{"title": "An Autoencoder Approach to Learning Bilingual Word Representations", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Cross-language learning allows us to use training data from one language to build models for a different language. Many approaches to bilingual learning require that we have word-level alignment of sentences from parallel corpora. In this work we explore the use of autoencoder-based methods for cross-language learning of vectorial word representations that are aligned between two languages, while not relying on word-level alignments. We show that by simply learning to reconstruct the bag-of-words representations of aligned sentences, within and between languages, we can in fact learn high-quality representations and do without word alignments. Since training autoencoders on word observations presents certain computational issues, we propose and compare different variations adapted to this setting. We also propose an explicit correlation maximizing regularizer that leads to significant improvement in the performance. We empirically investigate the success of our approach on the problem of cross-language test classification, where a classifier trained on a given language (e.g., English) must learn to generalize to a different language (e.g., German). These experiments demonstrate that our approaches are competitive with the state-of-the-art, achieving up to 10-14 percentage point improvements over the best reported results on this task.", "text": "problem cross-language test classiﬁcation classiﬁer trained given language must learn generalize different language experiments demonstrate approaches competitive state-of-the-art achieving percentage point improvements best reported results task. languages english plenty annotated resources disposal better natural language processing capabilities languages fortunate terms annotated resources. example high quality taggers parsers sentiment analyzers already available english case many languages hindi marathi bodo farsi urdu etc. situation acceptable past languages dominated digital content available online elsewhere. however ever increasing number languages today made important accurately process natural language data lesser-fortunate languages also. obvious solution cross-language learning allows training data language build models different language. many approaches bilingual learning require word-level alignment sentences parallel corpora. work explore autoencoder-based methods cross-language learning vectorial word representations aligned languages relying word-level alignments. show simply learning reconstruct bag-of-words representations aligned sentences within languages fact learn high-quality representations without word alignments. since training autoencoders word observations presents certain computational issues propose compare different variations adapted setting. also propose explicit correlation maximizing regularizer leads significant improvement performance. empirically investigate success approach another option exploit unlabeled data available language. context vectorial text representations proven useful multiple tasks it’s shown meaningful representations capturing syntactic semantic similarity learned unlabeled data. along labeled data representations allow exploit unlabeled data improve generalization performance given task even allowing generalize vocabulary observed labeled data majority previous work vectorial text representations concentrated monolingual case recent work started looking learning word document representations aligned across languages aligned representations potentially allow resources resource fortunate language develop capabilities resource deprived language example common representation model learned representing english german documents classiﬁer trained annotated english documents used classify german documents reuse resources across languages tried past projecting parameters learned annotated data language another language projections enabled bilingual resource machine translation system. recent attempts learning common bilingual representations eliminate need system. bilingual representations applied variety problems including crosslanguage document classiﬁcation phrase-based machine translation common property approaches word-level alignment translated sentences leveraged e.g. derive regularization term relating word embeddings across languages methods eliminate need system also outperform based projection approaches. alignment bilingual corpora training. unlike previous approaches require aligned sentences rely word-level alignments simpliﬁes learning procedure. propose bilingual autoencoder model learns hidden encoder representations paired bag-of-words sentences informative original bag-of-words also predictive other. word representations easily extracted encoder used context supervised task. speciﬁcally demonstrate quality representations task crosslanguage document classiﬁcation labeled data available language another one. we’ll approach able reach state-of-the-art performance achieving percentage point improvements best previously reported results. bag-of-words representation sentence. speciﬁcally word index ﬁxed vocabulary words. bag-of-words order words within correspond word order original sentence. wish learn d-dimensional vectorial representation words training sentence bag-of-words {x}t propose achieve using autoencoder model encodes input bag-of-words representations words present followed nonlinearity. speciﬁcally matrix matrix whose columns vector representations word. encoder’s computation involve summing columns word bag-of-word. note encoder function then using decoder autoencoder trained optimize loss function measures predictive original bag-of-words encoder representation different variations consider design encoder/decoder choice loss function. must careful however certain choices inappropriate training word observations intrinsically sparse high-dimensional. paper explore compare different approaches described next sub-sections. previous autoencoder architecture worked binary vectorial representation input bag-of-word. second autoencoder architecture investigated considered architecture instead works representation directly. notice implies scaling pre-activation vary bags-ofwords depending many words contains. thus we’ll optionally consider using average representations opposed xi|φ) efﬁciently speciﬁcally we’d like avoid procedure scaling linearly vocabulary size since large practice. precludes procedure would compute numerator denote sequence internal nodes path root given word always corresponding root. denote vector associated left/right branching choices path means path branches left internal node reconstruction. thus convert bag-of-words ﬁxed-size sparse binary vector word present otherwise representation obtain encoder representation multiplying word representation matrix element-wise non-linearity sigmoid hyperbolic tangent d-dimensional bias vector. encoding thus involves summing word representation words present least bag-ofword. note that since binary bag-of-words highdimensional training procedure aims reconstructing complete binary bag-of-word slow. since later training millions sentences training individual sentence bag-of-words expensive. thus propose simple trick exploits bagof-words structure input. assuming performing mini-batch training simply propose merge bagsof-words mini-batch single bag-of-word revert back stochastic gradient descent. resulting effect update efﬁcient stochastic gradient descent number updates training epoch divided mini-batch size. we’ll experimental section we’ve found trick still produces good word representations sufﬁciently reducing training time. note that additionally could used stochastic approach proposed dauphin reconstructing binary bag-of-words representations documents improve efﬁciency training. similarly extended tree-based autoencoder. notice share bias nonlinearity across encoders encourage encoders languages produce representations scale. sentence either languages want able perform reconstruction original sentence languages. particular given representation language we’d like decoder perform reconstruction language another decoder reconstruct language again decoders form proposed either section decoders language parameters encoder/decoder decomposition structure allows learn mapping within language across languages. speciﬁcally given pair train model construct construct reconstruct reconstruct follow approach experiments optimize corresponding losses training. bilingual encoder proposed enriched ensuring embeddings learned given pair highly correlated. achieve adding correlation term objective function. speciﬁcally could optimize correlation encoder representations learned scaling factor ensures three terms loss function range. note approach could used either binary bag-of-words tree-based reconstruction autoencoders. learn language speciﬁc word representation matrices described above construct document representations using columns vector representations words languages. given document written language containing words represent tf-idf weighted words’ representations full binary tree words number different decoder vocabulary size since words bag-of-words outputs required decoder. course worst case scenario since words share internal nodes paths decoder output computed once. organizing words tree larochelle lauly used random assignment words leaves full binary tree found work well practice. -dimensional bias vector matrix. left/right branching probability thus modeled logistic regression model applied encoder representation input bag-of-words let’s assume sentence bag-of-words source language associated bagof-words sentence translated target language human expert. assuming training pairs we’d like learn representations languages aligned pairs translated words similar representations. achieve this propose augment regular autoencoder proposed section that sentence representation given language reconstruction attempted original sentence language. speciﬁcally deﬁne language speciﬁc word representation matrices corresponding languages words respectively. also number words vocabulary languages different. word representations however size languages. binary reconstruction autoencoder bag-of-words representations extracted encoder becomes figure illustration binary reconstruction error based bilingual autoencoder learns reconstruct binary bag-ofwords english sentence barked french translation chien japp´e. recent work considered problem learning bilingual representations words usually relied word-level alignments. klementiev propose train simultaneously neural network languages models along regularization term encourages pairs frequently aligned words similar word embeddings. similar approach different form regularizer neural network language models work speciﬁcally investigate whether method rely word-level alignments learn comparably useful multilingual embeddings context document classiﬁcation. looking generally neural networks learn multilingual representations words phrases mention work showed useful linear mapping separately trained monolingual skip-gram language models could learned. however rely speciﬁcation pairs words languages align. mikolov also propose method training neural network learn useful representations phrases context phrase-based translation model. case phrase-level alignments required. figure illustration tree-based bilingual autoencoder learns construct bag-of-words english sentence barked french translation chien japp´e. horizontal blue line across input-to-hidden connections highlights fact connections share parameters techniques proposed paper enable learn bilingual embeddings capture cross-language similarity words. propose evaluate quality embeddings using task crosslanguage document classiﬁcation. follow setup used klementiev compare method. follows. labeled data documents language available train classiﬁer however interested classifying documents different language test time. achieve this leverage bilingual corpora importantly labeled document-level categories. bilingual corpora used instead learn document representations languages encouraged invariant translations language another. hope thus successfully apply classiﬁer trained document representations language directly document representations language english german language pair experiments. learning bilingual embeddings used english german section europarl corpus contains roughly million parallel sentences. mentioned earlier unlike klementiev word alignments parallel sentences. pre-processing used klenote klementiev word-aligned europarl corpus ﬁrst learn interaction matrix words languages. interaction matrix used multitask learning setup induce bilingual embeddings english german sections reuters rcv/rcv corpora. note documents parallel. document assigned categories pre-deﬁned hierarchy topics. following klementiev consider documents assigned exactly level categories topic hierarchy. topics ccat ecat gcat mcat number documents sampled klementiev english german respectively. contrast klementiev require stage approach directly learn embeddings europarl corpus parallel. further addition europarl corpus also considered feeding rcv/rcv documents autoencoders. non-parallel documents used reinforce monolingual embeddings effect amount data used klementiev model/training procedure completely different. next cross language classiﬁcation experiments follow setup used klementiev speciﬁcally single-topic documents training single-topic documents testing language. documents also pre-processed using similar procedure used europarl corpus. train bilingual word representations sentence pairs extracted europarl-v languages optionally also monolingual documents rcv/rcv reinforce monolingual embeddings klementiev used averaged perceptron train multi-class classiﬁer epochs experiments report results sensitive number epochs). english german vocabularies contained words respectively. document represented tf-idf weighted linear combination word’s embeddings described section words belonging vocabulary considered. different autoencoder architectures optional correlation-based regularization term proposed earlier trained different models learning bilingual embeddings. models described below. we’ll bae-cr worse performing model thus experiment allow observe whether correlation regularization play important role improving quality representations. models trained epochs using data described earlier. results word embeddings size klementiev further speed training baecr bae-cr/corr merged mini-batches adjacent sentence pairs single training instance described section embeddings learned method. this perform small experiment select english words list english german words similar words table shows result experiment. example table shows cases german word closest given english word actually translation english word. also notice model able capture semantic similarity words embedding semantically similar words etc.) close other. results experiment suggest bilingual embeddings useful cross language classiﬁcation task indeed shown results presented next section. supplementary material also includes visualization word embeddings languages generated using t-sne dimensionality reduction algorithm klementiev model uses word embeddings learned multitask neural network language model regularization term encourages pairs frequently aligned words similar word embeddings. embeddings document representations computed described section here test documents translated language training documents using machine translation system. moses standard phrase-based system using default parameters -gram language model trained europarl corpus tions i.e. en-de de-en. bae-tr baecr observe bae-tr provides better performance comparable embeddings learned neural network language model klementiev which unlike bae-tr relies word-level alignments. also observe correlation regularization beneﬁcial. indeed able improve performance bae-cr make best performing method accuracy methods task. next evaluate effect varying amount supervised training data training classiﬁer either bae-tr bae-cr/corr klementiev embeddigns. experiment training sizes results en-de deen summarized figure figure respectively. observe bae-cr/corr clearly outperforms models almost data sizes. importantly performs remarkably well data sizes suggests indeed learns meaningful embeddings generalize well even data sizes. excellent performance bae-cr/corr suggests merging mini-batches single bags-of-words signiﬁcantly impact quality word embeddings. words need rely wordlevel alignments exact sentence-level alignment also essential reach good performances. thus natural effect using even coarser level alignments. check varying size merged minibatches bae-cr/corr bae-tr. cross language classiﬁcation results obtained using coarser alignments summarized table presented evidence meaningful bilingual word representations could learned without relying word-level alignments even successful fairly coarse sentence-level alignments. particular showed even though model word level alignments able outperform state word representation learning method exploits word-level alignments. addition also outperforms strong machine translation based baseline. observed using correlation based regularization term leads better bilingual embeddings highly correlated hence perform better cross language classiﬁcation tasks. future work would like investigate extensions bag-ofwords bilingual autoencoder bags-of-ngrams model would also learn representations short phrases. model particularly useful context machine translation system. alexandre klementiev ivan titov binod bhattarai. inducing crosslingual distributed representations words. proceedings international conference computational linguistics richard socher daniel christopher manning. bilingual word embeddings conference phrase-based machine translation. empirical methods natural language processing inducing multilingual taggers bracketers robust projection across aligned corpora. proceedings second meeting north american chapter association computational linguistics language technologies pages pittsburgh pennsylvania http//dx.doi.org/./.. dipanjan slav petrov. unsupervised part-ofspeech tagging bilingual graph-based projections. proceedings annual meeting association computational linguistics human language technologies pages portland oregon june rada mihalcea carmen banea janyce wiebe. learning multilingual subjective language cross-lingual projections. proceedings annual meeting association computational linguistics pages prague czech republic june association computational linguistics. xiaojun wan. co-training cross-lingual sentiment classiﬁcation. proceedings joint conference annual meeting international joint conference natural language processing afnlp pages suntec singapore august yann dauphin xavier glorot yoshua bengio. largescale learning embeddings reconstruction sampling. proceedings international conference machine learning pages omnipress would also like explore possibility converting bilingual model multilingual model learn common representations multiple languages given different amounts parallel data languages. references kristina toutanova klein christopher manning yoram singer. feature-rich part-of-speech tagging cyclic dependency network. proceedings conference north american chapter association computational linguistics human language technology volume naacl pages richard socher john bauer christopher manning andrew parsing compositional vector grammars. proceedings annual meeting association computational linguistics pages soﬁa bulgaria august association computational linguistics. joseph turian ratinov yoshua bengio. word representations simple general method semisupervised learning. proceedings annual meeting association computational linguistics pages association computational linguistics frederic morin yoshua bengio. hierarchical probabilistic neural network language model. proceedings international workshop artiﬁcial intelligence statistics pages society artiﬁcial intelligence statistics", "year": 2014}