{"title": "Multimodal Convolutional Neural Networks for Matching Image and Sentence", "tag": ["cs.CV", "cs.CL", "cs.NE"], "abstract": "In this paper, we propose multimodal convolutional neural networks (m-CNNs) for matching image and sentence. Our m-CNN provides an end-to-end framework with convolutional architectures to exploit image representation, word composition, and the matching relations between the two modalities. More specifically, it consists of one image CNN encoding the image content, and one matching CNN learning the joint representation of image and sentence. The matching CNN composes words to different semantic fragments and learns the inter-modal relations between image and the composed fragments at different levels, thus fully exploit the matching relations between image and sentence. Experimental results on benchmark databases of bidirectional image and sentence retrieval demonstrate that the proposed m-CNNs can effectively capture the information necessary for image and sentence matching. Specifically, our proposed m-CNNs for bidirectional image and sentence retrieval on Flickr30K and Microsoft COCO databases achieve the state-of-the-art performances.", "text": "figure multimodal matching relations image sentence. words phrases grass ball small black brown play ball correspond image areas grounding meanings. global sentence small black brown play ball grass expresses whole semantic meaning image content. brown play ball correspond image areas whole sentence small black brown play ball grass expressing complete semantic meaning associates whole image content. matching relations taken consideration accurate inter-modal matching image sentence. recently much research work focuses modeling image sentence matching relation speciﬁc level namely word level phrase level sentence level however best knowledge models fully exploit matching relations image sentence considering word phrase sentence level intermodal correspondences together. multimodal matching image sentence requires good representations image sentence. recently deep neural networks employed learn better image sentence representations. speciﬁcally convolutional neural networks shown powerful abilities image representation sentence representation however abilpaper propose multimodal convolutional neural networks matching image sentence. m-cnn provides end-to-end framework convolutional architectures exploit image representation word composition matching relations modalities. speciﬁcally consists image encoding image content matching learning joint representation image sentence. matching composes words different semantic fragments learns inter-modal relations image composed fragments different levels thus fully exploit matching relations image sentence. experimental results benchmark databases bidirectional image sentence retrieval demonstrate proposed m-cnns effectively capture information necessary image sentence matching. speciﬁcally proposed m-cnns bidirectional image sentence retrieval flickrk microsoft coco databases achieve state-of-the-art performances. associating image natural language sentence plays essential role many applications. describing image natural sentences useful image annotation caption retrieval image query sentences convenient helpful natural image search applications association between image sentence formalized multimodal matching problem semantically correlated image sentence pairs produce higher matching scores uncorrelated ones. multimodal matching relations image sentence complicated happen different words sentence grass ball denote objects image. phrases describing objects attributes activities black brown small black paper propose novel multimodal convolutional neural network framework image sentence matching problem. training image sentence pairs proposed m-cnns able retrieve rank images given natural sentence query vice versa. core contributions ﬁrstly studied image sentence matching problem. employ convolutional architectures summarize image compose words sentence different semantic fragments learn matching relations interactions image composed fragments. complicated matching relations image sentence fully studied proposed m-cnn letting image composed fragments sentence meet interact different levels. validate effectiveness m-cnns bidirectional image sentence retrieval experiments achieve performances superior state-of-the-art approaches. long thread work association image text. early work usually focuses modeling correlation image annotating words phrases models cannot well capture complicated matching relations image natural sentence. recently association image sentence studied bidirectional image sentence retrieval automatic image captioning bidirectional image sentence retrieval hodosh proposed kcca discover shared feature space image sentence. however highly non-linear inter-modal relations cannot well exploited based shallow representations image sentence. recent papers seek better representations image sentence deep architectures. socher proposed employ semantic dependency-tree recursive neural network sentence semantic space image representation association measured distance space. stacked fully connected layers together represent sentence used deep canonical correlation analysis matching images text. klein used fisher vector sentence representation. kiros proposed skip-thought vector encode sentence matching image. such global level matching relations image sentence studied representing sentence global vector. however neglect local fragments sentence correspondences image content. compared karpathy work ﬁner level aligning fragments sentence regions image. plummer et.al used entities collect region-tophrase correspondences richer image-to-sentence models. local inter-modal correspondences image sentence fragments thus studied global matching relations considered. illustrated figure image content corresponds different fragments sentence local words global sentence. fully exploit inter-modal matching relations propose m-cnns compose words sentence different fragments fragments meet image different levels learn matching relations. automatic image captioning authors recurrent visual representation multimodal recurrent neural network multimodal neural language model neural image caption deep visual-semantic alignments long-term recurrent convolution networks learn relation image sentence generate caption given image. please note models naturally produce scores image-sentence association thus readily used bidirectional retrieval. image sentence representation image cnns demonstrated powerful abilities learn image representation image pixels achieved state-of-the-art performances image classiﬁcation object detection sentence thread neural networks sentence representation time-delay neural network recursive neural network recurrent neural network obtained sentence representation used sentence classiﬁcation image sentence retrieval language modeling text generation figure m-cnn architecture matching image sentence. image representation generated image cnn. matching composes words different fragments sentence learns joint representation image sentence fragments. summarizes joint representation outputs matching score. activation function image takes image input generates ﬁxed length image representation. successful image cnns image recognition used initialize image returns -dimensional activations fully connected layer immediately last relu layer. matrix dimension experiments. image thus represented ddimension vector νim. matching matching takes encoded image representation word representations input produces joint representation νjr. illustrated figure image content correspond sentence fragments varying scales adequately considered learnt joint representation image sentence. targeting fully exploiting inter-modal matching relation proposed matching cnns ﬁrstly compose words different semantic fragments image meet fragments learn inter-modal structures interactions. speciﬁcally different matching cnns designed make image interact composed fragments different levels generate joint representation word phrase level sentence level. detailed information matching cnns different levels introduced following subsections. three components proposed m-cnn fully coupled end-to-end image sentence matching framework parameters word representations) jointly learned supervision matching instances. threefold beneﬁts provided. firstly image tuned generate better image representation matching. secondly word representations tuned composition matching processes. thirdly matching composes word representations different fragments lets image representation meet fragments different levels fully exploit inter-modal matching correspondences image sentence. nonlinear projection image representations different matching cnns expected encode image content matching composed semantic fragments sentence. different variants matching fully exploit matching relations image sentence image representation meet interact different composed fragments sentence generate joint representation. order word-level matching relation image meet word-level fragments sentence learn interactions relations. moreover convolutional models consider convolution units local receptive ﬁeld shared weights adequately model rich structures word composition inter-modal interaction. word-level matching denoted matchcnnwd designed figure sequential layers convolution pooling joint representation image sentence generated input calculating matching score. convolution generally sequential input convolution unit feature type-f them) layer figure word-level matching cnn. word-level matching architecture. convolution units multimodal convolution layer matchcnnwd. dashed ones indicate zero padded word image representations gated convolution process. matchcnnwd targets exploring word-level matching relation multimodal convolution layer introduced letting image meet word-level fragments sentence. convolution unit multimodal convolution layer illustrated figure input multimodal convolution unit denoted vector representation word senνi tence encoded image feature matching word-level fragments sentence. hard input lead interaction words image representation ﬁrst convolution layer provides local matching signal word level. sentence perspective multimodal convolution composes words local receptive ﬁeld higher semantic representation phrase white ball. matching perspective captures learns multimodal convolution inter-modal correspondence image representation word-level fragments sentence. meanings word ball composed phrase white ball grounded image make inter-modal matching relations. moreover order handle natural sentences variable lengthes maximum length sentence ﬁxed matchcnnwd. zero vectors padded image word representation dashed ones figure gating function eliminate unexpected matching noise composed convolution process. max-pooling convolution layer maxpooling layer followed. taking two-unit window maxpooling example pooled feature obtained effects max-pooling two-fold. together stride max-pooling process lowers dimensionality representation half thus quickly making ﬁnal joint representation image sentence. helps ﬁlter undesired interaction relation image fragments sentence. take sentence figure example composed phrase chase matches closely image chase white. therefore imagine well-trained multimodal convolution unit generate better matching representation chase image. max-pooling process pool matching representation convolution pooling processes. convolution pooling processes explore summarize local matching signals explored word level. layers convolution pooling employed form matching decisions larger scales ﬁnally reach global joint representation. speciﬁcally paper another convolution max-pooling layers alternate summarize local matching decisions ﬁnally produce global joint representation matching reﬂects inter-modal correspondence image word-level fragments sentence. different matching word-level work solely words certain levels interacting image. without seeing image feature convolution process compose words receptive ﬁeld higher semantic representation maxpooling process ﬁlter undesired compositions. composed representations named phrase language perspective. image meet composed phrases reason inter-modal matching relations. resentation. name matching short phrases image matchcnnphs. long phrases sequential layer convolution pooling used summarize local matching joint representation. matching long phrases image named matchcnnphl. sentence-level convolutional matching denoted matchcnnst goes step composition defers matching sentence fully represented illustrated figure speciﬁcally image encodes image feature vector. sentence consisting three sequential layers convolution pooling represents whole sentence feature vector. multimodal layer concatenates image sentence representation together joint representation green field kick soccer ball high illustrated figure although word-level phrase-level fragments kick soccer ball correspond objects well activities image whole sentence needs fully represented make reliable association image. sentence layers convolution pooling used encode whole sentence feature vector representing semantic meaning. concatenating image sentence representation together matchcnnst non-trivial matching transfer representations modalities later fusing matching. m-cnns different matching cnns figure phrase-level matching composed phrases. short phrase composed layer convolution pooling. long phrase composed sequential layers convolution pooling. phase-level matching architecture. composed four words woman jean. composed short phrases present richer speciﬁc descriptions objects relationships compared single words woman jean. additional layer convolution maxpooling process short phrases long phrases composed four short phrases black grass woman figure compared composed short phrases single words long phrases present even richer higher semantic meanings speciﬁc description objects activities relative positions. order reason inter-modal relations image composed phrases multimodal convolution layer introduced performing convolution image phrase representations. input multimodal convolution unit composed phrase representation either short phrases multimodal convolution process produces phrase-level matching decisions. layers viewed fusion local phrase-level matching decisions joint representation captures local matching relations image composed phrase fragments. speciﬁcally short phrases sequential layers convolution pooling followed generate joint repcnnphl m-cnnst. fully exploit inter-modal matching relations image sentence different levels ensemble m-cnnen four variants summing matching scores generated m-cnns together. different image cnns overfeat take architecture also original parameters initialization. chopping softmax layer last relu layer output last fully-connected layer deemed image representation denoted conﬁgurations matchcnnwd matchcnnphs matchcnnphl matchcnnst outlined table three convolution layers three pooling layers fully connected layers four networks. ﬁrst convolution layer matchcnnwd second convolution layer matchcnnphs third convolution layer matchcnnphl multimodal convolution layers blend image representation fragments sentence together compose higher level semantic representation. matchcnnst concatenates image sentence representation together leave interaction ﬁnal mlp. matching cnns designed ﬁxed architectures need accommodate maximum length input sentences. evaluations maximum length word representations initialized skip-gram model dimension joint representation denotes parameters denotes correlated image-sentence pair randomly sampled uncorrelated image-sentence pair notational meaning varies matching task image retrieval query sentence denotes natural sentence denotes image; sentence retrieval query image opposite. object force matching score correlated pair greater uncorrelated pair margin simply training process. stochastic gradient descent minibatches optimization. order avoid overﬁtting early-stopping dropout used. relu used activation function throughout m-cnns. section evaluate effectiveness mcnns bidirectional image sentence retrieval. begin describing datasets used evaluation followed brief description competitor models. m-cnns bidirectional evaluate performances image retrieval sentence retrieval. datasets sentence datasets varying sizes characteristics. flickrk dataset consists images collected flickr. image accompanied sentences describing image content. database provides standard training validation testing split. flickrk dataset consists images collected flickr. image also accompanied sentences describing content image. images depict varying human activities. used public split training validation testing. microsoft coco dataset consists training validation images categories labeled total instances. image also compared models recently developed models performances bidirectional image sentence retrieval speciﬁcally devise sdt-rnn dcca deep fragment m-rnn mnlm dvsa lrcn devise deep fragment regarded working word-level phrase-level respectively. sdt-rnn dcca regarded working sentence-level embed image sentence semantic space. models namely mnlm m-rnn dvsa lrcn originally proposed automatic image captioning also used retrieval directions. adopt evaluation metrics fair comparison. speciﬁcally bidirectional retrieval report median rank closest ground truth result list well computes fraction times correct result found among items. performances proposed m-cnns bidirectional image sentence retrieval flickrk flickrk microsoft coco illustrated table highlight best performance evaluation metric. flickrk performs best suggesting strong beneﬁcial bias fisher vector modeling sentences obvious training data relatively scarce. proposed m-cnn performs inferiorly still superior methods. reason suggested results larger datasets mainly insufﬁcient training samples. flickrk consists images insufﬁcient adequately tuning parameters convolutional architectures m-cnns. flickrk microsoft coco datasets training samples mcnnen outperforms competitor models terms metrics illustrated table. moreover except slightly outperforms mcnnen image retrieval task measured except lack training samples another possible reason uses better image compared vgg. discussed section performance image greatly affects performance bidirectional image sentence retrieval. ages) best performing competitor model becomes tasks. m-rnn-vgg outperform m-cnnen sentence retrieval task measured comes image retrieval mcnnen consistently better competitor models. possible reason m-rnnvgg designed caption generation particularly good ﬁnding suitable sentence given image. possible reason flickrk entities speciﬁcally presented bounding boxes corresponding entity manually labeled. such much information available image retrieval. paint building table outdoor front union jack three like paint person table outdoor building front table three paint building like person front outdoor jack union niﬁcantly improved compared flickrk flickrk. firstly demonstrates sufﬁcient training samples parameters convolutional architecture m-cnn adequately tuned. secondly dvsa outperforms proposed m-cnnen sentence retrieval terms image retrieval m-cnnen signiﬁcantly consistently outperforms competitor models. proposed m-cnnwd devise target exploiting word-level inter-modal correspondences image sentence. however devise treats word equally average word vectors representation sentence m-cnnwd image interact word compose higher semantic representations signiﬁcantly outperforms devise. sdt-rnn proposed mcnnst exploit matching image sentence sentence level. however sdt-rnn encodes sentence recursively feature vector based pre-given dependency tree m-cnnst works ﬂexible deep fragment proposed m-cnnphs m-cnnphl match image sentence fragments phrase levels. however deep fragment uses edges dependency tree model sentence fragments making unable describe complex relations sentence. example deep fragment parses relative complex phrase black brown relations m-cnnphs handles phrase whole compose higher semantic representation. moreover m-cnnphl readily handle longer phrases reason grounding meanings image. consequently better performances m-cnnphs m-cnnphl obtained compared deep fragment. moreover observed m-cnnst consistently outperform m-cnns. sentence well summarize natural sentence make better sentencelevel association image m-cnnst. m-cnns captures matching relations word phrase levels. matching relations considered together fully depict inter-modal correspondences image sentence. thus m-cnnen achieves best performances indicates m-cnns different levels complementary capture complicated image sentence matching relations. overfeat initialize image m-cnn retrieval tasks. observed mcnns signiﬁcantly outperform overfeat large margin consistent performance classiﬁcation imagenet clearly retrieval performance depends heavily efﬁcacy image might explain good performance flickrk. moreover region features used encoding image regions feature vectors used image fragments deep fragment dvsa. future consider incorporate image cnns m-cnns make accurate inter-modal matching. m-cnns compose words different semantic fragments sentence inter-modal matching different levels therefore posses ability word composition. speciﬁcally want check whether mcnns compose words random orders semantic fragments matching image content. demonstrated table matching scores image accompanied sentence greatly decrease random reshufﬂe words. fairly strong evidence m-cnns compose words natural sequential order high semantic representations thus make inter-modal matching relations image sentence. proposed multimodal convolutional neural networks matching image sentence. proposed m-cnns rely convolution architectures compose different semantic fragments sentence learn interaction image composed fragments different levels therefore fully exploit inter-modal matching relations. experimental results bidirectional image sentence retrieval demonstrate consistent state-ofthe-art performances proposed models.", "year": 2015}