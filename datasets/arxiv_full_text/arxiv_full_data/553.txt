{"title": "Encoding Source Language with Convolutional Neural Network for Machine  Translation", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "The recently proposed neural network joint model (NNJM) (Devlin et al., 2014) augments the n-gram target language model with a heuristically chosen source context window, achieving state-of-the-art performance in SMT. In this paper, we give a more systematic treatment by summarizing the relevant source information through a convolutional architecture guided by the target information. With different guiding signals during decoding, our specifically designed convolution+gating architectures can pinpoint the parts of a source sentence that are relevant to predicting a target word, and fuse them with the context of entire source sentence to form a unified representation. This representation, together with target language words, are fed to a deep neural network (DNN) to form a stronger NNJM. Experiments on two NIST Chinese-English translation tasks show that the proposed model can achieve significant improvements over the previous NNJM by up to +1.08 BLEU points on average", "text": "recently proposed neural network joint model augments n-gram target language model heuristically chosen source context window achieving state-of-the-art performance smt. paper give systematic treatment summarizing relevant source information convolutional architecture guided target information. different guiding signals decoding speciﬁcally designed convolution+gating architectures pinpoint parts source sentence relevant predicting target word fuse context entire source sentence form uniﬁed representation. representation together target language words deep neural network form stronger nnjm. experiments nist chinese-english translation tasks show proposed model achieve signiﬁcant improvements previous nnjm bleu points average. learning continuous space representation source language attracted much attention traditional statistical machine translation neural machine translation various models mostly neural network-based proposed representing source sentence mainly encoder part encoder-decoder framework quite recent work encoding relevant part source sentence decoding process notably neural network joint model extends n-grams target language model additionally taking ﬁxed-length window source sentence achieving state-ofthe-art performance statistical machine translation. paper propose novel convolutional architectures dynamically encode relevant information source language. model covers entire source sentence effectively properly summarize relevant parts guided information target language. guiding signals decoding speciﬁcally designed convolution architectures pinpoint parts source sentence relevant predicting target word fuse context entire source sentence form uniﬁed representation. representation together target words deep neural network form stronger nnjm. since proposed joint model purely lexicalized integrated decoder feature. variants joint model also proposed coined name tagcnn incnn different guiding signals used decoding process. integrate proposed joint models state-of-the-art dependency-to-string translation system evaluate effectiveness. experiments nist chinese-english translation tasks show model able roadmap remainder paper start brief overview joint language model section convolutional encoders component which described detail section section discuss decoding algorithm proposed models. experiment results reported section followed section related work conclusion. joint model encoders illustrated figure consists encoder namely tagcnn incnn represent information source sentences nn-based model predicting next words representations encoders history words target sentence inputs. afﬁliated source word presidential zˇongtˇong tagcnn generates incnn generates takes \"holds parliament and\" input give conditional probability next word e.g. convolutional models generic encoder basic architecture generic encoder illustrated figure ﬁxed architecture consisting layers layer- input layer takes words form embedding vectors. work maximum length sentences words. sentences shorter that zero padding beginning sentences. convolution shown figure convolution layer- operates sliding windows words similar deﬁnition windows carries higher layers. formally source sentence input x={x··· convolution unit feature type-f them) layer- aligned target word take aligned source words afﬁliated source words. unaligned word inherit afﬁliation closest aligned word preference given right since word alignment many-to-many target word multi afﬁliated source words. gating previous cnns including tasks take straightforward convolution-pooling strategy fusion decisions based values feature-maps. essentially soft template matching works tasks like classiﬁcation harmful keeping composition functionality convolution critical modeling sentences. paper propose separate gating unit release score function duty convolution focus composition. take types gating layer- take local gating non-overlapping windows feature-maps convolutional layer- representation segments layer- take global gating fuse segments global representation. found gating strategy considerably improve performance tagcnn incnn pooling. local gating layer- every gating window ﬁrst original input layer- merge input gating network. example windows word word layer- concatenated vector consisting embedding word input local gating network determine weight convolution result windows weighted output layer-. training encoders encoders including tagcnn incnn discussed right below trained joint language model described section along following parameters training procedure identical neural network language model except parallel corpus used instead monolingual corpus. seek maximize log-likelihood training samples sample every target word parallel corpus. optimization performed conventional back-propagation implemented stochastic gradient descent mini-batches. tagcnn inherits convolution gating generic modiﬁcation input layer. shown figure tagcnn append extra tagging embedding words input layer indicate whether afﬁliated words extended word embedding treated regular word-embedding convolutional neural network. particular encoding strategy extended embed complicated dependency relation source language described section particular activated parameterized training predicting target words. words supervised signal words predict layers back-propagation importance afﬁliated words source language learn proper weight make tagged words stand adjust parameters tagcnn accordingly optimal predictive performance. joint model pinpoint parts source sentence relevant predicting target word already learned word alignment. unlike tagcnn directly tells location afﬁliated words encoder incnn sends information proceeding words target side convolutional encoder help retrieve information relevant predicting next word. essentially particular case attention model analogous automatic alignment mechanism attention signal state generative recurrent neural network decoder. n−k) injected every convolution window source language sentence illustrated figure speciﬁcally window indexed input convolution given concatenated vector work transform vector concatenated word-embedding words {en−k en−k} sigmoid activation function. layers convolution gating incnn retrieve relevant segments source sentences compose transform retrieved segments representation recognizable predicting words target language. different tagcnn incnn uses information proceeding words hence provides complementary information augmented joint language model tagcnn. empirically veriﬁed using feature based tagcnn based incnn decoding greater improvement. figure illustration dependency tree three head-dependents relations shadow example head-dependents relation rule level example head rule indicates substitution site replaced subtree whose root part-of-speech underline denotes leaf node. joint model purely lexicalized therefore integrated decoders feature. hierarchical decoder adopt integrating method proposed devlin inherited n-gram language model performing hierarchical decoding leftmost rightmost words constituent stored state space. extend state space also include indexes afﬁliated source words edge words. aligned target word take aligned source words afﬁliated source words. unaligned word afﬁliation heuristic adopted devlin paper integrate joint model state-of-the-art dependency-to-string machine translation decoder case study test efﬁcacy proposed approaches. brieﬂy describe dependency-to-string translation model description system. paper state-of-the-art dependency-to-string decoder also hierarchical decoder. dependency-to-string model employs rules represent source side head-dependents relations target side strings. head-dependents relation composed head dependents dependency trees. figure shows dependency tree three hdrs example rule level example head rule rules constructed head-dependents relations. rules translation rules reordering rules. head rules used translating source words. adopt decoder proposed meng variant depstr translation easier implement comparable performance. basically extract rules ghkm algorithm. decoding procedure given source dependency tree decoder transverses post-order. bottom-up chart-based decoding algorithm cube pruning used k-best items node. translation probabilities rules; lexical translation probabilities plex plex rules; rule penalty exp; pseudo translation rule penalty exp; target word penalty exp; n-gram language model plm; baseline decoder contains ﬁrst eight features. pseudo translation rule ensure complete translation matched rules found decoding. weights features tuned minimum error rate training dependency-to-string decoder rule-threshold stack-threshold rule-limit stack-limit setup data training data extracted data. keep sentence pairs length source part longer words covers sentence. bilingual training data consist sentence pairs containing million chinese words million english words. development nist test sets ﬁltering length limit. preprocessing word alignments obtained giza++ corpora directions using grow-diag-ﬁnal-and balance strategy adopt language modeling toolkit train -gram language model modiﬁed kneser-ney smoothing xinhua portion english gigaword corpus parse chinese sentences stanford parser projective dependency trees. optimization training neural network limit source target vocabulary frequent words chinese english covering approximately corpus respectively. out-of-vocabulary words mapped special token unk. used stochastic gradient descent train joint model setting size minibatch joint models used -word target history dimension word embedding attention signal incnn convolution layers apply ﬁlters. ﬁnal representation encoders vector dimension ﬁnal layer joint model standard multi-layer perceptron softmax layer. setting model comparisons tagcnn incnn joint language models additional decoding features dependency-to-string baseline system compare neural network joint model source context words implementation open source toolkit default conﬁguration except global settings described section since tagcnn incnn models source-to-target left-to-right take source-to-target left-to-right type nnjm comparison. call type nnjm bbn-jm hereafter. although bbn-jm originally tested hierarchical phrase-based string-to-dependency fairly versatile readily integrated depstr. clearly table tagcnn incnn improve upon depstr baseline bleu outperforming bbn-jm setting respectively bleu averaged nist indicate tagcnn incnn individually provide discriminative information decoding. worth noting incnn appears informative afﬁliated words suggested word alignment conjecture following facts table bleu- scores nist mt-test mt-test moses dependency-to-string baseline system different features depstr neural network joint model generic tagcnn incnn combination tagcnn incnn. boldface numbers superscript indicate results signiﬁcantly better bbn-jm depstr baseline respectively. stands adding corresponding feature depstr. incnn avoids propagation mistakes artifacts already learned word alignment; guiding signal incnn provides complementary information evaluate translation. moreover tagcnn incnn used decoding increase winning margin bbn-jm bleu points indicating models different guiding signals complementary other. role guiding signal slight surprising generic also achieve gain bleu similar bbn-jm since intuitively generic encodes entire sentence representations general optimal representation joint language model. reason conjecture yields fairly informative summarization sentence makes loss resolution relevant parts source senescence. said guiding signal tagcnn incnn crucial power cnn-based encoder easily seen difference bleu scores achieved generic tagcnn incnn. indeed signal already learned word alignment tagcnn gain bleu generic counterpart incnn guiding signal proceeding words target gain saliently bleu. dependency head tagcnn section study whether tagcnn beneﬁt encoding richer dependency structure source language input. speciﬁcally dependency head words used improve tagcnn model. described section tagcnn append tagging embedding words input layer tags whether afﬁliated source words. incorporate dependency head information extend tagging rule section another tagging word-embedding original tagcnn indicate whether part dependency heads afﬁliated words. example embedding afﬁliated source word dependency head word extended input tagcnn would contain afﬁliated source word root sentence append second tagging since root dependency head. table help dependency head information improve tagcnn bleu points averagely test sets. gating max-pooling section investigate extent gating strategy improve translation performance pooling comparisons incnn model case study. implementation incnn max-pooling replace local-gating max-pooling size global gating max-pooling then mean outputs k-pooling ﬁnal input layer-. guarantee input dimension layer- architecture gating. table clearly gating strategy improve translation performance max-pooling bleu points. moreover -pooling yields performance better -pooling. conjecture useful relevant parts translation mainly concentrated words source sentence better extracted larger pool size. seminal work neural network language model traced bengio monolingual text. recently extended devlin include additional source context modeling target sentence clearly related work however important differences instead selecting context window model covers entire source sentence automatically distill context relevant target modeling; convolutional architecture effectively leverage guiding signals vastly different forms nature target. prior model also work representing source sentences neural networks including work typically entire sentence vector used later rnn/lstm-based decoder generate target sentence. demonstrated section repmodel especially incnn inspired automatic alignment model proposed ﬁrst effort apply attention model machine translation sends state decoding attentional signal source obtain weighted embedding source words summary relevant context. contrast incnn uses different attention signal extracted proceeding words partial translations importantly convolutional architecture therefore highly nonlinear retrieve summarize relevant information source. proposed convolutional architectures obtaining guided representation entire source sentence used augment n-gram target language model. different guiding signals target side devise tagcnn incnn tested enhancing dependency-to-string bleu points baseline bleu points state-of-the-art future work consider encoding complex linguistic structures enhance joint model.", "year": 2015}