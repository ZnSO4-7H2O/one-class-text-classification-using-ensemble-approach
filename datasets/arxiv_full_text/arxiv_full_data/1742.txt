{"title": "A Generative Word Embedding Model and its Low Rank Positive Semidefinite  Solution", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Most existing word embedding methods can be categorized into Neural Embedding Models and Matrix Factorization (MF)-based methods. However some models are opaque to probabilistic interpretation, and MF-based methods, typically solved using Singular Value Decomposition (SVD), may incur loss of corpus information. In addition, it is desirable to incorporate global latent factors, such as topics, sentiments or writing styles, into the word embedding model. Since generative models provide a principled way to incorporate latent factors, we propose a generative word embedding model, which is easy to interpret, and can serve as a basis of more sophisticated latent factor models. The model inference reduces to a low rank weighted positive semidefinite approximation problem. Its optimization is approached by eigendecomposition on a submatrix, followed by online blockwise regression, which is scalable and avoids the information loss in SVD. In experiments on 7 common benchmark datasets, our vectors are competitive to word2vec, and better than other MF-based methods.", "text": "existing word embedding methods categorized neural embedding models matrix factorization based methods. however models opaque probabilistic interpretation mf-based methods typically solved using singular value decomposition incur loss corpus information. addition desirable incorporate global latent factors topics sentiments writing styles word embedding model. since generative models provide principled incorporate latent factors propose generative word embedding model easy interpret serve basis sophisticated latent factor models. model inference reduces rank weighted positive semideﬁnite approximation problem. optimization approached eigendecomposition submatrix followed online blockwise regression scalable avoids information loss svd. experiments common benchmark datasets vectors competitive wordvec better mf-based methods. introduction task word embedding model distribution word context words using corresponding vectors euclidean space. regression relevant statistics derived corpus vectors recovered best statistics. vectors commonly referred embeddings capture semantic/syntactic regularities words. based link function objective function developed. reasonableness link function impacts quality obtained embeddings different link functions amenable different optimization algorithms different scalability. based forms link function optimization techniques methods divided classes traditional neural embedding models recent rank matrix factorization methods. neural embedding models softmax link function model conditional distribution word given context function embeddings. normalizer softmax function brings intricacy optimization usually tackled gradient-based methods. pioneering work later mnih hinton propose three different link functions. however interaction matrices embeddings models complicate slow training hindering trained huge corpora. mikolov mikolov greatly simplify conditional distribution embeddings interact directly. implemented well-known wordvec trained efﬁciently huge corpora. obtained embeddings show excellent performance various tasks. low-rank matrix factorization methods include various link functions optimization methods. link functions usually softmax functions. methods reconstruct certain corpus statistics matrix product rank factor matrices. objective usually minimize reconstruction error optionally constraints. line research levy goldberg wordvec essentially stochastic weighted factorization word-context pointwise mutual information matrix. factorize matrix directly method. pennington propose bilinear regression function conditional distribution weighted problem bigram logfrequency matrix formulated. gradient descent used embeddings. recently based intuition words organized semantic hierarchies yogatama hierarchical sparse regularizers matrix reconstruction error. similar techniques faruqui reconstruct pretrained embeddings using sparse vectors greater dimensionality. dhillon apply canonical correlation analysis word matrix context matrix canonical correlation vectors matrices word embeddings. stratos stratos assume brown language model prove bigram occurrences equivalent ﬁnding transformed solution language model. arora assume hidden discourse vector random walk determines distribution current word. slowly evolving discourse vector puts constraint embeddings small text window. maximum likelihood estimate embeddings within text window approximately reduces squared norm objective. limitations current word embedding methods. ﬁrst limitation mfbased methods words context words different sets embeddings employ singular value decomposition obtain rank approximation word-context matrix factorizes information lost learned embeddings capture signiﬁcant regularities appendix gives example work properly. second limitation generative model documents parametered embeddings absent recent development. although based generative processes generative processes deriving local relationship embeddings within small text window leaving likelihood document undeﬁned. addition learning objectives models e.g. even clear probabilistic interpretation. generative word embedding model documents easier interpret analyze importantly provides basis upon documentlevel global latent factors document topics sentiments writing styles incorporated principled manner better model text distribution extract relevant information. based considerations propose unify embeddings words context words. link function factorizes three parts interaction embeddings capturing linear correlations words residual capturing nonlinear noisy correlations unigram priors. reduce overﬁtting gaussian priors embeddings residuals apply jelinek-mercer smoothing bigrams. furthermore model probability sequence words assume contributions context word approximately thereby generative model documents constructed parameterized embeddings residuals. learning objective maximize corpus likelihood reduces weighted low-rank positive semideﬁnite approximation problem matrix. block coordinate descent algorithm adopted approximate solution. algorithm based eigendecomposition avoids information loss brings challenges scalability. exploit sparsity weight matrix implement efﬁcient online blockwise regression algorithm. seven benchmark datasets covering similarity analogy tasks method achieves competitive stable performance. throughout paper always uppercase bold letter denote matrix lowercase bold letter denote vector normal uppercase letter denote scalar constant normal lowercase letter denote scalar variable. suppose vocabulary {s··· consists words vocabulary size. suppose s··· sorted decending order frequency i.e. frequent least frequent. document sequence words corpus collection documents {d··· dm}. vocabulary word mapped vector n-dimensional euclidean space. document sequence words referred text window denoted wi··· wi+l wiwi+l shorthand. text window chosen size word deﬁnes context wi−c··· wi−. referred focus word. context word wi−j focus word comprise bigram wi−j section formulate probability sequence words function embeddings. start link function bigrams building blocks long sequence. link function extended text window context words ﬁrst-order approximation actual probability. rationale originates idea product experts suppose different types semantic/syntactic regularities encoded different dimensions exp{v vsi} exp{vsil means effects different regularities probability combined multiplying together. independent joint probability presence correlations actual joint probability would scaling scale factor reﬂects much positively negatively correlated. within scale factor captures linear interactions residual asisj captures nonlinear noisy interactions. applications interest. hence bigger magnitude relative asisj better. note assume asisj provides ﬂexibility agreeing asymmetry bigrams natural languages. time imposes symmetric part gaussian priors embeddings employed regression empirical bigram probabilities practical issue arises bigrams zero frequency constituting words become less frequent. zero-frequency bigram necessarily imply negative correlation constituting words; could simply result missing data. case even smoothing force asisj negative number making overly long. increased magnitude embeddings sign overﬁtting. hyperparameter increases frequency decreases. gaussian priors residuals wish captures much correlations possible. thus smaller asisj better. addition frequent corpus less noise empirical distribution thus residual asisj heavily penalized. subsets roots deeply information theory. intricate problem could redundant information synergistic information among conditioning variables functions pmi. based analysis complementing roles types pointwise information assume approximately equal cancel computing pointwise interaction information. appendix detailed discussion. following assumption pmi+pmi proceed assume text generated markov chain order i.e. word depends words within context size given hyperparameter generative process whole corpus penalize residual asisj sisj nonnegaf tive monotonic transformation referred weighting function. denote total penalty residuals square weighted frobenius norm sisj ccut chosen frequent bigrams identical words usually much smaller probability collocate. hence reﬂect true correlation word itself constraints embeddings. eliminate effects setting another measure reduce impact missing data apply commonly used jelinekmercer smoothing smooth empirical conditional probability unigram probability psmoothed smoothed bigram empirical accordingly joint probability deﬁned practice yields good results. obtained embeddings begin degrade indicating smoothing distorts true bigram distributions. link function text window previous subsection regression link function bigram probabilities established. section adopt ﬁrst-order approximation based information theory extend link function longer sequence w··· decomposing distribution conditioned random variables conditional distributions subroutine algorithm approximate eigendecomposition dense logarithm transformation. eigendecomposition dense matrix requires space time difﬁcult scale large vocabulary. addition majority words vocabulary infrequent tikhonov regularization necessary them. observed that words become less frequent fewer fewer words appear around form bigrams. remind vocabulary {s··· sorted decending order frequency hence lower-right blocks sparse cause blocks contribute much less penalty relative regions. therefore blocks could ignored regression without sacriﬁcing much accuracy. intuition leads following online blockwise regression. basic idea select small frequent words core words partition remaining noncore words sets moderate sizes. bigrams consisting core words referred core bigrams correspond top-left blocks embeddings core words learned approximately using algorithm top-left blocks embeddings core words embeddings noncore words turn. ignoring lower-right regions correspond bigrams noncore words quadratic terms noncore embeddings ignored. consequently ﬁnding embeddings becomes weighted ridge regression problem solved efﬁciently closedform. finally combine embeddings embeddings whole vocabulary. details follows positive semidefinite rank finding minimizes equivalent ﬁnding rank-n weighted positive semideﬁnite approximant subject tikhonov regularization. problem admit analytic solution solved using local optimization methods. first consider simpler case words vocabulary enough frequent thus tikhonov regularization unnecessary. case becomes unregularized optimization problem. adopt block coordinate descent algorithm approach problem. original algorithm generic rank-n matrix weighted approximation problem tailor constraining matrix within positive semideﬁnite manifold. summarize learning algorithm algorithm entry-wise product. suppose eigenvalues returned eigen decomposition descending order. extracts rows issue initialize srebro suggest point local optimum thus requires iterations. however also local optimum setting converges slowly too. setting usually weighted aver/ elementage wise product division respectively. columns independent thus separate weighted ridge regression problem whose solution columns corresponding respectively; models trained english wikipedia snapshot march removing nontextual elements non-english words billion words left. used default hyperparameters hyperwords training ppmi svd. wordvec glove singular trained default hyperparameters. embedding sets psd-reg-k psdunreg-k trained using online blockwise regression. sets contain embeddings frequent words based core words. psd-unreg-k traind i.e. disabling tikhonov regularization. psd-reg-k trained regularization sparsity increases. contrast batch learning performance performance psd-k listed contains core embeddings only. psd-k took advantages contains much less false candidate words test tuples evaluated missing words thus scores comparable others. sparse trained psd-k-reg input embeddings default hyperparameters. except rare words included many rare words frequency making test pairs invalid. word analogy datasets msr’s analogy dataset containing questions google’s analogy dataset questions. after ﬁltering questions involving out-of-vocabulary words i.e. words appear less times corpus instances instances google left. analogy questions answered using cosadd well cosmul proposed levy results table shows results tasks. wordvec signiﬁcantly outperformed methods analogy tasks. ppmi performed much worse analogy tasks reported probably sub-optimal hyperparameters. suggests performance unstable. embeddings yielded sparse systematically degraded compared embeddings contradicting claim method psd-reg-k performed well consistently best similarity tasks. performed worse wordvec analogy tasks still better mf-based methods. comparing psd-unreg-k tikhonov regularization brings performance boost across tasks. addition similarity tasks online blockwise regression degrades slightly compared batch factorization. performance gaps analogy tasks wider might explained fact hard cases counted psd-k’s evaluation paper inspired link functions previous works support information theory propose link function text window parameterized embeddings words residuals bigrams. based link function establish generative model documents. learning objective embeddings maximizing posterior likelihood given corpus. objective reduced weighted low-rank positive-semideﬁnite approximation subject tikhonov regularization. adopt block coordinate descent algorithm jointly online blockwise regression algorithm approximate solution. seven benchmark sets learned embeddings show competitive stable performance. future work incorporate global latent factors generative model topics sentiments writing styles develop elaborate models documents. learning latent factors important summary information documents would acquired useful various applications. thank omer levy thomas mach peilin zhao mingkui zhiqiang chunlin helpful discussions insights. research supported national research foundation prime minister’s ofﬁce singapore futures funding initiative administered interactive digital media programme ofﬁce. suppose bigram matrix interest. embeddings derived rank approximation keeping largest singular values/vectors. singular values correspond negative eigenvalues undesirable correlations might captured. following example approximating matrix. vocabulary consists words singular values/vectors kept yield identical embeddings embeddings point direction suggesting positively correlated. however actually negatively correlated second corpus. inconsistency principal eigenvalue negative corresponding singular value/vector kept. using eigendecomposition redundant information refers reduced uncertainty knowing value conditioning variables synergistic information reduced uncertainty ascribed knowing values conditioning variables cannot reduced knowing value variable alone figure different types information among random variables mutual information redundant information synergistic information between conditioning respectively. pointwise counterpart mutual information similarly concepts pointwise counterparts obtained dropping expectation operator. speciﬁcally pointwise interaction information deﬁned pint know pint recover mutual information variable subsets recover joint distribution information prdn pointwise synergistic information psyn higherorder magnitudes usually much smaller terms. assume approximately equal thus cancel computing pint. given this pint case three words pint leads pmi+pmi. references eneko agirre enrique alfonseca keith hall jana kravalova marius pas¸ca aitor soroa. study similarity relatedness using distributional wordnet-based approaches. proceedings human language technologies annual conference north americhapter association computational linguistics pages association computational linguistics. yoshua bengio holger schwenk jean-s´ebastien sen´ecal fr´ederic morin jeanluc gauvain. neural probabilistic language models. innovations machine learning pages springer. elia bruni gemma boleda marco baroni nam-khanh tran. distributional semantics technicolor. proceedings annual meeting association computational linguistics long papers-volume pages association computational linguistics. paramveer dhillon dean foster lyle ungar. multi-view learning proceedings adword embeddings cca. vances neural information processing systems pages evgeniy gabrilovich yossi matias ehud rivlin zach solan gadi wolfman eytan ruppin. placing search context concept revisited. trans. inf. syst. january. omer levy yoav goldberg dagan. improving distributional similarity lessons learned word embeddings. transactions association computational linguistics wayne zhao jing jiang jianshu weng jing ee-peng hongfei xiaoming comparing twitter traditional media using topic models. advances information retrieval pages springer. andriy mnih geoffrey hinton. three graphical models staproceedings tistical language modelling. international conference machine learning pages acm. jeffrey pennington richard socher christopher manning. glove global vectors word representation. proceedings empiricial methods natural language processing eugene agichtein shaul markovitch. word time computing word relatedness using temporal semantic analysis. proceedings international conference world wide pages york usa. acm. stratos do-kyum michael collins daniel hsu. spectral algorithm learning class-based n-gram models natural language. proceedings association uncertainty artiﬁcial intelligence. nicholas timme wesley alford benjamin flecker john beggs. synergy redundancy multivariate information measures experimentalist’s perspective. journal computational neuroscience mingkui ivor tsang yang chengqi zhang qinfeng shi. scalable maximum margin matrix factorization active riemannian subspace search. proceedings ijcai", "year": 2015}