{"title": "Stability of Topic Modeling via Matrix Factorization", "tag": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "abstract": "Topic models can provide us with an insight into the underlying latent structure of a large corpus of documents. A range of methods have been proposed in the literature, including probabilistic topic models and techniques based on matrix factorization. However, in both cases, standard implementations rely on stochastic elements in their initialization phase, which can potentially lead to different results being generated on the same corpus when using the same parameter values. This corresponds to the concept of \"instability\" which has previously been studied in the context of $k$-means clustering. In many applications of topic modeling, this problem of instability is not considered and topic models are treated as being definitive, even though the results may change considerably if the initialization process is altered. In this paper we demonstrate the inherent instability of popular topic modeling approaches, using a number of new measures to assess stability. To address this issue in the context of matrix factorization for topic modeling, we propose the use of ensemble learning strategies. Based on experiments performed on annotated text corpora, we show that a K-Fold ensemble strategy, combining both ensembles and structured initialization, can significantly reduce instability, while simultaneously yielding more accurate topic models.", "text": "topic models provide insight underlying latent structure large corpus documents. range methods proposed literature including probabilistic topic models techniques based matrix factorization. however cases standard implementations rely stochastic elements initialization phase potentially lead different results generated corpus using parameter values. corresponds concept instability previously studied context k-means clustering. many applications topic modeling problem instability considered topic models treated deﬁnitive even though results change considerably initialization process altered. paper demonstrate inherent instability popular topic modeling approaches using number measures assess stability. address issue context matrix factorization topic modeling propose ensemble learning strategies. based experiments performed annotated text corpora show k-fold ensemble strategy combining ensembles structured initialization signiﬁcantly reduce instability simultaneously yielding accurate topic models. topic models discover latent semantic structure topics within corpus documents derived co-occurrences words across documents. popular approaches topic modeling involved application probabilistic algorithms latent dirichlet allocation recently non-negative matrix factorization approaches also successfully applied identify topics unstructured text standard formulations algorithms include stochastic elements initialization phase prior optimization phase produces local solution. random component affect ﬁnal composition topics found rankings terms describe topics. problematic seeking capture deﬁnitive topic modeling solution given corpus represents fundamental instability algorithms different runs algorithm data produce different outcomes. problem widely studied context partitional clustering algorithms k-means tends converge numerous local minima depending choice starting condition case topic modeling instability manifest distinct aspects. ﬁrst observed examining topic descriptors multiple runs. term rankings change considerably certain terms appear disappear completely runs. secondly issues instability also observed examining degree documents associated topics across different runs algorithm corpus. cases inconsistencies potentially alter interpretation perception given topic model. also clear individual treated deﬁnitive summary underlying topics present data. generally speaking comparative evaluation topic modeling approaches researchers tend focus either coherence topic descriptors extent topics accurately coincide ground truth categories human annotations however researchers considered evaluation different approaches point view stability across multiple runs. paper quantitatively assess extent standard randomly-initialized algorithms unstable respect topics produce diverse collection text corpora. propose measures capture distinct aspects instability outlined above. focus addressing issue context matrix factorization exploring strategies involve improved initialization ensemble learning. particular propose combined approach motivated traditional concept k-fold cross-validation yield stable results also often producing accurate coherent models. rest paper structured follows. section provide overview relevant work topic modeling general area cluster analysis. section discuss problem topic model instability detail describing three measures quantify instability topic models. section propose ensemble approaches address issue subsequently evaluated different text corpora section finally section conclude paper ideas future work. topic models attempt discover hidden thematic structure within unstructured collection text without relying form training data. models date back early work latent semantic analysis deerwester proposed applying decompose document-term matrix uncover associations terms concepts data. basic terms topic model consists topics represented ranked list stronglyassociated terms document corpus also associated topics varying degrees. considerable research topic modeling focused probabilistic methods topic viewed probability distribution words documents mixtures topics widely-applied probabilistic topic modeling approach different approximation methods proposed inference including variational inference markov chain monte carlo approximation algorithms converge different local maxima data commonly-used implementation provided mallet software package relies fast gibbs sampling initial state determined user-speciﬁed random seed. alternative algorithms non-negative matrix factorization also effective discovering topics text corpora unsupervised approach reducing dimensionality non-negative matrices. working document-term matrix goal approximate matrix product non-negative factors dimensions. rows factor interpreted topics deﬁned non-negative weights terms corpus vocabulary. ordering provides topic descriptor form ranking terms relative corresponding topic. columns matrix provide membership weights documents respect topics. advantages traditional methods fewer parameter choices involved modeling process also tendency identify coherent topics commonly initialized assigning random nonnegative weights entries factors applying optimization process alternating least squares factors iteratively improved reduce approximation error local minimum reached. result values initial pair factors signiﬁcant impact values ﬁnal factors even large number iterations performed. alternative initialization schemes focused increasing accuracy ﬁnal factors using structured process seeding using prior clustering algorithm another approach non-negative double singular value decomposition chooses initial factors based sparse approximation original data matrix. shown particularly effective sparse data text basic form nndsvd contains stochastic element technically converge pair factors time although depends underlying implementation being used. partitional clustering algorithms k-means kmedoids inherent stability problem. algorithm data data drawn source repeatedly frequently achieve different results run. variation either poor random seeds leading convergence different local minima result perturbations data widely-adopted approach dealing issue adopt better cluster initialization strategy either fully deterministic least produces less variation random initialization simultaneously yielding useful clusterings. popular initialization approach proposed arthur vassilvitskii referred k-means++ involves choosing initial seed item random ﬁrst cluster center choosing subsequent cluster center probability proportional squared distance items nearest existing cluster centers. improve resulting clustering process repeated several different initial seed items. strategy deterministic tend yield consistent results across multiple runs. researchers also proposed fully deterministic strategies initial cluster centers determined based embedding methods coming alternative strategy reducing instability unsupervised learning ensemble clustering techniques based premise combining large diverse sets clusterings produce stable accurate solution ensemble approaches usually divided different stages. firstly collection base clusterings generated typically repeatedly applying algorithm k-means random initialization full dataset random samples data secondly integration function applied combine base clusterings single consensus clustering. common integration strategies utilized leverage information ensemble regarding level co-association pairs items. underlying idea behind items frequently assigned together different clusterings naturally belong underlying group resulting consensus clustering represents approximation average clustering among ensemble members. hadjitodorov demonstrated trade-off diversity quality cluster ensembles proposed number measures quantify diversity ensembles. work regarding optimality consistency solutions produced clustering biclustering algorithms previously carried domains however general area matrix factorization initial work ensemble approaches. includes using hierarchical scheme combine multiple factorizations study protein networks generation ensembles factorizations boosting-like approach recent work also looked using stability means identifying appropriate number topics given corpus applying text data however studies investigated extent problems introduced instability context topic modeling. section introduce three measures assessing stability collection topic models demonstrate standard approaches prone produce unstable results applied text corpora. discussed section standard implementations topic modeling approaches commonly employ stochastic initialization prior optimization. result models produce vary quite considerably different runs. regardless whether applying probabilistic non-probabilistic algorithms observe variation manifests ways relation term-topic associations document-topic associations. former ranking terms describe topic change signiﬁcantly runs. latter documents strongly associated given topic closely associated alternative topic another run. extreme cases consequence manifestations topics appear disappear across different runs algorithm. presents challenge domain experts seek gain reliable insight particular corpus documents. depending topics resulting given algorithm interpretation data change considerably. however implications variation rarely discussed topic modeling literature particularly context matrix factorization. actually quantify level stability/instability present collection topic models generated runs corpus propose three measures reﬂect aspects topic model stability described above. measures general sense applied models generated using either probabilistic matrix factorization algorithms. topics present topic models similar naturally expect prominent terms appearing topic descriptors models similar. formally represent topic single model number top-ranked terms calculate descriptor union terms across topics denote measuring symmetric difference descriptor sets different models broadly gauge similarity models. useful capture variance descriptor level terms appear disappear runs. formally given topic models containing topics represented terms calculate descriptor difference value indicates identical descriptor sets value indicates topic descriptors models share common terms all. given collection topic models calculate average descriptor difference firstly illustrate issue term instability consider topic models generated runs randomlyinitialized ﬁxed number topics fig. shows adsd scores algorithm number terms topic descriptors increases observe that even relatively relaxed measure exists substantial variation terms appearing models algorithms. represent topic using terms adsd score high nmf. even extend topic descriptors adsd gives overall measure difference models account cases topics mixed across different runs algorithm therefore propose measure compares similarity between topic models based pairwise matching process topic level. important topics appear disappear different runs also helps capture variance individual topic level. denotes ranked terms i-th topic build measure agreement complete topic models containing topics. construct similarity matrix entry indicates agreement x-th topic ﬁrst model y-th topic second model calculated using eqn. best match rows columns optimal permutation found time solving minimal weight bipartite matching problem using hungarian method this produce term stability score second manifestation topic model instability relates document-topic associations. measure extent associations document topics varies across different runs look dominant topic every document. convert document-topic associations disjoint partition taking maximum value document. compare similarity partitions generated runs using standard clustering agreement measures. utilizing information allows observe terms game team season play coach games points players against football game season team coach play games points league football players game season coach team football league giants play jets players game season team yankees games play mets nets left league game team season play games players coach yankees time terms game season team yankees games nets play points players coach game team season nets points games coach play knicks players game team season nets points games play coach knicks giants game nets team season coach points knicks jets giants play game team season nets points games play knicks coach kidd explore term instability inspecting topic-term stability topics. example table refers separate runs related topic lda. algorithms clear ordering terms topic change considerably also possible terms completely disappear different runs. using corpus also explore partition stability afforded lda. fig. plots distributions agreement scores pairs partitions corresponding models produced algorithm. partitions identical document assignments would yield score however pairs partitions achieve nmf. case unique pairs partitions none achieve perfect agreement achieve score average agreement scores runs overall pnmi scores algorithms respectively indicating considerable variation outputs algorithms across runs. again manually inspecting top-ranked documents topics models reveals extent variation. table lists identiﬁers documents assigned topics related sport selected runs table observe that similar case terms ordering documents also subject inherent instability. examples demonstrate production robust reliable topic models important also necessary emphasize stability sole requirement useful topic modeling algorithm. observed ben-hur context partitional clustering situations stability simply indicative algorithm’s tendency converge given local solution regardless quality solution. context could initialize factors deterministic arbitrary non-negative values. however redundant stability unlikely provide useful model. therefore next section propose techniques yield solutions stable also accurate i.e. topics semantically coherent provide useful insight content corpus. propose ensemble methods topic modeling matrix factorization utilized address issue stability also potentially producing accurate topic models corpus unstructured text. apply ensemble learning topic modeling form layers matrix factorization. fig. shows overview method naturally divided steps similar existing strategies ensemble clustering integration transform base topic models suitable intermediate representation apply ﬁnal produce single ensemble topic model represents ﬁnal output method. unsupervised ensemble procedures typically seek encourage diversity view improving quality information available integration phase therefore create diverse base topic models encourage diversity relying inherent instability random initialization generate base model populating factors values based different random seed applying case ﬁxed prespeciﬁed value number topics factor base topic model stored later use. generated collection factorizations second step create representation corpus form topic-term matrix matrix created stacking transpose factor generated ﬁrst step illustrated fig. factor consists topics terms wm}. construct topic-term matrix often expect similar topics appearing different runs. however identical respect terms wish leverage variance. important note process combining factors order independent. results matrix corresponds topic base topic models column term original corpus. entry holds weight association term relation single topic base model. topic descriptors explicitly capture variance between base topic models. improve quality resulting topics generate initial factors using nndsvd initialization input parameter specify ﬁnal number topics typically value used generation step. resulting factor provides weights terms ensemble topics top-ranked terms column used descriptors topic. produce weights original documents corpus fold documents ensemble model applying projection document-term matrix random initialization means models correspond poor local minima accuracy. furthermore given number possible initial factors could generated still potential several runs complete ensemble process yield somewhat different ﬁnal results. therefore consider improved initialization generate accurate base models also using structured strategy create models order reduce variability. strategy based traditional kfold cross-validation performed evaluation supervised learning. case randomly divide corpus documents folds equal size. folds excluded turn apply nndsvd initialization documents remaining folds yielding models. reduce variability repeat process rounds using different splits data yielding total topic models generated large subsample corpus. collection base topic models integrated described section produce ﬁnal topic model. full summary approach given fig. section comprehensively assess problem instability topic modeling standard approaches diverse collection corpora examine extent superior initialization ensemble methods improve stability nmf-based approaches also yielding accurate models. experiments diverse corpora including high-quality long texts user-generated content. corpora human annotated ground truth topical categories allowing evaluate model accuracy. datasets consist news articles individual mainstream news sources categorized subject matter. datasets consist pages wikipedia dump categorized associated wikiproject. eight datasets previously used topic modeling evaluations also include popular -newsgroups dataset ground truth categories correspond individual newsgroups evaluate performance social media data include newly-collected corpus experiments known topics dataset consists tweets prominent twitter accounts. accounts manually assigned different categories document corpus corresponds concatenation tweets posted single user given week period march february corpus contains user documents. detailed summary corpora used experiments provided table pre-processing corpora terms appearing documents ﬁltered. single list common english stop-words datasets. operates bag-of-words text representations applied frequency values. documents transformed logbased term frequency-inverse document frequency vectors document length normalization subsequently applied produce ﬁnal document-term matrix. random initialization using fast alternating least squares variant proposed provided sckit-learn toolkit non-random nndsvd initialization also description general news articles corpus news articles published guardian corpus news articles published irish times subset york times annotated corpus subset york times annotated corpus subset wikipedia dump articles assigned labels based high level wikiproject. subset wikipedia dump articles labeled ﬁne-grained wikiproject sub-groups. parameters random nndsvd initialization maximum number iterations default. random case different random seed used populate values initial factors process repeated runs. parameters algorithm additional hyperparameters. mallet default values maximum number iterations different random seed used initialize gibbs sampling process. process repeated runs. basic ensemble ﬁrst ensemble approach integrate collection members generated random initialization. ﬁnal number topics number ground truth categories dataset. entire process repeated times allow assess stability. k-fold ensemble second ensemble approach apply rounds folds thus also yielding collection ensembles members integration determined above. entire process repeated times. assess stability collection models generated algorithm term-based measures adsd using terms topic document-level pnmi measure results measures shown tables respectively. across three measures observe nndsvd kfold approaches clearly yield stable results. methods produce models perfect stability majority datasets i.e. yield models topicterm document-topic associations remain same. expected randomly-initialized approaches perform worst inherent instability caused stochastic elements identiﬁed standard deviation scores. basic ensemble approach yields high stability smaller corpora variation different runs term document level larger corpora. here random initialization used generating ensemble members still leading variation results ﬁnal ensemble integration phase even ensemble members. however structured nature generation phase kfold approach effectively negates problem. interesting observe that -newsgroups -topics datasets contain noisier user-generated content larger number underlying topics k-fold ensemble approach yields higher levels stability nndsvdinitialized nmf. suggests combining subsampling element ensemble process structured nndsvd initialization produces reliable solution. important note widely-used implementation nndsvd provided sckit-learn toolkit used experiments relies approximate truncated singular value decomposition method involving randomization order make applicable large data matrices. resulting decompositions often identical always case. computing full would eliminate instability trade-off running time requirements decomposing large high-dimensional document-term matrix would increase dramatically. primary focus work model stability. noted section stability without meaningful coherent topics unlikely useful. therefore consider quality models produced methods terms accuracy coherence. table model stability comparison average descriptor difference scores topic modeling approaches. corpus bbc-sport guardian- irishtimes- nytimes- nytimes- wikipedia-high wikipedia-low -newsgroups -topics topic coherence coherence refers overall quality semantic relatedness terms appearing topic descriptor. range measures proposed literature employ widely-used measure normalized pointwise mutual information uses term co-occurrence counts full corpus measure average coherence topics given model based terms descriptors. evaluations terms. table shows proposed basic ensemble k-fold approaches perform best. however noted cases differences average coherence scores small. noticeable approach nmf-based approaches reﬂect tendency produce generic less semantically-coherent terms provide clearer measure model quality next consider quality methods evaluating partition accuracy respect ground truth labels. table shows means standard deviations scores methods corpora. best-performing algorithms nndsvd-initialized k-fold ensemble approach although varies dataset. randomly-initialized algorithms exhibit considerable variation quality models produce indicated standard deviation scores worse average ensemble svd-based methods exception case applied york times corpora. investigate differences algorithms performed series statistical tests results presented previous section. carried non-parametric friedman’s aligned rank test measures previously reported test presence statistically signiﬁcant differences results amongst algorithms across datasets. tests returned p-values respectively. indicates statistically signiﬁcant differences conﬁdence level exist results achieved different algorithms measure except partition accuracy determine statistically signiﬁcant difference proposed k-fold algorithm topic modeling approaches respect four remaining measures performed series friedman’s aligned rank pairwise post tests k-fold approach used control. results tests reported table interesting statistically signiﬁcant difference conﬁdence level proposed approach randomly initialized topic modeling algorithms across measures along previously reported performance algorithm suggests k-fold approach produces stable higher quality topic models. statistical difference proposed k-fold approach ensemble nndsvd indicate three measures similar producing deterministic solutions notion strengthened similar performance regarding measures previously reported. also interesting note statistical difference k-fold approach regards coherence likely based approaches generating topics lower coherence timing algorithms vary depending hardware implementation utilized useful case obtain estimate much longer proposed ensemble approaches take respect traditional topic modeling algorithms. topic modeling approach times table posthoc p-values based friedman’s aligned rank pairwise test four measures statistically signiﬁcant using k-fold approach control algorithm. **** average times reported table experiments carried machine .ghz cores ram. running times impacted numerous factors including number documents corpus dimensionality corresponding documentterm matrix number topics selected corpus. expected clear ensemble approaches take considerably longer algorithms. naturally underlying nature generation step iterations generated. mallet implementation widely used topic modeling observe considerably slower majority experiments fact frequently slower proposed k-fold approach utilizes structured slower initialization step. discussed main criteria evaluating topic modeling algorithms separately. evaluation model quality examining topic coherence partition accuracy evaluation model stability examining term stability document stability. however important note output criteria considered together evaluations highlight topic modeling approaches perform well respect criterion performing poorly respect other. example seen results produced nndsvd nytimes- dataset. term document stability perfect stability scores achieved however take account partition accuracy dataset quality solution good initially appears. nndsvd initialization actually performs worst regards accuracy case score similarly randomly-initialized out-performs approaches york times corpora terms partition accuracy corresponding stability scores consistently poor across measures following discussion redundant stability section results raise interesting problem that strive produce stable results possible also case results poor quality model quality standpoint. among newly-proposed approaches interesting observe basic ensemble approach perform well k-fold approach even though based similar ensemble process. case former promote diversity generating base ensemble members using randomly-initialized motivated previous work supervised unsupervised ensemble learning however larger datasets stochastic nature approach tends cause ﬁnal results contain degree variance across different runs overall ensemble. contrast combining structured document subsampling nndsvd initialization generate ensemble member k-fold approach exhibits little instability across runs experiments indicated results tables ﬁndings correspond hadjitodorov demonstrated moderate level diversity leads useful ensembles cluster analysis. overall among techniques considered experiments k-fold ensemble approach produces best models taking account quality stability. observed scores lower certain datasets still performs better methods respect half corpora. strategy also appears handle noisy usertopic modeling methods widely applied range domains analyze unstructured text researchers often consider effect random initialization models produced methods. paper demonstrated that methods result signiﬁcant variations topics produced multiple runs corpus. effect manifested term document level potentially lead different human interpretations underlying thematic structure data. address issue instability context investigated extent improved algorithm initialization ensemble strategies produce stable models almost potentially accurate insightful. compared performance approaches regards different metrics measure stability accuracy coherence topics. results indicate k-fold ensemble approach afforded stable accurate models although initializating based approximation document-term matrix also provide clear improvement standard methods. concern arises application ensemble learning techniques general relates scalability. ensemble techniques described paper naturally parallelized considerable scope reducing computation time required generate ensemble. potentially promising idea investigate context relates concept snapshot ensembles supervised learning single algorithm allowed converge several local minima optimization process providing contribution overall ensemble. approach might also used yield stable topic models matrix factorization reduce computational expense.", "year": 2017}