{"title": "Learning in Riemannian Orbifolds", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "Learning in Riemannian orbifolds is motivated by existing machine learning algorithms that directly operate on finite combinatorial structures such as point patterns, trees, and graphs. These methods, however, lack statistical justification. This contribution derives consistency results for learning problems in structured domains and thereby generalizes learning in vector spaces and manifolds.", "text": "learning riemannian orbifolds motivated existing machine learning algorithms directly operate ﬁnite combinatorial structures point patterns trees graphs. methods however lack statistical justiﬁcation. contribution derives consistency results learning problems structured domains thereby generalizes learning vector spaces manifolds. statistical data analysis learning riemannian orbifolds motivated applications data want learn naturally represented ﬁnite combinatorial structures point patterns trees graphs. examples structural pattern recognition learn structured data include estimating central points distribution graphs mean median central clustering graphs learning graph quantization multilayer perceptrons graphs retrospect structure space framework proposed theoretically justiﬁes approaches sense actually minimize empirical risk function structures. since minimizing empirical risk function usually computationally intractable ultimate challenge consists constructing eﬃcient algorithms capable return optimal least suboptimal solutions. point view statistical pattern recognition however ultimate goal determine good solution empirical risk function rather discover true unknown structure data respect distribution. according perspective regard solutions empirical risk functions estimators true unknown population parameter. statistical structural pattern recognition lack consistency results existing estimators population parameters. consequence methods structural pattern recognition directly operate domain graphs still statistical justiﬁcation. ﬁrst contribution paper establishes suﬃcient conditions consistency estimators deﬁned empirical risk functions attributed graphs. regard graphs points structure space structure space quotient euclidean space permutation group. beneﬁt structure space framework provides enough mathematical structure diﬀerential geometry time preserves full relational information graphs. comparison innovations follows first extend suitable concept generalized diﬀerentiability sense norkin functions graphs. second prove stronger result underlying empirical risk functions graphs generalized diﬀerentiable rather locally lipschitz. third equipped results apply consistency theorem ermoliev norkin generalized diﬀerentiable loss functions. finally using examples show standard methods statistical pattern recognition generalized consistent learning algorithms graphs. second contribution shifts terminology structure spaces general notion orbifold. informally orbifolds topological spaces locally modeled quotients manifolds ﬁnite group actions. such structure spaces simplest examples riemannian orbifolds. shifting focus orbifolds provides view problem following beneﬁts first notion orbifold strongly emphasizes exploit diﬀerential geometric tools graphs namely charting lifting riemannian geometry. second using notion orbifold integrates structure space framework established mathematical ﬁeld providing access useful concepts results insights. third notion orbifold indicates theory generalized structures locally live quotient manifold ﬁnite group action. fourth since orbifolds generalize euclidean spaces manifolds framework establishes consistency stochastic generalized gradient learning also standard stochastic gradient learning euclidean spaces unifying umbrella learning riemannian orbifolds. section aims outlining problem learning structured data order motivate learning riemannian orbifolds. illustrative example consider problem estimating mean distribution attributed graphs. attributed graphs. begin describing structures want learn attributes distinguished element denoting null void element. attributed graph tuple consisting ﬁnite nonempty vertices attribute function elements edges denote attributed graphs attributes vertex attributed graph often referred attribute function thus obtain alignment adding isolated vertices null-attribute. aligned vertices. denote inﬁnite alignments pairwise alignment graphs triple consisting alignments together bijective mapping graph edit distance. dissimilarity fundamental concept machine learning. here consider graph edit distance common choice measuring structural variation given graphs. several distance measures reported structural pattern recognition literature derived special cases graph edit distance function. examples geometric graph distance functions distances based maximum common subgraph including graph subgraph isomorphism distance function deﬁned attributes. edit cost decomposed deletion cost insertion cost substitution cost vertices edges non-null attributes. since distance function occur pairs non-edges deﬁnition minimal pairwise alignments therefore safely ignored. observe deletion vertices also deletes edges respective vertices incident graph edit distance deﬁned edit path minimal cost optimization variable random variable probability distribution pga. since distribution graphs usually unknown goal learning minimize risk basis empirical data. point problems learning domain graphs consider counterpart minimizing risk euclidean vector space goal minimize expected risk based independent identically distributed random points probability measure since loss function continuously diﬀerentiable interchange integral gradient valid short digression vector spaces return problem minimizing expected risk graph spaces. opposed vector spaces following factors complicate learning graphs statistically consistent graph edit distance general not-diﬀerentiable; neither well-deﬁned addition graphs notion derivative functions graphs known. therefore address following questions extend gradient-based learning problems euclidean spaces minimize expected risk learning problem structured inputand/or output-space statistically consistent way? ansatz answer questions identify graphs points riemannian orbifold extend concept generalized diﬀerentiability sense norkin order apply methods stochastic optimization non-diﬀerentiable non-convex loss functions. following orbifold triple consisting euclidean space permutation group acting orbifold chart call elements structures since represent combinatorial structures graphs. capital letters denote structures write vector vector representation structure vector representation representation space riemannian orbifolds attributed graphs arise considering equivalence classes matrices representing graph. identify graphs points orbifold technical assumptions simplify mathematical treatment necessary. this graph distance space graph edit distance make following assumptions consider assumptions detail. conditions eﬀect graph edit distance provided appropriate feature attributes found. restricting ﬁnite dimensional euclidean feature spaces necessary deriving consistency results applying methods stochastic optimization. limiting maximum size graphs arbitrarily large number aligning smaller graphs graphs oder purely technical assumptions simplify mathematics. machine learning problems limitation practical impact neither bound needs speciﬁed explicitly extension graphs identical order needs performed. applying theory actually require order graphs bounded. assumptions mind construct riemannian orbifold attributed graphs. hn×n -matrices elements feature space graph completely speciﬁed representation matrix elements permutation group acting regarding arbitrary matrix representation graph orbit consists possible matrices represent identifying orbits attributed graphs attributed graphs bounded order riemannian orbifold. orbifold. derive intrinsic metric enables riemannian geometry. note case graph orbifolds intrinsic metric special graph edit distance based generalization concept maximum common subgraph. graph metric occurs various diﬀerent guises common choice proximity measure example suppose attributed graphs edges attribute vertices attribute optimal alignment kernel induced standard inner product number edges maximum common subgraph equation states length minimizing geodesic therefore intrinsic metric coincides inﬁmum length admissible curves addition topology induced metric coincides quotient topology induced topology euclidean space mappings. orbifold mapping mapping underlying spaces. lift mapping representation spaces since orbifold form deﬁne orbifold function function lift function satisfying lift invariant group actions implies gradients vector representations structure namely gradient orbifold function thus gradient well-deﬁned structure pointing direction steepest ascent. section extends concept generalized diﬀerentiability sense norkin orbifold functions. begin introducing generalized diﬀerentiable functions. ﬁnite-dimensional euclidean space. function generalized diﬀerentiable multi-valued neighborhood suppose orbifold function. generalized differentiable lift generalized diﬀerentiable vector representations project subdiﬀerential deﬁned projection proves upper semicontinuity vector representations projecting finally prove satisﬁes subderivative property suppose γ-invariance since generalized diﬀerentiable deﬁne showing tends faster zero proves subderivative property vector representations projecting putting results together yields generalized diﬀerentiable example graph space graph edit distance.we identify riemannian orbifold graph edit distance distance function deﬁned suppose edit costs edit paths generalized diﬀerentiable. distance generalized diﬀerentiable. expected risk function optimization variable random variable probability measure loss function measures performance learning system parameter given observable event assume loss generalized diﬀerentiable integrable expectation taken respect probability space since distribution observable events usually unknown expected risk function neither computed minimized directly. addition loss function neither convex diﬀerentiable. ﬁeld stochastic approximation provides methods minimize consistent general conditions. since interchange integral generalized gradient valid mild assumptions minimize expected risk according following stochastic generalized gradient method since orbifolds generalize euclidean spaces manifolds consistency theorem also valid standard machine learning algorithms euclidean spaces diﬀerentiable cost function non-diﬀerentiable cost function section extends typical examples statistical data analysis learning problems vector spaces structured domains. assume riemannian orbifold optimal alignment kernel parameter space space observable data. parameter space consists augmented parameter structures weight structure bias. observable data consists input structures together labels {±}. learning orbifold maps. example presents generic formulation learning functional relationships orbifolds supervised manner. since orbifolds generalize euclidean spaces setting covers various types functional relationships learned. non-standard examples include multi-layer perceptrons adaptive processing graphs learning predict structured data structure quantization. structure quantization generalizes vector quantization quantization structures. graphs number structure quantizer design techniques purpose central clustering already proposed. examples include competitive learning k-means well k-medoids algorithms contribution proves consistency learning structured domains reducing stochastic generalized gradient learning riemannian orbifolds. proposed framework applicable learning combinatorial structures point patterns trees graphs. retrospect proposed results provide theoretical foundation statistical justiﬁcation number existing learning methods directly operate domain graphs. addition orbifold framework provides generic technique generalize gradient-based learning methods structured domains. future work aims generalizing theory general riemannian orbifolds discontinuous graph edit distance functions.", "year": 2012}