{"title": "Norm-Based Capacity Control in Neural Networks", "tag": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "abstract": "We investigate the capacity, convexity and characterization of a general family of norm-constrained feed-forward networks.", "text": "investigate capacity convexity characterization general family normconstrained feed-forward networks. keywords feed-forward neural networks deep learning scale-sensitive capacity control statistical complexity capacity unregularized feed-forward neural networks function network size depth fairly well understood. hard-threshold activations vc-dimension hence sample complexity class functions realizable feed-forward network equal logarithmic factors number edges network corresponding number parameters. continuous activation functions vcdimension could higher fairly well understood still controlled size depth network. feedforward networks often trained kind explicit implicit regularization weight decay early stopping regularization exotic regularization drop-outs. effect regularization induced hypothesis class capacity? linear prediction know using regularization capacity class bounded terms norms dependence number edges e.g. understand well capacity ℓ-regularized linear predictors bounded terms norm alone even inﬁnite dimension. using weights high precision vastly different magnitudes possible shatter number points quadratic number edges activations sigmoid ramp hinge used even activations dimension still bounded size depth central question bound capacity feed-forward network terms norm-based regularization alone without relying network size even network size unbounded inﬁnite? type regularizers admit capacity control? capacity behave function norm perhaps network parameters depth? beyond central question capacity control also analyze convexity resulting hypothesis class—unlike unregularized size-controlled feed-forward networks inﬁnite magnitude-controlled networks potential yielding convex hypothesis classes convex class might easier optimize might convenient ways. paper focus networks rectiﬁed linear units natural types norm regularization bounding norm incoming weights unit bounding overall norm weights system jointly generalize single notion group-norm regularization take norm weights unit norm units. section present regularizer obtain tight understanding provides size-independent capacity control characterization induces convexity. apply generic results per-unit regularization overall regularization noting also forms regularization equivalent two. particular show per-unit regularization equivalent novel pathbased regularizer overall regularization two-layer networks equivalent so-called convex neural networks terms capacity control show per-unit regularization allows size-independent capacity-control per-unit ℓ-norm overall regularization allows size-independent capacity control even depth bounded. case even bound magnitudes system show exponential dependence depth unavoidable. aware prior work size-independent capacity control feedforward networks considered per-unit regularization per-unit regularization two-layered networks here extend scope signiﬁcantly provide broad characterization types regularization possible properties. particular consider overall norm regularization perhaps natural form regularization used practice hope study useful thinking about analyzing designing learning methods using feed-forward networks. another motivation complexity large-scale optimization often related scale-based dimensionbased complexity. understanding scale-based complexity depends exponentially depth network might help shed light understanding difﬁculties optimizing deep networks. preliminaries feedforward neural networks feedforward neural network computes function speciﬁed directed acyclic graph special input nodes incoming edges special output node vout outgoing edges weights edges activation function given input output values input units coordinates rely inputs ﬁxed bias coordinate) output value internal nodes deﬁned according forward propagation equation network said compute function fgwσ given graphs activation function consider hypothesis class functions {fgwσ computable using setting weights. refer size network overall number edges depth network length longest directed path in-degree network maximum in-degree vertex special case feedforward neural networks layered fully connected networks vertices partitioned layers directed edge every vertex layer every vertex layer index layers ﬁrst layer whose inputs input nodes last layer contains single output node—the number layers thus equal depth in-degree maximal layer size. denote layer layered fully connected network layers nodes layer also allow also shorthand layerσ layerσ. layered networks parametrized sequence matrices rh×d rh×h contains input weights unit layer focus mostly hinge relu activation currently popular σrelu max. activation speciﬁed implicitly referring relu. relu several convenient properties exploit shared activation functions non-negative homogeneity non-negative scalar input σrelu σrelu. property important allows scale incoming weights unit scale outgoing edges without changing function computed network. layered graphs means scale compensate scaling consider various measures magnitude weights measure induces complexity measure functions deﬁned fgwσ sublevel sets complexity measure form family hypothesis classes shorthand αdhσ referring layered graphs layer layer respectively frequently drop relu implicitly meant. binary function {±}d realized unit margin points shattered unit margin hypothesis class realized unit margin appears contexts. group regularizer imposes per-unit regularization constrain norm incoming weights unit separately regularizer overall weight regularizer constraining overall norm weights system. e.g. paying magnitudes weights network corresponds overall weight-decay square magnitudes weights measures therefore equivalent relus deﬁne level remainder sets family hypothesis classes refer simply section investigate convexity generalization properties hypothesis classes. order understand effect norm sample complexity bound recall rademacher complexity rademacher complexity classes measure capacity hypothesis class speciﬁc sample used bound difference empirical expected error thus excess generalization error empirical risk minimization complete treatment appendix exact deﬁnitions use). particular intuition rademacher complexity increases simply distributing weights among neurons supremum attained output neuron connected neuron highest rademacher complexity lower layer weights layer zero. complete proof appendix bound theorem depends magnitude weights captured also width network however dependence width disappears bound depends magnitude long happens e.g. overall regularization per-unit regularization whenever cases omit size constraint state theorem inﬁnite-width layered network next investigate tightness complexity bound theorem show dependence width indeed unavoidable. show bound rademacher complexity tight implied bound sample complexity tight even binary classiﬁcation margin binary inputs. this show shatter points {±}d using network small group-norm proof consider size subset vertices dimensional hypercube +}d. construct ﬁrst layer using units. unit unique weight vector consisting output positive value sign pattern input matches weight vector. second layer single unit connects units ﬁrst layer. dimensional sign pattern choose weights second layer network output desired sign unit margin. norm network establishes claim obtain norm unit margin adding layers unit layer connected previous layer unit weight. show dependence recursively replacing unit copies adding averaging unit that. speciﬁcally given layer network make copies output unit rectiﬁed linear activation layer output unit uniform weight copies layer. since operation change output network margin norm network understand lower bound ﬁrst consider bound without dependence width depth means depth sample complexity learning class scales shows polynomial dependence though lower exponent dependence theorem still consider complexity control function sample complexity least establishing control group-norm cannot avoid sample complexity depends exponentially depth. note construction factors theorem namely maxi kxik logarithmic next consider dependence width depth indeed width depth increase magnitude control decrease without decreasing capacity matching theorem offset depth. particular regime shatter arbitrarily large number points arbitrarily using indeed inﬁnite cannot ensure enough hidden units capacity generalization. convexity convex. finally establish sufﬁcient condition hypothesis classes independent speciﬁc represenreferring convexity functions tation. consider possibly regularized empirical risk minimization problem weights objective would never convex function weights even regularizer convex bound width network instead rely magnitude-control alone resulting hypothesis class indeed complexity measure convex calculating output layer. output unit weights coming f-side weights coming g-side. appendix show condition theorem complete proof also show homogeneous sufﬁcient convexity. per-unit path regularization section focus special case i.e. constrain norm incoming weights unit separately. per-unit ℓ-regularization studied bartlett koltchinskii panchenko bartlett mendelson showed generalization guarantees. two-layer network form relu activation also considered bach studied approximation ability suggested heuristics learning per-unit regularization two-layer network considered saul showed equivalent using speciﬁc kernel. introduce path regularization discuss equivalence per-unit regularization. controls norm used aggregate paths. motivate regularizer follows node high-weight paths going really don’t care much comes won’t much effect output. path-regularizer thus looks aggregated inﬂuence weights. important emphasize even layered graphs case weights γp∞. e.g. high-magnitude edge going unit non-zero outgoing edges affect high-magnitude edges different layers different paths. sense path regularization careful equivalence extend non-layered graphs since lengths different paths might different. again think path regularizer reﬁned regularizer taking account local structure. however consider dags depth notions equivalent capacity previously noted size-independent generalization bounds bounded depth networks bounded per-unit norm long known correspond specialization corollary case furthermore kernel view saul allows obtaining size-independent generalization bound two-layer networks bounded per-unit norm however lower bound theorem establishes beyond layers cannot ensure generalization without also controlling size network. convexity immediately consequence theorem per-unit regularization constrain network width convex norm. however discussed above depth meaningful collapses hardness since classes convex might hope might make learning computationally easier. indeed consider functional-gradient boosting-type strategies learning predictor class however bach points easy requires ﬁnding best target relu unit easy. indeed applying results hardness learning intersections halfspaces represented small per-unit norm using two-layer networks conclude that subject certain complexity assumptions possible efﬁciently learn corollary subject strong random assumptions daniely possible efﬁciently learn functions {±}d realizable unit margin moreover subject intractability ˜q-unique shortest vector problem possible efﬁciently learn functions {±}d realizable unit margin corollary theorem appendix either versions corollary precludes possibility learning time polynomial though still might possible learn poly time sublinear. theorem exists layered graph outg depth degree every internal node one. subgraph induced non-input vertices tree directed toward output vertex. theorem tells realize every function tree optimal perunit norm. think learning inﬁnite fully-connected layered network always restrict models non-zero-weight edges form tree. means using per-unit regularization incentive share lowerlevel units—each unit single outgoing edge used single down-stream unit. seems defy much intuition power using deep networks expect lower layers represent generic feature useful many higherlevel features. effect encouraging transfer learning different aspects function per-unit regularization therefore misses much inductive bias might like impose using deep learning ﬁrst sort vertices based topological ordering out-degree ﬁrst vertex zero. step ﬁrst pick vertex vector topological ordering. out-degree otherwise edge create copy vertex call edge connect incoming edges weights every ﬁnally delete vertex together incoming outgoing edges easy indicate fgiwi fgi−wi−. steps internal nodes out-degree hence subgraph induced non-input vertices tree. capacity corollary provides generalization guarantee independence width—we conclude weight decay tighter regularization need limit networks ﬁnite size however section layers regularizer degenerates leads inﬁnite capacity classes case even bound overall ℓ-norm complexity increases exponentially depth. convexity conditions theorem convexity ensured depth i.e. single unit conﬁrms ℓp-regularized linear prediction convex depth convexity regularization depth would need however values know degenerates inﬁnite capacity class control theorem width leaves interesting convex class. show explicit convex characterization showing equivalent so-called convex neural nets. convex neural nets inputs two-layer networks ﬁxed inﬁnite hidden layer consisting units weights base class second ℓ-regularized layer. since ﬁnite data weights second layer always taken ﬁnite support approach function countable support instead think network bottom layer constraint layer regularized. focusing kwkp corresponds imposing constraint bottom layer regularization layer yields following complexity measure corollary subject strong random assumptions daniely possible efﬁciently learn functions {±}d realizable unit margin moreover subject intractability ˜q-unique shortest vector problem possible efﬁciently learn functions {±}d realizable unit margin discussed relying magnitude-based regularization instead directly controlling network size thus allowing unbounded even inﬁnite width. still relied ﬁnite bound depth derivations. explicit dependence depth avoided replaced measure scale weights? already know cannot rely bound group-norm depth unbounded know theorem terms sample complexity necessarily increases exponentially depth allow arbitrarily deep graphs shrink toward zero without changing scale computed function. however controlling γ-measure equivalently path-regularizer arbitrarily-deep graphs sensible deﬁne minimization dag. theorem conclude γp∞. case sensible complexity measure collapse despite unbounded depth. obtain generalization guarantees class nγpq≤γ unfortunately even obtain width-independent bounds bound corollary still dependence even bounded. dependence avoided? however ramp homogeneous equivalent breaks down. obtain bound also relu? least inductive argument used proofs theorems cannot used avoid exponential dependence depth. this consider conv symmetric convex hull applied function class. order apply inductive argument without increasing complexity exponentially depth would need operation preserve rademacher complexity least non-negative convex cones however show simple example non-negative convex cone specify vectors corresponding evaluation different functions class points sample. construction points. consider conv case conv hard verify presented general framework norm-based capacity control feed-forward networks analyzed norm-based control sufﬁcient extent capacity still depends parameters. particular showed depth networks per-unit control overall regularization sufﬁcient capacity control without also controlling network size. contrast linear models weak dependence dimensionality two-layer networks per-unit also sufﬁcient capacity control. also obtained generalization guarantees perhaps natural form regularization namely regularization showed even control still necessarily exponential dependence depth. although additive µ-measure multiplication γ-measure equivalent optimum behave rather differently terms optimization dynamics understanding relationship them well novel path-based regularizer helpful practical regularization neural networks. although obtained tight characterization size-independent capacity control possible precise polynomial dependence margin-based classiﬁcation norm might tight likely improved though would require going beyond bounding rademacher complexity real-valued class. particular theorem gives bound per-unit regularization overall regularization although would expect later lower capacity. beyond open issue regarding depth-independent γ-based capacity control another γpq≤γ particularly interesting open question understanding expressive power function depth clearly going depth depth provides additional expressive power clear much additional depth helps. class already includes binary functions {±}d dense among continuous realvalued functions. γ-measure reduced increasing depth? viewed differently monotonically non-increasing functions continues decreasing? although seems obvious functions require high depth efﬁcient representation questions related decade-old problems circuit complexity might easy resolve. research partially supported grant iis- intel icri-ci award. thank colt anonymous reviewers pointing error statement lemma suggesting corrections.", "year": 2015}