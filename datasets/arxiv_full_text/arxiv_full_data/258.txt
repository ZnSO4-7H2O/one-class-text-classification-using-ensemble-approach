{"title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency", "tag": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "abstract": "In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence - both semantic and syntactic - but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of $6.28\\%$. This is comparable to the state-of-the-art $5.91\\%$ resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.", "text": "paper propose topicrnn recurrent neural network -based language model designed directly capture global semantic meaning relating words document latent topics. sequential nature rnns good capturing local structure word sequence semantic syntactic might face difﬁculty remembering long-range dependencies. intuitively long-range dependencies semantic nature. contrast latent topic models able capture global semantic structure document account word ordering. proposed topicrnn model integrates merits rnns latent topic models captures local dependencies using global dependencies using latent topics. unlike previous work contextual language modeling model learned endto-end. empirical results word prediction show topicrnn outperforms existing contextual baselines. addition topicrnn used unsupervised feature extractor documents. sentiment analysis imdb movie review dataset report error rate comparable state-of-the-art resulting semi-supervised approach. finally topicrnn also yields sensible topics making useful alternative document models latent dirichlet allocation. reading document short long humans mechanism somehow allows remember gist read far. consider following example u.s.presidential race isn’t drawing attention controversy united states it’s closely watched across globe. rest world think campaign already thrown surprise another? asked journalists take race country might hoping america’s next missing word text easily predicted human either president commander chief synonyms. various language models simple ngrams recent rnn-based language models solve problem predicting correctly subsequent word observed sequence words. good language model capture least important properties natural language. ﬁrst correct syntax. order prediction enjoys property often need consider preceding words. therefore correct syntax local property. word order matters case. second property semantic coherence prediction. achieve this often need consider many preceding words understand global semantic meaning sentence document. ordering words usually matters much less case. consider ﬁxed-size context window preceding words traditional n-gram neural probabilistic language models difﬁculties capturing global semantic information. overcome this rnn-based language models hidden states remember history word sequence. however none approaches explicitly model main properties language mentioned above correct syntax semantic coherence. previous work chelba jelinek exploit syntactic semantic parsers capture long-range dependencies language. paper propose topicrnn rnn-based language model designed directly capture long-range semantic dependencies latent topics. topics provide context rnn. contextual rnns received attention however models closest contextual model proposed mikolov zweig recent extension long-short term memory architecture models pre-trained topic model features additional input hidden states and/or output rnn. contrast topicrnn require pre-trained topic model features learned end-to-end fashion. introduce automatic handling stop words topic models usually difﬁculty dealing with. comparable model size topicrnn achieves better perplexity scores contextual model mikolov zweig penn treebank dataset moreover topicrnn used unsupervised feature extractor downstream applications. example derive document features imdb movie review dataset using topicrnn sentiment classiﬁcation. reported error rate close state-of-the-art despite labels adversarial training feature extraction stage. remainder paper organized follows section provides background rnn-based language models probabilistic topic models. section describes topicrnn network architecture generative process perform inference section presents per-word perplexity results penn treebank dataset classiﬁcation error rate imdb dataset. finally conclude provide future research directions section present background necessary building topicrnn model. ﬁrst review rnn-based language modeling followed discussion construction latent topic models. language modeling fundamental many applications. examples include speech recognition machine translation. language model probability distribution sequence words predeﬁned vocabulary. formally vocabulary sequence words language model measures likelihood sequence joint probability distribution traditional n-gram feed-forward neural network language models typically make markov assumptions sequential dependencies words chain rule shown limits conditioning ﬁxed-size context window. principle rnn-based models remember arbitrarily long histories provided enough capacity practice large-scale neural networks easily encounter difﬁculties optimization overﬁtting issues finding better ways model long-range dependencies language modeling therefore open research challenge. motivated introduction much long-range dependency language comes semantic coherence syntactic structure local phenomenon. therefore models capture long-range semantic dependencies language complementary rnns. following section describe family models called probabilistic topic models. probabilistic topic models family models used capture global semantic coherency provide powerful tool summarizing organizing navigating document collections. basic goal models groups words tend co-occur together document. groups words called topics represent probability distribution puts mass subset vocabulary. documents represented mixtures latent topics. posterior inference learned topics capture semantic coherence words cluster together simplest topic model latent dirichlet allocation assumes underlying topics distribution ﬁxed vocabulary. generative process follows first generate topics ∼iid dirichlet. document containing words independently generate document-level variables data prior distribution topic proportions dirichlet distribution; replaced many distributions. example correlated topic model uses log-normal distribution. topic models words models word order ignored. makes easier topic models capture global semantic information. however also reasons topic models perform well general-purpose language modeling applications word prediction. bi-gram topic models proposed higher order models quickly become intractable. another issue encountered topic models model stop words well. stop words usually carry semantic meaning; appearance mainly make sentence readable according grammar language. also appear frequently figure unrolled topicrnn architecture words document state time step stop word indicators latent representation input document unshaded convention. topicrnn model architecture compact form binary vector indicates whether word input document stop word not. indicates stop words blue indicates content words. next describe proposed topicrnn model. topicrnn latent topic models used capture global semantic dependencies focus modeling capacity local dynamics sequences. joint modeling hope achieve better overall performance downstream applications. model. topicrnn generative model. document containing words stop word indicator controls topic vector affects output. topic vector contribution output. otherwise bias favor words likely appear mixing measured product latent word vector vocabulary word. longrange semantic information captured directly affects output additive procedure. unlike mikolov zweig contextual information passed hidden layer rnn. main reason behind choice using topic vector bias instead passing hidden states enables clear separation contributions global semantics local dynamics. global semantics come topics meaningful stop words excluded. however stop words needed local dynamics language model. hence achieve separation global local binary decision model stop words. unclear achieve pass topics show unrolled graphical representation topicrnn figure denote model parameters parameter inference network introduce below. observations word sequences stop word indicators marginal likelihood sequence model inference. direct optimization equation intractable variational inference approximating marginal variational distribution marginalized variable construct variational objective function also called evidence lower bound follows following proposed variational autoencoder technique choose form inference network using feed-forward neural network |vc| term-frequency representation excluding stop words variational autoencoder inference network parameter feed-forward neural network relu activation units projects k-dimensional latent space. speciﬁcally denotes feed-forward neural network. weight matrices biases shared across documents. document resulting unique distribution document. output inference network distribution regard summarization semantic information similar topic proportions latent topic models. show role inference network figure training parameters inference network model jointly learned updated truncated backpropagation time using adam algorithm stochastic samples reparameterization trick towards generating sequential text computing perplexity. suppose given word sequence initial estimation generate next word compute probability distribution given online fashion. choose point estimate mean current distribution marginalizing stop word indicator unknown prior observing approximate distribution predicted word sample predictive distribution. update including stop word. however updating word prediction expensive sliding window done mikolov zweig compute perplexity approximate predictive distribution above. model complexity. topicrnn complexity size hidden layer vocabulary size dimension topic vector number parameters inference network. contextual mikolov zweig accounts including pre-training process might require parameters additional complexity. assess performance proposed topicrnn model word prediction sentiment analysis. word prediction penn treebank dataset standard benchmark assessing language models sentiment analysis imdb dataset also common benchmark dataset application. lstm cells experiments leading topicrnn topiclstm topicgru. figure inferred distributions using topicgru three different documents. content documents added appendix. shows topics picked depending input document. ﬁrst tested topicrnn word prediction task using penn treebank portion wall street journal. standard split sections used training sections validation sections testing vocabulary size includes special token rare words indicates sentence. topicrnn takes documents inputs. split data blocks sentences constitute documents done inference network takes input bag-of-words representation input document. reason vocabulary size inference network reduced excluding pre-deﬁned stop words. order compare previous work contextual rnns trained topicrnn using different network sizes. performed word prediction using recurrent neural network neurons table topicrnn counterparts exhibit lower perplexity scores across different network sizes reported mikolov zweig table shows per-word perplexity scores neurons. table table correspond per-word perplexity scores neurons respectively. results prove topicrnn generalization capabilities example need topicgru neurons achieve better perplexity stacking lstms neurons each neurons neurons. experiments used multilayer perceptron hidden layers hidden units layer inference network. number topics tuned depending size rnn. neurons used topics. neurons found topics optimal. used validation tune hyperparameters model. used maximum epochs experiments performed early stopping using validation set. comparison purposes apply dropout used layer counterparts word prediction experiments reported table epoch neurons takes minutes. neurons epoch completed less minutes. finally neurons epoch takes less minutes. experiments microsoft azure cores tesla gpus memory. first show randomly drawn topics table results correspond network neurons. also illustrate inferred topic distributions several documents topicgru figure similar standard topic models distributions also relatively peaky. next compare performance topicrnn baseline contextual using perplexity. perplexity thought measure surprise language model. deﬁned exponential average negative likelihood. table summarizes results different network sizes. learn three things tables. first perplexity reduced larger network size. second rnns context features perform better rnns without context features. third topicrnn gives lower perplexity previous baseline result reported mikolov zweig note compute perplexity scores word prediction sliding window compute move along sequences. topic vector used current batch words estimated previous batch words. enables fair comparison previously reported results another aspect topicrnn model studied capacity generate coherent text. this randomly drew document test used document seed input inference network compute expectation topics contained seed document reﬂected generated text. table shows generated text models learned imdb datasets. appendix examples. believe senior damages guarantee frustration stations rush minimum effect composite trading compound base inﬂated rate common charter report wells fargo inc. state control funds without openly scheduling university exchange rate downgraded said united cancer began critical increasing rate less country trade rate three months workers mixed head watched month acting surprisingly nothing good cant believe role appear stupid killer really help fell plot clearly gets clear movie mexico direction regarding ﬁlms walk model full full unlabelled wrrbm wrrbm mnb-uni mnb-bi svm-uni svm-bi nbsvm-uni seq-bown-cnn nbsvm-bi paragraph vector sa-lstm joint training lstm tuning dropout lstm initialized wordvec embeddings sa-lstm linear gain lm-tm sa-lstm virtual adversarial topicrnn performed sentiment analysis using topicrnn feature extractor imdb dataset. data consists movie reviews internet movie database website. data split training testing. among training reviews unlabelled labelled carrying either positive negative sentiment. test reviews labelled. trained topicrnn random training reviews used remaining reviews validation. learn classiﬁer passed labelled training reviews learned topicrnn model. concatenated output inference network last state reviews compute feature vectors. used feature vectors train neural network hidden layer hidden units sigmoid activation function predict sentiment exactly done mikolov train topicrnn model used vocabulary size mapped words token. took stop words create input inference network. used units layers inference network used layers units per-layer figure clusters sample movie reviews imdb dataset using topicrnn feature extractor. used k-means cluster feature vectors. used reduce dimension visualization purposes. negative review green positive review. rnn. chose step size deﬁned topics. regularization dropout. trained model epochs used validation tune hyperparameters model track perplexity early stopping. experiment took close hours macbook quad-core ram. appendix visualization topics learned data. table summarizes sentiment classiﬁcation results topicrnn methods. error rate close state-of-the-art despite labels adversarial training feature extraction stage. approach similar mikolov features extracted unsupervised one-layer neural trained classiﬁcation. figure shows ability topicrnn cluster documents using feature vectors created sentiment analysis task. reviews positive sentiment coloured green reviews carrying negative sentiment shown red. shows topicrnn used unsupervised feature extractor downstream applications. table shows generated text models learned imdb datasets. appendix examples. overall generated text imdb encodes negative sentiment. paper introduced topicrnn rnn-based language model combines rnns latent topics capture local global dependencies words. global dependencies captured latent topics serve contextual bias rnn-based language model. contextual information learned jointly parameters maximizing evidence lower bound variational inference. topicrnn yields competitive per-word perplexity penn treebank dataset compared previous contextual models. reported competitive classiﬁcation error rate sentiment analysis imdb dataset. also illustrated capacity topicrnn generate sensible topics text. future work study performance topicrnn stop words dynamically discovered training. also extend topicrnn applications capturing context important dialog modeling. successful allow model performs well across different natural language processing applications. chelba mikolov schuster brants koehn robinson. billion word benchmark measuring progress statistical language modeling. arxiv preprint arxiv. merriënboer gulcehre bahdanau bougares schwenk bengio. learning phrase representations using encoder-decoder statistical machine translation. arxiv preprint arxiv. j.-y. cao. dependence language model information retrieval. proceedings annual international sigir conference research development information retrieval pages yang zhou hierarchical recurrent neural network document modeling. proceedings conference empirical methods natural language processing pages maas daly pham huang potts. learning word vectors sentiment analysis. proceedings annual meeting association computational linguistics human language technologies-volume pages association computational linguistics following notation vocabulary size number hidden units number topics dimension inference network hidden layer. table gives dimension parameters topicrnn model ’the’ ’market’ ’has’ ’grown’ ’relatively’ ’quiet’ ’since’ ’the’ ’china’ figure left ’crisis’ ’but’ ’if’ ’the’ ’japanese’ ’return’ ’in’ ’force’ ’their’ ’ﬁnancial’ ’might’ ’could’ ’compensate’ ’to’ ’some’ ’extent’ ’for’ ’local’ ’investors’ ’<unk>’ ’commitment’ ’another’ ’and’ ’critical’ ’factor’ ’is’ ’the’ ’u.s.’ ’hong’ ’kong’ \"’s\" ’biggest’ ’export’ ’market’ ’even’ ’before’ ’the’ ’china’ ’crisis’ ’weak’ ’u.s.’ ’demand’ ’was’ ’slowing’ ’local’ ’economic’ ’growth’ ’<unk>’ ’strong’ ’consumer’ ’spending’ ’in’ ’the’ ’u.s.’ ’two’ ’years’ ’ago’ ’helped’ ’<unk>’ ’the’ ’local’ ’economy’ ’at’ ’more’ ’than’ ’twice’ ’its’ ’current’ ’rate’ ’indeed’ ’few’ ’economists’ ’maintain’ ’that’ ’global’ ’forces’ ’will’ ’continue’ ’to’ ’govern’ ’hong’ ’kong’ \"’s\" ’economic’ ’<unk>’ ’once’ ’external’ ’conditions’ ’such’ ’as’ ’u.s.’ ’demand’ ’swing’ ’in’ ’the’ ’territory’ \"’s\" ’favor’ ’they’ ’argue’ ’local’ ’businessmen’ ’will’ ’probably’ ’overcome’ ’their’ ’worries’ ’and’ ’continue’ ’doing’ ’business’ ’as’ ’usual’ ’but’ ’economic’ ’arguments’ ’however’ ’solid’ ’wo’ \"n’t\" ’necessarily’ ’<unk>’ ’hong’ ’kong’ \"’s\" ’million’ ’people’ ’many’ ’are’ ’refugees’ ’having’ ’ﬂed’ ’china’ \"’s\" ’<unk>’ ’cycles’ ’of’ ’political’ ’repression’ ’and’ ’poverty’ ’since’ ’the’ ’communist’ ’party’ ’took’ ’power’ ’in’ ’as’ ’result’ ’many’ ’of’ ’those’ ’now’ ’planning’ ’to’ ’leave’ ’hong’ ’kong’ ’ca’ \"n’t\" ’easily’ ’be’ ’<unk>’ ’by’ ’<unk>’ ’improvements’ ’in’ ’the’ ’colony’ \"’s\" ’political’ ’and’ ’economic’ ’climate’ ’it’ ’said’ ’the’ ’man’ ’whom’ ’it’ ’did’ ’not’ ’name’ ’had’ ’been’ ’found’ ’to’ ’have’ ’the’ ’disease’ ’after’ ’hospital’ ’tests’ ’once’ ’the’ ’disease’ ’was’ ’conﬁrmed’ ’all’ ’the’ ’man’ \"’s\" ’associates’ ’and’ ’family’ ’were’ ’tested’ ’but’ ’none’ ’have’ ’so’ ’far’ ’been’ ’found’ ’to’ ’have’ ’aids’ ’the’ ’newspaper’ ’said’ ’the’ ’man’ ’had’ ’for’ ’long’ ’time’ ’had’ ’chaotic’ ’sex’ ’life’ ’including’ ’relations’ ’with’ ’foreign’ ’men’ ’the’ ’newspaper’ ’said’ ’the’ ’polish’ ’government’ ’increased’ ’home’ ’electricity’ ’charges’ ’by’ ’and’ ’doubled’ ’gas’ ’prices’ ’the’ ’ofﬁcial’ ’news’ ’agency’ ’<unk>’ ’said’ ’the’ ’increases’ ’were’ ’intended’ ’to’ ’bring’ ’<unk>’ ’low’ ’energy’ ’charges’ ’into’ ’line’ ’with’ ’production’ ’costs’ ’and’ ’compensate’ ’for’ ’rise’ ’in’ ’coal’ ’prices’ ’in’ ’<unk>’ ’news’ ’south’ ’korea’ ’in’ ’establishing’ ’diplomatic’ ’ties’ ’with’ ’poland’ ’yesterday’ ’announced’ ’million’ ’in’ ’loans’ ’to’ ’the’ ’ﬁnancially’ ’strapped’ ’warsaw’ ’government’ ’in’ ’victory’ ’for’ ’environmentalists’ ’hungary’ \"’s\" ’parliament’ ’terminated’ ’multibillion-dollar’ ’river’ ’<unk>’ ’dam’ ’being’ ’built’ ’by’ ’<unk>’ ’ﬁrms’ ’the’ ’<unk>’ ’dam’ ’was’ ’designed’ ’to’ ’be’ ’<unk>’ ’with’ ’another’ ’dam’ ’now’ ’nearly’ ’complete’ ’miles’ ’<unk>’ ’in’ ’czechoslovakia’ ’in’ ’ending’ ’hungary’ \"’s\" ’part’ ’of’ ’the’ ’project’ ’parliament’ ’authorized’ ’prime’ ’minister’ ’<unk>’ ’<unk>’ ’to’ ’modify’ ’agreement’ ’with’ ’czechoslovakia’ ’which’ ’still’ ’wants’ ’the’ ’dam’ ’to’ ’be’ ’built’ ’mr.’ ’<unk>’ ’said’ ’in’ ’parliament’ ’that’ ’czechoslovakia’ ’and’ ’hungary’ ’would’ ’suffer’ ’environmental’ ’damage’ ’if’ ’the’ ’<unk>’ ’<unk>’ ’were’ ’built’ ’as’ ’planned’ ’in’ ’hartford’ ’conn.’ ’the’ ’charter’ ’oak’ ’bridge’ ’will’ ’soon’ figure right ’be’ ’replaced’ ’the’ ’<unk>’ ’<unk>’ ’from’ ’its’ ’<unk>’ ’<unk>’ ’to’ ’park’ ’<unk>’ ’are’ ’possible’ ’citizens’ ’in’ ’peninsula’ ’ohio’ ’upset’ ’over’ ’changes’ ’to’ ’bridge’ ’negotiated’ ’deal’ ’the’ ’bottom’ ’half’ ’of’ ’the’ ’<unk>’ ’will’ ’be’ ’type’ ’while’ ’the’ ’top’ ’half’ ’will’ ’have’ ’the’ ’old’ ’bridge’ \"’s\" ’<unk>’ ’pattern’ ’similarly’ ’highway’ ’engineers’ ’agreed’ ’to’ ’keep’ ’the’ ’old’ ’<unk>’ ’on’ ’the’ ’key’ ’bridge’ ’in’ ’washington’ ’d.c.’ ’as’ ’long’ ’as’ ’they’ ’could’ ’install’ ’crash’ ’barrier’ ’between’ ’the’ ’sidewalk’ ’and’ ’the’ ’road’ ’<unk>’ ’<unk>’ ’drink’ ’carrier’ ’competes’ ’with’ ’<unk>’ ’<unk>’ ’<unk>’ ’just’ ’got’ ’easier’ ’or’ ’so’ ’claims’ ’<unk>’ ’corp.’ ’the’ ’maker’ ’of’ ’the’ ’<unk>’ ’the’ ’chicago’ ’company’ \"’s\" ’beverage’ ’carrier’ ’meant’ ’to’ ’replace’ ’<unk>’ ’<unk>’ ’at’ ’<unk>’ ’stands’ ’and’ ’fast-food’ ’outlets’ ’resembles’ ’the’ ’plastic’ ’<unk>’ ’used’ ’on’ ’<unk>’ ’of’ ’beer’ ’only’ ’the’ ’<unk>’ ’hang’ ’from’ ’<unk>’ ’of’ ’<unk>’ ’the’ ’new’ ’carrier’ ’can’ ’<unk>’ ’as’ ’many’ ’as’ ’four’ ’<unk>’ ’at’ ’once’ ’inventor’ ’<unk>’ ’marvin’ ’says’ ’his’ ’design’ ’virtually’ ’<unk>’ ’<unk>’ text refcorp bond fund might point rate house national wall restraint property pension fund sold willing zenith guaranteed million short-term rates maturities around products deposit posted yields slightly greatest likely nice movies various david proves story always well scary friend high strange love lacks even perfect worst movies come gave rock whatever possible kyle different reasons rock still music", "year": 2016}