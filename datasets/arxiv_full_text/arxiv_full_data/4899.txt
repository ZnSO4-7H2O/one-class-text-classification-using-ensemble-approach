{"title": "Variational Intrinsic Control", "tag": ["cs.LG", "cs.AI"], "abstract": "In this paper we introduce a new unsupervised reinforcement learning method for discovering the set of intrinsic options available to an agent. This set is learned by maximizing the number of different states an agent can reliably reach, as measured by the mutual information between the set of options and option termination states. To this end, we instantiate two policy gradient based algorithms, one that creates an explicit embedding space of options and one that represents options implicitly. The algorithms also provide an explicit measure of empowerment in a given state that can be used by an empowerment maximizing agent. The algorithm scales well with function approximation and we demonstrate the applicability of the algorithm on a range of tasks.", "text": "paper introduce unsupervised reinforcement learning method discovering intrinsic options available agent. learned maximizing number different states agent reliably reach measured mutual information options option termination states. instantiate policy gradient based algorithms creates explicit embedding space options represents options implicitly. algorithms also provide explicit measure empowerment given state used empowerment maximizing agent. algorithm scales well function approximation demonstrate applicability algorithm range tasks. paper provide answer question intrinsic options available agent given state options meaningfully affect world. deﬁne options policies termination condition primarily concerned consequences states environment reach upon termination. options available agent independent agent’s intentions things possible agent achieve. purpose work provide algorithm aims discover many intrinsic options using information theoretic learning criterion training procedure. differs traditional approach option learning goal small number options useful particular task limiting oneself working relatively small option spaces makes credit assignment planning long time intervals easier. however argue operating larger space intrinsic options alluded above fact useful even though space vastly larger. first number options still much smaller number action sequences since options distinguished terms ﬁnal states many action sequences reach state. second learn good representational embeddings options similar options close representational space rely power generalization. embedded spaces planner needs choose neighborhood space containing options sufﬁciently similar consequences. idea goal state embeddings along universal value function reaching goals introduced schaul work allowed agent efﬁciently represent control many goals generalize goals. however goals assumed given. paper extends work provides mechanism learning goals preserving embedded nature. least scenarios algorithm useful. classical reinforcement learning case aims maximize externally provided reward explained above. case rather learning options uniformly represent control agent combine extrinsic reward intrinsic control maximization objective biasing learning towards high reward options. second scenario long-term goal agent state maximal available intrinsic options objective empowerment options consists agent knows use. note theoretical options agent possible something unable learn thus maximize empowerment agent needs simultaneously learn control environment well needs discover options available agent fact states control according current abilities states expects achieve control learning. able learn available options thus fundamental becoming empowered. compare commonly used intrinsic motivation objective maximizing amount model-learning progress measured difference compression experience learning empowerment objective differs fundamental manner primary goal understand predict observations control environment. important point agents often control environment perfectly well without much understanding exempliﬁed canonical model-free reinforcement learning algorithms agents model action-conditioned expected returns. focusing understanding might signiﬁcantly distract impair agent reducing control achieves. algorithm viewed learning represent intrinsic control space agent. developing space seen acquiring universal knowledge useful accomplishing multitude different tasks maximizing extrinsic intrinsic reward overview useful references). analogous unsupervised learning data processing goal representations data useful tasks. crucial difference here however rather simply ﬁnding representations learn explicit policies agent choose follow. additionally algorithm explicitly estimates amount control different states intuitively total number reliably reachable states used empowerment maximizing agent. common criterion unsupervised learning data likelihood. given data various algorithms compared based measure. commonly established measure exists comparison unsupervised learning performance agents. primary difﬁculties unsupervised learning data known control agent exists environment needs order discover states dynamics contains. nevertheless able compare agents terms amount intrinsic control empowerment achieve different states. like multiple methods objectives unsupervised learning devise multiple methods objectives unsupervised control. data likelihood empowerment information measures likelihood measures amount information needed describe data empowerment measures mutual information action choices ﬁnal states. therefore suggest maximum likelihood unsupervised learning mutual information options ﬁnal states unsupervised control. information measure introduced empowerment literature along methods measuring arimoto recently mohamed rezende proposed algorithm utilize function approximation deep learning techniques operate high-dimensional environments. however algorithm considers mutual information sequences actions ﬁnal states. corresponds maximizing empowerment open loop options agent priori decides sequence actions advance follows regardless environment dynamics. obviously often limits performance severely agent cannot properly react environment tends lead signiﬁcant underestimation empowerment. paper provide perspective measure instantiate novel algorithms closed loop options actions conditioned state. show number tasks signiﬁcantly increase intrinsic control improve estimation empowerment. paper structured follows. first formally introduce notion intrinsic control derivation mutual information principle. that describe algorithm intrinsic control explicit options argue viability experimental section. last touch intrinsic control implicit options demonstrate scales even better. conclude short discussion merits approach possible extensions. section explain represent intrinsic options corresponding objective optimize. deﬁne option element space associated policy chooses action state following policy special termination action terminates option yields ﬁnal state consider following example spaces takes ﬁnite number values simplest case separate policy followed. binary vector length captures combinatorial number possibilities. d-dimensional real vector. space options inﬁnite. expected policies nearby similar practice. need express knowledge regions option space consider. imagine start state follow option environments policies typically stochastic might terminate different ﬁnal states different times. policy thus deﬁnes probability distribution consider different options. lead similar states inherently intrinsically seen different another. express knowledge regarding effective intrinsic option given state? help answer question consider example discrete option case three options assume always leads state always lead state would like really intrinsic options sample options order maximize behavior diversity would half time choose half time relative choice frequencies matter example. express choices probability distribution call controllability distribution. intuitively maximize intrinsic control choose maximize diversity ﬁnal states while given controlling precisely possible ensuing ﬁnal states are. latter given expressed negative probability needs averaged subtracting quantities yields objective wish optimize mutual information options ﬁnal states probability distribution mutual information symmetric second line contains reverse expression. expression intuitive interpretation associated able tell options apart infer ﬁnal states. options upon reaching state infer option executed rather reaching state infer option rather said intrinsically different options. would like maximize options achieve large entropy time wish make sure options achieve intrinsically different goals inferred ﬁnal states. entails maximizing average second term advantage formulation absence term formulation difﬁcult obtain would integrate rewriting derivation however term introduced arrived using bayes’ rule. quantity inherent environment obtaining bayes’ reverse difﬁcult. however interpretation prediction ﬁnal state would fortuitous could train separate function approximator infer quantity. fortunately exactly variational bound provides section provide simple algorithm maximize variational bound introduced above. throughout assume distributions policies possible functions parameterized using recent function approximation techniques neural networks state representations formed observations using recurrent neural networks. however calculate mutual information options ﬁnal observations instead ﬁnal states leave latter future work. algorithm provides outline basic training loop. sample follow policy till termination state regress towards calculate intrinsic reward reinforcement learning algorithm update maximize reinforce option prior based algorithm derived appendix note appears determining distribution terminal states give intuitive explanation algorithm. state agent tries option available options goal choose actions lead state inferred well possible using option inference function infer option well means options don’t lead state often therefore option intrinsically different others. goal expressed intrinsic reward agent reinforcement learning algorithm policy gradients q-learning train policy maximize reward. ﬁnal state updates option inference function towards actual chosen also reinforces prior based reward reward high choose option often. note also keep prior ﬁxed example uniform gaussian distribution. then different values result different behavior learning. intrinsic reward equals average logarithm number different options agent given state empowerment state. follows deﬁnition mutual information expression take sample however also provide intuitive explanation. essentially negative logarithm number different choose however different things. region large deﬁnes region similar options. empowerment essentially equals number regions total region given taking logarithm ratio total number options number options within region gives q/pc train using policy gradients training estimate baseline lower variance weight updates baseline tracks expected return demonstrate behavior algorithm simple example two-dimensional grid world. agent lives grid actions move down right left stay put. environment noisy following manner agent takes step probability agent pushed random direction. follow algorithm choose discrete space options. option prior uniform goal therefore learn policy would make options different states possible. measured function which state reached tries infer option followed. episode intrinsic reward particular option inferred correctly conﬁdence close zero negative reward large wrong however negative reward small. choosing options random order large reward average different options need reach substantially different states order able infer chosen option. grid world plot locations given option inferred locations option navigates. shown figure rectangle corresponding different option intensity denoting probability predicting given option. thus indeed different options learn navigate different localized places environment. example q-learning learn policy. general express function different options running corresponding input states neural network outputting nactions values option action. update options time efﬁciently triplet experience st+. experiment linear function approximator terminate options ﬁxed probability could also universal value function approximation continuous option spaces still calculate passing input neural network update result several randomly sampled options time. option space option embedding itself. figure learning navigate grid world. left standard grid world valid actions indicated green arrows green disc right square corresponds different option shows probability predicting option different locations environment. negative logarithm values proportional intrinsic reward given option trying maximize therefore locations large intensity show locations given option terminates. second environment also grid world special properties. consists parts narrow corridor connected open square blue denoting walls. however square simple grid world somewhat dangerous. types cells arranged checkerboard lattice sub-lattice left right actions actually move agent adjacent states sub-lattice actions agent picks action these falls state figure learning navigate grid world trap states. left dangerous grid world cells different checkerboard sub-lattices different sets valid actions taking wrong actions teleports agent top-left corner indicated cross small probability escaping time step. blue region represents barrier forming narrow corridor left. furthermore valid action taken probability agent actually move. therefore agent commits sequence actions quickly lose track sub-lattice fall empowerment left corner state. hand observes environment always take actions safely navigate square. bottom classical empowerment considers effect action sequences small empowerment inside square sequence respond changes environment noise agent instead prefers ‘safe’ left corridor. demonstrated bottom ﬁgure shows exact empowerment different locations action sequence lengths square area empowerment amount empowerment goes increasing sequence length. closed loop policies hand agent learned navigate different locations square shown right ﬁgure. stuck long time. furthermore move adjacent state happens probability. this agent doesn’t observe environment quickly loses information lattice thus inevitably falls empowerment state. show computed exact empowerment values location open-loop policy using blahut-arimoto algorithm horizons result shown ﬁgure hand agent observes environment knows sub-lattice always choose action doesn’t fall. thus safely navigate square. experiments agent indeed accomplishes this learns options inside square classical empowerment maximizes mutual information sequences actions ﬁnal states. maximizes objective function corresponds maximizing empowerment space open loop options. options agent ﬁrst commits sequence actions blindly follows sequence regardless environment does. contrast closed-loop option every action conditioned current state. show using open-loop options lead severe underestimation empowerment stochastic environments resulting agents reach low-empowerment states. demonstrate effect ’dangerous grid world’ environment section using open loop options length agent center environment would exponentially growing probability reset function option length resulting estimation empowerment quickly decreases option length highest value inside corridor top-left corner shown figure consequence agent would prefer inside corridor top-left corner away center grid world. great contrast open loop case using closed loop options empowerment grow quadratically option length resulting agents prefer staying center grid world. example might seem contrived actually quite ubiquitous real world. example navigate around city whether walking driving quite safely. instead committed sequence actions ahead would almost certainly driving crash another car. importance closed loop nature policies indeed well understood. demonstrated open loop policies even measure empowerment. advantages algorithm relatively simple uses closed loop policies general function approximation naturally formulated combinatorial options spaces discrete continuous model-free. primary problem found algorithm difﬁcult make work practice function approximation. suggest might reasons this. first intrinsic reward noisy changing agent learns. makes difﬁcult policy learn. algorithm worked speciﬁed simple environments used linear function approximation small ﬁnite number options. however failed neural networks substituted. still succeeded ﬁxing intrinsic reward period time learning policy vice versa. however replacing small option space continuous made training even difﬁcult runs succeeded. problems related deep reinforcement learning order make learning work well function approximation needs store large number experiences memory replay them. possible work direction would good practices training algorithm general function distribution approximations. second problem exploration. agent encounters state like there might correspond option hasn’t considered therefore increase control. however gets there option inference function learned yet. likely inferring incorrect option therefore giving reward therefore discouraging agent going there. overall objective maximized agent control algorithm difﬁculty maximizing objective functions intrinsic reward policy match good expressing options region familiar with seems fail push state regions. hence introduce algorithm formulation section address issues. address learning difﬁculties algorithm action space option space. gives inference function grounded targets makes easier train. sensible makes policy easier train. elements algorithm follows. controllability prior policy algorithm simply become policy denote internal state calculated implementation state recurrent network. function algorithm infer action choices made knowing ﬁnal observation thus becomes internal state calculated logarithm number action choices effectively different distinguished based observation ﬁnal state given introduce algorithm maximize expected cumulative number distinct actions maximizing intrinsic return setting maximization control substantially simpliﬁed. consider experience generated policy. learning supervised learning problem inferring action choices even random action policy terminates different states thus able train experiences mimicking decisions happen lead thought choosing among lead diverse states experiments follow following functions policies every input passed fully connected layer neural network standard rectiﬁer non-linearity resulting embedding passed lstm recurrent outputs policy probabilities actions. concatenate embedding pass another layer neural obtain state this together hidden state lstm passed another lstm network outputs probabilities actions. facts worth highlighting here. first algorithm applies general partially observable environments since policies build internal states. however generally ﬁnal states environment instead observations states agent reach. leave aspect future work. second policy thought implicit option. however embedding ﬁnal observation thought explicit option policy policy implementing option. experiments test algorithm several environments. ﬁrst grid world size four rooms random action policy length leads ﬁnal distribution width states whose distance initial state distributed approximately according gaussian within room. order environment size agent rarely crosses different room narrow doors rooms. figure shows trajectories learned algorithm indeed extended spanning large parts environment. furthermore many trajectories cross different rooms passing straight narrow doors without difﬁculty. interesting policy conditioned ﬁnal state policy actually followed given notion ﬁnal state explicitly. implicitly learns navigate doors different parts environment. maximize intrinsic control distribution ﬁnal points reaches uniform among points reachable given state. tell every point point equally well. figure shows distribution points reached algorithm trajectories length starting different points environment. example left square shows point distribution starting left corner. distribution indeed roughly uniform among reachable points. average empowerment learning reaches nats corresponds different reachable states. second experiment three dimensional simulated environment given time agent sees particular view environment color image. ﬁgure shows example trajectories agent follows using policy trajectories seem intuitively much ‘consistent’ random action policy would produce. average empowerment achieved trajectories length nats corresponds reaching different states. figure intrinsic control implicit options. left center environment four room grid world. left blue denotes beginning trajectory green end. algorithm learns trajectories extend environment accurately pass doors rooms. center green shows distribution points trajectories length trajectory given square starts location denotes coordinates room picture. points cover reachable points nearly uniformly. right column shows trajectory three dimensional simulated environment. agent observes images. again obtain non-random trajectories rotate move large amount. average amount intrinsic control achieved nats corresponding different states. third environment grid world contains blocks agent push. blocks cannot pass boundary agent cannot push blocks time. case visual space small combinatorially many possibilities. figure shows typical trajectory. agent pushes ﬁrst block down goes around second block pushes goes third pushes down arrives ﬁnal position. average empowerment nats corresponds able reach different states. figure pushing blocks. left agent lives grid world push blocks around. image shows typical trajectory agent pushes blocks various locations pushes ﬁrst block down goes around second pushes goes third pushing down moves ﬁnal position. right empowerment function training time world trajectories length actions blocks. beginning training agent ﬁrst learns different locations ‘realizes’ control blocks. average empowerment three blocks reached nats corresponds able reach states. prevalent feature real world many elements beyond control falling leaves trafﬁc. important features algorithms intrinsic options represent things agent actually control model complexities real world algorithms model-free. demonstrate property introduce environments elements beyond agent’s control. ﬁrst environment four room grid world used above distractors moving around random live different input feature planes. distractors affect agent agent observe them. agent needs learn ignore them. agent reaches amount empowerment without distractors second environment consists pairs mnist digits forming image. actions affect classes digits. ﬁrst action doesn’t change classes next increase/decrease class ﬁrst digit next increase/decrease class second digit class chosen random digit class selected. thus environment visually complex small control space. example trajectories followed agent shown figure left. empowerment obtained agent policies length actions corresponds states. controllable states environment agent achieves nearly maximum possible intrinsic control. figure left environment consists possible pairs mnist digits. agent control classes digits able increase decrease class ﬁrst second digit speciﬁc instance digit sampled random. digit column ﬁgure shows trajectory. sample trajectory respective actions shown left inset. agent learns navigate space achieving nearly maximum possible control different states. note agent digit class learn tell different classes apart control. right extrinsic reward function training time pure reinforce different amounts unsupervised interaction environment. environment four room grid world reward placed location next compare agents utilizing open closed loop options. grid world environment noise follows. agent takes step environment pushes agent random direction. closed loop policy correct environment noise example following strategy always going towards goal open loop policy agent less less information environment time goes cannot navigate reliably towards actual target location. implement open loop agent using algorithm feed starting states recurrent network. table shows comparison. closed loop agent indeed performs much better. table open closed loop options. agents follow open close loop options grid world noise show empowerment exponential average number nats approximately denotes many different states agent reach. finally primary focus paper unsupervised control provide proof-of-principle experiment shows learned policies help learning extrinsic reward. consider following situation. agent placed environment without told objective agent opportunity explore learn control environment. amount time agent told objective extrinsic reward limited amount time collect much reward possible. could number ways learned policies maximize extrinsic reward. case simply combine intrinsic extrinsic rewards reinforce policy algorithm large constant four room grid world different periods time place reward location figure shows reward collected episode different amounts unsupervised pre-training using pure reinforce without maximum control objective that indeed agent learns collect reward signiﬁcantly faster opportunity interact environment. paper introduce formalism intrinsic control maximization unsupervised option learning. presented algorithms framework analyzed diverse range experiments. demonstrated importance closed loop policies estimating empowerment. additionally also demonstrated usefulness unsupervised learning intrinsic control extrinsic reward maximization. alexander klyubin daniel polani chrystopher nehaniv. empowerment universal agentcentric measure control. ieee congress evolutionary computation volume ieee tejas kulkarni karthik narasimhan ardavan saeedi joshua tenenbaum. hierarchical deep reinforcement learning integrating temporal abstraction intrinsic motivation. arxiv preprint arxiv. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature shakir mohamed danilo jimenez rezende. variational information maximisation intrinsically motivated reinforcement learning. advances neural information processing systems schaul daniel horgan karol gregor david silver. universal value function approximators. proceedings international conference machine learning alexander vezhnevets volodymyr mnih john agapiou simon osindero alex graves oriol vinyals koray kavukcuoglu. strategic attentive writer learning macro-actions. arxiv preprint arxiv. taking gradient respect straightforward. obtain sample sample following policy till termination. directly take gradient gradient standard likelihood maximization. standard policy gradient update sample pretend training generative model observed maximum likepc lihood multiply resulting gradient corrected reward baseline aims predict trained regressing values towards observed thus given episode obtain larger expected reward likelihood generating increased vice versa. particular case sampled empowerment state tends towards expected empowerment state. finally would like update policy. keeping ﬁxed would like know change policy increase given sampled experience value becomes reinforcement learning algorithm maximize reward. show functional forms used algorithm policies sequence observations actions. assume computed embedding ﬁnal observation computations time step following figure empowerment grid world distractors grid world without different sizes sequence lengths. distractors move around environment agents sees interact them. curve shows agent learns completely ignore focus controllable part environment. figure displays graph showing agent learns ignore distractors affect intrinsic control. environment grid world points move different feature plane affect agent except visual distractor. tested several environment sizes option lengths.", "year": 2016}