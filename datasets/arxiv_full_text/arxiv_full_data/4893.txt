{"title": "The Sum-Product Theorem: A Foundation for Learning Tractable Models", "tag": ["cs.LG", "cs.AI"], "abstract": "Inference in expressive probabilistic models is generally intractable, which makes them difficult to learn and limits their applicability. Sum-product networks are a class of deep models where, surprisingly, inference remains tractable even when an arbitrary number of hidden layers are present. In this paper, we generalize this result to a much broader set of learning problems: all those where inference consists of summing a function over a semiring. This includes satisfiability, constraint satisfaction, optimization, integration, and others. In any semiring, for summation to be tractable it suffices that the factors of every product have disjoint scopes. This unifies and extends many previous results in the literature. Enforcing this condition at learning time thus ensures that the learned models are tractable. We illustrate the power and generality of this approach by applying it to a new type of structured prediction problem: learning a nonconvex function that can be globally optimized in polynomial time. We show empirically that this greatly outperforms the standard approach of learning without regard to the cost of optimization.", "text": "inference expressive probabilistic models generally intractable makes difﬁcult learn limits applicability. sumproduct networks class deep models where surprisingly inference remains tractable even arbitrary number hidden layers present. paper generalize result much broader learning problems inference consists summing function semiring. includes satisﬁability constraint satisfaction optimization integration others. semiring summation tractable sufﬁces factors every product disjoint scopes. uniﬁes extends many previous results literature. enforcing condition learning time thus ensures learned models tractable. illustrate power generality approach applying type structured prediction problem learning nonconvex function globally optimized polynomial time. show empirically greatly outperforms standard approach learning withregard cost optimization. introduction graphical models compact representation often used target learning probabilistic models. unfortunately inference exponential treewidth common measure complexity. further since inference subroutine learning graphical models hard learn unless restricted treewidth real-world problems exhibit property. recent research however shown probabilistic models fact much expressive remaining tractable particular sum-product networks poon domingos class deep probabilistic models consist many layers hidden variables unbounded treewidth. despite this inference spns guaranteed tractable structure parameters effectively accurately learned data paper generalize extend ideas behind spns enable learning tractable high-treewidth representations much wider class problems including satisﬁability max-sat model counting constraint satisfaction marginal inference integration nonconvex optimization database querying ﬁrst-order probabilistic inference. class problems address viewed generalizing structured prediction beyond combinatorial optimization include optimization continuous models others. instead approaching domain individually build long line work showing despite apparent differences problems fact much common structure dechter mceliece wilson green dechter mateescu bacchus namely consists summing function semiring. example boolean semiring product operations disjunction conjunction deciding satisﬁability summing boolean formula truth assignments. inference summation states max-product semiring etc. begin identifying proving sum-product theorem unifying principle tractable inference states simple sufﬁcient condition summation tractable semiring factors every product disjoint scopes. representations like graphical models conjunctive normal form consisting single product sums would allow trivial models; deep representations like spns negation normal form provides remarkable ﬂexibility. based sum-product theorem develop algorithm learning representations satisfy condition thus guaranteeing learned functions tractable expressive. demonstrate power generality approach applying type structured prediction problem learning nonconvex function optimized polynomial time. empirically show greatly outperforms standard approach learning continuous function without regard cost optimizing also show number existing novel results corollaries sum-product theorem propose general algorithm inference semiring deﬁne novel tractable classes constraint satisfaction problems integrable optimizable functions database queries present much simpler proof tractability tractable markov logic. sum-product theorem begin introducing notation deﬁning several important concepts. denote vector variables value domain denote subsets variables domains xaxa cartesian product domains variables respectively. denote assignments restrictions indicate compatibility write scope function variables takes input. deﬁnition commutative semiring nonempty operations product deﬁned satisfy following conditions associative commutative identity elements distributes absorbing commutative semiring function semiring ﬁnite refer function sum-product function. deﬁnition sum-product function semiring variables constant univariate functions following function product spfs spfs. computes mapping represented rooted directed acyclic graph leaf node labeled function non-leaf node labeled either referred product node respectively. spfs compatible compute mapping; i.e. spfs. size number edges graph. rooted node represents sub-spf notice restricting leaf functions univariate incurs loss generality mapping compatible indicator function value argument true otherwise spfs similar arithmetic circuits leaves functions instead variables. darwiche used arithmetic circuits data structure support inference bayesian networks discrete variables. important subclass spfs decomposable. deﬁnition product node decomposable scopes children disjoint. decomposable product nodes decomposable. decomposability simple condition deﬁnes class functions inference tractable. theorem every decomposable summed time linear size. proof. proof recursive starting leaves spf. decomposable commutative semiring every leaf node summed constant time labeled either constant univariate function. node summation. {ci} children sub-spfs summations xv\\i domain variables xv\\xi. children disjoint scopes equations require associativity commutativity associativity distributivity properties semiring. thus node summed domain time linear number children summed time linear size. computed constant time leaf function evaluated constant time true semirings considered. also assume take constant time elements semiring true common semirings. appendix details. complexity summation related notions complexity treewidth common relevant complexity measure across domains consider. deﬁne treewidth ﬁrst deﬁne junction trees related class spfs. deﬁnition junction tree variables tuple rooted tree subsets variables vertex contains subset variables ∪ici every pair vertices path separator edge deﬁned junction tree provides schematic constructing speciﬁc type decomposable called tree-like note tree-like tree however many nodes multiple parents. deﬁnition tree-like variables constructed junction tree functions {ψi} contains following nodes node indicator value variable node value product node value cluster node value xsij separator single root node product node node compatible corresponding values compatible; i.e. sij. nodes connected follows. children root product nodes root children product node compatible nodes child constant node value indicator nodes node closer root children node compatible product nodes child connected separator sij. tree-like junction tree difﬁcult decomposable since indicators variable appear level node computes ssjk indicator children combined children thus tree-like spfs provide method decomposing spf. tree-like compatible cannot assert independencies hold deﬁnition variables pairwise-disjoint subsets conditionally independent given similarly junction tree incompatible asserts independencies variables conditionally independent given separates variables separates removing vertices pair vertices connected. inference complexity commonly parameterized treewidth deﬁned junction tree size largest cluster minus one; i.e. maxi∈t |ci| treewidth minimum treewidth junction trees compatible notice deﬁnitions junction tree treewidth reduce standard ones treewidth bounded inference efﬁcient must exist compatible tree-like bounded treewidth. note trivial junction tree single cluster compatible every spf. corollary every bounded treewidth summed time linear cardinality scope. space limitations proofs provided appendix tree-like spfs type compatible size exponential treewidth; however many compatible spfs. fact compatible spfs exponentially smaller compatible tree-like spf. corollary every summed time linear cardinality scope bounded treewidth. given existing work tractable high-treewidth inference perhaps surprising results exist literature level generality. relevant preliminary work kimmig proposes semiring generalization arithmetic circuits knowledge compilation address learning. main results show summation circuits decomposable either deterministic based idempotent takes time linear size whereas show decomposability alone sufﬁcient much weaker condition. fact variables deterministic circuits exponentially larger never smaller non-deterministic circuits note decomposable circuits made deterministic introducing hidden variables imply properties equivalent. even restricted speciﬁc semirings logical probabilistic inference poon domingos results previously shown formally although foreshadowed informally. further existing semiring-speciﬁc results make clear semiring properties required tractable high-treewidth inference. results thus simpler general. further sumproduct theorem provides basis general algorithms inference arbitrary spfs learn. inference non-decomposable spfs inference arbitrary spfs performed variety ways efﬁcient others. present algorithm summing adapts structure thus take exponentially less time constructing summing compatible tree-like imposes uniform decomposition structure. root node summed calling sumspf pseudocode shown algorithm sumspf simple recursive algorithm summing decomposable sumspf simply recurses bottom sums leaf functions evaluates upward pass. decomposable sumspf decomposes product node encounters summing algorithm spf. input node root sub-spf decomposition achieved many different ways base method common algorithmic pattern already occurs many inference problems consider resulting general semiring-independent algorithm summing spf. decompose shown algorithm chooses variable appears scope multiple children; creates |xt| partially assigned simpliﬁed copies sub-spf assigned value multiplies indicator ensure ever non-zero evaluated; replaces {svi}. node scope re-used across drastically limit amount duplication occurs. furthermore simpliﬁed removing nodes became setting variables chosen heuristically; good heuristic minimizes amount duplication occurs. similarly sumspf heuristically orders children lines good ordering ﬁrst evaluate children return absorbing value sumspf break lines occurs. general decomposing hard resulting decomposed exponentially larger input although good heuristics often avoid this. many extensions sumspf also possible detail later sections. understanding inference non-decomposable spfs important future work extending learning even challenging classes functions particularly without obvious decomposability structure. learning tractable representations instead performing inference intractable model often simpler learn tractable representation directly data gens domingos general problem consider learning decomposable semiring i.i.d. instances drawn ﬁxed distribution dx×r certain problems constraint satisfaction inference desired quantity argument sum. recovered single downward pass recursively selects children product node active child node learning domain corresponds generalization learning structured prediction formally problem learn instances arg⊕y∈y arg⊕y∈y here arbitrarily structured input inference variables learning problems solved algorithm schema present minor differences subroutines. focus former discuss latter below alongside experiments learning nonconvex functions that construction efﬁciently optimized. shown sum-product theorem tractable inference identify decomposability structure spf. difﬁculty however general structure varies throughout space. example protein folds exist conformations protein particular amino acids energetically independent conformations amino acids directly interact amino acids longer interact. suggests simple algorithm call learnspf ﬁrst tries identify decomposable partition variables successful recurses subset variables order ﬁner-grained decomposability. otherwise learnspf clusters training instances grouping analogous decomposition structure recurses cluster. either variables small enough summed number instances small contain meaningful statistical information learnspf simply estimates current instances. learnspf generalization learnspn simple effective structure learning algorithm. learnspf actually algorithm schema instantiated different variable partitioning clustering leaf creation subroutines different semirings problems. successfully decompose variables learnspf must partition refer approximate decomposability. probabilistic inference mutual information pairwise independence tests used determine decomposability experiments decomposable partitions correspond connected components graph variables correlated variables connected. instances clustered virtually clustering algorithm naive bayes mixture model k-means experiments. instances also split conditioning speciﬁc values variables sumspf decision tree. similarly leaf functions estimated using appropriate learning algorithm linear regression kernel density estimation. section present preliminary experiments learning nonconvex functions globally optimized polynomial time. however particular application learnspf used learn tractable representation problem consists summation semiring. following section brieﬂy discuss common inference problems correspond summing speciﬁc semiring. each demonstrate beneﬁt sum-product theorem relate core algorithms sumspf specify problem solved learnspf. additional details semirings found appendix. table provides summary relevant inference problems. applications speciﬁc semirings logical inference consider boolean semiring logical disjunction logical conjunction variable boolean leaf functions literals logical negation) spfs correspond exactly negation normal form dag-based representation propositional formula exponentially smaller sentence standard representation conjunctive disjunctive normal form never larger summation propositional satisﬁability problem determining exists satisfying assignment thus tractability decomposable nnfs follows directly sum-product theorem. corollary satisﬁability decomposable decidable time linear size. satisﬁability arbitrary determined either decomposing expanding using solver. dpll stantable inference problems correspond summing speciﬁc semiring details variables leaf functions core algorithm instance sumspf. denote natural real numbers. subscript denotes restriction non-negative numbers subscript denotes inclusion denotes universe relations arity denotes polynomials coefﬁcients appendix information mpe-sat generic-join dard algorithm solving instance sumspf speciﬁcally dpll recursive algorithm level chooses variable computes f|x=∨f|x= recursing disjunct f|x=x assigned value thus level recursion dpll corresponds call decompose. learning boolean semiring well-studied area includes problems learning boolean circuits learning sets rules however learned rule sets typically encoded large knowledge bases making reasoning intractable. contrast decomposable tractable expressive formalism knowledge representation supports rich class polynomial-time logical operations including thus learnspf semiring provides method learning large complex knowledge bases encoded decomposable therefore support efﬁcient querying could greatly beneﬁt existing rule learning systems. constraint satisfaction. constraint satisfaction problem consists constraints {ci} variables constraint assignsolving consists ments variables. ﬁnding assignment satisﬁes straint. constraints functions sets tuples unary tuples discrete boolean semiring i.e. or-and network generalization decomposable decomposable oan. summation chang mackworth rollon solution recovered downward pass recursively selects non-zero child node children node. corollary follows immediately. corollary every decomposable solved time linear size. thus inference efﬁcient sufﬁces expressible tractably-sized decomposable oan; much weaker condition treewidth. like dpll backtracking-based search algorithms csps also instances sumspf further spfs number semirings correspond various extensions csps including fuzzy probabilistic weighted csps learnspf csps addresses variant structured prediction speciﬁcally learning training data structured object representing solution. learnspf solves problem guaranteeing learned remains tractable. much simpler attractive approach existing constraint learning methods lallouet uses inductive logic programming tractability guarantees. probabilistic inference many probability distributions compactly repreψi sented graphical models potential variables known partition function main inference problems graphical models compute probability evidence variables x\\xe. partition function unnormalized probability empty evidence unfortunately computing generally intractable. building number earlier works poon domingos introduced sum-product networks class distributions inference guaranteed tractable. non-negative real sum-product semiring graphical model unnormalized probability evidence variables computed replacing leaf function s|xj constant summing spn. corollary follows immediately sum-product theorem. corollary probability evidence decomposable computed time linear size. similar result ﬁnding probable state non-evidence variables also follows sum-product theorem. important consequence sum-product theorem decomposability sole condition required tractable; previously completeness also required expands range tractable spns simpliﬁes design tractable representations based them tractable probabilistic knowledge bases existing algorithms inference graphical models correspond different methods decomposing loosely clustered tree-based conditioning compilation methods sumspf generalizes. details provided appendix learnspf spns corresponds learning probability distribution samples y)}. note case deﬁned implicitly empirical frequency dataset. learning parameters structure spns fast-growing area research rooshenas lowd peharz adel refer readers references details. integration optimization spfs generalized continuous domains variable domain semiring subset sum-product theorem hold additional conditions computable constant time leaf nodes children xv\\c domain xv\\c xv\\xc. integration. non-negative real sum-product semiring summation continuous variables corresponds integration accordingly generalize spfs follows. measures respectively integrable releaf function spect satisﬁes summation corresponds computing dµn. dµv\\c must integrable nodes children dµv\\c ﬁnite support xv\\c xv\\c corollary follows immediately. corollary every decomposable real variables integrated time linear size. thus decomposable spfs deﬁne class functions exact integration tractable. sumspf deﬁnes novel algorithm integration based recursive problem decomposition exponentially efﬁcient standard integration algorithms trapezoidal monte carlo methods dynamically decomposes problem recursion level caches intermediate computations. detail provided appendix semiring learnspf learns decomposable continuous samples integrated efﬁciently domain thus learnspf provides novel method learning integrating complex functions partition function continuous probability distributions. nonconvex optimization. summing continuous min-sum min-product max-sum maxproduct semirings corresponds optimizing continuous objective function. results hold these focus real minsum semiring summation min-sum function corresponds computing minx simply terms. satisfy assume minxj∈xj computable constant time trivially satisﬁed min. corollary follows immediately. corollary global minimum decomposable found time linear size. sumspf provides outline general nonconvex optimization algorithm sum-of-terms functions. recent rdis algorithm nonconvex optimization achieves exponential speedups compared algorithms instance sumspf values chosen multi-start gradient descent variables decompose chosen graph partitioning. friesen domingos however specify tractability conditions optimization; thus corollary deﬁnes novel class functions efﬁciently globally optimized. nonconvex optimization learnspf solves variant structured prediction variables predict continuous instead discrete training data structured object representing nonconvex function vector values specifying global minimum function. learnspf learns function miny∈y computed efﬁciently decomposable. detail provided section experiments evaluated learnspf task learning nonconvex decomposable min-sum function training solutions instances highly-multimodal test function consisting terms. learning instead terms learn general mathematical form optimization problem resulting learned problem tractable whereas original terms not. test function learn variant rastrigin function standard highly-multimodal test function global optimization consisting multidimensional sinusoids quadratic basins. function parameters determine dependencies variables location minima. test learnspf sampled dataset function instances y)}m distribution miny∈y learnspf partitioned variables based connected components graph containing node edge nodes correlated measured spearman rank correlation. instances clustered running k-means values preliminary test learnspf learn leaf functions learned min-sum function instead evaluating minimizing leaf node evaluated minimized test function variables scope leaf node ﬁxed corresponds perfectly learned leaf nodes scopes leaf nodes accurately reﬂect decomposability otherwise large error incurred. study effect learning decomposability structure isolation error learning leaf nodes. function used comparison also perfectly learned. thresholds respectively. dataset split training samples test samples miny comparison purposes. training computed miny function test ﬁrst minimizing leaf function multi-start l-bfgs performing upward downward pass figure shows result minimizing learned evaluating test function compared running multistart l-bfgs directly test function reporting minimum found optimizations ﬁxed amount time learnspf accurately learned decomposition structure test function allowing much better minima optimized since optimizing many small functions leaves requires exploring exponentially fewer modes optimizing full function. additional experimental details provided appendix conclusion paper developed novel foundation learning tractable representations semiring based sumproduct theorem simple tractability condition inference problems reduce summation semiring. developed general inference algorithm algorithm learning tractable representations semiring. demonstrated power generality approach applying learning nonconvex function optimized polynomial time type structured prediction problem. showed empirically learned function greatly outperforms continuous function learned without regard cost optimizing also showed sum-product theorem speciﬁes exponentially weaker condition tractability treewidth corollaries include many previous results literature well number novel results. acknowledgments research partly funded grants n--- afrl contract fa---. views conclusions contained document authors interpreted necessarily representing ofﬁcial policies either expressed implied afrl united states government. bacchus dalmao pitassi value elimination bayesian inference backtracking search. proceedings annual conference uncertainty artiﬁcial intelligence chang mackworth generalization generalized consistency constraint satisfaction constraint-based inference. proceedings ijcai workshop modeling solving problems constraints friesen domingos recursive decomposition nonconvex optimization. yang woolridge proceedings international joint conference artiﬁcial intelligence aaai press sang bacchus beame kautz pitassi combining component caching clause learning effective model counting. proceedings international conference theory applications satisﬁability testing taskar chatalbashev koller guestrin learning structured prediction models large margin approach. proceedings international conference machine learning decomposable summation complexity decomposable size commutative semiring |xi| cost elements further denote complexity evaluating unary leaf function maxv∈ssumj∈ch |xv\\xj| ssum sprod sleaf product leaf nodes respectively children |sleaf| |ssum| certain simple spfs little internal structure many input variables worst case complexity summing quadratic occurs rare restrictive case term node however semiring idempotent minsum max-product semirings term always equal thus computation necessary. alternatively semiring supports multiplication division sum-product semiring complexity reduced ﬁrst computing product variables dividing needed. semiring neither properties identity summations still computed single preprocessing pass since constants independent input variables. semirings we’ve studied quadratic cost occur include completeness. logical inference model counting. model counting problem computing number satisfying assignments boolean formula. model count obtained translating boolean semiring counting sum-product semiring summing deﬁnition semiring semiring involves replacing node node node node leaf function returns returns respectively. however simply summing translated function compute incorrect model count satisfying assignment counted multiple times; occurs idempotence semirings differs i.e. either semiring idempotent vice versa. exactly semirings idempotent must deterministic ensure summing gives correct model count. deﬁnition node deterministic supports children disjoint. deterministic nodes deterministic. support function points deterministic decomposable follows sum-product theorem model count computed efﬁciently. corollary model count deterministic decomposable computed time linear size. algorithms cachet dpll also instances sumspf since extend dpll level recursion decomposing independent components solving separately caching model count component. component decomposition corresponds decomposable product node sumspf component caching corresponds connecting sub-spf multiple parents. notice nodes created decompose deterministic. max-sat. max-sat problem computing maximum number satisﬁable clauses assignments. generalized nnfs follows. deﬁnition assignment. number literal true otherwise. node children. node children. max-sat problem computing maximum number root assignments reduces standard max-sat. max-sat solved translating max-sum semiring summing clearly i.e. max-sum network. corollary follows immediately sum-product theorem. corollary max-sat decomposable computed time linear size. max-sat arbitrary computed ﬁrst translating calling sumspf extended perform branch bound traversing spf. allows sumspf prune sub-spfs relevant ﬁnal solution greatly reduce search space. addition dpll-based solvers max-sat references therein) instances sumspf. relevant however mpe-sat algorithm sang since probabilistic inference marginal inference tree-based methods include junction-tree clustering variable elimination correspond constructing junction tree summing corresponding tree-like spn. conditioning algorithms recursive conditioning value elimination and/or search dpll traverse space partial assignments recursively conditioning variables values. algorithms vary ﬂexibility variable ordering decomposition caching comparison) instances sumspf fully-dynamic variable ordering value elimination dpll does ﬁxed ordering variants recursive conditioning and/or search. decomposition caching correspond decomposable product nodes connecting sub-spns multiple parents respectively inference graphical models performed compilation arithmetic circuit discrete domains rooshenas lowd showed spns equivalent spns always smaller equal size. continuous domains however unlikely even relationship exists would require inﬁnite number indicator functions. furthermore existing compilation methods require ﬁrst encoding graphical model restrictive languages make exponentially slower sumspf. finally tractability properties established guarantee compiling inference tractable generalized semirings. mpe. beyond computing probability evidence another probabilistic inference problem ﬁnding probable state non-evidence variables given evidence maxxe evidence x\\xe. value computed translating non-negative max-product semiring maximizing resulting state recovered downward pass recursively selecting highest-valued child node children product node translating model counting must selective correspond children often interest state hidden observed variables. done linear time requires decomposable making explicit multiplying child indicator makes resulting selective. integration optimization integration non-decomposable spfs decompose must altered select ﬁnite number values trapezoidal rule approximate integration. values chosen using grid search lipschitz continuous grid spacing error incurred approximation bounded pre-speciﬁed amount. signiﬁcantly reduce number values explored sumspf combined approximate decomposability since sumspf treat non-decomposable product nodes decomposable avoiding expensive call decompose incurring bounded integration error. relational inference ﬁnite constants complete relation arity i.e. tuples cartesian product times. universe relations arity power relation containing empty tuple. since union distributes join associative commutative semiring relations natural join empty relation given extensional database {ri} containing relations arity referred union-join network query composed union joins unary tuples j=cj leaves unary tuples combined unions joins form full query variables deﬁnes intensional qans corresponds summation maximum number variables involved particular join joins corollary follows immediately since decomposable join cartesian product. corollary qans decomposable computed time linear size note smaller treewidth since composes ﬁnal output relation many small relations relational form determinism. since size depends input relations query statement combined complexity queries deﬁned ujns. regarding expressivity selection implemented join relation contains tuples satisfy selection predicate projection immediately supported since union distributes projection straightforward extend results theorem allow ujns contain projection nodes. ujns projection correspond non-recursive datalog queries decomposable ujns tractable sub-class. thus sumspf deﬁnes recursive algorithm evaluating non-recursive datalog queries genericjoin algorithm recent join algorithm achieves worst-case optimal performance recursing individual tuples instance decompose. relational probabilistic models tractable probabilistic knowledge base class object declarations classes form forest objects form tree subparts given leaf class object. class declaration class speciﬁes subparts parts {pi} subclasses subs {si} attributes atts {ai} relations rels {ri}. subparts parts every object class must speciﬁed name class number unique copies. class subclasses must belong exactly subclasses weights specify distribution subclasses. every attribute domain weight function relation form part relations specify relationships hold among subparts. weight deﬁnes probability relation true. relation also apply object whole instead parts. object declarations introduce evidence specifying object’s subclass memberships attribute values relations well specifying names path object object part decomposition. tpkb objects properties possible world subtree values attributes relations. literals class membership attribute relation atoms negations thus specify subclasses object truth value relation value attribute. single object objects descendants. objects class unnormalized distribution possible subworlds deﬁned recursively relation hard otherwise construction deﬁnes literals. sum-product theorem hand possible greatly simplify two-page proof tractability given niepert domingos show here. prove computing tractable sufﬁces show decomposable decomposed efﬁciently. ﬁrst note four factors decomposable since ﬁrst second product subparts therefore subfunctions disjoint scopes third fourth products attributes relations respectively decomposable none share variables. remains show factors decomposed respect without increasing size spn. denote number object declarations class declarations relation rules respectively. corresponding size since object constant number edges pairs non-decomposable. alternatively pairs non-decomposable. global minimum function sampled uniformly interval length line zero-mean additive gaussian noise thus instance highly nonconvex decomposable respect certain variables respect others. instances structure decomposability variables different instances different decomposability structure learnspf must ﬁrst group function instances similar decomposability structure identify structure order learn min-sum function applicable instance training data. proof. commutative semiring bounded treewidth tree-like compatible junction tree treewidth size largest cluster |xv| further root one-to-one correspondence separator instantiations xsij nodes cluster instantiations product nodes number edges obtained counting edges correspond edge summing edges follows. construction edge corresponds product nodes {ck}; children leaf nodes nodes {sjk}; children {sjk} product nodes {cj}. deﬁnition {cj} single parent |xcj| edges {sjk} {cj}. further |ck| leaf node children |ch| node children |xck||) dα|) edges between {ck} {sjk}. addition also edges root product nodes thus since tree edges size dα|) since decomposable size then sum-product theorem summed time furthermore compatible summed time claim follows. identical. induction step assume maxxi child maxxi ch)). product node maxx maxx node maxx∈x{maxci maxci{maxxi∈xi maxx second equality occurs selective. summing state recovered downward pass takes linear time. corollary partition function tpkb computed time linear size. proof. sources non-decomposability object subclasses contain relation attribute note cannot contain part since classes descendant class hierarchy never part name. cases shared relation pushed subclass distributing subclass subclass makes product relations non-decomposable affect decomposability objects. decomposed follows. product relations contains non-decomposable factor pushed however since literal thus simply weight results decomposable structure different weight. weight function attribute class domain weight function attribute pushed render decomposable simply replace element-wise product weight functions. again decomposability achieved simply updating weight functions. decomposing adds linear number edges original non-decomposable size corresponding decomposable |k|. thus sum-product theorem computing partition function tpkb takes time linear |k|. proof. counterexample. vector variables commutative semiring |xi| summed time linear decomposable size time treewidth pairwise-disjoint subsets domains xaxbxc conditionally independent given thus smallest junction tree compatible complete clique seen follows. without loss generality ﬁrst variables ixi∈a∪b ···⊗ ψm)⊕···⊕ ⊗···⊗ ψrm). factor terms right-hand side must common factors; however general different factors. thus conditional independencies node corresponding node sat) induction. base case leaf node holds trivially. induction step sat) child node multiplication node sat) decomposable. node addition node sat) deterministic children logically disjoint. proof. selective decomposable max-product version size also selective decomposable. clarity assume evidence since trivial incorporate. sum-product theorem maxx takes time linear size node corresponding node remains show maxx maxx induction base case leaf holds trivially", "year": 2016}