{"title": "Exploring the Design Space of Deep Convolutional Neural Networks at  Large Scale", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "In recent years, the research community has discovered that deep neural networks (DNNs) and convolutional neural networks (CNNs) can yield higher accuracy than all previous solutions to a broad array of machine learning problems. To our knowledge, there is no single CNN/DNN architecture that solves all problems optimally. Instead, the \"right\" CNN/DNN architecture varies depending on the application at hand. CNN/DNNs comprise an enormous design space. Quantitatively, we find that a small region of the CNN design space contains 30 billion different CNN architectures.  In this dissertation, we develop a methodology that enables systematic exploration of the design space of CNNs. Our methodology is comprised of the following four themes.  1. Judiciously choosing benchmarks and metrics.  2. Rapidly training CNN models.  3. Defining and describing the CNN design space.  4. Exploring the design space of CNN architectures.  Taken together, these four themes comprise an effective methodology for discovering the \"right\" CNN architectures to meet the needs of practical applications.", "text": "recent years research community discovered deep neural networks convolutional neural networks yield higher accuracy previous solutions broad array machine learning problems. knowledge single cnn/dnn architecture solves problems optimally. instead right cnn/dnn architecture varies depending application hand. cnn/dnns comprise enormous design space. quantitatively small region design space contains billion diﬀerent architectures. would like thank parents encouraging pursue interested would like thank wife steena monteiro accomplished computer science researcher last several years worth brainstorming sessions collaboration encouragement. thanks mike syphers introducing scientiﬁc research years inspiring pursue research career. would like thank advisor kurt keutzer helpful encouraging energizing thoughtful. kurt exceeded expectations scientiﬁc advisor also thank kurt mentor topics including management human behavior especially entrepreneurship. would also like thank dissertation committee members sara mcmains krste asanovi´c feedback encouragement. thanks trevor darrell jitendra malik ross girshick encouraging participate projects even ﬁrst-year grad student brought questions solutions table. honor work many great collaborators time berkeley. thanks david sheﬃeld michael anderson teaching much know benchmarking accelerating software. thanks song really collaboration designing small neural network models using model compression techniques. much gratitude many colleagues collaborators including steena monteiro matthew moskewicz katya gonina saurabh gupta peter anting shen sammy sidhu macmillen peter sergey karayev mehdi maasoumy pierluigi nuzzo mangpo phothilimthana evan shelhamer long yangqing donohue richard zhang mahsa kamali john hart matei stroila tarek abdelzaher bichen benjamin elizalde khalid ashraf ning zhang larry zitnick fang rupesh srivastava mitchell john platt deng piotr doll´ar jianfeng xiaodong john platt larry zitnick geoﬀ zweig ryan farrell. would like thank sponsors. last years research partially funded industrial sponsors aﬃliates including intel google huawei nokia nvidia oracle hewlett-packard samsung. ﬁrst three years funded ndseg fellowship enabled maximum freedom research direction. research also partially funded darpa award number hr--. research used resources ridge leadership computing facility ridge national laboratory supported oﬃce science u.s. department energy contract de-ac-or. finally thanks shirley salanio helping navigate non-obvious logistical nuances successfully ﬁling dissertation berkeley. also days leading dissertation ﬁling deadline spending much time deepscale oﬃce mountain view thanks anting shen daisyca hand-delivering dissertation documents berkeley. thus computer science research development focused problems mathematical solution clear step-by-step procedure derived. approaches feasible popular computer science topics applications including databases numerical linear algebra encryption programming systems. however extremely broad space important problems closed-form solution known. right equation take pixels image understand contents? equation take audio frequencies interpret speech? nature evolution developed solutions problems foolproof mathematical solution series steps write comprehensively solve problems. problems procedural mathematical solution known often turn machine learning broadly deﬁne enabling computer automatically learn without explicitly programmed. history ﬁeld machine learning churned numerous approaches including decision trees clustering hidden markov models support vector machines neural networks. given problem mathematical/procedural solution known machine learning practitioners often done combination experimenting several families machine learning algorithms attempting design algorithms. figure show number widely-studied machine learning problems along high-accuracy solutions problems since opportunity play role dramatic transformation landscape machine learning algorithms. last years machine learning community discovered deep neural networks give higher accuracy previous solutions broad array problems. compared figure notice year highest-accuracy solutions problems based dnns. however that’s single conﬁguration able singlehandedly solve problems high accuracy. rather dnns broad family algorithms comprised multiple layers automatically-learned data transformations layers trained end-to-end fashion. thus design architectures driven primarily human intuition empirical experimentation overarching theory horizon supplant experimental craft. therefore search right architectures replacing broader algorithmic exploration many application areas. dissertation propose codify best practices undertaking search right architectures. many machine learning researchers spent entire careers optimizing metric accuracy. accuracy often deﬁned percentage examples held-out test machine learning system recognize classify correctly. last years rise dnns brought enormous improvements accuracy. example pascal object detection dataset pre-deep-learning methods achieved roughly mean-average precision pascal- validation set. today dnnbased object detectors able achieve much higher accuracy. example dnn-based faster r-cnn object detector achieved mean-average precision pascal- validation set. relative improvement double accuracy best pre-deeplearning solutions. short dnns enabled enormous improvements accuracy application areas computer vision. by-product computer vision ready prime time number practical industrial applications. deploying solutions real applications achieving high accuracy necessary usually suﬃcient enable application’s success. number factors time required train model speed model inference mode energy-eﬃciency system whole important enabling productive workﬂows deployable applications. summarize factors figure give detailed account factors chapter take-away that explore design space architectures aiming high accuracy searching tradeoﬀs accuracy practical metrics. research industrial settings many engineers gearing apply deep learning respective problem domains. tools resources objectives teams obtain dive process? identiﬁed four ingredients success deep learning. enumerate ingredients figure discuss detail following subsections. figure ingredients success deep neural networks. advanced stateof-the several ingredients contributions including firecaﬀe rapid training framework squeezenet cnn/dnn architectures boda framework portable eﬃcient inference. training data fuel enables deep neural networks learn. enormous amounts training data dnns able achieve unprecedented accuracy variety problems. however dnns real products product developers receive steady stream feedback dnn-based solution failing correctly. example picture developer autonomous vehicle technology. end-users report vehicle acts confused speciﬁc situations night driving snow horizontal stop lights. addressing issues? techniques disposal collect training data dark lighting conditions inclement weather unusual types stop lights. anticipate developers dnn-based products constantly iterating identifying edge-cases failure cases collecting annotating data cover cases retraining models deploying updated models. engage quantitative discussion training size aﬀects dnn’s accuracy target application. imagenet-k widely-used dataset provides training million labeled images image hand-labeled belonging category total imagenet-k categories images dataset provides roughly training images category. labor-intensive label million images. logical question training models quantity labeled training images aﬀect model’s accuracy? supplementary material nips paper yosinski evaluated precisely question. figure show result experiment yosinski point figure separate training alexnet architecture trained speciﬁc number training examples category. example point corresponding x-axis trained subset imagenet-k consisting categories images category. y-axis accuracy deﬁned percentage images validation model classiﬁed correctly. observe training data leads higher accuracy. even right-hand side graph training images category shows improvement roughly percentage-point training images category. broadly general consensus unless already achieving accuracy increasing size diversity training typically leads higher accuracy. dissertation primarily publicly-available datasets training data. said industrial applications dnns require substantial quantities labeled data representative task hand. biggest challenge building custom training datasets cost management overhead getting workers apply appropriate annotations data. kurt keutzer’s research group berkeley developed tool beaverdam ability rapidly train models important reasons. first rapid training enables developers quickly search right models. second rapid training enables workﬂow keep ever-expanding training datasets. enabling rapid training require appropriate computational hardware eﬃcient implementations appropriate architectures eﬃcient distributed computation cluster servers. discuss approaches rapid training chapter right models must give right tradeoﬀs eﬃciency metrics energy compute footprint memory footprint achieving accuracy level required endapplication. discuss approaches exploring models identifying right models chapter right models need deploy them. opportunities eﬃcient implementations cpus gpus fpgas asics custom silicon. cases need right portable eﬃcient software implementation. dissertation primarily rely existing computational kernels nvidia cudnn library eﬃcient inference. cudnn kernels developed large team engineers nvidia highly eﬃcient nvidia hardware. however cudnn kernels exclusively support nvidia processing hardware. achieving high eﬃciency inference hardware remains challenge much research industrial community. joint work matthew moskewicz jannesari kurt keutzer berkeley developed boda software framework. boda provides computational kernels competitive cudnn nvidia hardware boda’s kernels also eﬃcient hardware platforms qualcomm gpus. information boda available motivated search right architectures proposed ingredients vital carrying search. recipe book leverage ingredients cook world-beating deep neural network architectures? section begin describe recipe book. deep neural networks comprise enormous design space. overarching theme objective dissertation design architectures dnn-based systems meet particular design goals. dnns recently become widely studied areas broad design spaces fruitfully explored. area computational hardware architecture comprises enormous design space continuously explored numerous semiconductor companies years. section summarize number insights hardware architectural design practices adapt dnns. broad design space computational hardware spectrum lies ﬁxedfunction circuit spectrum lies general-purpose processor. fixed-function circuits routinely developed goal providing cheap-to-manufacture energy-eﬃcient low-latency solutions speciﬁc problems bluetooth wireless communication network packet routing bitcoin mining challenge that ﬁxed-function circuit designed execute particular algorithm future changes algorithm likely require entirely circuit produced. hand general-purpose processors relatively easy program ability execute broad range computations. downside processors typically draw energy well-designed ﬁxed-function circuit processors expensive manufacture. options extremes ﬁxedfunction circuits general-purpose processors computational hardware solutions middleground ﬁxed-function circuits processors commonly called application-speciﬁc processors application-speciﬁc processors developed variety applications including speech recognition computer vision graphics software-deﬁned wireless even given problem broad design space asips address problem. design space include asips already produced/fabricated; designs considered fabricated; designs considered all. point asip design space processor unique characteristics number pipeline stages number processing elements number vector lanes. design point also speciﬁc tradeoﬀs energy-eﬃciency throughput latency manufacturing cost metrics. mescal methodology codiﬁes best practices navigating design space produce high-quality asip hardware. mescal methodology recommends beginning choosing benchmarks target results benchmarks. then achieve desired results cornerstone mescal design space exploration consists identifying evaluating broad range points design space. series case studies asip hardware ethernet network processing mescal book explores dimensions design space number processing elements pipeline depth width vector lanes. broadly speaking characteristics asip design space commonalities characteristics design space. asips dnns comprise enormous design spaces. practical deployments design spaces asips dnns present tradeoﬀs terms speed energy quality metrics following table enumerate themes reference include analogous theme mescal methodology. dissertation devote chapter themes. table notice that mescal methodology theme eﬃciently evaluate points hardware design space dissertation rapidly training dnns. reader wouldn’t term evaluating refer inference rather training phase dnns? here’s problem test dataset evaluate accuracy must trained ﬁrst imagenet-k dataset typical train making passes corpus million images. contrast evaluate single pass images required. thus training computationally expensive stage required evaluate accuracy architecture. fortunately evaluate metrics besides accuracy computational requirements much lower. metrics training speed inference speed throughput energy eﬃciency dnns evaluated using microbenchmarks take seconds most. architecture accuracy computationally-intensive metric evaluate must train neural evaluate accuracy. words exploring broad range architectures training model acute computational bottleneck evaluating model. began research career mid-s working computational physics topics simulation particle colliders gradually moved working computational eﬃciency aspects physics simulations working physics-oriented distributed computing frameworks well single-node acceleration aside computational physics also worked directly core parallel/eﬃcient computing problems scheduling tasks single-gpu multi-node distributed environments well problem estimating predicting latency sparse-matrix computations early work distributed scheduling motivated maximizing computational eﬃciency well guaranteeing hard real-time constraints safety-critical applications aircraft electronics. later co-authored paper directly topic aircraft electronics time conducted research computer vision applications image stitching facial analysis work berkeley focused intersection computer vision machine learning computational eﬃciency. computational eﬃciency side work spanned analyzing energy-eﬃciency computer vision algorithms eﬃcient feature extraction amortizing cost computation object detection unifying eﬃciently-computed features object detection approaches convolution ubiquitous computational patterns computer vision communication-minimizing convolution kernels delivered speedup nvidia’s nvidia performance primitives library since time nvidia developed eﬃcient convolution kernels cudnn library individuals nvidia tell closed-source cudnn implementation drew inspiration communication-minimizing convolution approach. joint work matthew moskewicz kurt keutzer released report boda framework provides eﬃcient convolution kernels software eﬃcient neural network computation. nvidia gpus found boda’s convolution eﬃciency competitive nvidia’s cudnn software boda also executes eﬃciently gpus vendors qualcomm. addition several contributions focused developing accurate approaches problems ﬁne-grained visual classiﬁcation logo recognition audio-based concept recognition visual object detection image captioning ﬁnal years berkeley focused problems accelerating training deep neural networks computer vision understanding exploring design space deep neural networks taken together contributions form basis holistic strategy eﬃciently exploring design space deep neural network architectures focal point dissertation. dissertation chapter reﬁnement firecaﬀe paper accelerating training chapter reﬁnement squeezenet paper introduced novel squeezenet architecture also explored variety architectures design. chapters discussions published previous work. years many computer architects evaluated technology using narrow benchmarks metrics cycle count. cycle count useful metric mescal book contends computer architectures must evaluated representative benchmarks metrics. ideally benchmarks metrics representative end-applications computer architecture aims enable. like days computer architecture cycle count erroneously considered suﬃcient metric machine learning computer vision research predominantly focused single type metric accuracy. context computer vision accuracy typically refers ability machine learning method correctly classify detect segment otherwise understand visual imagery. however computer vision methods becoming quite accurate number metrics must considered developing computer vision technology deployed real systems. computer vision methods quite computationally intensive every application explicit implicit limits factors speed must achieved quantity energy available cost quantity hardware used. chapter propose holistic metrics enable benchmarking methodology covers accuracy well computational eﬃciency. section search computationally-intensive applications dnns. discover that applying neural networks high-dimensional data images video computational requirements extremely high. next section visit dnns’ requirements large quantities expensive labeled training data describe alleviate aspects problem transfer learning. then section address question besides accuracy metrics useful evaluating dnns applied practical applications? section describe benchmarks metrics evaluate engineering team’s progress well individual engineers’ progress toward end-goal achieving speciﬁc tradeoﬀ accuracy choosing representative benchmarks enough identify average case middleof-the-road benchmarks. instead goal choosing benchmarks identify worst-case challenging cases focus optimizing cases. applying ideology present topic goal take situations training and/or inference currently intractably slow computational eﬃciency skills accelerate dnn-based applications much faster time scale. mind sought computationally time-consuming applications dnns. speciﬁcally looking applications following properties table identiﬁed computationally-intensive applications make large publicly-available datasets four domains text audio images video. summarize results table figure main observation table applications data sample higher dimensionality training takes longer. example training text takes hours training video takes month words video application took time train text application. factor higher-dimensional input data necessitate higher-dimensional models computational complexity. text application table authors didn’t report number computational operations data sample appears text fewer computational operations video dnn. diﬀerence computational complexity among applications dramatic that even though text dataset data samples video dataset training video still slower training text dnn. finally notice video trained gpus text trained gpu. best case training gpus would faster training gpu. therefore apples-to-apples comparison video training much time-consuming text training despite fact text training dataset training samples video training dataset. especially compared text application image video applications table highly computationally intensive meet requirement requirement outset goals computationally-intensive applications/benchmarks work accelerating applications. examples table computer vision applications large-scale image video datasets tend demand extremely high computational cost. mind focus rest dissertation application neural networks computer vision. today almost neural networks computer vision deep also make extensive convolution layers. clear would best-case speedup scaling gpus. however factors gpu-to-gpu communication overhead make diﬃcult achieve truly linear speedup scaling multiple gpus. computer vision deep convolutional neural networks often referred dcnns simply cnns. point forward dissertation primarily term describe neural networks applied computer vision. turn attention back table probably imagine summarizing four domains applications table requires many simpliﬁcations omissions. following subsections provide details numbers table calculated. feel satisﬁed understanding table recommend skipping ahead section past text analysis algorithms often relied hand-engineered features n-grams. however since publication wordvec paper deep neural networks gone widespread creating feature representations text data. wordvec approach used improve accuracy number text-based applications document search internet click prediction. provide background wordvec approach. wordvec requires large corpus text data example many sentences collected wikipedia internet text. unlike audio imaging video topics discussed chapter training wordvec text data require human-annotated labels. instead wordvec training problem phrased self-supervised way. then sentence remove word model predict word best sentence. correct response model would predict word original sentence. response considered wrong correct response backpropagated model. ultimately produces model predict missing words sentences. self-supervised rather requiring human annotator labels derived corpus training data. research community discovered representation learned training procedure broadly applicable high-quality feature representation variety problems. long take train wordvec model? original wordvec paper trained model dataset contained total billion words wordvec authors reported training wordvec model billion words single server information provided type hardware used server. however zhao report amount time required train wordvec model observed experiments. admittedly isn’t entirely clear whether model zhou dimensions conﬁguration model presented original wordvec paper. nevertheless zhao reported training wordvec model single nvidia titan rate words second equates roughly hours complete epoch training. used numbers zhao wordvec entry table dnns widely used research commercial speech recognition systems. approaches training perform speech recognition requires large quantity appropriate knowledge largest publicly available corpus word-labeled audio data fisher corpus hours data long take train high-accuracy dataset? literature found articles report training times fisher corpus. however researchers baidu reported time required train even larger dataset. baidu researchers used -hour dataset includes hours fisher corpus plus hours proprietary data collected labeled baidu plus number smaller datasets switchboard librispeech train -hour dataset using single nvidia authors reported takes approximately weeks train high-accuracy model. given takes weeks train full -hour baidu dataset knowing fisher corpus contains much training data estimate would take half week train high accuracy using fisher corpus knowledge largest publicly-available dataset. image classiﬁcation core problem computer vision. goal automatically assign labels image. image zoomed-in primary object valid classiﬁcation label could even speciﬁc breed dog. image zoomed-out scene reasonable label might park oﬃce depending contents image. researchers developed large datasets labeled data training evaluating image classiﬁcation methods. largest image classiﬁcation datasets imagenet-k categories images average labeled training images category total million labeled training images architecture called alexnet imagenet image classiﬁcation competition imagenet large dataset cnns often quite computationally intensive. reported training took approximately week. result achieved training single nvidia using well-tuned computational kernels. full imagenet data corpus least million images image labeled categories ﬁxed number epochs training full imagenet corpus take times longer total weeks execution time researcher begins training model time training complete. imagenet-k slightly million labeled images training set. however sports-m dataset nearly million labeled videos training video comprised hundreds frames minute average video dataset minutes long. labels sports-m rather coarse. rather labeling individual objects individual frames creators sports-m opted label video. speciﬁcally video labeled particular activity performed video. dataset called sports-m activities types sports total diﬀerent types sports exact. training models sports-m dataset typical end-goal automatically predict label video test set. long take train model recognize types sports played videos? karpathy trained alexnet-based model sports-m dataset authors trained model using custom settings downsampling factor input images number images clips training video. rather attempting calculate training time based alexnet-imagenet experiments previous subsection simply report training times karpathy follows. karpathy able train high-accuracy model sports-m month using cluster gpus. previous section found cnn/dnn training tends time-consuming computationally expensive large-scale computer vision recognition problems images video compared text audio typically less computational overhead. dedicated remainder dissertation accelerating rethinking improving eﬃciency accuracy cnns applied computer vision. element many today’s computer vision workﬂows transfer learning discuss following paragraphs. achieve highest possible accuracy cnn/dnns typically need large supply training data. computer vision state-of-art cnns often trained supervised learning methods. supervised learning methods typically require training data annotated humans. practical question avoid need human annotators annotate enormous datasets application would like develop high-accuracy models? image classiﬁcation application training data relatively inexpensive gather. because train image classiﬁcation need label image. contrast problem object detection problem automatically localizing classifying object image. typically train perform object detection image must hand-annotated rectangle class label object image. substantially work simply annotating class label image. given much work goes annotating image popular publicly-available object detection training datasets kitti pascal fewer annotated images. frustrating cnns typically require large quantity training data achieve highest possible accuracy. however recall much less labor-intensive annotate data image classiﬁcation. mind popular approach ingest enough data object detection follows. first train image classiﬁcation enormous volume data million images imagenet-k. partway training procedure switch training image classiﬁcation training object detection kitti coco virat data). switch classiﬁcation detection necessary change choice loss function knowledge natural images learned classiﬁcation transferred training object detector. training protocols involve switch called transfer learning. broadly training approach involves transferring model learned domain adapting model domain described transfer learning approach. overwhelming majority research machine learning computer vision neural networks focused getting best possible results type metric accuracy. however train models tractable productive timeframe deploy models real-time without unbounded quantities hardware must apply discipline evaluating computational characteristics models execution environments. section present menu metrics enable disciplined evaluation computational properties dnns. completeness also present overview strategies evaluating accuracy dnns. list metrics table orient metrics especially toward computer vision many metrics applied cnn/dnn applications well. further also show table diﬀerent aspects environment interact metrics. notice metrics inﬂuenced aspects environment metrics isolated aspects environment. bear mind metrics necessarily useful applications rather goal present broad playbook evaluate cnns cnn-based systems deploying real application. machine learning methods dnns employed classify extract meaning unstructured data text audio images video. methods better others accurately extracting meaning data. measuring accuracy machine learning method? general template measure accuracy involves test data samples experimenter knows ground truth labels samples learning method not. evaluate method’s accuracy trained model applied test method’s accuracy level computed based well method understood test set. exact choice compute accuracy depends problem hand. review techniques measuring accuracy various problem domains. widely-studied problem computer vision image classiﬁcation automatically classify whole image oﬃce park restaurant etc. dnns classiﬁcation methods trained learn mapping image category label. evaluate accuracy classiﬁer? image test belongs exactly category accuracy reported simply percentage correct widely-studied problem computer vision localized detection objects. goal automatically draw rectangles around objects image automatically assign correct labels rectangle localized detection methods exhibit extreme behavior example cover entire image rectangles choose draw rectangles all. range outcomes summarized single metric accuracy? number approaches begin organizing method’s detections false positives true positives false negatives true negatives information metrics average precision false positives image computed. metrics summarize object detector’s accuracy single metric. number additional metrics calculating accuracy problem domains. accuracy semantic segmentation assigning object labels pixels image often evaluated intersection union metric accuracy speech recognition methods evaluated per-frame per-word basis. likewise accuracy video analysis methods evaluated timescale ranging per-frame per-video accuracy methods automatically assigning captions sentences images commonly evaluated metrics bilingual evaluation understudy metric evaluation translation explicit ordering general terms bleu meteor designed quantify correctness algorithmically generated caption’s word usage word ordering number words respect database ground-truth captions. optical algorithms accuracy often measured angular error metric. today’s architectures present wide range computational requirements. example lenet architecture performs million ﬂoating-point arithmetic operations classify image. spectrum vgg- architecture performs mflops classify image. applied image lenet performs mflops classify image. applying vgg- input image requires computation applying lenet input image. even input image size vgg- requires computation lenet. dnns comprise enormous design space points design space require orders magnitude computation others. focused quantity computation required classify images inference phase. turn attention quantity computation needed train dnn. rule thumb computation required image training computation required inference inference think quantity computation terms ops/frame. however training multiple passes training often performed. good think quantity computation consider total number arithmetic operations required complete training number epochs could image ﬁrst fully-connected layer lenet operates input size xxchannels ﬁlter also xxchannels. adapt lenet image make layer convolution layer ﬁlters size xxchannels average pool output layer xxchannels. monly implemented -bit ﬂoating-point math less commonly integer math high-end nvidia perform tflops/sec running tflops/sec lenet could classify frames/sec vgg- could classify frames/sec frames/sec. however guarantee oﬀ-the-shelf implementation achieve peak eﬃciency tflops/sec. well-tuned libraries boda cudnn fbﬀt neon shown achieve peak depending choice hardware dimensions layer. computational resource limited quantity computation perform second. however often-overlooked factor computational resource also limited quantity communication perform second. deﬁne communication quite broadly. variety communication movement data server other example using hardware networking platform ethernet inﬁniband. variety communication within server diﬀerent layers storage memory hierarchy. thus classify varieties data movement communication. illustrate example conﬁguration hardware pertaining levels data movement figure landmark rooﬂine model paper williams showed that depending method choice hardware theoretical best-case execution time limited either computation communication. interpret broadly mean following limiting factor speed algorithm/program execute mind important analyze quantity computation also quantity communication required architecture. speciﬁc hardware implementations also study speed i.e. computation second communication second discuss next section. that brieﬂy consider quantity communication diﬀers training inference cnns. figure multicore server. diagram xeon intel broadwell core. diagram includes several levels communication infrastructure caches memory interconnects. portions diagram borrowed transferring input data registers transferring activations gradients registers transferring parameters registers transferring model updates across servers quantities computation communication properties architecture orthogonal underlying hardware implementation executes dnn. speed holistic metric impacted number factors including architecture software implementation choice computational hardware. following example illustrates diﬀerence measuring quantity computation measuring speed. inference vgg- performs gflops frame quantity computation. majority flops attributed performing convolution calculations. rate expect processor perform flops? every processor theoretical peak best-case throughput flops/s. example nvidia titan theoretical peak tflops/s. study high-performance computing applications implemented supercomputers found average application runs approximately processor’s peak eﬃciency rate tflops/s inference phase would frames/sec. however shown boda work accelerating convolutions gpus feasible implement certain problem sizes convolution peak more nvidia qualcomm gpus. peak inference phase would frames/sec speedup frames/sec. learn simple example every architecture requires particular quantity computation speciﬁc software implementation inﬂuence order-of-magnitude speed flops executed. learned that addition choice architecture software implementation major impact computational speed. choice computational hardware also impact? nvidia’s maxwell microarchtecture underlying computational hardware several nvidia’s current products. nvidia produces system small maxwell-based peak -bit computation rate tflops/s high nvidia produces titan peak -bit computation rate tflops/s even provably optimal software implementation achieves hardware’s peak tflops/s would slower frame rate titan answer question choice computational hardware order-of-magnitude impact speed execute cnn. summarize architecture software implementation choice computational hardware contribute speed executed. subsection focused speed inference phase chapter present detailed discussion factors determine speed training. experimental autonomous vehicles caltech’s alice vehicle implement vision/perception methods using in-vehicle server racks draw power rumored google’s autonomous suvs also onboard computers draw multiple kilowatts power. number conversations people inside automakers issue. make autonomous driving feasible economical automotive oems suppliers urgently wish achieve high-quality perception much lower computational power budgets prototype mass-produced autonomous vehicles. toyota recently created toyota research institute billion research center focuses driver assistance autonomous driving technologies computer vision core area focus. recent keynote gill pratt toyota research institute explained humans fairly good driving cars human body resting power draw order consumed human brain contrast prototype autonomous cars mentioned earlier section draw multiple kilowatts power execute visual perception methods real-time. pratt team working decrease power required perform computer vision retaining suﬃciently high accuracy safe semi-autonomous fully-autonomous driving using less power directly translates dissipating less heat. automotive drone embedded applications system designers typically prefer passive processor cooling therefore low-power design goal. power also good proxy number problem dimensions. numerous applications require hardware small applications also often require hardware cheap. low-power computational hardware often small cheap targeting lower power hardware likely meet system-level goals cost size. much today’s computer vision research still focuses accuracy sole metric. however encouraging recent studies used power motivating metric addition accuracy. example cavigelli benini recently proposed origami design accelerator authors evaluated work terms accuracy power operations-perwatt likewise number hardware papers e.g. ovtcharov reported power footprints. design automation conference hosted low-power image recognition challenge organizers challenge published highlights iccad goal classify imagenet test images minutes wattage used classifying images accurately possible. organizers competition choose limits power time? without limit time competitors could simply slow computation point particular power budget following combination time limit wattage eﬀect specifying limited budget energy. consider case particular processor execute visual recognition method frames second drawing power. reduce power envelope naive approach would simply slow computation delivers best-case improvement less power draw however also reduces frame rate fps. clearly power isn’t suﬃcient metric capture goals related computational eﬃciency metric easily gamed degrading frame rate. fortunately single metric capture eﬃciency goals energy joule deﬁned achieve power budget require joules frame. convolution ﬁlter contains multiple parameters. example consider ﬁlter ilterh ilterw represented -bit ﬂoating-point numbers ﬁlter parameters. prior training parameters typically initialized random distribution. then goal training process learn numerical values parameters model training able generalize beyond training set. modern architectures comprise tens hundreds layers layer hundreds ﬁlters. architectures alexnet vgg- hundreds megabytes model parameters. however figure chapter parameters necessarily lead higher accuracy. further speciﬁc level accuracy particular dataset often identify several models able achieve level accuracy. given equivalent accuracy architecture fewer parameters number advantages scalability distributed training. distributed data-parallel training communication overhead directly proportional number parameters model short smaller models train faster requiring less communication. panies tesla periodically copy models servers customers’ cars. alexnet would require communication server car. smaller models require less communication making frequent updates feasible. feasible fpga embedded deployment. fpgas often less on-chip memory oﬀ-chip memory storage. inference suﬃciently small model could stored directly fpga instead bottlenecked memory bandwidth required transfer model parameters onto chip previous sections learned that beyond accuracy often necessary optimize cnn-based system several metrics order meet system-level design goals. build best possible system necessary co-design multiple levels cnn-based system including architecture software implementation computational hardware. explore design implement systems often necessary pull team experts bring deep knowledge multiple areas team ideal following ways evaluating progress toward end-goals section describe approach aiming full-stack team overall goal involve achieving particular level accuracy aggressively budget energy aggressively high target speed/throughput. section describe approach enables individual team members sub-teams evaluate contribution progress toward goals. system-level benchmarking promotes creativity codesign algorithms software hardware. consider example system-level benchmarking autonomous driving domain. autonomous road vehicles present huge opportunity save human lives reduce unproductive time commuting. ability automatically perform real-time visual perception vehicle’s surroundings prerequisite safe autonomous driving road vehicles. figure show perception system overall stack autonomous driving systems. drive safely perception systems must deliver high accuracy. addition accuracy perception systems subject number practical constraints. example real-time computation crucial automotive perception applications. useful detect drifted lane crashed time computer vision system identiﬁed lane departure. likewise traﬃc light detection limited value vehicle figure overview implement autonomous driving systems described contacts major automaker. today perception module often limiting factor overall system’s level safety. already violated light time vision system detected light. addition need real-time computation vehicle limited supply energy onboard. electric vehicle battery stores limited amount energy autonomous driving system energy-hungry cannibalizes energy needed move long distances. even internal-combustion vehicle ﬁnite supply fuel tank installing highcapacity alternator generate power would eﬀect reducing vehicle’s range fuel economy. discussed section high-accuracy perception algorithms real-time companies like google rumored server racks much instantaneous power draw equates mjoules/hour. designing perception system practical automotive deployments worthwhile specify target outcomes several metrics including accuracy energy peak power draw model size consider problem designing perception system autonomous road vehicle. future expect safety standards autonomous vehicles vehicle manufacturers also automobile safety agencies iihs nhsta euroncap. today accuracy achieved visual perception module limiting factors safety semi-autonomous autonomous vehicles. design objectives deployable system meets exceeds highest safety rating? first determine accuracy metrics accuracy levels required meet desired level safety. safety standards still it’s entirely clear levels accuracy required following steps applicable systems variety accuracy operating-points. next minimum frame rate needed perform perception real-time. long achieve desired frame rate accuracy level using less energy always better. baseline system required achieve desired frame rate speciﬁc accuracy level worthy goal would preserve accuracy frame rate envelope freedom modify algorithm focus problem converting real-time data feeds cameras sensors understanding environment. meanwhile much work done improving sensors mapping path planning control. software implementation choice hardware target improved energy-eﬃciency without compromising accuracy frame rate. words metric energy transcends boundaries algorithm software hardware optimizing metric promotes creativity codesign multiple layers system. overall goal reduce energy frame consider dynamics team working toward goal. typical case team would include least hardware architect least software architect least architect. three team members work focus identifying usable oﬀ-the-shelf solutions developing entirely solutions likely pursuing mixture oﬀ-the-shelf custom solutions. team member want evaluate contributions toward common cause improving energyeﬃciency. refer exercise per-component benchmarking. team follows terms ops/sec. further hardware architect minimize power needed peak ops/sec. power envelope include processor itself also memory power requirements. erty only software hardware further quantity computation allows architect chart progress reducing computational requirements. model size metric helps architect understand level memory hierarchy model parameters reside. tempting isolate his/her contributions per-component benchmarks. software architect maximize speed relevant problem sizes representative hardware. hardware developed benchmarking need occur simulation-based fgpa-based prototypes proposed hardware. ideally software architect communicate frequently hardware architect architect trading notes hardware limitations problem dimensions implemented eﬃciently. discussed sections software+hardware system always achieve peak ops/sec problem sizes software hardware architects constantly exchanging notes architect problem dimensions. mescal book points many computer architectures developed evaluated using inadequate benchmarks representative applications. especially time mescal book written common architectures chips evaluated based number clock cycles needed execute series instructions. ideally instruction stream would least derived motivating application sometimes instructions chosen arbitrarily. surprisingly application-speciﬁc processors developed vague benchmarks objectives processors don’t necessarily deliver improvements end-application speed eﬃciency. antidote phenomenon mescal book advocates system-level benchmarking. type benchmarking calls representative algorithms implementations inputs evaluating hardware. evaluating top-to-bottom system metrics speed energy-eﬃciency correctness sure-ﬁre understand hardware perform application. discussion benchmarks metrics evaluating deep learning systems advocated system-level benchmarking. well aligned mescal methodology. example speed cnn/dnn system inﬂuenced levels stack including dimensionality input data neural architecture software hardware. likewise energy frame inﬂuenced factors. metrics traded accuracy system-level benchmarks promote co-design levels stack develop best possible system. system-level benchmarking vital type benchmarking cases per-component benchmarks useful too. example accuracy cnn/dnn model independent software hardware implemented model’s accuracy analyzed independently rest system. likewise number arithmetic operations property alone. further peak power envelope purely property hardware. per-component metrics useful individual teams organization evaluate individual contributions. however north star guiding light always system-level metrics represent end-application behavior. cnn’s accuracy changes moving diﬀerent software implementation hardware platform considered bug. however opportunities co-design leveraging hardware eﬃciently computations precision working mitigate possible drop accuracy. since publication alexnet variety deep neural network architectures googlenet network-in-network developed rapid pace. natural training testing dataset ﬁxed architecture primarily responsible improvements accuracy. words race improvements accuracy image classiﬁcation contemporary problems computer science become race development architectures. bottleneck development architectures? development architectures human research endeavor creativity element. however impact architectural variations dnns number layers ﬁlter dimensions forth hard predict experimentation required assess impact. high-accuracy deep neural network model googlenet take weeks train modern gpu. true even leveraging deep neural network primitives like cudnn maxdnn fbﬀt boda operate near theoretical peak computation second achievable gpus. thus training time challenge root development architectures. sentiment voiced jeﬀrey dean google recent keynote address given considerable resources available google researchers dean’s comments indicate simply throwing computational resources problem suﬃcient solve training problem. following spend little time dimensionalizing current problems training upside potential problems solved. particular example long training times limiting pace research productization consider following. imagenet-k million training images distributed across diﬀerent category labels. ﬁrst-hand conversations engineers executives know several internet companies internal databases containing billions images hundreds thousands diﬀerent category labels. long training times companies facing serious delays bringing dnn-based solutions market. accelerated training solutions would address major pain point companies. argued accelerating training would beneﬁt applications dnns today. consider ways accelerating training would allow dnnbased techniques applied entirely ways. number situations crucial incorporate data model real time. example reinforcement learning enables robots learn things minimal supervision. recent study levine applied state-of-the-art dnn-based techniques enable robot teach build lego structures screw bottle caps technique eﬀective robot indeed learn screw bottle caps. however takes hours robot learn screw bottle caps majority time spent training. faster training would enable reinforcement learning applications move toward real-time. deep neural networks used ever-broadening variety problems including classifying detecting objects images writing sentences images identifying actions videos performing speech recognition gaining semantic understanding text anticipate sophisticated reinforcement learning systems robotics eventually leverage modalities ideally real-time. work focus directly problem training. since single-gpu eﬃciency reached hard limits hardware next frontier accelerating training scale across compute cluster. chapter present firecaﬀe scales training across cluster gpus speedups compared single gpu. strategy scaling training focus reducing communication overhead make number design choices toward goal. example fast interconnects inﬁniband cray gemini accelerate communication among gpus. also show reduction trees faster method communication using parameter servers. parallelization strategy high-accuracy architectures require less communication. section describe choice hardware evaluating scalable training section introduces factors analyzing communication among workers. describe tradeoﬀs parallelism strategies section section explains certain high-accuracy architectures particularly amenable parallelism. section describe approach eﬃciently implementing distributed training. section describe good practices facilitate comparison scalable training techniques present speedups training googlenet architectures imagenet. section describes approaches complimentary firecaﬀe accelerating training. conclude section useful feasible experiment scalability computations using theoretical scale models. however demonstration veriﬁcation correctness realworld scalability proposed firecaﬀe system requires using concrete hardware platforms. speed data sent nodes consideration selecting hardware platform scalable training. because faster interconnect nodes scale achieve without dominated communication overhead. hardware manufacturers cray mellanox address developing high-bandwidth low-latency interconnects substantially faster typical ethernet connections. example titan supercomputer ridge leadership computing facility high bandwidth latency cray gemini interconnect communication among servers. titan supercomputer total servers nvidia kepler-based server mind choose olcf titan supercomputer tuning evaluating firecaﬀe. research relatively small slices overall capacity titan training run. additional computational capacity enables conduct multiple training runs concurrently training utilizes gpus. considering -node slices titan found interconnect speed similar provided nodes slice connected single inﬁniband-class switch. minimize confusion deﬁne terminology. data structures convolution layer data parameters present simple illustration figure explicitly terminology following sets words synonyms also sometimes terms activations data interchangeably. fully-connected layers special case convolution layers ilterh datah ilterw dataw deﬁne epoch pass training data. finally word performance ambiguous write terms speciﬁc metrics accuracy training time. deep neural network training comprised iterating phases forward backward propagation. forward phase batch data items taken training attempts classify them. comes backward phase present preliminaries later dissertation reasoning data volume communicate distributed training. equation show calculate total size weights convolution fully-connected layers combined. output channel number ﬁlters layer dictates number input channels layer words layer number input channels numf iltl− number output channels numf iltl. next equation expresses size activations produced layers combined. gradient updates deﬁne model parallelism case worker gets subset model parameters workers communicate exchanging data gradients exchanging activations note |∇w| |∇d|; words weights maximize training scalability goal select parallelism strategy requires lowest possible quantity communication servers. choice whether ideal data parallelism model parallelism depends strongly dnn’s architectural characteristics. commonly-used architectures speech recognition consist primarily fully-connected layers activations parameters spatial resolution typical batch sizes fully-connected models often similar quantity weights activations example observe table property holds true msft-speech architecture computer vision popular accurate models consist primarily convolution layers spatial resolution ﬁlters smaller resolution activations. convolutional models data parallelism typically preferable requires less communication |∇w| much smaller |∇d| typical batch sizes. notice computer vision dnns table property. firecaﬀe enable data parallelism across cluster gpus produces ample speedups training popular deep convolutional neural network architectures. illustrate data parallel approach figure conﬁguration gpus contain full model parameters. worker gets subset batch. gpus compute share weight gradients. gradients calculated locally added together using either parameter server reduction tree communication popular deep convolutional neural network architectures computer vision amenable data parallel training others. might na¨ıvely assume models parameters would produce higher classiﬁcation accuracy. evaluate assumption consider figure plot total size parameters bytes versus top- imagenet accuracy several popular architectures. observe network-in-network alexnet similar accuracy fewer parameters alexnet. likewise googlenet similar accuracy googlenet fewer parameters. wonder architectural choices googlenet fewer parameters alexnet vgg? answer twofold. first googlenet judicious ﬁlters spatial resolution many ﬁlters googlenet resolution instead larger. second alexnet fully-connected layer parameters googlenet smaller fully-connected layers fully-connected layers. summary models fewer parameters amenable scalability data parallel training still delivering high accuracy. therefore rest chapter focus eﬀorts accelerating training models fewer parameters maintaining high accuracy. data-parallel distributed training strategy requires communication among workers forward pass. backward pass traditional single-gpu implementation sums weight gradients images batch uses weight gradient update model. distribute backward pass compute cluster subset batch. strategy communicating gradients appoint node parameter server. remaining worker nodes assigned subset batch perform forward backward-propagation. backward pass workers send gradient updates parameter server. then parameter server computes gradients. finally parameter server sends summed gradients workers workers apply gradient updates local copies model. illustrate parameter server communication pattern figure words parameter server’s communication time scales linearly increase number workers; doubling number workers leads least communication time gradient update. conﬁrm experimentally figure parameter server experiments figure implemented fully synchronous parameter server following characteristics. parameter server arbitrarily-selected server cluster servers workers; parameter server worker servers identical hardware. batch workers send weight gradients parameter server parameter server computes parameter server sends back workers. number ways augment parameter server greater scalability. example single parameter server became bottleneck microsoft adam google distbelief deﬁned pool nodes collectively behave parameter server. bigger parameter server hierarchy gets looks like reduction tree. made wonder could achieve greater scalability implement gradient aggregation reduction tree? various common patterns communication parallel programs; among common patterns frequently occurring allreduce. pattern occurs worker produces data values must globally reduced produce single result value single value must element-wise vector addition since computation exactly allreduce communication pattern convenient existing library support operations. many possible implementations allreduce share property time taken perform operation scales number workers intuitively allreduce algorithms binomial reduction tree and/or butterﬂy communication patterns internally possible allreduce implementation strategies binomial reduction tree particularly easy reason theoretical level. rest section focus allreduce communication implemented reduction tree. practice base depends branching factor reduction tree basic idea straightforward parameter server communication overhead scales linearly reduction tree communication much eﬃcient scales logarithmically conﬁrm experimentally reduction trees scale eﬃciently parameter servers figure illustrating parameter servers reduction trees communicate weight figure gradients. ﬁgure show summing-up weight gradients. distribute weight gradient sums going back tree. figure comparing communication overhead parameter server reduction tree. network-in-network architecture worker contributes gradient updates. section evaluate firecaﬀe accelerate training cluster gpus. train googlenet network-in-network servers titan supercomputer leveraging firecaﬀe’s reduction tree data parallelism begin describing evaluation methodology analyze results. evaluate speed accuracy training publicly-available dataset. recent study azizpour applied dnns diﬀerent visual recognition challenge datasets including human attribute prediction ﬁne-grained ﬂower classiﬁcation indoor scene recognition accuracy obtained azizpour ranged scene recognition human attribute prediction. accuracy dnns machine learning algorithms depends highly speciﬁcs application dataset applied. thus researchers report improvements training speed accuracy proprietary datasets clear compare improvements related literature. example baidu amazon recently presented results accelerating training. amazon baidu reported training time numbers proprietary dataset it’s clear compare results related literature. contrast conduct evaluation publicly-available dataset imagenet-k contains million training images image labeled containing object categories. imagenet-k widely-studied dataset easily compare accuracy training speed scalability results studies data. report hyperparameter settings weight initialization momentum batch size learning rate. glorot shown seemingly-subtle hyperparameter settings weight initialization impact speed accuracy produced training. training network-in-network initialize weights gaussian distribution centered standard deviation convolution layers std=. layers. initialize bias terms constant value weight decay momentum settings derived caﬀe conﬁguration ﬁles released authors frustratingly google’s technical reports googlenet training details batch size momentum learning rate disclosed. fortunately guadarrama reproduced googlenet released details training protocols. train googlenet momentum=. weight decay=. xavier given architecture number strategies increase accuracy albeit substantial computational cost. strategy train multiple independent copies architecture diﬀerent random number generator seed initializing parameters. test time dnns used ensemble dnns test data test data item dnn’s classiﬁcation activations averaged. example using ensemble googlenet dnns szegedy achieved percentage-point accuracy improvement imagenet compared single googlenet baseline technique augment data adding deformations color variations training and/or testing focus chapter show speedup training single models compare reported baselines. hence avoid using exotic data augmentation ensembles multiple dnns. experiments resize images training time crop randomized oﬀset test time classify crop center image; settings also commonly used alexnet network-in-network architectures. measure speedups respect single-server baseline. order meaningfully measure much accelerated training adding gpus must representative baseline e.g. single gpu. reporting results begin considering time required train single report multi-gpu speedups respect single-gpu baseline. recent study microsoft reported training custom architecture cluster servers. sound impressive microsoft report time model would take train single server. could microsoft achieved speedup going server servers speedup could isn’t clear information provided microsoft’s paper. illustrates importance measuring speed scalable training systems respect single-server baseline. measure accuracy respect single-server baseline. experience hyperparameters learning rate batch size selected aggressively model converge quickly fall short state-of-art accuracy. therefore experiments train multi-gpu models reach single-gpu accuracy baseline; validates accelerate training without degrading accuracy. however cluster-scale multi-gpu training experiments baidu flickr training stopped prematurely dnns converge. leaves wondering whether baidu flickr multi-gpu training experiments would reproduced accuracy produced single gpu. avoid type confusion evaluate speed accuracy firecaﬀe training respect single-server/single-gpu baseline. using settings described krizhevsky alexnet achieves top- imagenet-k accuracy epochs training. epochs training also converges top- accuracy. training iteration time-consuming google krizhevsky developed scheme accelerating alexnet training using multiple gpus within single server krizhevsky’s strategy uses data parallelism convolution layers model parallelism fully-connected layers. show table krizhevsky achieves nearlinear acceleration gpus shown scale beyond single server. reasons don’t entirely understand krizhevsky’s accuracy drops percentage points multi-gpu training firecaﬀe scale training gpus scale communication time computation approximately equal. begin using learning rate batch size settings reported caﬀe conﬁguration released authors batch size initial learning rate reduce factor twice training. using conﬁguration reproduce single-gpu accuracy hours training gpus. ﬁxed number epochs increasing batch size reduces number times need communicate weight gradients thus reducing overall training time. mind train batch size increase batch size increase learning rate equal proportion. example batch size initialize learning rate conﬁguration train hours gpus. increasing batch size achieved substantial speedup came price reducing ﬁnal accuracy accuracy could regained batch size retaining substantial speed advantage training epochs. finally gpus achieve speedup single-gpu training. compared firecaﬀe cuda-convnet framework google runs single-server/multi-gpu platform multi-server distributed platform. addition cuda-convnet google developed tensorflow framework also supports single-server/multi-gpu training distributed multi-server training. thus google released training speed results multi-gpu tensorflow. twitter also experimented scaling training gpus speed accuracy results released. tencent theano facebook published alexnet singleserver/multi-gpu training times slower google firecaﬀe seen literature training alexnet/nin-scale models multi-server/multi-gpu setting. gpus firecaﬀe least faster train alexnet/nin-scale models aforementioned results. ultra-deep models googlenet produce higher accuracy present even bigger challenge terms training time. internally google trained googlenet cluster servers reported time required complete training fortunately guadarrama reproduced googlenet caﬀe released googlenet caﬀe conﬁguration ﬁles guadarrama trained epochs using batch size initial learning rate settings single-gpu googlenet training experiments. instead occasionally reducing learning rate guadarrama used polynomial learning rate learning rate gradually reduced every iteration training. speciﬁcally iter )power power googlenet training runs. running caﬀe singlegpu googlenet takes days train imagenet-k producing top- accuracy top- accuracy. slightly lower top- single-model accuracy reported google interesting whether open-source caﬀe community eventually able reproduce surpass google’s googlenet accuracy. here single-gpu caﬀe googlenet accuracy baseline reproduce rapidly cluster gpus. consider accelerate googlenet training using firecaﬀe. initially tried googlenet batch size cluster wasn’t enough work batch keep cluster saturated. learned earlier chapter larger batch sizes lead less frequent communication therefore enable scalability distributed setting. modifying batch size breuel krizhevsky found choice learning rate crucial order preserve high accuracy. trained separate versions impact learning rate accuracy achieved googlenet training. figure separate training trained scratch. experiments lr=. achieved highest accuracy lr=. learn beyond random-guess level accuracy. batch size lr=. lr=. googlenet didn’t ever learn anything beyond random-chance accuracy test set. using lr=. produced top- imagenet-k accuracy lr=. produced finally declare victory lr=. achieved accuracy matches accuracy baseline used batch size illustrate outcome learning rate experiments figure batch size ﬁxed number epochs firecaﬀe gpus train googlenet faster single gpu. move batch size lr=. batch size lr=. googlenet takes epochs converge absolute training speedup show results table words firecaﬀe train googlenet hours gpus compared days single gpu. finally gpus achieve speedup single-gpu googlenet training matching single-gpu accuracy. discussed related work throughout chapter provide brief survey additional techniques accelerate deep neural network training. several following techniques could used concert firecaﬀe accelerate training. architectures discussed chapter ﬂoating-point operations forward backward propagation reside convolution layers accelerating convolution getting gpu. recently number techniques developed accelerate convolution gpus. unlike cpus nvidia gpus inverted memory hierarchy register larger cache. volkov demmel pioneered communication-avoiding strategy accelerate matrix multiplication gpus staging much data possible registers maximizing data reuse. iandola extended communication-avoiding techniques accelerate convolution; cudnn maxdnn extended techniques accelerate convolution. boda framework extended techniques execute eﬃciently hardware produced companies nvidia. firecaﬀe coupled current future hardware convolution libraries speedups. reducing quantity data communicated batch useful increase speed scalability training. inherent tradeoﬀ here gradients aggressively quantized training speed goes model’s accuracy compared nonquantized baseline. firecaﬀe uses -bit ﬂoating-point values weight gradients jeﬀrey dean stated recent keynote speech google often uses -bit ﬂoating-point values communication servers training along lines wawrzynek used -bit weights -bit activations distributed neural network training going step further seide used -bit gradients backpropagation albeit drop accuracy trained model finally related strategy reduce communication servers discard gradients whose numerical values fall certain threshold. amazon presented thresholding strategy recent paper scaling training speech recognition however amazon’s evaluation uses proprietary dataset clear type thresholding impacts accuracy compared well-understood baseline. section discussed strategies compressing quantizing data communicate distributed training. also series studies applying dimensionality reduction dnns trained. jaderberg zhang compress weights models albeit substantial reduction model’s classiﬁcation accuracy. combination pruning quantization huﬀman encoding compress weights pretrained models reduction accuracy. thus algorithms able accelerate dnns test time. long training times impose severe limitation progress deep neural network research productization. accelerating training several beneﬁts. first faster training enables models trained ever-increasing dataset sizes tractable amount time. accelerating training also enables product teams bring dnn-based products market rapidly. finally number compelling use-cases real-time training robot selflearning. compelling applications focus problem accelerating training work culminated firecaﬀe distributed training system. approach accelerating training scale three pillars. first select network hardware achieves high bandwidth servers inﬁniband cray interconnects ideal this. second selecting communication algorithm reduction trees eﬃcient scalable traditional parameter server approach. third optionally increase batch size reduce total quantity communication training identify hyperparameters allow reproduce small-batch accuracy training large batch sizes. three pillars helped achieve near-linear speedup number leading deep neural network architectures. particular achieved speedup training speedup googlenet training cluster. design choices speciﬁed human architect. architecture delivers speciﬁc tradeoﬀs terms accuracy computational complexity energy-eﬃciency model size metrics. true handful commonly-used computational primitives many ways arranging primitives deep neural network architecture. composition total number layers selected architect considering recent architectures layers residual networks exponentially large number permutations diﬀerent types layers composed. further within layer number dimensions selected architect e.g. convolution layer number ﬁlters spatial resolution ﬁlters user-selectable. thus design space architectures enormous. accuracy controlled architecture training protocol training data quantity computation image controlled entirely architecture model size i.e. number parameters model controlled entirely terms accuracy computation speed and/or thus given speciﬁc aggressive goals achieving goals requires comprehensive understanding architectural space. chapter organized follows. section describe building blocks modern cnns. section explain geometrical properties individual layers. stacking multiple layers form end-to-end leads certain geometrical properties passed layer next. section provide intuitive mental model understanding changing dimensions layer impact overall quantity computation cnn. explain ways modifying dimensions layer impacts dimensions downstream layers sections clearly cnns comprise large design space large provide intuition section chapter focuses architectures number additional design choices inﬂuence time accuracy level achieved training choice initialize model parameters prior training model solver approach used train model. summarize present literature design choices section conclude section forty years convolution used widely image processing computer vision. example process obtaining image camera involves convolution using ﬁlter called bayer mask likewise straightforward ways detect edges sobel filter consists entirely convolving image particular ﬁlter canny edge detection also involves convolving image ﬁlter typically ﬁlter numerical values ﬁlter based gaussian distribution examples mind surprise convolution ingredient applying neural networks computer vision problems. convolutional neural networks apply many layers convolution e.g. classiﬁcation conv layer)). previous examples ﬁlters numerical values hand-selected engineers convolutional neural networks learn ﬁlters’ numerical values training data. recall high-level objectives dissertation include decreasing number parameters cnns accelerating training decreasing computational requirements. want decrease computational requirements cnns must ﬁrst understand calculate amount computation performed individual layers cnns. cnns googlenet arithmetic operations occur convolution layers understanding dimensionality computational overhead convolution layers particularly important. cnns convolution layers main data structures parameters input data parameters contained convolution ﬁlters numerical values parameters learned automatically training. input data output previous layer; ﬁrst convolution layer network consists data samples training testing set. illustrate figure describe detail below. parameters convolution layer dimensions ilterh ilterw numf ilt. ﬁlters spatial resolution ilterhxf ilterw e.g. ﬁlter multiple channels example input channels ﬁlter finally usually multiple convolution ﬁlters ﬁlter learns diﬀerent pattern identify diﬃcult humans reason dimensions recommend taking extra care avoid forgetting every convolution layer multiple ﬁlters ﬁlters multiple channels total quantity parameters input data ﬁrst convolution layer. training batched stochastic gradient descent dimensions datah dataw batch. batch size user-selected batch consistent convolution layers cnn. ﬁrst layer input data batch data samples train datah dataw height width number channels training images. input data later convolution layers. consider layer input data output previous layer li−. thus height width datah dataw determined output previous layer. number channels input data determined number ﬁlters li−. finally batch size user-selected constant layers cnn. convolutional neural networks common take image input produce vector output. indeed default cnns googlenet alexnet squeezenet take image input produce vector classiﬁcations output. convert data spatial resolution data structure without spatial resolution answer quite simple gradually downsample downsample once; rather deep network typically downsample every layers. figure pooling stride= preserves input dimensionality here visualizing height width activation plane height width ﬁlters show batch size number channels. describe typical approach downsampling layers must ﬁrst introduce term stride. discussed several dimensional terms section number ﬁlters height width ﬁlters stride interval ﬁlters applied input data. stride means place convolution ﬁlter every location input data producing activation height width equivalent input di−. illustrate height width activations stride= case figure however stride= place convolution ﬁlter every location thus convolution layer stride= produces activation roughly height width input data roughly fewer pixels channel di−. note architect could select diﬀerent strides height width dimensions however dissertation always simply term stride denote horizontal vertical stride layer. pooling. idea using strided operations limited convolution. usually think sliding-window operations applied every window valid compute sliding-window computation stride integer greater equal convolutional neural networks alexnet squeezenet etc.) common technique apply strided pooling. commonly-used type pooling max-pooling figure pooling stride= downsamples height width factor here visualizing height width activation plane height width ﬁlters show batch size number channels. pooling example intuition applies convolution types layers. neighborhood take maximum-intensity pixel channel. strategy average-pooling simply consists taking average pixel value channel neighborhood. architecting pooling layers cnns sensible deﬁne neighborhood size smaller stride otherwise data disregarded downsampling. beyond average-pooling max-pooling computational photography researchers devised much complex mechanisms downsampling content-aware seam-carving however downsampling algorithms easily diﬀerentiable. backpropagation-based training applicable necessary downsampling mechanisms derivative computed. nevertheless applying alternative downsampling methods cnns would interesting area future work. finally presented typical strategies allow cnns gradually downsample image vector cases goal actually produce activation instead single vector. case semantic segmentation problem assigning category label every pixel image therefore output ought pixel map. indeed researchers addressed problem cnns downsample sometimes called fully convolutional networks finally common stride stride literally series convolution layers nothing else model would simply linear system. linear systems fairly constrained subset achieved deep networks. express non-linear functions common introduce nonlinearity output convolution layer cnn. nonlinearity functions sometimes called activation functions. note that diagrams cnns activation function often omitted brevity. also implementations frameworks caﬀe activation function implemented layer cnn. historical recent literature number activation functions proposed applied cnns. attempt provide exhaustive survey activation functions provide examples below quantity computation required converge. given changing per-layer activation functions change number epochs training required converge particular level accuracy. accuracy achieved training. given changing per-layer activation functions change accuracy achieved training. pathological cases certain choices activation function lead learning anything remaining accuracy level. ramiﬁcations activation function almost entirely constrained training phase little impact computational requirements inference. quantity computation required calculate typical activation function max) extremely trivial compared running typical convolution layer data. therefore quantity computation cnns dominated layers convolution activation functions typically represent much less total quantity computation. training loss function sometimes called objective function typically applied cnn. ideal choice loss function highly dependent problem hand. without going deep mathematical details main questions architect must beginning search right loss function classiﬁcation regression? want train solve classiﬁcationregression-style problem? classiﬁcation formulation typical there labels every data sample needs assigned labels. image classiﬁcation example classiﬁcation objective functions sensible. hand regression formulation typical every every data sample sample needs analyzed output ﬂoating-point number loss functions logistic regression applied either classiﬁcation regression loss functions e.g. softmax speciﬁc classiﬁcation. note many loss functions used cnns designed speciﬁcally training rather loss functions developed areas mathematical optimization appropriated researchers. examples classiﬁcation regression loss functions found loss function multiple loss functions? it’s typical optimize particular loss function it’s possible multiple loss functions. common example optimize classify objects simultaneously optimize localize objects taken together loss functions train perform object detection. machine learning loss function often formulated achieving zero loss training done simply memorizing training set. problematic real world tends diverse training set. case machine-learned model memorizes training delivers poor accuracy test called overﬁtting. regularization refers family techniques used combat overﬁtting. roughly speaking method regularizer makes diﬃcult model memorize training set. cases regularization applied training inference. reality haven’t given enough information answer question. need know number channels input data discussed section layer number input channels determined number ﬁlters previous layer ﬁrst layer special case number input channels chi= determined number channels input data cnn. keep math simple ﬁrst layer numf ilti− shorthand number channels input data cnn. turn attention back present example. previous layer ﬁlters input channels numf ilti ilterwi ilterhi consider convolution layer ﬁlters size ilterhi ilterwi stridei= zero-padding note stride applies height width dimensions. also input data datahi− datawi− chi− finally assume batch size dimensionality output data produced discussed calculate dimensionality quantity parameters activations convolution layer. given dimensionality parameters dimensionality activations convolution layer produces output enough information calculate quantity computation layer. discussed previous sections architect charged deﬁning certain dimensions convolution layer dimensions include height width ﬁlters number ﬁlters previous layer number ﬁlters current layer height width output activations using dimensions compute number arithmetic operations convolution layer follows concentrated calculating quantity computation required inference. inference requires computing forward pass only training requires computing forward backward pass. backward pass requires computation forward quantity computation entire forward pass. therefore image data sample training requires three times quantity computation required inference words determine quantity computation data sample training procedure simply calculate equation multiply result much research conducted aspects cnns solvers adding residual connections skip layers cnns rethinking initialize parameters prior training cnns areas research impact time required train accuracy achieved. however topics aﬀect inference speed comes deploy real application? part solvers parameter initialization relevant training. residual connections appeared recent literature negligible eﬀect inference speed account tiny portion overall quantity computation. factors inﬂuence speed quantity computation required inference time? simply architecture dimensionality input data wholly responsible determining quantity computation required inference. knowledge papers documents provide practical intuition layer dimensions impact overall computational footprint. remedy this codiﬁed mental model understanding changing dimensions impact cnn’s quantity computation inference. given number ways modify interest maximizing accuracy minimizing quantity computation optimizing metrics. many modiﬁcations make modiﬁcations impact quantity computation tend deal number channels ﬁlters layers height width activations passed layer next. ways modifying include initially seem daunting understand types modiﬁcations aﬀect quantity computation required perform inference cnn. fortunately modiﬁcations distilled straightforward mental model. architectural modiﬁcations induce either local change global change dimensions deﬁne terms follows. local change. deﬁne local change case modiﬁcation layer aﬀects dimensionality layer possibly li+. examples local change include changing number channels input data changing number ﬁlters convolution layer changing number categories classiﬁed global change. contrast local change deﬁne global change case modiﬁcation layer aﬀects dimensionality layer subsequent layers examples global change include changing resolution input images changing strides convolution pooling layer modiﬁcations considered described either local change global change. following sections describe detail type modiﬁcation impacts total quantity computation cnn. ﬁrst state ﬁndings front begin present examples changing dimensions impacts computational footprint begin presenting initial architecture. selected networkin-network initial architecture. default conﬁguration takes image input predicts image’s category output. terms accuracy competitive highest-accuracy submissions imagenet image classiﬁcation challenges. highest accuracy submission imagenet googlenet draws inspiration architecture. present overview architecture figure provide detailed dimensions architecture table observe convolution layers. convolution layers multiple ﬁlters ﬁlters range xxchannels xxchannels. layer ﬁlters size. beginning gradually downsamples height width activations downsampling accomplished maxaverage-pooling well convolution stride cnns alexnet downsampling xxchannels done partway last convolution layers operate without spatial resolution. however every convolution layer spatial resolution greater ﬁnal convolution layer global average pooling means takes average channel’s grid downsample xxchannels. design choice nice property regardless input image height width able output output vector size xxchannels. relu performed convolution layer. training dropout performed convolution layer. batch normalization used nin. initial layer provides batch -channel images. pooling layer stride stride mean discussed section number ﬁlters convolution layer deﬁnes number input channels convolution layer li+. table avoid redundancy simply list number output channels layer. observe table inference batch size images produces output activations ﬁlters requires computation. following sections show modiﬁcations impact quantity activations quantity ﬁlters quantity computation required inference. computer vision algorithms often applied color images. color images typically represented grid pixels pixel channels green blue short. cnns alexnet ﬁlter ﬁrst convolution layer height width plus channels accept images. channels rgb-d computer vision. sensors lidar provide depth maps essentially grid pixels pixel represents depth sensor. using depth maps common operate -channel rgb-d images fourth channel depth. perform visual recognition rgb-d images approach conﬁgure channels ﬁrst convolution layer. researchers represent rgb-d images higher numbers channels. example gupta featurize depth multiple channels information pixel horizontal disparity height ground surface normal channels used input cnn. channels video recognition. videos frame typically represented image. video frames temporal locality; enabling visual recognition algorithms leverage locality lead higher recognition accuracy. approach recurrent models long-short term memory networks keep track temporal relationship frames. lstms applied video recognition problems identifying actions videos automatically answering questions images assigning captions images alternative approach simply concatenate multiple video frames together channel dimension. example system identify human activities/actions performed trecvid video dataset concatenated groups video frames input case cnn’s conv layer input channels. aforementioned applications oﬀ-the-shelf architecture alexnet modiﬁed target application diﬀerent number channels input data. important consideration changes impact quantity computation performed cnn? walk straightforward case changing number input channels take increase number input channels factor think concatenating video frames together channel dimension feeding single input data item. increase number channels input data factor quantity computation inference change? table show impact modiﬁcation architecture quantity computation. observe that conv layer quantity computation increases factor layers conv quantity computation unchanged modiﬁcation. looking end-to-end perspective modiﬁcation leads computation overall. increase number channels input data factor time required train model change? table observe change requires computation quantity parameters remains essentially unchanged. data-parallel training approach described chapter quantity communication sent received server determined quantity parameters cnn. therefore answer question stated above increase number channels input data training ﬁxed number processors slightly slower. however ratio communication increased means scale modiﬁed cnn’s training problem processors nearly regain time-to-solution achievable original model. finally note quantity output aﬀect communication requirements data-parallel distributed training therefore directly impact scalability speed training multiple processors. turn attention number ﬁlters layer implications number ﬁlters cnn’s computational properties. sections discussed relationship channels ﬁlters cnns. work example change number ﬁlters convolution layer within observe impact quantity computation performed whole. network-in-network conv layer ﬁlters. observe table conv output channels. isn’t coincidence; number ﬁlters layer dictates number output channels produced layer. next since conv input channels ﬁlter conv channels. also happens architects chose ﬁlters conv ﬁlters channels. conv layer also input channels output channels. conv layer input channels architects decided ﬁlters conv layer. architects also decided spatial resolution ﬁlters conv. putting together conv layer ﬁlters size learned observe three properties number ﬁlters layer choice number ﬁlters layers comprises huge design space. mind worthwhile understand number ﬁlters layer impacts overall computational overhead cnn. explore this begin architecture increase number ﬁlters conv layer explore impacts computational characteristics cnn. increase number ﬁlters conv factor quantity computation inference change? within conv quantity computation increases factor calculate using equation also discussed earlier section number ﬁlters layer deﬁnes number input channels layer li+. therefore ﬁlter conv channels leading computation conv. however layers conv conv quantity computation unchanged. show table overall quantity computation increases result increasing number ﬁlters conv since increasing number ﬁlters layer impacts quantity computation convolution layers describe local change architecture. increase number ﬁlters conv factor time required train model change? modiﬁcation negligible increase computation model size training model would slightly time-consuming. since communication computation increased amount problem’s distributed scalability remains unchanged. cnns ﬁlter spatial resolution size range height width input data ﬁlters applied. architects free select ﬁlters size range. note that could design every ﬁlter unique size it’s common ﬁlters within layer spatial resolution. chapter discuss choice ﬁlter resolution aﬀects accuracy. though focus understanding modifying ﬁlter resolution aﬀects computational requirements. architecture ﬁlters range spatial resolution. chapter learn choice ﬁlter resolution impact accuracy. though focus understanding changing resolution ﬁlters aﬀects computational requirements. keeping running example increasing dimension next consider increasing height width ﬁlters layer aﬀects cnns computational requirements. increase ﬁlter resolution conv factor increase amount zero-padding border pixel border pixels quantity computation inference change? table show modiﬁcation architecture impacts quantity computation. observe table that conv layer quantity computation increases factor following convolution layer notice quantity computation nearly unchanged. looking end-to-end perspective modiﬁcation leads computation overall. finally notice quantity computation conv conv layers decreased slightly compared original architecture output activations conv conv conv shrunk slightly take moment explain this. modest change quantity computation output activation resolution conv conv choice zero-padding modiﬁed conv layer. ﬁlters stride border pixel zero-padding leads input data output activations height width. similarly ﬁlters stride border pixels zeropadding leads input data output activations height width. however ﬁlters getting output resolution equivalent input resolution would require padding technique pixels padding left borders pixels padding bottom right borders. typically frameworks tensorflow natively support unequal padding diﬀerent sides example section used border pixels zero-padding sides modiﬁed conv layer. slightly smaller output activations conv conv conv leading slight drop quantity computation conv conv. increase ﬁlter resolution conv factor increase amount zero-padding border pixel border pixels time required train model change? modiﬁcation increase computation model size training model would slightly time-consuming. since communication computation increased amount problem’s distributed scalability remains unchanged. applications cnns include single-class multi-class classiﬁcation. output layer number ﬁlters equal number categories classiﬁed full-image classiﬁcation localized recognition/detection semantic segmentation applications. cases multi-class classiﬁcation aspect problem localized detection classiﬁes regions/windows images terms list object categories semantic segmentation classiﬁes pixels terms list visual categories. applications number ﬁlters ﬁnal layer equal number classes training set. authors allocated ﬁlters conv imagenet-k categories training test data. next consider case would like apply visual category recognition task categories categories images. increase number categories dataset factor quantity computation inference change? table show impact modiﬁcation architecture quantity computation. ﬁnal convolution layer ﬁlter category dataset. number ﬁlters conv increased leading computation conv. however dimensionality quantity computation layers remains unchanged. looking end-to-end perspective modiﬁcation leads computation overall. increase number categories dataset factor time required train model change? table observe change leads larger model size computation. therefore ratio communication decreased tells less scalable original model. words model require computation original model runtime dominated communication smaller number processors case original model. reasons using distributed data-parallel training model slower train original model. previous sections considered number local changes architectures. terminology local change modifying layer aﬀects dimensions layer cases layer li+. dimensions layers unaﬀected local change layer common pattern among local changes considered changes involve modifying dimensions layer’s ﬁlters channels. reason this number ﬁlters layer deﬁnes number input channels layer number ﬁlters bearing number input channels layers previous section changing various dimensions ﬁlters layer change computational cost layer cases layer li+. however changing dimensions activations produced layer impact computational cost downstream layers. call global changes architectures. following discuss examples global changes cnns. discussed section settings strides convolution pooling layer aﬀect dimensions output activations produced work example provide intuition phenomenon. begin original architecture notice pool stride consider case modify architecture removing pool max-pooling layer. quantity computation change modiﬁcation? first it’s important note computational overhead pooling layer roughly equivalent convolution layer ﬁlter quite amount overhead compared typical convolution layers tens hundreds ﬁlters. focusing quantity computation convolutional layers impacted removal pool layer. remove pool layer quantity computation inference change? table show impact modiﬁcation architecture quantity computation. remove pool layer height width input data conv decreases height width compared original network described table increase height width activations propagates subsequent layers conv conv conv beyond computation total less computation compared original architecture. computation increased ratio communication also increased therefore scale modiﬁed cnn’s training problem processors nearly regain time-to-solution achievable original model. cnn-based systems conventional wisdom higher resolution imagery tends allow deliver higher accuracy further applying cnns extremely complex scenes high resolution imagery especially critical achieving high accuracy. resolution input images aﬀect quantity computation cnn. apply larger input images. increase height width factor number input pixels increases double height width images used input quantity computation inference change? table show impact modiﬁcation architecture quantity computation. layers height width input data doubles total approximately input data layer. precisely follow equation determine output activation dimensions layer rounding leads activations slightly larger original version. convolution layer leads convolution ﬁlter applied roughly locations. overall leads computation running input images instead input images. double height width images used input time required train model change? table observe change leads change model size computation. computation increased ratio communication also increased therefore scale modiﬁed cnn’s training problem processors nearly regain time-to-solution achievable original model. previous examples modiﬁed height width input data ﬁrst layer modiﬁed height width activations produced layer li≥. observed modiﬁcations result changes dimensions quantity computation layer layer also changes downstream layers refer changes modifying results changes dimensions downstream layers global changes. sections described seven diﬀerent architectures. many architectures there? work concrete example number unique architectures exist particular sub-space design space. imagenet image-classiﬁcation competitions winning approaches consisted cnns fewer layers. imagenet competition winners vgg- -layer model googlenet model layers. paper vgg- simonyan zisserman presented vgg- architecture also explored variants vgg- architecture rest section architectural space deﬁned simonyan zisserman basis discussion large narrow corner overall architectural space architectures presented simonyan zisserman layers models layers andln share enumerate dimensions. remaining convolution layers following dimensional conﬁgurations many unique cnns create convolution layers aforementioned dimensions? mathematically layers layer diﬀerent dimensions. works billion unique architectures section shown that even given narrow range options type dimensions layers billions unique combinations layers. however truth section covered narrow corner design space. evidence design space much larger knowledge known limits upper-bounds several dimensions cnns size input data number layers number types layers invented. considering factors believe that practical purposes design space cnns considered domain text audio vision) ability obtain appropriate labeled data train models solve problem architect must carefully consider explore number design choices including following architecture dimensions design principles architectures discussed relatively little detail literature. thousands computer vision papers appeared last years relatively papers propose substantially architecture even rare papers propose evaluate many architectures. meanwhile choice architecture directly eﬀects quantity computation speed achieved training inference. speciﬁc ways architecture impacts computational footprint widely discussed literature. model parameter initialization solver approach model parameter initialization solver approaches widely discussed literature. optimization ﬁeld long history wide following many optimization experts recently refocused careers optimization methods neural networks. choice initialize model parameters choice solver approach impacts number epochs required achieve given level accuracy. thus impact quantity computation time required train models. however impact quantity computation achievable speed inference. summarize eﬀects table given architecture limited coverage literature broadest impact cnn’s computational requirements focused discussions chapter however completeness summarize current literature model parameter initialization solver approaches direct interested reader resources reading. prior applying solver stochastic gradient descent optimize cnn’s model parameters model parameters must ﬁrst initialized. choice initialize parameters left architect. said research community developed number widely-used approaches initializing parameters. straightforward widely-used parameterinitialization method selecting parameter’s value gaussian distribution. approach allows architect select standard deviation gaussian distribution. optionally diﬀerent standard deviation selected layer cnn. xavier parameter initialization. xavier parameter initialization method developed xavier glorot works follows. given layer xavier method deﬁnes fan-in number input channels layer. initialize parameters layer xavier method samples values uniform distribution values normalized based fan-in. xavier normalizes normal distribution inversely fan-in input channels layer smaller typical initial parameter value msra parameter initialization. kaiming microsoft research asia drew inspiration xavier parameter initialization develop parameter initialization method. xavier parameter initialization looks fan-in msra method takes account fan-in well fan-out normalize random distribution initial parameter values sampled. addition choice normalization diﬀerence msra xavier msra uses gaussian distribution rather normal distribution. initialization transfer learning. also possible initialize cnn’s parameters using parameter values previous training exercise. approach described section called transfer learning. parameters transfered supervised unsupervised learning approach. however transfer learning somebody train scratch reminder parameters hyperparameters. discussed approaches initializing parameters cnn. parameters values automatically learned training proceedure. however hyperparameters typically human charge architecting cnn. selected architecture initialized parameters next step train cnn’s parameters perform tasks identifying objects images. developing optimization approaches solvers train parameters extremely active area research. present current approaches training cnns. present literature stochastic gradient descent standard optimization methodology training cnns. typically applied cnns operates taking subset training data attempting classify computing gradients summarize mistakes made attempting classify data. backprogation applied train layer avoid mistake next time sees similar data sample. goal converging higher accuracy converging given accuracy level less computation number variants developed. summarize sgd-based methods follows. architects responsible several design decisions including choice architecture choice initialize model parameters choice solver approach training model. parameter initialization solver approaches widely covered literature useful guide designing architectures. remedy this focused chapter providing intuition dimensions layers dimensions impact characteristics end-to-end training inference. summarize intuition lessons modiﬁcations number channels number ﬁlters spatial resolution ﬁlters yield local changes cnn’s dimensions. local change layer impacts dimension layer cases li+. much recent research deep convolutional neural networks focused increasing accuracy computer vision datasets. given accuracy level typically exist multiple architectures achieve accuracy level. given equivalent accuracy architecture fewer parameters several advantages scalability distributed training. distributed data-parallel training communication overhead directly proportional number parameters model short small models train faster requiring less communication. panies tesla periodically copy models servers customers’ cars. practice often referred over-the-air update. consumer reports found safety tesla’s autopilot semi-autonomous driving functionality incrementally improved recent over-the-air updates however over-the-air updates today’s typical cnn/dnn models require large data transfers. alexnet would require communication server car. smaller models require less communication making frequent updates feasible. chip memory oﬀ-chip memory storage. inference suﬃciently small model could stored directly fpga instead bottlenecked memory bandwidth video frames stream fpga real time. further deploying cnns several advantages smaller architectures. mind focus directly problem identifying architecture fewer parameters equivalent accuracy compared well-known model. identiﬁed architecture call squeezenet. addition present attempt disciplined approach searching design space novel architectures. section review related work. then sections describe evaluate squeezenet architecture. that turn attention understanding architectural design choices impact model size accuracy. gain understanding exploring design space squeezenet-like architectures. section design space exploration microarchitecture deﬁne organization dimensionality individual layers modules. section design space exploration macroarchitecture deﬁne high-level organization layers cnn. finally conclude section short sections useful researchers well practitioners simply want apply squeezenet application. remaining sections aimed advanced researchers intend design architectures. overarching goal work identify model parameters preserving accuracy. address problem sensible approach take existing model compress lossy fashion. fact research community emerged around topic model compression several approaches reported. fairly straightforward approach denton apply singular value decomposition pretrained model developed network pruning begins pretrained model replaces parameters certain threshold zeros form sparse matrix ﬁnally performs iterations training sparse recently extended work combining network pruning quantization huﬀman encoding create approach called deep compression designed hardware accelerator called operates directly compressed model achieving substantial speedups energy savings. convolutions used artiﬁcial neural networks least years; lecun helped popularize cnns digit recognition applications late neural networks convolution ﬁlters typically height width channels dimensions. applied images ﬁlters typically channels ﬁrst layer subsequent layer ﬁlters number channels ﬁlters. early work lecun uses xxchannels ﬁlters recent architectures extensively ﬁlters. models network-in-network googlenet family architectures ﬁlters layers. trend designing deep cnns becomes cumbersome manually select ﬁlter dimensions layer. address this various higher level building blocks modules comprised multiple convolution layers speciﬁc ﬁxed organization proposed. example googlenet papers propose inception modules comprised number diﬀerent dimensionalities ﬁlters usually including plus sometimes sometimes many modules combined perhaps additional ad-hoc layers form complete network. term microarchitecture refer particular organization dimensions individual modules. microarchitecture refers individual layers modules deﬁne macroarchitecture system-level organization multiple modules end-to-end architecture. perhaps mostly widely studied macroarchitecture topic recent literature impact depth networks. simoyan zisserman proposed family cnns layers reported deeper networks produce higher accuracy imagenet-k dataset proposed deeper cnns layers deliver even higher imagenet accuracy choice connections across multiple layers modules emerging area macroarchitectural research. residual networks highway networks propose connections skip multiple layers example additively connecting activations layer activations layer refer connections bypass connections. authors resnet provide comparison -layer withbypass connections; adding bypass connections delivers percentage-point improvement top- imagenet accuracy. neural networks large design space numerous options microarchitectures macroarchitectures solvers hyperparameters. seems natural community would want gain intuition factors impact nn’s accuracy much work design space exploration focused developing automated approaches ﬁnding architectures deliver higher accuracy. automated approaches include bayesian optimization simulated annealing randomized search genetic algorithms credit papers provides case proposed approach produces architecture achieves higher accuracy compared representative baseline. however papers make attempt provide intuition shape design space. later chapter eschew automated approaches instead refactor cnns principled comparisons investigate architectural decisions inﬂuence model size accuracy. following sections ﬁrst propose evaluate small squeezenet architecture. next show model compression applied make squeezenet even smaller. then explore impact design choices microarchitecture macroarchitecture squeezenetlike architectures. section begin outlining design strategies architectures parameters. then introduce fire module building block build architectures. finally design strategies construct squeezenet comprised mainly fire modules. overarching objective chapter identify architectures parameters maintaining competitive accuracy. achieve this employ three main strategies designing architectures strategy replace ﬁlters ﬁlters. given budget certain number convolution ﬁlters choose make majority ﬁlters since ﬁlter fewer parameters ﬁlter. strategy decrease number input channels ﬁlters. consider convolution layer comprised entirely ﬁlters. learned section total quantity parameters layer maintain small total number parameters important decrease number ﬁlters also decrease number input channels ﬁlters. decrease number input channels ﬁlters using squeeze layers describe next section. strategy downsample late network convolution layers large activation maps. convolutional network convolution layer produces output activation spatial resolution least often much larger height width activation maps controlled size input data choice layers downsample architecture. commonly downsampling engineered architectures setting convolution pooling layers early layers network large strides layers small activation maps. conversely layers network stride strides greater concentrated toward network many layers network large activation maps. intuition large activation maps lead higher classiﬁcation accuracy else held equal. indeed applied delayed downsampling four diﬀerent architectures case delayed downsampling higher classiﬁcation accuracy strategies judiciously decreasing quantity parameters attempting preserve accuracy. strategy maximizing accuracy limited budget parameters. next describe fire module building block architectures enables successfully employ strategies deﬁne fire module follows. fire module comprised squeeze convolution layer feeding expand layer convolution ﬁlters; illustrate figure liberal ﬁlters fire modules application strategy section expose three tunable dimensions fire module fire module number ﬁlters squeeze layer number ﬁlters expand layer number ﬁlters expand layer. fire modules less squeeze layer helps limit number input channels ﬁlters strategy section describe squeezenet architecture. illustrate figure squeezenet begins standalone convolution layer followed fire modules ending ﬁnal conv layer gradually increase number ﬁlters module beginning network. squeezenet performs max-pooling stride layers conv conv; relatively late placements pooling strategy section present full squeezenet architecture table brevity omitted number details design choices squeezenet table figure provide design choices following. intuition behind choices found papers cited below. learning rate throughout training described details training protocol please refer caﬀe conﬁguration ﬁles located here https//github.com/deepscale/squeezenet. ﬁlter resolutions around this implement expand layer separate convolution layers layer ﬁlters layer ﬁlters. then concatenate outputs layers together channel dimension. numerically equivalent implementing layer contains ﬁlters. released squeezenet conﬁguration ﬁles format deﬁned caﬀe framework. however addition caﬀe several frameworks emerged including mxnet chainer keras torch native format representing architecture. said libraries underlying computational back-ends cudnn mkl-dnn research community ported squeezenet architecture compatibility number software frameworks turn attention evaluating squeezenet. model compression papers reviewed section goal compress alexnet model trained classify images using imagenet dataset. therefore alexnet associated model compression results basis comparison evaluating squeezenet. table review squeezenet context recent model compression results. svd-based approach able compress pretrained alexnet model factor diminishing top- accuracy network pruning achieves reduction model size maintaining baseline top- top- accuracy imagenet deep compression achieves reduction model size still maintaining baseline accuracy level squeezenet achieve reduction model size compared alexnet meeting exceeding top- top- accuracy alexnet. summarize aforementioned results table smaller model size best eﬀorts model compression community maintaining exceeding baseline accuracy. open question been small models amenable compression small models need representational power aﬀorded dense ﬂoating-point values? applied deep compression squeezenet using addition results demonstrate deep compression works well architectures many parameters also able compress already compact fully convolutional squeezenet architecture. using -bit quantization finally note deep compression uses codebook part scheme quantizing parameters -bits precision. therefore commodity processors trivial achieve speedup -bit quantization using scheme developed deep compression. however developed custom hardware eﬃcient inference engine compute codebook-quantized cnns eﬃciently addition months since released squeezenet gysel developed strategy called ristretto linearly quantizing squeezenet bits speciﬁcally ristretto computation bits stores parameters activations -bit data types. using ristretto strategy -bit computation squeezenet inference gysel observed less percentage-point drop accuracy using -bit instead -bit data types. chapter proposed architectural design strategies small models followed principles create squeezenet discovered squeezenet smaller alexnet equivalent accuracy. however squeezenet models reside broad largely unexplored design space architectures. sections explore several aspects design space. divide architectural exploration main topics microarchitectural exploration macroarchitectural exploration compared ristretto deep compression enables aggressive compression drop accuracy. hopeful future reﬁnements ristretto approach incorporate sparsity matching dense ﬂoating-point model’s accuracy enabling direct computation sparse quantized domain. squeezenet fire module three dimensional hyperparameters deﬁned section squeezenet fire modules total dimensional hyperparameters. broad sweeps design space squeezenet-like architectures deﬁne following higher level metaparameters control dimensions fire modules cnn. deﬁne basee number expand ﬁlters ﬁrst fire module cnn. every fire modules increase number expand ﬁlters incre. words fire module number expand ﬁlters basee shared fire modules) percentage expand ﬁlters words pctx finally deﬁne number ﬁlters squeeze layer fire module using metaparameter called section proposed decreasing number parameters using squeeze layers decrease number input channels seen ﬁlters. deﬁned squeeze ratio ratio number ﬁlters squeeze layers number ﬁlters expand layers. design experiment investigate eﬀect squeeze ratio model size accuracy. experiments squeezenet starting point. squeezenet experiments following metaparameters basee incre pctx train multiple models model diﬀerent squeeze ratio range figure show results experiment point graph independent model trained scratch. squeezenet sr=. point ﬁgure. ﬁgure learn increasing beyond increase imagenet top- accuracy model model. accuracy plateaus sr=. setting sr=. increases model size without improving accuracy. section proposed decreasing number parameters replacing ﬁlters ﬁlters. open question important spatial resolution cnns? architectures spatial resolution layers’ ﬁlters; googlenet network-in-network ﬁlters layers. googlenet authors simply propose speciﬁc quantity ﬁlters without analysis. here attempt shed light proportion ﬁlters aﬀects model size accuracy. following metaparameters experiment basee incre vary pctx words fire module’s expand layer predeﬁned number ﬁlters partitioned turn knob ﬁlters mostly mostly previous experiment models fire modules following organization layers figure show results experiment figure note models figure figure architecture pctx figure top- accuracy plateaus using ﬁlters increasing percentage ﬁlters leads larger model size provides improvement accuracy imagenet. explored design space microarchitecture level i.e. contents individual modules cnn. explore design decisions macroarchitecture level concerning high-level connections among fire modules. inspired resnet explored three diﬀerent architectures simple bypass architecture adds bypass connections around fire modules requiring modules learn residual function input output. resnet implement bypass connection around fire input fire equal operator elementwise addition. changes regularization applied parameters fire modules resnet improve ﬁnal accuracy and/or ability train full model. limitation that straightforward case number input channels number output channels same; result half fire modules simple bypass connections shown middle diagram same number channels requirement can’t complex bypass connection illustrated right figure simple bypass just wire deﬁne complex bypass bypass includes convolution layer number ﬁlters equal number output channels needed. note complex bypass connections extra parameters model simple bypass connections not. addition changing regularization intuitive adding bypass connections would help alleviate representational bottleneck introduced squeeze layers. squeezenet squeeze ratio meaning every squeeze layer fewer output channels accompanying expand layer. severe dimensionality reduction limited amount information pass squeeze layers. however adding bypass connections squeezenet open avenues information around squeeze layers. trained squeezenet three macroarchitectures figure compared accuracy model size table ﬁxed microarchitecture match squeezenet described table throughout macroarchitecture exploration. complex simple bypass connections yielded accuracy improvement vanilla squeezenet architecture. interestingly simple bypass enabled higher accuracy improvement complex bypass. adding simple bypass connections yielded increase percentage-points top- accuracy percentage-points top- accuracy without increasing model size. alexnet without compression. since released chapter technical report colleague song collaborators experimented squeezenet model compression. using approach called dense-sparse-dense model compression training regularizer improve accuracy producing compressed squeezenet parameters percentage-points accurate imagenet-k also producing uncompressed squeezenet parameters percentage-points accurate compared results table context chapter focused imagenet target dataset. however become common practice apply imagenet-trained representations variety applications ﬁne-grained object recognition logo identiﬁcation images generating sentences images imagenet-trained cnns also applied number applications pertaining autonomous driving including pedestrian vehicle detection images videos well segmenting shape road think squeezenet good candidate architecture variety applications especially small model size importance. fact since original publication squeezenet techncial report done follow-on work squeezedet builds upon squeezenet address problem localized object detection. december squeezedet models deﬁne state-of-the-art results accuracy model size frame rate kitti object detection dataset. squeezenet several cnns discovered broadly exploring design space architectures. hope squeezenet inspire reader consider explore broad range possibilities design space architectures perform exploration systematic manner. cnn/dnn models become best approach solving variety problems text audio visual domains unprecedented accuracy. among domains visual applications often computationally-intensive use-cases cnn/dnns. interest choosing challenging computationally-intensive problems focused dissertation primarily application cnns visual recognition applications. images) appear single best architecture ideal computer vision problems possible constraints accuracy energy-eﬃciency metrics. further cnns comprise enormous design space. architecture design space presents tradeoﬀs terms computational requirements communication overhead distributed training energy-eﬃciency accuracy problem hand design space ﬁrst large design space computer scientists explored. design space computer processor hardware architectures example large complex design space. mescal book codiﬁed number best practices themes exploring design space computational hardware architectures. drew inspiration mescal book organized dissertation exploring design space architectures. quite similar mescal themes four themes comprehensively deﬁning benchmarks metrics. rapidly training cnn/dnns. deﬁning describing cnn/dnn design space. exploring design space cnn/dnn architectures. cnn/dnns comprise enormous design space. therefore exploring cnn/dnn design space don’t know you’re looking you’re unlikely mind it’s useful deﬁne clear goals terms benchmarks metrics. benchmarks representative target application. example goal recognize vehicles road training testing data include annotated road scenes. important metrics include accuracy also quantity computation quantity communication latency energy-eﬃciency building deployable highly eﬃcient cnn-based system sensible bring together architecture team software architecture team hardware architecture team. measure individual teams’ contributions team evaluated quantity computation accuracy hardware team evaluated power consumption best-case peak throughput. hardware cnns evaluated individually diﬃcult evaluate software team’s contributions isolation. evaluate whole organization’s progress toward eﬃcient full-stack solution useful measure metrics energyeﬃciency speed. long training times impose severe limitation progress deep neural network research productization. accelerating training several beneﬁts. first faster training enables models trained ever-increasing dataset sizes tractable amount time. accelerating training also enables product teams bring cnn-based products market rapidly. compelling applications focus problem accelerating training work culminated firecaﬀe distributed training system. firecaﬀe achieved speedup training googlenet model. enabled train googlenet hours gpus instead weeks gpu. achieving result turned attention identifying training evaluating architectures speciﬁc goals mind. firecaﬀe’s rapid training functionality enabled productively explore design space cnns minimal turnaround time experiment. cnn/dnn architecture deﬁnes quantity computation required data sample training inference. context cnn-based system architecture impacts metrics including accuracy quantity computation quantity communication latency energy-eﬃciency model size. therefore preparing explore design space architectures useful ﬁrst understand design architecture impacts metrics. surprisingly conducting literature survey able practical guide design architecture impacts behavior cnn-based system. address this developed guide explains reason design choices. looking architecture intimidating reason potential change would aﬀect metrics quantity computation. however found changes architecture produces following eﬀects date seen intuition like written anywhere motivated explain intuition analysis topic. better understanding architectural dimensions enable reader reason clearly region design space would like explore. aforementioned information place able design-space exploration methodology practice. following challenge ourselves produce small models achieve competitive accuracy computer vision benchmark. challenge problem benchmark imagenet-k image classiﬁcation dataset metrics accuracy model size. bonus small models trained larger scale using distributed data-parallel training enabled conduct exploration quickly. chose benchmarks independent choice hardware software executing cnn. however section teams since adapted small models achieve system-level goals developing fpga-based systems model entirely on-chip. early process ﬁrst deﬁned region design space explore. best deﬁne region design space likely contain small models e.g. designs fire modules make abundant ﬁlters alternate many ﬁlters layer. within design space identiﬁed squeezenet model fewer parameters alexnet achieves alexnet-level accuracy imagenet-k dataset. further applying model compression squeezenet obtained model smaller alexnet produces alexnet-level accuracy imagenet-k. much content dissertation published ﬁrst time pages. meanwhile published versions chapters within last twelve months despite fact recently released publications work begun make impact research literature. learned chapter common cnns take multiple weeks train single gpu. slow turnaround time training impedes progress research productization. therefore last couple years distributing training multiple processors become increasingly active area research. retrospectively appears release firecaﬀe paper late subsequent publication cvpr turning point researchers approached distributed training problems. early prior embarking development firecaﬀe surveyed literature distributed cnn/dnn training. course literature survey observed following trends. range parallelism strategies. model parallelism staple many papers microsoft google researchers stanford nvidia even designed architecture optimized model parallel training cluster gpus implicitly) manifest destiny variant asynchronous communication would emerge best choice. papers include hogwild follow-on works well work microsoft google common metric number processors utilized. example distbelief paper google deep image paper baidu reported number processors used diﬃcult interpret results papers used proprietary dataset reported time-to-convergence dataset. next subsection recent works done type evaluation publicly-available datasets makes easier make comparisons diﬀerent approaches. choice networking hardware used communicate servers. papers using slow connections servers severe limitations speed scalability system whole papers authors compared approaches mentioned previous subsection work firecaﬀe paper advocated following data parallelism synchronous communication evaluation time-to-convergence predetermined accuracy level publicly-available dataset using fastest communication hardware available. early pioneers approaches; research papers distributed cnn/dnn training released last months we’ve noticed following trends emerge beneﬁts focusing data parallel communication distributed training. papers published researchers gravitated toward data-parallel training cnns. perhaps interestingly google brain team advocate model parallelism earlier paper recently used data parallelism distributed training results released iclr limiting factor asynchronous communication typically need central parameter server. mitigate this advocated synchronous communication enables collective communication approaches reduction trees. parameter server slows linearly number workers collective communication slows logarithmically number workers enabling eﬃciently larger scale. approach begun take hold recent research literature. intel recently published latest results distributed training servers communication fully synchronous incidentally individual intel told firecaﬀe paper used bible intel teams charge producing results further version theano cnn/dnn framework supports synchronous data-parallel communication released mid- finally recent paper google used fully-synchronous communication distributed training even provided approach elegantly address fault tolerance synchronous communication paradigm following strategy evaluating distributed training approach. given speciﬁc dataset speciﬁc target accuracy level goal train desired accuracy level rapidly possible. everything else system architect choice architecture choice computational hardware choice communication hardware benchmarking methodology appears catching recent papers intel google methodology benchmarking. encouraged system architects fastest communication hardware available them. universities large corporations sometimes datacenters containing number diﬀerent compute clusters. employees deciding existing compute clusters distributed cnn/dnn training approach would encourage compute cluster fastest network connections bridging servers cluster. designing ordering compute clusters approach would encourage system infrastructure team spend substantial portion budget purchase fast communication hardware. thinking appears catching recent distributed training results intel used gbit/s inﬁniband network hardware scaling servers discussed chapter order distribute training cnn/dnn model multiple servers necessary communicate among servers training process. applications described general literature distributed computing need communication among servers quite common. distributed computing literature software optimization overlapping communication computation studied twenty-ﬁve years firecaﬀe results overlap communication computation. training particular particular hardware platform requires total hours communication hours computation firecaﬀe results would took hours train. however purine multi-gpu training framework proposed form communicationcomputation overlapping. basic idea follows. backward pass whenever layer ﬁnishes computing data gradients purine would stage gradients communication. however gradients needed time compute forward pass allows communicated computation performed layers. consider best-case speedup applying approach. best case would layers exactly amount communication exactly amount computation best case scenario time communicate data servers identical time required perform gflops computation server. ideal case overlapping approach described purine would lead precisely speedup nonoverlapping training. however cnns described dissertation substantially parameters ﬁnal layers early layers layers vary computational requirements given practical cases deviate best case scenario described overlapping communication computation would lead less speedup serialized communication computation. said opportunity address designing architectures expose overlapping opportunities making layers homogenous communication computation requirements. note that many researchers struggled implement cnns eﬃciently cpus found it’s possible achieve order tflop/s executing cnns using well-tuned implementations. last four years marked resurgence neural networks computer vision chieﬂy unprecedented accuracy modern deep neural networks provide. frenzy hardware architects developed variety accelerators computational platforms targeted speciﬁcally cnn/dnns. observed recurring theme work team hardware architects begin taking oﬀ-the-shelf architecture alexnet engineer accelerator designed oﬀ-the-shelf model mind. fact following work fpgaasic-based accelerators reports speed eﬃciency results solely alexnet architecture eyeriss shidiannao motamedi ovtcharov certainly merit comparing eﬃciency diﬀerent computational hardware approaches architecture. however paper krizhevsky quite clear alexnet model designed leverage problem sizes ideal nvidia using computational hardware especially designing hardware critical design cnns execute eﬃciently hardware. words achieve best possible results various metrics ideal co-design architecture software implementation computational hardware. encouragingly researchers begun make progress holistically designing architectures software hardware. problem mentioned chapter parameters large cnns alexnet typically on-chip today’s processors fpgas requiring timeenergy-intensive communication oﬀ-chip dram. building previous work deep compression recently developed eﬃcient inference engine accelerator able retain compressed versions alexnet vgg- on-chip srams dramatically reducing need oﬀ-chip memory traﬃc. accelerators process dense representations parameters data operates directly sparse representation produced deep compression. authors point models squeezenet reduces memory requirements. work begun directly impact researchers co-design cnns hardware. gschwend recently released report implementation zynqnet comprises custom cnns custom hardware implementation executing inference xilinx zynq fpgas report gschwend says squeezenet basis topology good fpga-based implementation. tiny parameter could even on-chip sram medium-sized fpga optimizations relatively easy thanks fast training cycles clear network structure. rather simply porting squeezenet fpga accelerator gschwend used squeezenet starting point developing zynqnet family architectures. goal eﬃciently targeting fpgas gschwend outlined four particular design objectives used zynqnet architectures while gschwend argues design objectives particularly critical enabling straightforward eﬃcient fpga implementations objectives also enable eﬃcient execution hardware cpus gpus. power-of-two sizes wherever possible dimensions. gschwend mentions that fpga multiplications divisions power calculated inexpensive shift operations enables optimizations addressing image caches accelerator. downsampling activations produced layers replace max-pooling strided convolution wherever possible eliminate need consume chip-area max-pooling hardware gschwend found situations improves accuracy imagenet-k image classiﬁcation. equalize quantity activations layer capacity produced module cnn. helps conserve memory space fpga. addition gschwend found cases change also leads higher accuracy. culminates architecture gschwend able implement fpga. beyond opportunities improving speed eﬃciency appears co-designing architecture fpga hardware implementation actually saved gschwend substantial amount engineering eﬀort. appendix gschwend’s report mentions work mentioned developed gschwend -month master’s thesis project. contrast eyeriss shidiannao group authors spent multiple years team engineers develop optimize hardware architectures alexnet. gschwend’s work demonstrates designing right architecture lead substantial savings hardware-engineering eﬀort. works zynqnet encouraging anticipate orders-of-magnitude improvements remain table several metrics choose co-design architecture software hardware. jordan boyd-graber philip resnik holistic sentiment analysis across languages multilingual supervised latent dirichlet allocation empirical methods natural language processing naptali tsuchiya nakagawa topic dependent class based language model evaluation automatic speech recognition ieee spoken language technology workshop xiaodan zhuang stavros tsakalidis shuang pradeep natarajan rohit prasad prem natarajan compact audio representation event detection consumer media interspeech elizalde friedland i-vector representation acoustic environments audio-based video event detection user generated content ieee international symposium multimedia dario amodei rishita anubhai eric battenberg carl case jared casper bryan catanzaro jingdong chen mike chrzanowski adam coates greg diamos erich elsen jesse engel linxi christopher fougner tony awni hannun billy patrick legresley libby sharan narang andrew sherjil ozair ryan prenger jonathan raiman sanjeev satheesh david seetapun shubho sengupta wang zhiqian wang chong wang xiao dani yogatama zhan zhenyao deep speech end-to-end speech recognition english mandarin arxiv. khalid ashraf benjamin elizalde forrest iandola matthew moskewicz julia bernd gerald friedland kurt keutzer audio-based multimedia event detection dnns sparse sampling icmr mark everingham gool christopher williams john winn andrew zisserman pascal visual object classes challenge international journal computer vision forrest iandola khalid ashraf matthew moskewicz kurt keutzer firecaﬀe near-linear acceleration deep neural network training compute clusters cvpr forrest iandola song matthew moskewicz khalid ashraf william dally kurt keutzer squeezenet alexnet-level accuracy fewer parameters <.mb model size arxiv. matthew moskewicz forrest iandola kurt keutzer boda-rtc productive generation portable eﬃcient code convolutional neural networks mobile computing platforms arxiv. sharan chetlur cliﬀ woolley philippe vandermersch jonathan cohen john tran bryan catanzaro evan shelhamer cudnn eﬃcient primitives deep learning arxiv. zeijl eikenbroek vervoort setty tangenherg shipton kooistra keekstra belot visser bosma blaakmeer bluetooth radio .mu;m cmos ieee journal solid-state circuits codrescu anderson venkumanhanti zeng plondke koob ingle tabony maule hexagon architecture optimized mobile multimedia communications ieee micro chen wang chen temam high-throughput accelerator ubiquitous machine-learning international conference architectural support programming languages operating systems interactive python mercury monte carlo particle transport code international conference mathematics computational methods applied nuclear science engineering forrest iandola schuemann jungwook shin bruce faddegon harald paganetti joseph perl representing range compensators topas monte carlo system european workshop monte carlo treatment planning forrest iandola fatemeh saremi tarek abdelzaher praveen jayachandran aylin yener real-time capacity networked data fusion international conference information fusion fatemeh saremi praveen jayachandran forrest iandola yusuf sarwar tarek abdelzaher schedulability time composability multisensor data aggregation networks international conference information fusion steena monteiro forrest iandola daniel wong stomp statistical techniques optimizing modeling performance blocked sparse matrix vector multiplication international symposium computer architecture high performance computing mehdi maasoumy pierluigi nuzzo forrest iandola maryam kamgarpour alberto sangiovanni-vincentelli claire tomlin optimal load management system aircraft electric power distribution ieee conference decision control forrest iandola matthew moskewicz sergey karayev ross girshick trevor darrell kurt keutzer densenet implementing eﬃcient convnet descriptor pyramids arxiv. forrest iandola david sheﬃeld michael anderson phitchaya mangpo phothilimthana kurt keutzer communication-minimizing convolution registers icip forrest iandola david sheﬃeld michael anderson phitchaya mangpo phothilimthana kurt keutzer minimizing memory communication image convolution registers technology conference fang saurabh gupta forrest iandola rupesh srivastava deng piotr dollar jianfeng xiaodong margaret mitchell john platt lawrence zitnick geoﬀrey zweig from captions visual concepts back cvpr christopher cieri david miller kevin walker ﬁsher corpus resource next generations speech-to-text international conference language resources evaluation andrej karpathy george toderici sanketh shetty thomas leung rahul sukthankar fei-fei large-scale video classiﬁcation convolutional neural networks cvpr panayotov chen povey khudanpur librispeech corpus based public domain audio books international conference acoustics speech signal processing tsung-yi michael maire serge belongie lubomir bourdev ross girshick james hays pietro perona deva ramanan lawrence zitnick piotr dollar microsoft coco common objects context eccv sangmin anthony hoogs amitha perera naresh cuntoor chia-chih chen jong taek saurajit mukherjee aggarwal hyungtae larry davis eran swears xioyang wang qiang kishore reddy mubarak shah carl vondrick hamed pirsiavash deva ramanan jenny yuen antonio torralba song anesco fong amit roy-chowdhury mita desai large-scale benchmark dataset event recognition surveillance video cvpr roozbeh mottaghi xianjie chen xiaobai nam-gyu seong-whan sanja fidler raquel urtasun alan yuille role context object detection semantic segmentation wild ieee conference computer vision pattern recognition banerjee lavie automatic metric evaluation improved correlation human judgments workshop intrinsic extrinsic evaluation measures machine translation and/or summarization christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich going deeper convolutions arxiv. nicolas vasilache johnson micha¨el mathieu soumith chintala serkan piantino yann lecun fast convolutional nets fbﬀt performance evaluation arxiv. lars cremean tully foote jeremy gillula george hines dmitriy kogan kristopher kriechbaum jeﬀrey lamb jeremy leibs laura lindzey christopher rasmussen alexander stewart joel burdick richard murray alice information-rich autonomous vehicle high-speed desert navigation journal field robotics kalin ovtcharov olatunji ruwase joo-young jeremy fowers karin strauss eric chung accelerating deep convolutional neural networks using specialized hardware microsoft research whitepaper yung-hsiang alan kadin alexander berg thomas conte erik debenedictis rachit garg ganesh gingade bichlien hoang yongzhen huang boxun jingyu huizi junran peng tianqi tang elie track jingqiu wang wang wang rebooting computing low-power image recognition challenge international conference computer-aided design jiantao wang song kaiyuan boxun erjin zhou jincheng tianqi tang ningyi song wang huazhong yang going deeper embedded fpga platform convolutional neural network international symposium fpga awni hannun carl case jared casper bryan catanzaro greg diamos erich elsen ryan prenger sanjeev satheesh shubho sengupta adam coates andrew deep speech scaling end-to-end speech recognition arxiv. anantharaj foertter joubert wells approaching exascale application requirements olcf leadership computing https//www.olcf.ornl.gov/wp-content/ uploads///olcf_requirements_tm__final.pdf yangqing evan shelhamer donahue sergey karayev jonathan long ross girshick sergio guadarrama trevor darrell caﬀe convolutional architecture fast feature embedding arxiv. jeﬀrey dean greg corrado rajat monga chen matthieu devin mark ranzato andrew senior paul tucker yang quoc andrew large scale distributed deep networks nips rajeev thakur rolf rabenseifner william gropp optimization collective communication operations mpich international journal high performance computing applications hossein azizpour sharif razavian josephine sullivan atsuto maki stefan carlsson from generic speciﬁc deep representations visual recognition cvpr deep vision workshop mart´ın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeﬀrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoﬀrey irving michael isard yangqing rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg man´e rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda vi´egas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng tensorflow large-scale machine learning heterogeneous systems google technical report john gale theodore williams light adaptation temperature eﬀects piii retinal response analysis two-state model proceedings national academy sciences jiuxiang zhenhua wang jason kuen lianyang amir shahroudy bing shuai ting xingxing wang gang wang recent advances convolutional neural networks arxiv. yue-hei matthew hausknecht sudheendra vijayanarasimhan oriol vinyals rajat monga george toderici beyond short snippets deep networks video classiﬁcation cvpr donahue l.a. hendricks guadarrama rohrbach venugopalan saenko darrell long-term recurrent convolutional networks visual recognition description cvpr paul over george awad fiscus martial michel alan smeaton wessel kraaij trecvid goals tasks data evaluation mechanisms metrics trecvid workshop song xingyu huizi jing ardavan pedram mark horowitz william dally eﬃcient inference engine compressed deep neural network international symposium computer architecture tianqi chen yutian naiyan wang minjie wang tianjun xiao bing chiyuan zhang zheng zhang mxnet ﬂexible eﬃcient machine learning library heterogeneous distributed systems arxiv. dipankar sasikanth avancha dheevatsa mudigere karthikeyan vaidyanathan srinivas sridharan dhiraj kalamkar bharat kaul pradeep dubey distributed deep learning using synchronous stochastic gradient descent arxiv. song pool sharan narang huizi shijian tang erich elsen bryan catanzaro john tran william dally regularizing deep neural networks dense-sparsedense training arxiv. donahue yangqing oriol vinyals judy hoﬀman ning zhang eric tzeng trevor darrell decaf deep convolutional activation feature generic visual recognition arxiv. bichen forrest iandola peter kurt keutzer squeezedet uniﬁed small power fully convolutional neural networks real-time object detection autonomous driving arxiv. dave andersen alex smola junwoo park ahmed vanja josifovski james long eugene shekita bor-yiing scaling distributed machine learning parameter server operating systems design implementation minjie wang tianjun xiao jianpeng jiaxing zhang chuntao hong zheng zhang minerva scalable highly eﬃcient training platform deep learning nips workshop distributed machine learning matrix computations zhongyang zheng wenrui jiang gang edward chang speedo parallelizing stochastic gradient descent deep convolutional neural network nips workshop machine learning systems cevdet aykanat fusun ozguner fikret ercal ponnuswamy sadayappan iterative algorithms solution large sparse systems linear equations hypercubes ieee transactions computers y.-h. chen krishna emer vivian eyeriss energy-eﬃcient reconﬁgurable accelerator deep convolutional neural networks ieee international conference solid-state circuits zidong robert fasthuber tianshi chen paolo ienne ling xiaobing feng yunji chen olivier temam shidiannao shifting vision processing closer sensor acm/ieee international symposium computer architecture mohammad motamedi philipp gysel venkatesh akella soheil ghiasi design space exploration fpga-based deep convolutional neural networks design automation conference chen zhang peng guangyu bingjun xiao yijin guan jason cong optimizing fpga-based accelerator design deep convolutional neural networks international symposium field-programmable gate arrays jost tobias springenberg alexey dosovitskiy thomas brox martin riedmiller striving simplicity convolutional international conference learning representations", "year": 2016}