{"title": "C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0  Dataset", "tag": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "abstract": "Visual Question Answering (VQA) has received a lot of attention over the past couple of years. A number of deep learning models have been proposed for this task. However, it has been shown that these models are heavily driven by superficial correlations in the training data and lack compositionality -- the ability to answer questions about unseen compositions of seen concepts. This compositionality is desirable and central to intelligence. In this paper, we propose a new setting for Visual Question Answering where the test question-answer pairs are compositionally novel compared to training question-answer pairs. To facilitate developing models under this setting, we present a new compositional split of the VQA v1.0 dataset, which we call Compositional VQA (C-VQA). We analyze the distribution of questions and answers in the C-VQA splits. Finally, we evaluate several existing VQA models under this new setting and show that the performances of these models degrade by a significant amount compared to the original VQA setting.", "text": "visual question answering received attention past couple years. number deep learning models proposed task. however shown models heavily driven superﬁcial correlations training data lack compositionality ability answer questions unseen compositions seen concepts. compositionality desirable central intelligence. paper propose setting visual question answering test question-answer pairs compositionally novel compared training question-answer pairs. facilitate developing models setting present compositional split dataset call compositional analyze distribution questions answers c-vqa splits. finally evaluate several existing models setting show performances models degrade signiﬁcant amount compared original setting. automatically answering questions visual content considered holy grails artiﬁcial intelligence research. visual question answering poses rich challenges spanning various domains computer vision natural language processing knowledge representation reasoning. stepping stone visually grounded dialog intelligent agents past couple years received attention. various datasets proposed different groups number deep-learning models developed however shown despite recent progress today’s models heavily driven superﬁcial correlations training data lack compositionality ability answer questions unseen compositions seen concepts. instance model said compositional correctly answer without seeing question-answer pair training perhaps seen training. order evaluate extent existing models compositional create compositional split dataset called compositional dataset created re-arranging train splits dataset question-answer pairs c-vqa test split compositionally novel respect c-vqa train split i.e. pairs c-vqa test split present c-vqa train splits concepts constituting pairs test split present train split. fig. shows examples c-vqa splits. since c-vqa test split contains pair similar pairs present c-vqa train split. c-vqa train split contains pairs consisting concepts plate color evaluating model setting helps testing whether model capable learning disentangled representations different concepts whether model compose concepts learned training correctly answer questions novel compositions test time. please section details c-vqa splits. demonstrate difﬁculty c-vqa splits report performance several existing models c-vqa splits. experiments show performance models drops signiﬁcantly trained evaluated train test splits c-vqa compared models trained evaluated train splits original please section details experiments. visual question answering. several papers proposed visual question answering datasets train test machines task visual understanding span time size datasets become larger questions becomes free form open-ended. instance earliest datasets considers questions generated using templates consists ﬁxed vocabulary objects attributes etc. also consider questions whose answers come closed world. generate questions automatically using image captions answers belong following four types object number color location. consist free form open-ended questions. datasets dataset used widely train deep models. performance models increased steadily past years test similar distribution data points training set. however careful examination behaviors models reveals models heavily driven superﬁcial correlations training data lack compositionality partly training contains strong language priors data-driven models learn easily perform well test consists similar priors training without truly understanding visual content images easier learn biases data truly understand images. order counter language priors goyal balance every question dataset collecting complementary images every question. thus every question dataset similar images different answers question. clearly language priors signiﬁcantly weaker dataset. however balancing test compositionality train test distributions similar. order test whether models learn concept individually irrespective correlations data perform well test different distribution correlations compared training propose compositional split dataset call compositional-vqa compositionality. ability generalize novel compositions concepts learned training desirable intelligent system. compositionality studied various forms vision community. zero-shot object recognition using attributes based idea composing attributes detect novel object categories recently studied compositionality domain image captioning focusing structured representations study compositionality visual question answering questions answers open-ended free-form natural language. work closest study compositionality domain vqa. however dataset synthetic limited number objects attributes. contrary c-vqa splits consist real images questions hence involve variety objects attributes well activities scenes etc. andreas developed compositional models consist different modules specialized particular task. modules composed together based question structure create model architecture given question. although compositional design models evaluated speciﬁcally compositionality. c-vqa splits used evaluate models test degree compositionality. fact report performance neural module networks c-vqa splits compositionality setting proposing type zero-shot test pairs novel. types zero-shot also explored. propose setting test questions contain atleast unseen word. propose answering questions unknown objects c-vqa splits created re-arranging training validation splits dataset splits created question-answer pairs c-vqa test split seen c-vqa train split cases concepts compose c-vqa test pairs seen c-vqa train split c-vqa splits created using following procedure order reduce similar questions form. instance what color cones? what color cones? reduced form reduction achieved using simple text processing removal stop words lemmatization. reduced grouping questions reduced form ground truth answer grouped together. instance grouping done merging pairs train splits. greedily re-splitting greedy approach used redistribute data points c-vqa train test splits maximize coverage test concepts c-vqa train split making sure pairs repeated test train splits. procedure loop groups created above every iteration current group c-vqa test split unless group already assigned c-vqa train split. always maintain concepts belonging groups c-vqa test split covered groups belonging c-vqa train split. groups assigned either splits group covers majority concepts group c-vqa train split. approach results unique c-vqa test concepts covered c-vqa train split. coverage taking account frequency occurrence concept c-vqa test split. table shows number questions images answers train splits dataset train test splits c-vqa dataset. number questions number answers c-vqa splits similar splits. however number images c-vqa splits splits. c-vqa splits image present train test sets. note three questions every image dataset splitting c-vqa done based pairs based images. consider following questions associated image what color cones? what time possible what color cones? gets assigned c-vqa train split what time gets assigned c-vqa test split. result image corresponding questions would present train test splits c-vqa. verify sharing images across splits make problem easier randomly split train+val random-train random-val. trained evaluated deeper lstm norm model splits. setup leads increase model performance compared train setup. section analyze distributions questions answers c-vqa train test splits differ train splits. question distribution. fig. shows distribution questions based ﬁrst four words questions train test splits c-vqa dataset. splitting dataset compositionally lead signiﬁcant differences distribution questions across splits keeping distributions qualitatively similar splits quantitatively question strings split also present train split whereas percentage c-vqa splits. figure distribution questions ﬁrst four words random sample questions c-vqa train split c-vqa test split ordering words starts towards center radiates outwards. length proportional number questions containing word. white areas words contributions small show. answer distribution. fig. shows distribution answers several question types what color what sport many etc. train test splits c-vqa dataset. distributions answers given question type signiﬁcantly different. however dataset distribution given question type similar across train splits instance tennis frequent answer question type what sport c-vqa train split whereas skiing frequent answer question type c-vqa test split. however splits tennis frequent answer train splits. similar differences seen question types well what animal what brand what kind what type what are. quantitatively pairs split also present train split whereas percentage c-vqa splits report performances following models trained c-vqa train split evaluated c-vqa test split compare setting models trained train split evaluated split deeper lstm question normalized image model proposed channel model channel processes image channel processes question. image image channel extracts activations last hidden layer vggnet normalizes them. question question channel extracts hidden state cell state activations last hidden layers -layered lstm resulting -dim encoding question. image features obtained image channel question features obtained question channel linearly transformed dimensions fused together element-wise multiplication. fused vector passed fully-connected layer multi-layered perceptron ﬁnally outputs -way softmax score frequent answers training set. entire model except learned end-to-end cross-entropy loss. neural module networks model designed compositional nature. model consists composable modules module speciﬁc role given image natural language question image ﬁrst decomposes question linguistic substructures using parser. structures determine modules need composed together layout create network answering question. resulting compound networks jointly trained. test time image question forward propagated dynamically composed network outputs distribution answers. addition network composed using different modules also uses lstm encode question added elementwise representation produced last module nmn. combined representation passed fully-connected layer output softmax distribution answers. lstm encodes priors training data models syntactic regularities singular plural stacked attention networks widely used models vqa. model different models uses multiple hops attention image. given image natural language question uses question obtain attention image. attended image combined encoded question vector becomes query vector. query vector used obtain second round attention image. query vector obtained second round attention passed fully-connected layer obtain distribution answers. hierarchical question-image co-attention networks performing models vqa. addition modeling attention image model also models attention question. image question attention computed hierarchical fashion. attended image question features obtained different levels hierarchy combined passed fully-connected layer obtain softmax distribution space answers. multimodal compact bilinear pooling model real image track challenge uses multimodal compact bilinear pooling predict attention image features also combine attended image features question features. combined features passed fully-connected layer obtain softmax distribution space answers. table performance existing models drops signiﬁcantly c-vqa setting compared setting. note even though neural module networks architecture compositional design performance suffers c-vqa. posit additional lstm encoding question encode priors dataset. c-vqa priors learned train unlikely generalize test set. also note models suffer larger drop performance compared neural module networks. another interesting observation table ranking models based overall performance changes c-vqa. outperforms deeper lstm norm whereas c-vqa models outperform san. also note change ranking models different types answers instance number questions outperforms models except hiecoatt however c-vqa models except outperform mcb. examining accuracies models different question types shows performance drop c-vqa larger question types others. neural module networks stacked attention networks hierarchical question-image co-attention networks questions starting what room largest drop drop drop drop hiecoatt. questions c-vqa test split correct answers living room correct answers questions c-vqa train split models tend answer c-vqa test questions seen training note living room seen training questions which room this?. deeper lstm norm model multimodal compact bilinear pooling model largest drop questions drop deeper lstm norm drop model. questions c-vqa test split correct answer whereas correct answer questions c-vqa train split again models tend answer c-vqa test questions question types resulting signiﬁcant drop performance models what color many people there conclusion introduce novel setting visual question answering compositional visual question answering. setting question-answer pairs test compositionally novel compared question-answer pairs training set. create compositional split dataset called c-vqa facilitates training compositional models. show similarities differences c-vqa splits. finally report performances several existing models c-vqa splits show performance models drops signiﬁcantly compared original setting. suggests today’s models handle compositionality well c-vqa splits used benchmark building evaluating compositional models. yash goyal tejas khot douglas summers-stay dhruv batra devi parikh. making matter elevating role image understanding visual question answering. arxiv preprint arxiv. justin johnson bharath hariharan laurens maaten fei-fei lawrence zitnick ross girshick. clevr diagnostic dataset compositional language elementary visual reasoning. arxiv preprint arxiv. abhishek satwik kottur khushi gupta singh deshraj yadav josé m.f. moura devi parikh dhruv batra. visual dialog. proceedings ieee conference computer vision pattern recognition harm vries florian strub sarath chandar olivier pietquin hugo larochelle aaron courville. guesswhat? visual object discovery multi-modal dialogue. arxiv preprint arxiv. nasrin mostafazadeh chris brockett bill dolan michel galley jianfeng georgios spithourakis lucy vanderwende. image-grounded conversations multimodal context natural question response generation. corr abs/. ranjay krishna yuke oliver groth justin johnson kenji hata joshua kravitz stephanie chen yannis kalantidis li-jia david shamma visual genome connecting language vision using crowdsourced dense image annotations. arxiv preprint arxiv. yuke oliver groth michael bernstein fei-fei. visualw grounded question answering images. proceedings ieee conference computer vision pattern recognition pages mateusz malinowski mario fritz. multi-world approach question answering real-world scenes based uncertain input. advances neural information processing systems pages haoyuan junhua zhou zhiheng huang wang talking machine? dataset methods multilingual image question. advances neural information processing systems pages akira fukui dong park daylen yang anna rohrbach trevor darrell marcus rohrbach. multimodal compact bilinear pooling visual question answering visual grounding. emnlp christoph lampert hannes nickisch stefan harmeling. learning detect unseen object classes between-class attribute transfer. computer vision pattern recognition cvpr ieee conference pages ieee dinesh jayaraman kristen grauman. decorrelating semantic visual attributes resisting urge share. proceedings ieee conference computer vision pattern recognition pages", "year": 2017}