{"title": "Generalized Dropout", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.NE"], "abstract": "Deep Neural Networks often require good regularizers to generalize well. Dropout is one such regularizer that is widely used among Deep Learning practitioners. Recent work has shown that Dropout can also be viewed as performing Approximate Bayesian Inference over the network parameters. In this work, we generalize this notion and introduce a rich family of regularizers which we call Generalized Dropout. One set of methods in this family, called Dropout++, is a version of Dropout with trainable parameters. Classical Dropout emerges as a special case of this method. Another member of this family selects the width of neural network layers. Experiments show that these methods help in improving generalization performance over Dropout.", "text": "deep neural networks often require good regularizers generalize well. dropout regularizer widely used among deep learning practitioners. recent work shown dropout also viewed performing approximate bayesian inference network parameters. work generalize notion introduce rich family regularizers call generalized dropout. methods family called dropout++ version dropout trainable parameters. classical dropout emerges special case method. another member family selects width neural network layers. experiments show methods help improving generalization performance dropout. large-scale tasks like image classiﬁcation general practice recent times train large convolutional neural network models. even large datasets risk overﬁtting runs high large model size. result strong regularizers required restrict complexity models. dropout stochastic regularizer widely used recent times. however rule proposed heuristic objective reducing co-adaption among neurons. result it’s behaviour well understood. gharamani showed dropout implicitly performs approximate bayesian inference making bayesian neural net. bayesian neural nets view parameters neural network random variables rather ﬁxed unknown quantities. result exists distribution possible values parameter take. placing appropriate prior random variables possible restrict model’s capacity implicitly perform regularization. theoretical attractiveness methods tools probability theory work models. advantages bnns offer plain neural nets? first inherently capture uncertainty model parameters well predictions. second ideal learning small amounts data. third bayesian approach advantage distilling complex assumptions model form prior distributions. inference bnns typically intractable. result often uses approximations posterior distribution. mcmc variational inference popular methods performing approximations. recent times emerged preferred method performing approximation scalable large models. using common assume independence model parameters. neural networks assumption seem unnecessarily stringent. weights particular ﬁlter highly correlated produce speciﬁc patterns however different ﬁlters more-or-less independent compute different features. fact might even advantageous enforce independence different ﬁlters reduce co-adaptation among features. work strive enforce independence among features rather weights. section shall formally introduce notion bnns also discuss proposed method. denote neural network function parameters given input neural network produces probability distribution possible labels classiﬁcation problem. given training data parameter vector updated using bayes’ rule. computing posterior equation intractable complicated neural network structure. variational inference deﬁne another distribution called variational distribution approximates posterior distribution used instead posterior equation inference. common assumption working variational distribution mean-ﬁeld assumption requires states parameters independent. assumption certainly true especially feature-level parameters highly correlated. works like denil explicitly show parameters predicted given parameters hinting large amount correlation present. trying enforce independence among weights adverse effects. difﬁcult overcome independence assumption within framework next section introduce approach overcome difﬁculty. multiplicative gates feature neuron neural network figure gates modulate output neuron. parameters denoted also assume intuitively gates control relative importance particular feature viewed next layer. relative importance fundamentally uncertain given particular feature. hence useful think gate parameters random variables. shall crystallize assumptions form choices variational distribution. ﬁrst place following prior-hyperprior pair gate parameters prior regular parameters note products possible variables deﬁned network. here denotes bernoulli parameters also needs estimated along given variational inference deﬁne forms variational distributions use. note even though make independence assumption weights overcome disadvantages described previous section effectively bayesian respect using delta distribution. also note parameter distributions true using different parameters distributions could make formulation powerful parameter simplicity. write equations describing variational approximation using deﬁnitions above. objective solve equation observe exhaustive summation equation intractable large models. popular method deal monte-carlo approximation summation. however even infeasible large models. result approximate single monte-carlo sample. words perform given assumptions approximations discussed above write complete objective function solve. since variational distributions delta distributions shall instead notations simplicity. expression above used fact beta distribution. form objective function gates constitutes generalized dropout regularizer. brieﬂy look behaviour beta distribution various values shown figure shall refer speciﬁc cases different versions dropout++. reasons discussed later shall refer last case stochastic architecture learning figure illustration proposed method binary stochastic multiplicative gates. here refers weights refers gates. note behaviour beta distribution different values note dropout++ becomes indistinguishable classic dropout obtain dropout rates simply ensure next section shall discuss another algorithm called architecture learning relates method above. srinivas babu recently introduced method learn width depth neural network architectures. also additional learnable parameters similar gates. also objective function following form notation. note objective function looks similar except instead another difference heaviside threshold select rather sampling bernoulli. observe equivalent taking maximum likelihood sample bernoulli distribution. given similarities found name corresponding method stochastic architecture learning stochastic version algorithm described above. surprisingly motivation arrive algorithm completely different intended minimize number neurons network. arrive similar formulation purely bayesian perspective. section shall attempt provide intuitive explanation generalized dropout. going back fig. neuron augmented gate learns values enforced regularizers well parameter clipping. forward pass treat gate values probabilities toss coin probability. output coin toss used block allow neuron outputs. result learning important features tend higher probability values unimportant features. test time perform sampling. rather simply real-valued probability values gate variables. approximation called re-scaling used classical dropout well. different generalized dropout methods intuitively place restriction gate values learnt. example dropout++ encourages gate values close important ones high. hand dropout++ encourages gates values close intuitively means dropout++ restricts capacity layer large amount whereas dropout++ hardly changes anything. hand encourages neurons close either contrast methods produces neural network layers close deterministic neurons close almost never ’on’ close almost always ’on’. dropout++ also unique sense doesn’t place restriction gate values. result require hyper-parameters method. bayesian perspective prior beliefs gate values non-informative prior dropout++ case. dropout++ encourages values close regularization constants increased gate values penalized heavily. limiting case dropout deviation probability value inﬁnitely penalized. given formalism stochastic gate variables unclear might compute error gradients them. bengio investigated problem binary stochastic neurons empirically veriﬁed efﬁcacy different solutions. conclude simplest computing gradients straight-through estimator works best overall. involves simply back-propagating stochastic neuron identity function. sampling step given bernoulli gradient another issue consideration ensuring always lies valid bernoulli parameter. bengio sigmoid activation experiments showed clipping functions worked better. thought ‘linearized’ sigmoid. clipping function given following expression. shall discuss apply convolutional layers. assume output feature convolutional layer i.e; feature maps size classical dropout samples bernoulli random variables performs pointwise multiplication output feature map. follow generalized dropout well. however wish perform architecture selection like architecture learning need select subset feature maps. case gate variables multiplying output feature map. gate close zero entire feature map’s output becomes close zero test time. selecting feature maps determine ﬁlters previous layer essential. plenty works extend dropout. dropconnect stochastically drops weights instead neurons obtain better accuracy ensembles networks. stated earlier using independence assumption weights correct. indeed dropconnect shown work fully connected layers. standout version dropout dropout rate depends output activations layer. variational dropout proposes bayesian interpretation gaussian dropout rather canonical multiplicative dropout. considering multiplicative dropout make important connections architecture learning neuron pruning. gharamani showed bayesian interpretation binary dropout show test performance improves performing monte-carlo averaging rather re-scaling. simplicity re-scaling method test time generalized dropout. work seen extension work considering hyper-prior along bernoulli prior. hinton camp ﬁrst introduced variational inference making neural networks bayesian. recent work graves blundell investigated notion using different priors relevant approximations large networks. probabalistic backpropagation algorithm inferring marginal posterior probabilities special classes bayesian neural networks. method different methods bayesian weights whereas bayesian respect gates. section perform experiments generalized dropout family test usefulness. first perform wide variety analysis generalized dropout family. later study speciﬁc applications method. perform experiments primarily using theano lasagne. shall analyze behaviours different members generalized dropout family ones useful. experiments mnist dataset standard lenet-like architecture consists convolutional layers ﬁlters fully connected layers neurons. nothing particularly special architecture simply standard analyze method. investigate whether generalized dropout indeed advantage dropout terms accuracy. here apply dropout generalized dropout last fully connected layer. experiments reveal network considered accuracies achieved generalized dropout method always strictly better dropout shown figure indicates regularization power dropout comes independence assumption variational inference rather particular values dropout parameter. surprising result shall advantage paper. however note small data-sizes dropout++ seems advantageous dropout possibly dropout++ forces neurons capacity value parameters. inspired results dropout++ look relationship using different layer-widths fully connected layer learnt gate parameters. intuitively natural assume larger layers learn lower gate values whereas smaller layers learn much higher values wish overall capacity layer remain roughly same. experiments conﬁrm intuition shown figure figure dropout++ methods perform dropout large data dropout++ seems work better small data. dropout++ adapt different layer sizes result optimal performance. initialization dropout++ parameters crucial. result initialize favourably drastically increase training speed. dropout++ convolutional layers learns selectively attend parts image rather full image. dropout++ best suited gate pruning better automated pruning given steep slope. table architecture selection capability method lenet-like baseline. ﬁrst architecture performance baseline lenet network. larger ratios result smaller networks cost network performance. also test ﬂexibility translates higher accuracy numbers ﬁxed dropout value indeed case. small layer-widths dropout tends remove many neurons dropout++ adjusts it’s parameter values account small layer-widths shown figure table applying dropout++ layer standard networks decreases error rate dropout++ learnt values close result dropout performs similar learnt value. dropout++ init learnt values close resnets small values dropout makes training difﬁcult. initialization good parameters known play key-role generalization deep learning systems. test whether holds newly introduced generalized dropout parameters well different initializations generalized dropout parameters. example simply initialize gates single constant value. expected choice initialization much less crucial compared setting dropout value shown figure choice initialization however affects training time. example empirically observed dropout much slower therefore helpful higher dropout rates facilitate faster training. help faster training dropout++ simply initialize i.e; start network dropout gradually learn much dropout add. observe indeed helps training time time provides ﬂexibility dropout shown figure point focussed using generalized dropout fully connected layers. similar effects hold apply convolutional layers well. here visualize learnt parameters convolutional layers. first dropout++ input layer. resulting gate parameters shown figure observe similar effect dropout++ ﬁrst convolutional layer shown figure shows average gate convolutional ﬁlters layer. cases observe dropout++ learns selectively attend centre image rather towards corners. multiple advantages. first looking corners feature potentially decrease model evaluation time. second breaks translation equivariance implicit convolutions case certain spatial locations important ﬁlter others. could helpful using cnns face images ﬁlter need look everywhere image. locally connected layers previously used works deepface dropout++ could offer natural incorporate assumption. shall attempt stochastic architecture learning automatically learn required layer width network. inherent assumption initial architecture over-complete sub-set neurons sufﬁcient similar performance. ﬁrst learn parameters network using regularizer later prune neurons gates parameters. figure shows learns gate parameters often close either resulting much sharper rise compared methods. sharp rise criterion select width layer. observe varying parameter encourages method smaller architectures sometimes cost accuracy shown table studied various properties generalized dropout performing various experiments lenet. shall shift larger networks test effectiveness dropout++. modern networks mainly dropout fully connected layers simply owing much powerful regularizers batch normalization. shall take networks simply dropout++ layer increase accuracy. perproposed generalized dropout family methods generalize dropout-like behaviour. methods family dropout++ adaptive version dropout. stochastic architecture learning another methods performs architecture selection. uninformed choice dropout parameter usually hurts performance. dropout++ helps setting useful parameter value regardless factors layer width initialization. experiments show generally beneﬁcial simply dropout++ every layer deep network.", "year": 2016}