{"title": "Coupled Compound Poisson Factorization", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We present a general framework, the coupled compound Poisson factorization (CCPF), to capture the missing-data mechanism in extremely sparse data sets by coupling a hierarchical Poisson factorization with an arbitrary data-generating model. We derive a stochastic variational inference algorithm for the resulting model and, as examples of our framework, implement three different data-generating models---a mixture model, linear regression, and factor analysis---to robustly model non-random missing data in the context of clustering, prediction, and matrix factorization. In all three cases, we test our framework against models that ignore the missing-data mechanism on large scale studies with non-random missing data, and we show that explicitly modeling the missing-data mechanism substantially improves the quality of the results, as measured using data log likelihood on a held-out test set.", "text": "present general coupled compound poisson factorization capture missing-data mechanism extremely sparse data sets coupling hierarchical poisson factorization arbitrary data-generating model. derive stochastic variational inference algorithm resulting model examples framework implement three different data-generating models—a mixture model linear regression factor analysis—to robustly model non-random missing data context clustering prediction matrix factorization. three cases test framework models ignore missing-data mechanism large scale studies non-random missing data show explicitly modeling missing-data mechanism substantially improves quality results measured using data likelihood held-out test set. statistical theory missing data developed little rubin starts important distinction between missing-data pattern missing-data mechanism. missing-data pattern indicator matrix describes matrix values missing. missingdata mechanism hand captures relationship missing-data pattern data generating model. distribution missingnessencoding model depend observed data yobs missing data ymis missing-data mechanism characterized missing completely random contrast missing random indicates data mcar maximum likelihood estimates data generating model parameters change taken account cases missing-data mechanism said ignorable. data nmar however becomes effective parameter estimation consider joint likelihood data generating model missingness-encoding model. bayesian framework missingness-encoding model represented probabilistic model; model coupled arbitrary data generating model. missing-data mechanism non-ignorable hypothesize identifying mechanism correctly improve inference joint data generating model. theoretical machine learning literature missingdata problem often discussed within limited attribute observability framework assumed learner controls attributes observe. models address variety data generating models. chechik considered situation learner control attribute observability linear regression setting. similarly hazan proposed non-probabilistic algorithm classiﬁcation problem lowrank assumption extreme sparsity. probabilistic models low-rank approximation extremely sparse matrices abundant include probabilistic matrix factorization non-negative matrix factorization variants. applied side motivating missing-data problem extreme sparsity collaborative ﬁltering—creating predictive ranking items user given observations users’ preferences—where capturing missingdata pattern crucial accurate ranking table common additive exponential dispersion models. gaussian gamma inverse gaussian poisson binomial negative binomial distributions additive form. gamma distribution refer shape rate parameters respectively. state-of-the-art probabilistic collaborative ﬁltering models based either poisson factorization probabilistic counterpart instance weighted matrix factorization model binarized data heteroscedastic variance term. exposure matrix factorization model uses model conditioned bernouilli exposure matrix capture missingdata pattern another successful implicit feedback model hierarchical poisson factorization building hierarchical compound poisson factorization uses encode missing-data pattern extends structure general additive exponential dispersion model generate data hcpf ﬂexible model used factorize continuous realvalued non-negative data well non-negative discrete data. increasing popularity pf-based methods collaborative ﬁltering attributed factors. first gamma-poisson distributions conjugate whereas gaussian-bernoulli distributions models not. thus probabilistic inference pf-based models large scale problems straightforward computationally tractable second hcpf hierarchical structure models user activity item popularity natural way. particular gopalan show heavy tail gamma priors accurately capture user behavior item popularity using posterior predictive checks. equally important complex aspect implicit feedback models missing-data mechanism. marlin zemel give ample empirical evidence motivating need showing performance beneﬁts explicitly modeling nmar missing-data mechanisms collaborative ﬁltering models. recommendation systems largely driven development collaborative ﬁltering models include nmar data; however problem nmar data exist broad range analytic tasks underdeveloped now. motivated observations develop explicit nmar missing data mechanistic model coupled arbitrary generative model framework. show beneﬁts including explicit missing-data mechanistic model three speciﬁc generative model tasks mixture models clustering linear regression prediction latent factor models matrix factorization. start data generating models additive exponential dispersion model output. large collection models includes gaussian gamma binomial mixture models linear regression models among others. explore relationship data generating model missingness-encoding model identify need missing-data mechanism capture heteroscedastic relationship observations missing-data pattern. speciﬁcally empirically show variance observation function probability missingness observation three large data sets. address issue propose coupled compound poisson factorization framework missing-data mechanism nmar data extreme sparsity. prove ccpf model reduces data generating model missingness-encoding model ignorable. ﬁrst implication result sufﬁcient update parameters data-generating model using nonmissing entries only. second implication statistician able describe data generating model ignoring possible impact missingness-encoding model heteroscedasticity. heteroscedasticity exists data missingness-encoding model within ccpf framework accurately capture control complexity analytic task. figure exploratory analysis heteroscedasticity. scatter plots normalized variance residuals test data sets ﬁtting model non-missing entries respect qunatiles probability non-missingness calculated ﬁtting model binarized full matrix amazon movielens yelp data sets expected scaling versus probability non-missingness taking values orange exponential linkage function purple linear linkage function. start describing family additive exponential dispersion models deﬁning characteristic additivity. exponential dispersion models primarily used error distributions generalized linear models comprehensive treatment theory edms see]jorgensentheory. additive edms subfamily edms includes gaussian gamma inverse gaussian poisson binomial negative binomial distributions among others following basbug engelhardt deﬁne additive edms follows. deﬁnition family distributions deﬁnition above additive edms sharing base log-partition function natural parameter. following theorem makes statement concrete theorem sequence additive edms exploit property provide theoretical justiﬁcation compounding coupling framework. importantly inference compound poisson additive edms relatively straightforward availability conditional marginal densities coupling framework arbitrary data generating model additive dispersion parameter conjugate prior hierarchical model parameters examples data generating models include gaussian poisson gamma binomial mixture models linear regression models. first investigate relationship missingdata pattern data-generating model. model three different data sets mcar assumption i.e. non-missing entries sampled during training. also model binarized full matrix data sets. test non-missing entries calculate residuals probability non-missingness table data statistics number samples attributes non-missing entries ratio missing entries total number entries maximum minimum observation values. discretize data equalsized bins based sample quantiles probability nonmissingness calculate variance residuals fig. shows scatter plot variance within quantiles probability non-missingness amazon movielens yelp data sets. clearly linear/exponential relationship three data sets. also test heteroscedasticity ﬁtting double generalized linear model residuals using probability non-missingness regressor. conclude strong heteroscedasticity three data sets address heteroscedastic structure data analysis missing observations present framework nmar missing-data mechanism handle largescale extremely sparse data additional desirable properties compatibility low-rank assumption missing-data pattern convergence homoscedastic model almost surely missing missingness-encoding model entry missing draw model zero; otherwise nij. hcpf ﬂexible observation model i.e. however hcpf assumes natural parameter observations; therefore data-generating model ﬁxed distribution complex structure mixture model regression model. framework assume arbitrary data generating model whose output additive edm. couple missingness-encoding data generating models scaling dispersion parameter linkage function full generative model coupled compound poisson factorization follows first summarize model assumptions behind used capture missing-data pattern. extension poisson factorization essentially non-negative matrix factorization model. imagine movie ratings data entry missing user rated movie. setting missing-data pattern approximated interaction matrix entry latent poisson random variable interaction variable interaction contributions nijk poisson distributed rank assumption implies exist latent groups users factor weight models membership group. different another random variable modeling active user user active factor weight interaction contribution nijk adjusted accordingly. similarly exist latent group movies factor weight models membership movie genre. popularity variable controls many tion movie ratings user group dislike movie genre down-vote. furthermore latent factors movies users terms interaction vastly different latent factors terms preference. brings data generating model. ccpf ability choose arbitrary data-generating model movie ratings perhaps another matrix factorization model. model relationship missingness-encoding model data-generating model linkage function linear missing-data mechanism obtain linear relationship probability missingness dispersion parameter linkage function cnij expectation zero truncated poisson distribution given fig. purple lines indicate expected scaling probability non-missingness changes seen ﬁgure inverse linear relationship two. words probability non-missingness increases expected dispersion decreases linearly. compatible empirical ﬁndings fig. times movie rated. point interactions groups users groups movies non-negative. group users ‘not interact’ movie genre hence nijk cannot negative. different performing matrix factorizacomputationally tractable learning stochastic variational inference minimizes lower bound expected posterior likelihood variational distribution. mean ﬁeld variational distribution ccpf given distributions variational approximation generative distribution. gamma distributions shape rate parameter. make multinomial representation poisson factors update variational parameters data generating model need sufﬁcient statistic similarly need sufﬁcient statistics update variational parameters missingness-encoding model. algorithm general ccpf framework summarized algorithm apply ccpf framework three classes data generating models mixture models matrix factorization linear regression. model compare ccpf data-generating models ignore missing-data mechanism. case ccpf approach outperforms comparisons terms likelihood held-out nonmissing entries. mixture models linear regression matrix factorization three useful data generating models analysis high-dimensional data. mixture models represent partitioning observations subgroups used exploratory data analysis. matrix factorization decomposes matrix observations lower dimensional matrices allowing observation original matrix represented weighted linear combination lower dimensional space. linear regression models relationship regressors observations; case observations multivariate large number dimensions. three models applied data depending objectives analysis. goal ccpf account missing data various modeling paradigms. note ccpf applicable linear regression settings full access regressors multivariate observations missing. applicable data missing covariates. cnij implies that probability non-missingness increases expect greater dispersion. fixing standard compound poisson additive model inverse relationship probability non-missingness dispersion. expectation given fig. orange lines show expected scaling respect probability non-missingness different values expected scaling increases exponentially probability non-missingness increases. expected scaling decreases exponentially since dispersion always positive needs careful choosing exponential decay small might better choice linear relationship certain data sets seen fig. another takeaway fig. expected scaling converges probability non-missingness becomes following theorem provide stronger result. theorem zero-truncated poisson random variable parameter i.i.d. additive random variables pψκ) anrandom variable log-partition function natural parameter deﬁne linear linkage exponenfunction tial linkage function n+yn probability densities theorem implies distribution observation missing almost surely converges homoscedastic observation model i.e. model building aspect useful property since design data-generating model independently simply plug missingness-encoding model capture suspected heteroscedasticity. another corollary theorem parameters data generating model table non-missing test likelihood mixture models. likelihood non-missing test entry clustering performed across samples attributes. stand gaussian mixture model poisson mixture model respectively. ccpf-gmm ccpf-pmm variants models framework. using expected range well maximum likelihood estimates assumption ﬁxed. inverse gamma prior shape parameter scale parameter note inverse gamma conjugate prior exponential dispersion models saddle-point approximation additionally gaussian prior zero mean standard deviation linkage parameter calculate estimates using stochastic gradient descent smoothed gradients taking average gradients. ﬁrst consider gaussian mixture models spherical gaussian priors poisson mixture models gamma priors data-generating models. derive algorithms modiﬁed accommodate coupling ccpf-gmm ccpf-pmm movielens amazon netﬂix yelp data sets. perform clustering across users items separately. continuous gene expression data ccpf-gmm identify cluster individuals genes ﬁrst take gene expression levels. compare models performance terms test likelihood. table observe ccpf-gmm outperforms amazon netﬂix yelp geuvadis clustering samples attributes. movielens beating ccpf-gmm. comparisons consistent pattern. explained previous observation impact missingness-encoding model data-generating model signiﬁcant dispersion parameter signiﬁcant mean term. case scaling dispersion parameter affects variance term. scales mean variance observation; analyze four user behavior data sets gene expression data set; table outlines characteristics each. user behavior data includes multiple ratings data sets amazon contains ratings food netﬂix consists movie ratings yelp comprises venue ratings; ratings three data sets range also consider movielens another movie data ratings ranging data movie corresponding exogenous variables representing association predetermined tag. weights calculated crowd sourced tag-movie association data behavior data always bounded— wordpress social media interaction data users blogs response number likes user given blog. geuvadis data shows ccpf applicable beyond discrete user behavior data; gene expression data genes individuals. hyper parameters missingness-encoding model followed method presented hcpf empirical study small data sets. estimate sparsity level number nonmissing entries sparsity level calculated heavy tail gamma priors table non-missing test likelihood factorization models. stands probabilistic matrix factorization stands hierarchical poisson factorization. ccpf-pmf ccpf-hpf variants models framework. regression model well coupled version movielens data attribute vector weights discussed earlier. goal capture user preferences arbitrary tags information within collaborative ﬁltering setting. table observe coupling improves regression performance. also note regression approach achieves highest test likelihood among mixture model matrix factorization approaches. work present coupled compound poisson factorization models missing-data mechanism extremely sparse data sets coupling missingness-encoding model arbitrary datagenerating model. derive stochastic variational inference algorithm ccpf models examples framework implement instances mixture model linear regression matrix factorization. compare model data generating models ignores missing-data mechanism large scale studies show explicitly modeling missing-data mechanism substantially improves test likelihood metrics relevant analysis interest. table comparison regression models. likelihood rmse values hierarchical linear regression models movielens data set. ccpf-regr. variant regression model framework. next considered major matrix factorization models. probabilistic counterpart regularized penalty terms relate spherical gaussian priors factors another probabilistic matrix factorization model factor contributions non-negative previously utilized missingnessencoding model; however also trained non-missing entries. derive algorithm accommodating coupling missingness-encoding model ccpf-pmf ccpf-hpf movielens amazon netﬂix yelp wordpress data sets. table summarizes comparison models terms test likelihood. ratings data sets ccpf-pmf best performing algorithm clear margin cases. similar mixture model analysis observe substantial improvement ccpf-pmf transition ccpf-hpf transition. social media activity data wordpress exhibits different characteristic— poisson models outperform ccpf-pmf. attributed highly dispersed poisson-like response distribution. finally considered hierarchical linear regression data-generating model. attribute vector item describe data-generating model user unique coefﬁcient vector gaussian prior coefﬁcient vectors maximum likelihood estimate coefﬁcient vector response ﬁxed kukliansky doron shamir ohad. attribute efﬁcient linear regression distribution-dependent sampling. proceedings international conference machine learning mcauley julian john leskovec jure. amateurs connoisseurs modeling evolution user expertise online reviews. proceedings international conference world wide references basbug mehmet engelhardt barbara hierarchical compound poisson factorization. proceedings international conference machine learning july http//arxiv.org/ abs/.. yifan koren yehuda volinsky chris. collaborative ﬁltering implicit feedback datasets. data mining icdm’. eighth ieee international conference ieee", "year": 2017}