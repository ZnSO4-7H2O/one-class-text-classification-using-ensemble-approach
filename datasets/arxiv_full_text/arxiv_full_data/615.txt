{"title": "Semi-Supervised Phoneme Recognition with Recurrent Ladder Networks", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "Ladder networks are a notable new concept in the field of semi-supervised learning by showing state-of-the-art results in image recognition tasks while being compatible with many existing neural architectures. We present the recurrent ladder network, a novel modification of the ladder network, for semi-supervised learning of recurrent neural networks which we evaluate with a phoneme recognition task on the TIMIT corpus. Our results show that the model is able to consistently outperform the baseline and achieve fully-supervised baseline performance with only 75% of all labels which demonstrates that the model is capable of using unsupervised data as an effective regulariser.", "text": "abstract. ladder networks notable concept ﬁeld semi-supervised learning showing state-of-the-art results image recognition tasks compatible many existing neural architectures. present recurrent ladder network novel modiﬁcation ladder network semi-supervised learning recurrent neural networks evaluate phoneme recognition task timit corpus. results show model able consistently outperform baseline achieve fully-supervised baseline performance labels demonstrates model capable using unsupervised data eﬀective regulariser. doubt recent success deep learning tied rising availability labelled data. tasks image text classiﬁcation greatly beneﬁted availability still number domains e.g. speech recognition majority research community free access large amounts labelled data. promising approach towards problem semi-supervised learning models trained labelled data improved training unlabelled data. recent methods graph-supported training sparse autoencoders sssae) especially ladder network stacked denoising autoencoder shortcut connections show promising results semisupervised training feed-forward neural networks. shown deliver state-of-the-art results semi-supervised image classiﬁcation still compatible many existing feed-forward neural networks however novel architecture explored complex sequential tasks speech recognition recurrent neural network architectures like gated recurrent units current state art. therefore propose novel recurrent ladder network architecture evaluate timit phoneme recognition benchmark introduce novel recurrent layer decoder order better-suited abstractions semi-supervised learning test noise injection schemes tailored support recurrent dynamics increase regularising nature rln. results show hyper-parameter optimization model able signiﬁcantly outperform baseline experiments using unsupervised data regulariser achieves fully-supervised baseline performance training labelled data. basic idea architecture depicted fig. make autoencoders expressive adding shortcut connections encoder decoder. decoder layer able combine preactivation encoder layer reconstruction previous decoder layer means short-circuits enforce learning intermediate layers i.e. denoising autoencoder. ensure noise removed decoder’s reconstruction compared encoder’s preactivation added unsupervised objective function total amount layers preactivation vector l-th encoder layer without noise l-th decoder layer reconstruction noisy input. hyper-parameter controls targeted similarity encoder decoder layers prevents short-circuits punishing direct copies noisy data weighting diﬀerence layers. semisupervised learning encoder path also used supervised task i.e. output evaluated supervised objective function csup combined unsupervised objective function cdae csemsup csup cdae. using encoder supervised task shortcuts help reconstruction needed information also retrieved shortcuts combinator function models responsible creating reconstruction l-th layer help reconstruction previous layer shortcut value l-th layer i.e. ˆz). function attempt remove noise help previous reconstruction infer inverse mapping combination both. section elaborate modelling choices rln. order extend original support recurrence encoder noise injection scheme decoder adapted since recurrent layers additional context layers. overall proposing noise injection methods decoder variants supervised baseline model encoder since encodes task closely full means using unsupervised data. resulting model combinations no-decoder feed-forward noise no-decoder recurrent noise recurrent decoder feed-forward noise recurrent noise well feed-forward decoder feed-forward noise recurrent noise recurrent layers even receiving noisy output previous layer eﬀectively amplifying noise even further. therefore apply noise preactivation shortcut without direct perturbation context memory. hidden layer noisy counterpart therefore updated follows activation function input input weight matrix noise injection method referred recurrent noise another method noise injection tested referred feed-forward noise inject additional noise recurrent layer i.e. feed-forward layers injected noise recurrent layers not. input weights hidden-to-hidden weights preactivation recurrent decoder noisy preactivation l-th encoder layer time-step shortcut. second modelling option simply feed-forward network decoder batch normalisation heavily used normalisation layer-wise reconstruction cost normalisation layer activations. considered problematic recurrent networks introduction recurrent batch normalisation since potentially requires tuning another hyper-parameter decided model without batch normalisation exception layer-wise reconstruction cost function computed exactly described rasmus evaluate timit phoneme recognition benchmark widely used test corpus allows comparing architecture previous approaches. audio samples corpus reduced dimensionality using librosa compute frequency cepstral components ﬁrst second derivative frames frame skip similar related work -dimensional feature vectors normalised zero mean unit variance. grouped easily confused phonemes english phoneme alphabet described halberstadt resulting phoneme classes predict. connectionist temporal classiﬁcation supervised cost csup solve problem label alignment. phoneme error rate used evaluation computed using levenshtein distance label sequences predictions normalised total length label sequences. fig. overview feed-forward noise recurrent noise injection schemes encoders introduced subsection well recurrent decoder feed-forward decoder layouts introduced subsection combining encoder decoder layouts gives total model variants including no-decoder baselines nd-rn nd-ffn. build supervised unsupervised training sets keep input data unsupervised training reduce supervised drawing samples full dataset least represented phonemes drawn minimum number times prevent under-representing class keeping distribution intact. cycle supervised dataset match sizes unsupervised similar implementation rasmus activation feed-forward output layer softmax activation well inverse layers decoder. noisy softmax output used classify phonemes training additional regularisation. since performance encoder likely correlate performance hyper-parameters including layer sizes learning rate determined empirically grid search using encoder described section i.e. also serves baseline. cost weights overview results seen fig. diﬀerent modelling choices directly compared other. overall best results hyper-parameter optimization supervised data split well results approaches shown table fig. comparison achieved variants varying amount labelled data noise standard deviation data point represents mean whiskers cover conﬁdence interval. higher needed fewer labels prevent overﬁtting. seen table consistently outperforms baseline conﬁguration even fully-supervised training able achieve performance baseline less labelled data shows complements encoder well demonstrates compatibility existing models. average models perform better models fewer labels suggesting recurrent decoder better ﬁltering noise. also explains models work better higher compared ffd. noise injection method chosen greatly impact overall performance. performance curves roughly concave shift towards stronger noise less available labels network overﬁts easily fewer labels prevented higher noise. performance degrades higher network needs trained signiﬁcantly longer remove noise chosen training parameters allow. recurrent noise injection expected achieve better regularisation additional noise recurrent layer not. observing encoder layers found outputs often diﬀered signiﬁcantly causes unrecoverable perturbations recurrent layers applying equally strong noise layers instead noise relative layer’s output. employing batch normalisation might solve this hypothesised related work normalising preactivation layer unit variance adding noise makes change variance relative preactivation therefore coupling noise layer activation strength beneﬁt reducing search space signiﬁcantly. predict lead increase performance using fewer labels. even though best results slightly lower ranked compared related approaches model signiﬁcantly fewer parameters therefore hypothesise increase parameters complex layer architectures result even better performance. indicated best achieving similar results bi-directional long short-term memory parameters) using half labels. shown recurrent ladder network able perform good similarly parametrised blstm models using labelled data demonstrating rln’s ability eﬀectively regularise using unsupervised training data. current state-of-the-art methods performed better overall come surprise given models times parameters. argue could potentially closed scaling models demonstrated blstm models graves proposed recurrent decoder proved better denoising feed-forward decoder. additionally found recurrent noise injection perform expected hypothesise needs help normalisation work eﬃciently. future would also like take advantage semi-supervised learning abilities conjunction complex recurrent models bidirectional attention-based rnns utilise unlabelled data even eﬀectively explore learning framework scales complex temporal dynamics challenging tasks end-to-end speech recognition question answering. acknowledgments. authors gratefully acknowledge partial support german research foundation project european union project secure hamburg landesforschungsf¨orderungsprojekt cross.", "year": 2017}