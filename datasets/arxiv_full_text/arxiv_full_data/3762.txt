{"title": "Self-Expressive Decompositions for Matrix Approximation and Clustering", "tag": ["cs.IT", "cs.CV", "cs.LG", "math.IT", "stat.ML"], "abstract": "Data-aware methods for dimensionality reduction and matrix decomposition aim to find low-dimensional structure in a collection of data. Classical approaches discover such structure by learning a basis that can efficiently express the collection. Recently, \"self expression\", the idea of using a small subset of data vectors to represent the full collection, has been developed as an alternative to learning. Here, we introduce a scalable method for computing sparse SElf-Expressive Decompositions (SEED). SEED is a greedy method that constructs a basis by sequentially selecting incoherent vectors from the dataset. After forming a basis from a subset of vectors in the dataset, SEED then computes a sparse representation of the dataset with respect to this basis. We develop sufficient conditions under which SEED exactly represents low rank matrices and vectors sampled from a unions of independent subspaces. We show how SEED can be used in applications ranging from matrix approximation and denoising to clustering, and apply it to numerous real-world datasets. Our results demonstrate that SEED is an attractive low-complexity alternative to other sparse matrix factorization approaches such as sparse PCA and self-expressive methods for clustering.", "text": "abstract—data-aware methods dimensionality reduction matrix decomposition low-dimensional structure collection data. classical approaches discover structure learning basis efﬁciently express collection. recently self expression idea using small subset data vectors represent full collection developed alternative learning. here introduce scalable method computing sparse self-expressive decompositions seed greedy method constructs basis sequentially selecting incoherent vectors dataset. forming basis subset vectors dataset seed computes sparse representation dataset respect basis. develop sufﬁcient conditions seed exactly represents rank matrices vectors sampled unions independent subspaces. show seed used applications ranging matrix approximation denoising clustering apply numerous real-world datasets. results demonstrate seed attractive low-complexity alternative sparse matrix factorization approaches sparse self-expressive methods clustering. data-driven methods sparse matrix factorization sparse dictionary learning approximate data vectors sparse linear combinations small basis elements. simple approaches provide extremely efﬁcient representation dataset bases learned often points different lowdimensional geometric structures thus lead degraded classiﬁcation/clustering performance revealing low-dimensional structure data express itself—to represent element dataset terms small subset samples. selfexpression already successfully used context classiﬁcation clustering low-rank matrix approximation contrast learning basis self expression provides provable means low-dimensional subspace structures discovered idea underlying selfexpressive approaches clustering sparse subspace dyer konrad k¨ording dept. physical medicine rehabilitation dept. applied math northwestern university chicago usa. raajen patel richard baraniuk dept. electrical computer eng. rice university houston goldstein computer science dept. university maryland college park clustering low-rank representations represent dataset terms signals collection. dataset rm×n containing data vectors dimensions computes representation rn×n sparse matrix zeros along diagonal. resulting sparse matrix interpreted afﬁnity matrix data vectors subspace presumed another sparse representations. dataset clustered applying spectral clustering methods graph laplacian |vt|. self-expressive methods like provide principled segmentation low-dimensional subspaces applying methods datasets challenging. require construction storage afﬁnity matrix dataset size even low-complexity greedy methods used populate afﬁnity matrix ssc-omp clustering data requires solving eigenvalue problem entire afﬁnity matrix intractable large such development efﬁcient solutions decomposing large datasets essential clustering discovering lowdimensional structures data. paper develop scalable approach sparse matrix factorization built upon idea using samples data express itself. approach refer self-expressive decomposition consists main steps. ﬁrst step select data samples sequentially selecting columns incoherent columns selected previous iterations. this method called oasis oasis operates subset gram matrix thus used quickly select columns without computing entire gram matrix. second step vectors selected ﬁrst step basis compute sparse representation dataset using faster variant orthogonal matching pursuit method describe seed detail sec. provide pseudocode alg. demonstrate seed provides effective strategy matrix approximation theory practice particular thm. provides sufﬁcient condition oasis return subset columns captures full range data condition called exact matrix recovery highlights attractive properties proposed approach column selection naturally selects linearly independent columns thus provides highly efﬁcient representation rank matrices provides estimate representation error remaining dataset iteration. error estimate used stop algorithm explicit evaluation error intractable. following analysis demonstrate seed applied and/or solve numerous problems including matrix approximation clustering denoising outlier detection evaluate performance seed applications several real-world datasets including three image datasets collection neural signals motor cortex. results demonstrate seed provides scalable alternative sparse decomposition methods spca nearest neighbor methods fraction computational cost. paper organized follows. sec. provide background column subset selection sparse recovery sparse subspace clustering. sec. introduce seed provide motivating examples complexity analysis. sec. develop sufﬁcient condition exact matrix recovery seed rank matrices datasets living unions independent subspaces. sec. study performance seed four applications matrix approximation denoising sparse representation-based learning outlier detection. finally concluding remarks details approach column selection appendix denote matrices uppercase bold script vectors lowercase bold script. write entry matrix xij. denote column-wise concatenation denote left pseudoinverse orthogonal projection onto span columns indexed deﬁned frobenius norm deﬁned support vector supp indexes nonzero elements. sparsity equals |supp|. denote columns indexed x-s. denote entry-wise multiplication colsum returns sums columns argument. dimension union subspaces {si}p i=si dimension signal subspace equals matrix contains objective aims columns best approximate leastsquares sense. collection signals kdimensional subspace invertible matrices rm×k yield exact matrix recovery i.e. unfortunately believed np-hard since requires brute force search sub-matrix provides best approximation. however large body literature random adaptive column selection emerged past years uniform random sampling easiest well-studied sampling method number adaptive selection criteria proposed reduce number samples required achieve target approximation error. adaptive approaches include leverage-based sampling sequential errorbased selection approaches leverage sampling requires computing low-rank data matrix determine columns exert inﬂuence lowrank approximation. computing so-called leverage scores columns dataset columns drawn randomly based upon leverage score. strategies select columns based upon well approximated current sample probability selecting column proportional xi−πs. strategies highly effective practice methods costly require computing residual error matrix selection step. contrast proposed column selection strategy requires computing operating matrix step iteration number -norms rows penalizing rows minimize number non-zero rows turn minimizes number columns needed represent dataset. approach known reveal representative columns collections data also hyperspectral unmixing however approach requires solving matrix cannot used directly enforce sparsity entries seed group sparsity norms known produce dense estimates within group. algorithm sparse self-expressive decomposition input dataset rm×n maximum number columns select termination criterion step termination criterion step output normalized basis rm×l sparse coefﬁcient matrix rl×n step column subset selection select columns oasis normalize selected columns form rm×l. step greedy sparse recovery solve column respect stack result corresponding column rl×n leads provable guarantees exact feature selection condition every data point represented using data within subspace guarantees require exists least linearly independent columns span k-dimensional subspace dataset. occurs provides complete reference subspace. sec. iv-c show lies union independent subspaces seed guaranteed return columns provides complete reference subspaces present dataset. step seed select columns form good low-dimensional approximation dataset. this employ method called accelerated sequential incoherence selection adaptive strategy selects columns incoherent another. oasis originally designed compute rank factorizations positive semideﬁnite kernel matrices used wide range machine learning applications. here show oasis used novel column selection ﬁnding rank approximation gram matrix motivating example show samples selected oasis random sampling dataset consisting faces various illumination conditions observe oasis returns images highly varied terms illumination select columns incoherent selected previous iterations. contrast random sampling returns images highly redundant illumination conditions input input signal dictionary containing unit-norm vectors columns termination condition output sparse coefﬁcient vector initialize residual input signal select column maximally correlated sparse recovery methods form approximation consisting small number nonzero coefﬁcients greedy methods sparse recovery orthogonal matching pursuit select columns iteratively subtracting contribution selected atom current signal residual. selection process repeated stopping criterion satisﬁed either target sparsity reached residual magnitude becomes smaller pre-speciﬁed value pseudocode algorithm given alg. number methods learning multiple subspaces data idea self-expression represent data terms signals collection; methods lead state-of-the-art clustering performance unions subspaces instance sparse subspace clustering factorizes dataset rm×n solving following -minimization problem user parameter controls error self-expressive approximation. idea underlying datapoint represented linear combination small number points dataset. coefﬁcient matrix computed interpreted graph entry matrix represents edge point dataset; strength edge represents likelihood points live subspace. forming symmetric afﬁnity matrix +|vt| spectral clustering performed graph laplacian afﬁnity matrix obtain labels points dataset fig. column selection oasis. step oasis project onto span select column produces largest deviation compared corresponding diagonal entry i.e. select column maximal deviation w−b|. proposed column selection strategy depends knowledge shaded regions depend gray shaded region containing inner products unsampled columns question. iteration needs compute coefﬁcient matrix speed na¨ıve implementation performing rank- updates coefﬁcient matrix every time column added. sake completeness provide brief description pseudocode oasis appendix. full discussion oasis application rank kernel matrix approximation. step greedy sparse recovery step seed compute sparse representations columns terms columns selected step given this ﬁrst normalize columns unit -norm; rm×l denote corresponding matrix normalized datapoints. without loss generality reorder compute sparse decomposition vector containing -norm column entry. columns computed solving accelerated version designed efﬁciently compute sparse representations batch signals called batch orthogonal matching pursuit unlike convex optimizationbased approaches sparse recovery -norm constrain either total approximation error column constrain sparsity introduce variant seed used outlier detection clustering applications. variant modiﬁes compute sparse representation sampled signals reorder write sparse matrix contains sparse representations unsampled signals fig. incoherence sampling face images. face images subject selected oasis random sampling images selected oasis represent wide range diverse illumination conditions whereas random sampling selects number similar illumination conditions. motivate selection criterion used oasis suppose already selected columns indexed without loss generality reorder leastsquares approximation terms subsampled columns given assume contains linearly independent columns thus inversion possible. sec. iv-a show oasis guaranteed select linearly independent columns thus assumption justiﬁed. however discrepancy projection scalar quantities provides measure poorly represents candidate column thus without computing entire column measuring projection onto span instead approximate inﬂuence current approximation measuring discrepancy using insight estimate employ following greedy strategy decide column approximation iteration runtime complexity required select columns step seed requiring storage matrix step step seed must compute sparse approximation datapoint runtime complexity matrix kml. thus complexity computing equals small roughly thus total complexity steps given contrast complexity seed approaches clustering complexity computing nearest neighbors collection data points complexity ssc-omp dominated forming sparse representation column respect m×n− matrix runtime complexity. section develop sufﬁcient conditions exact matrix recovery occurs projection onto subspace spanned subset gives back exactly i.e. prove main result exact matrix recovery begin ﬁrst proving iteration oasis algorithm selects samples linearly independent selected previous iterations show application oasis gram matrix guaranteed provide exact matrix recovery. assume rank exact matrix recovery occurs contains least linearly independent columns lemma below provide sufﬁcient condition describes oasis return linearly independent columns. proof. proceed induction. denote columns already selected previous iterations denote square matrix consisting entries selected column indices columns selected. assume invertible since consists linearly independent columns consider selecting column forming given column vector corresponding inner products newly selected column previously selected columns equal gii. matrix invertible provided schur complement non-zero. schur complement ∆k+. thus nonzero contains linearly independent columns thus column drawn must also linearly independent. long initialize oasis linearly independent columns guaranteed select linearly independent columns provided corresponding unselected columns. result follows induction. remark. lemma guarantees oasis return linearly independent columns steps long selection criterion holds exact reconstruction occurs. unfortunately pathological case algorithm fails columns selected algorithm terminate early. possible construct pathological matrices occurs observed early termination practice. following theorem shows entries gram matrix drawn continuous random distribution algorithm succeeds probability theorem suppose entries gram matrix drawn continuous random distribution. assume oasis initialized randomly selecting fewer columns oasis succeeds generating linearly independent columns probability proof. begin noting randomly chosen initialization columns gram matrix full rank probability matrix random matrix drawn continuous distribution singular matrices positive co-dimension thus measure probability choosing linearly dependent vectors chance thus zero. suppose columns already selected wish show always possible select column result follows induction. denotes diagonal entry column construction condition holds columns k−bi known however columns continuous random variable thus probability holding selects columns spans space thus select linearly independent columns also guarantees columns linearly independent. idea made precise following. proof. recall gram matrix deﬁnition rank columns indexed equals rank rank rank. thus rank implies rank assumption equals rank full dataset therefore rank exact recovery guaranteed. state main result matrix recovery oasis proof. prove thm. must simply combine lemma lemma precise lemma states oasis return linearly independent columns provided thus long algorithm terminate implies return linearly independent columns indexed index using lemma oasis returns subset columns rank exact recovery guaranteed based upon corresponding subset guarantees ssc-omp rely assumption dataset provides complete reference low-dimensional subspace present data make precise assume points drawn union subspaces i=si provides complete reference contains least points follows deﬁnition exact matrix recovery that union independent subspaces whenever yields exact matrix recovery guaranteed also provides complete reference main condition required prove thus seed returns subset data least columns k-dimensional subspaces compute corresponding covering radius subspace details). thus long exact recovery occurs apply theory produce guarantees occurs decomposition obtained seed. result follows combining thm. condition exact recovery thm. describe datasets used evaluations. face dataset consists images subjects faces various illumination conditions resulting dataset size hyperspectral dataset consists images salinas scene image contains spatial information scene different spectral band. image pixels removing pixels without labels total dataset size dataset consists types vegetation spectral signatures associated class low-dimensional mnist dataset contains images handwritten digits results dataset size neuro dataset consists ﬁring rates neurons motor area collected time points monkey performing center-out reach task moving center position targets resulting dataset produces data matrix size union-of-subspaces dataset synthetic dataset consisting signals drawn union subspaces dimension -dimensional overlap collection outliers created generating random gaussian vectors. results dataset size points ﬁrst subspace points second subspace outlier points. evaluations mnist datasets used openmpi implementation seed written c++. parallelizes oasis faster column selection batch faster sparse representation computation. experiments utilized total processor cores core. smaller datasets evaluations matlab single desktop processor. well-studied application approximation rank matrices seed utilizes fast sequential algorithm column selection approach provides effective strategy matrix approximation. show sec. iv-a oasis selects linearly independent columns step thus obtains exact recovery rank matrices using samples contrast random leverage-based sampling exhibit signiﬁcantly slower decay approximation error. fig. approximation error versus size factorization. relative approximation error displayed function factorization size seed error-based sampling random sampling faces neuro mnist uos. smaller datasets also display error leverage sampling spca. results matrix approximation evaluate performance seed matrix approximation compute approximation error function decomposition size. compare error sampling dataset with oasis sequential error selection uniform random sampling possible leverage score sampling addition also compute error spca using generalized power method relative approximation error figure displays approximation error datasets function relative factorization size l/n. oasis achieves exact matrix recovery number points sampled equals rank. agreement result matrix recovery thm. result suggests guarantee exact matrix recovery overly restrictive seed produces linearly independent sample sets wide range real synthetic datasets. interestingly observe similar decay approximation error neuro synthetic datasets error achieved seed spca roughly equivalent trails behind seed also quickly achieves exact recovery random leverage sampling ﬂatline achieve exact recovery. suggests neuro dataset likely contain rank structures well outliers make random sampling signiﬁcantly less effective matrix approximation seed. sparse representation-based approaches classiﬁcation clustering leverage fact signals class another sparse representations. using fact sparsity patterns self-expression decomposition used cluster data using either spectral clustering consensus method fact recent studies shown that dataset lives union subspaces sparse representation point subspace consist points subspace decomposition provided seed cluster data using sparsity patterns however rectangular standard spectral clustering approaches square afﬁnity matrices cannot used. rather think representing edges bi-partite graph thus co-clustering methods used place standard graph clustering methods. spectral co-clustering algorithm introduced provides elegant relaxation problem ﬁnding minimum bi-partite graph. interesting consequence using spectral co-clustering approach that eventually solve eigenvalue problem minimum compute second largest singular vector matrix rather second smallest eigenvector matrix. enables exploit simple iterative methods leading singular vectors rather computing entire matrix. results sparse representation-based learning figure show visualization embedding union overlapping subspaces; show ﬁrst three coordinates embedding plot projection unsampled signals dots projection sampled signals blue stars. result provides evidence that co-clustering provides feasible efﬁcient strategy clustering data seed proposed sampling strategy capable separating data fewer samples random sampling. sparse representation-based clustering compute cost normalized vary size factorization cost normalized measure easy cluster bi-partite graph correct classes. cost deﬁned follows index rows corresponding points class index columns corresponding points class index points dataset. cost normalized class subsequent experiments compare average cost normalized column sampling-based approaches ssc-omp graph. note sscomp exhibit complexity make impractical large datasets. fig. normalized ratios face image database. normalized ratios size factorization collections subject’s faces sixty different illumination conditions twenty subject’s faces sixty different illumination conditions cases data full rank i.e. respectively. provides denoised version original dataset similar spirit nn-based denoising. however rather ﬁnding nearest neighbors applying simple averaging procedure seed ﬁnds optimal neighbors weights datapoint. motivating example show performance seed clustering hyperspectral image data denoising data seed random subset samples applying k-means original data. here observe signiﬁcant improvement clustering denoising data samples k-means clustering error denoising data seed respectively. clustering subset image points selected entire dataset small sample points observe nearly perfect clustering image seed. compare performance seed random sampling-based approach clustering original data without denoising. clustering error show clustering results obtained denoising data clustering error pca-based denoising based upon principal components fig. visualization co-clustering seed. visualization embedding union overlapping -dimensional subspaces pairs subspaces -dimensional intersection rank dataset left show embedding samples selected random sampling right show embedding seed. visualization draw ellipses around samples subspace display sampled points blue unsampled points neuro dataset achieve lower ratios column samplingbased approaches sampling samples. samples column sampling-based methods achieve ratios experiments kmax higher values kmax lower values observe ssc-omp provides best ratios. results suggest that leverage random sampling provide poor schemes matrix approximation column sampling approaches provide comparable performance terms ratios. fig. display normalized ratios faces dataset twenty different subjects. observe similar decay ratios datasets seed achieve normalized cuts less ssc-omp sample dataset. sscomp seed grows increase number samples performance leverage random sampling appears ﬂatline ratio methods. many datasets tested observe subsampling-based approaches produce smaller ratios ssc-omp methods. likely fact that size ensemble grows relative dimension underlying cluster graphs generated become weakly connected thus produce smaller ratios contrast sparse representations produced seed built upon self-expressive bases containing incoherent columns thus observe seed produces sparse graphs well connected thus produce smaller ratios results suggest seed provide strategy outlier detection simply thresholding columns based upon sparsity level. general determining appropriate threshold segment rank structures outliers challenging. however practice observe distribution column sparsity multi-modal; thus instead setting threshold explicitly k-means algorithm employed good threshold segment outliers. paper introduced seed scalable method sparse matrix factorization couples provable method column selection greedy sparse recovery demonstrated seed applied either assist solve numerous signal processing machine learning problems ranging matrix approximation denoising clustering outlier detection. addition developed sufﬁcient condition seed achieve exact matrix recovery sample number columns rank dataset numerical experiments shown result holds number real-world datasets i.e. obtain exact recovery sampling number columns equal matrix rank. stark contrast random sampling exact recovery cannot guaranteed. column sampling explored extensively machine learning literature task approximating rank matrices paper applied column selection class problems namely sparse representationbased clustering/classiﬁcation subspace clustering. thus important contribution work showing selfexpressive approaches used signal processing computer vision beneﬁt column selection approaches. demonstrated seed provides self-expressive bases amenable solving sparse representation-based learning subspace clustering. case dataset lies union independent subspaces shown condition exact recovery also implies fig. clustering hyperspectral images display results clustering section data ground truth k-means applied original data denoised data random selection columns denoised data obtained seed columns. clustering error lies single multiple lowdimensional subspaces seed discover outliers. idea behind using seed outlier detection sparely represent data point lies low-dimensional subspace small number data points required contrast sparsely represent outlier large number data points needed instance collection signals union k-dimensional subspaces k-dimensional subspace bounded exploit rank revealing property seed determine whether signal lies low-dimensional subspaces ensemble whether outlier. rather setting diagonal matrix alg. sparse coefﬁcients obtained solving objective compute utilize batch solve providing error tolerance algorithm. constraining error important goal sparsity level column determine whether outlier. compute sparse factorization compute number nonzeros column segment columns based upon user threshold. column dense declare outlier sufﬁciently sparse declare inlier. cases setting threshold segment data straightforward however cases setting threshold difﬁcult k-means learn threshold split data. results outlier detection fig. demonstrate rank revealing property seed applied dataset corrupted outliers. along bottom show sparse coefﬁcient matrices obtained seed random sampling spca coefﬁcient matrices show number nonzeros column. case spca because increase observe even smaller sparsity level signals living low-dimensional subspaces outliers. methods compute sparse coefﬁcients schur complement column vector. update formula allows formed updating requires inexpensive vector-vector multiplication. note invertible long non-zero guaranteed sampling rule algorithm terminates case approximation exact. consider calculation note step method evaluate values simultaneously computing entry-wise product matrix summing resulting columns. already formed needed iteration matrix next iteration obtained applying eqn. obtain update formula forms updating matrix previous iteration. update requires matrix-vector vector-vector products. application fast update rule perform incoherent sampling yields alg. accelerated version oasis numerical expeditions. authors would like thank azalia mirhoseini ebrahim songhori farinaz koushanfar helpful discussions assistance developing code running seed large datasets. thanks also matt perich miller mohammad azar collecting sharing neural data used evaluations. funded ccf- ccf- n--- muri wnf--. funded rmh. also funded grfp texas instruments distinguished graduate fellowship. inputs data matrix rm×n vector diag maximum number columns sample number columns sample initially non-negative stopping criterion initialize choose vector random starting indices. w−ct colsum |∆i| select least linearly independent columns k-dimensional subspace dataset. however providing bound covering radius column sampling methods open problem must solved prove stronger results feature selection similar ssc. extending analysis case approximately rank matrices unions overlapping subspaces noisy settings interesting directions future work. na¨ıve implementation column sampling approach described sec. iii-a inefﬁcient step requires matrix inversion form addition calculating errors fortunately done efﬁciently updating results previous step using block matrix inversion formulas rank- updates. provide derivation algorithm pseudocode alg. soltanolkotabi cand`es geometric analysis subspace clustering outliers ann. stat. vol. malik normalized cuts image segmentation ieee trans. pattern anal. mach. intell. vol. august patel goldstein dyer mirhoseini baraniuk oasis adaptive column sampling kernel matrix approximation rice university electrical computer engineering dept. technical report oct. chan bioucas-dias iordache greedy algorithms pure pixels identiﬁcation hyperspectral unmixing multiple-measurement vector viewpoint proc. europ. sig. processing conf. ieee georghiades belhumeur kriegman from many illumination cone models face recognition variable lighting pose ieee trans. pattern anal. mach. intell. vol.", "year": 2015}