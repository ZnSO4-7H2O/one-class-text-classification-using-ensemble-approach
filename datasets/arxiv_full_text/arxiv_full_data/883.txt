{"title": "Depth-Width Tradeoffs in Approximating Natural Functions with Neural  Networks", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "We provide several new depth-based separation results for feed-forward neural networks, proving that various types of simple and natural functions can be better approximated using deeper networks than shallower ones, even if the shallower networks are much larger. This includes indicators of balls and ellipses; non-linear functions which are radial with respect to the $L_1$ norm; and smooth non-linear functions. We also show that these gaps can be observed experimentally: Increasing the depth indeed allows better learning than increasing width, when training neural networks to learn an indicator of a unit ball.", "text": "provide several depth-based separation results feed-forward neural networks proving various types simple natural functions better approximated using deeper networks shallower ones even shallower networks much larger. includes indicators balls ellipses; non-linear functions radial respect norm; smooth non-linear functions. also show gaps observed experimentally increasing depth indeed allows better learning increasing width training neural networks learn indicator unit ball. deep learning form artiﬁcial neural networks seen dramatic resurgence past recent years achieving great performance improvements various ﬁelds artiﬁcial intelligence computer vision speech recognition. empirically successful theoretical understanding deep learning still limited best. emerging line recent works studied expressive power neural networks functions cannot represented networks given architecture particular focus trade-off network’s width depth hand wellknown large enough networks depth already approximate continuous target function arbitrary accuracy hand long evident deeper networks tend perform better shallow ones phenomenon supported intuition depth providing compositional expressibility necessary efﬁciently representing functions. indeed recent empirical evidence suggests even large depths deeper networks offer beneﬁts shallower networks demonstrate power depth neural networks clean precise approach prove existence functions expressed moderately-sized networks given depth cannot approximated well shallower networks even size much larger. however mere existence functions enough ideally would like show depth separation results using natural interpretable functions type expect neural networks successfully train proving depth necessary functions give clearer useful insight various neural network architectures cannot express practice. prove indicator euclidean unit ball easily approximated accuracy using -layer network neurons cannot approximated accuracy higher using -layer network unless width exponential fact show result generally indicator ellipsoid proof based reduction main result eldan shamir shows separation -layer -layer networks using complicated less natural radial function. show depth/width trade-off also observed experimentally speciﬁcally indicator unit ball learned quite well using -layer network using standard backpropagation learning function -layer network signiﬁcantly difﬁcult. theoretical result indicates performance approximation error issues. experiment also highlights fact separation result natural function well-approximated -layer network also learned well data using standard methods. finally prove member wide family non-linear twice-differentiable functions approximated accuracy using relu networks depth width cannot approximated similar accuracy constant-depth relu networks unless width least note similar result appeared online concurrently independently yarotsky liang srikant setting different question studying effect depth neural network received considerable attention recently studied various settings. many works consider somewhat different setting ours hence directly comparable. include networks plain-vanilla ones delalleau bengio martens medabalimi measuring quantities approximation error poole focusing approximation upper bounds measuring approximation error terms l∞-type bounds i.e. supx rather l-type bounds liang srikant note latter distinction important although bounds common approximation theory literature bounds natural context statistical machine learning problems moreover approximation lower bounds stronger sense lower bound easily translates lower bound lower bound vice versa. give trivial example relu networks always express continuous functions therefore never approximate discontinuous function sense easily approximate sense given continuous distribution. noteworthy paper setting telgarsky proves separation result expressivity relu networks depth depth holds even one-dimensional functions depth network shown realize saw-tooth function exp) oscillations whereas network depth would require width superpolynomial approximate constant. fact rely construction proofs results sec. side paper focus separation terms accuracy dimension rather parameter moreover construction relies highly oscillatory function lipschitz constant exponential almost everywhere. contrast paper focus simpler functions type likely learnable data using standard methods. separation results sec. closely related yarotsky liang srikant appeared online concurrently independently work proof ideas quite similar. however papers focused bounds rather bounds. moreover yarotsky considers class functions different positive results liang srikant consider networks employing relu threshold activations whereas consider purely relu network. another relevant insightful work poggio considers width depth provide general results expressibility functions compositional nature. however focus worse-case approximation general classes functions rather separation results terms speciﬁc functions here details setting somewhat orthogonal ours. general bold-faced letters denote vectors capital letters denote matrices probabilistic events. denotes euclidean norm -norm. denotes indicator function. standard asymptotic notation hide constants hide constants factors logarithmic problem parameters. neural networks. consider feed-forward neural networks computing functions network composed layers neurons neuron computes function form weight vector bias term non-linear activation function relu function max{ letting shorthand output layer deﬁne network arbitrary depth recursively represent matrix weights bias layer respectively. following standard convention multi-layer networks ﬁnal layer purely linear function bias i.e. oh−. deﬁne depth network number layers denote number neurons layer size layer. deﬁne width network maxi∈{...l} finally relu network neural network non-linear activations relu function. -layer -layer denote networks depth particular notation -layer relu network form approximation error. given function domain endowed probability distribution deﬁne quality approximation function )µdx ex∼µ refer approximation l-norm sense. results also consider approximation l∞-norm sense deﬁned supx∈x clearly upper-bounds approximation error deﬁned above discussed introduction lower bounds approximation error stronger lower bounds approximation error. indicators balls ellipsoids begin considering simplest possible function classes namely indicators balls ability compute functions necessary many useful primitives example determining distance points euclidean space threshold section show depth separation result functions although easily approximated -layer networks -layer network approximate high accuracy w.r.t. distribution unless width exponential dimension. formally stated following theorem theorem following holds positive universal constants network employing activation function satisfying assumptions eldan shamir non-singular matrix rd×d exists continuous probability distribution function computed -layer network width function formal proof thm. based reduction main result eldan shamir shows existence certain radial function probability distribution cannot expressed -layer network whose width less exponential dimension constant accuracy. closer look proof reveals function expressed indicators balls various radii. argue could accurately approximated given ball indicator respect distributions could approximated indicators whose hence reach contradiction. linear transformation argument show contradiction would occured could approximated indicators non-degenerate ellipse respect distribution. formal proof provided below result hand turn complete proof. consider function eldan shamir proven -layer network approximate w.r.t. better constant accuracy unless width exponential dimension particular written however since linear combination -layer neural networks width still -layer ˜fai) -layer network width approximates accuracy less hence picking sufﬁciently small contradiction result eldan shamir -layer network width smaller approximate constant accuracy sufﬁciently large dimension complement thm. also show indicator functions easily approximated -layer networks. argument quite simple using activation relu sigmoid layer approximate lipschitz continuous function bounded interval particular given vector apply construction coordinate seperately hence similarly approximate arbitrary ﬁxed matrices vectors finally -layer network second layer compute continuous approximation threshold function composing layers arbitrarily good approximation function w.r.t. continuous distribution network size scaling polynomially dimension required accuracy. theorem below formalize intuition simplicity focus approximating indicator unit ball theorem given activation function satisfying assumption eldan shamir continuous probability distribution exists constant dependent function expressible -layer network width figure experiment results depicting network’s root mean square error training validation function number batches processed. best viewed color. subsection empirically demonstrate indicator functions balls indeed easier learn -layer network compared -layer network indicates depth/width trade-off indicators balls predicted theory indeed observed experimentally. moreover highlights fact separation result simple natural functions learned reasonably well data using standard methods. experiment sampled data instances direction chosen uniformly random norm drawn uniformly random interval instance associated target value computed according target function another examples generated similar manner used validation set. training performed backpropagation using tensorflow library. used squared loss batches size networks chose momentum parameter learning rate starting decaying multiplicative factor every batches stopping results presented fig. clearly seen -layer network achieves signiﬁcantly better performance -layer networks. true even though networks signiﬁcantly larger parameters performance exact opposite might expected based parameter counting alone. moreover increasing width -layer networks exhibits diminishing returns performance improvement doubling width much larger doubling width indicates would need much larger -layer network match -layer width network’s performance. thus conclude network’s depth indeed plays crucial role -layer networks inherently suitable express indicator functions type studied. considered functions depending norm turn consider functions depending norm. focusing relu networks show certain separation result holding non-linear function depends input -norm theorem function give concrete example suppose cannot approximated linear function better \u0001-neighborhood taking layer network approximate function unless width ˜ω)}). side expressed exactly -layer output neuron simply identity function. argument would work piecewise-linear generally kind argument would work function exhibiting non-linear behavior points functions well-approximated -layer networks approximating -layer network lower bound size speciﬁed theorem. intuitively proof relies showing good -layer approximation must capture non-linear behavior close most points satisfying however -layer relu bj]+ piecewise linear non-linearities union hyperplanes ∪j{x implies most points s.t. must \u0001-close hyperplane however geometry ball neighborhood single hyperplane cover small portion ball need cover ball. using appropriate construction show required number hyperplanes least long exp) formal proof appears subsection note bound thm. weaker nature bound previous section lower bound polynomial rather exponential nevertheless believe point balls also pose geometric difﬁculty -layer networks conjecture lower bound considerably improved indeed moment know approximate function -layer networks better constant accuracy using less neurons. section establish depth separation result approximating continuously twice-differentiable functions using relu neural networks. unlike previous results paper separation depths larger depending required approximation error. also results respect uniform distribution mentioned earlier results techniques section closely related independent results yarotsky liang srikant emphasis rather approximation bounds focus somewhat different network architectures function classes. clearly functions difﬁcult approximate instead consider functions certain degree non-linearity sense hessians non-zero along direction signiﬁcant portion domain. formally make following deﬁnition deﬁnition denote uniform distribution function denote uniform distribution largest connected domain point curvature least along ﬁxed direction prototypical functions interested lower bounded constant stress results section hold equally well considering condition well however sake simplicity focus former condition appearing def. goal show depth separation result inidividually function attainable approximation error using relu neural network given depth width theorem function function expressible relu network depth maximal width holds theorem conveys tradeoff depth width approximating function using relu networks error cannot decay faster polynomially width bound deteriorates exponentially depth show later deterioration stem looseness bound well-behaved indeed possible construct relu networks approximation error decays exponentially depth. proof thm. appears subsection based series intermediate results. first show strictly curved function cannot well-approximated sense piecewise linear functions unless number linear regions large. ﬁrst establish necessary tools based legendre polynomials. prove result speciﬁc one-dimensional case including explicit lower bound target function quadratic strongly convex concave expand construction error lower bound figure relu approximation function obtained extracting bits. number linear segments grows exponentially number bits approximating network size. general dimension depending number linear regions approximating piecewise-linear function. finally note relu network induces piecewise-linear function bound number linear regions induced relu network given width depth combining previous lower bound yields thm. turn complement lower bound approximability result showing depth wide family functions thm. applies approximated exponentially high accuracy. speciﬁcally consider functions approximated using moderate number multiplications additions values intermediate computations bounded idea construction depth allows compute highly-oscillating functions extract high-order bits binary representation inputs. given bits compute product procedure resembling long multiplication shown fig. formally proven follows proof thm. begin observing using simple linear change variables assume without loss generality rescale interval back original domain error multiply factor requiring accuracy next compute size network required implement approximation. compute neurons required therefore computed using layers neurons each ﬁnally composing requires subsequent layer neurons. implement extractor therefore require network size using dummy neurons propagate architecture extracting signiﬁcant bits size adding ﬁnal component performing multiplication estimation require layers width respectively increase width propagate penultimate layer resulting network size thm. shows multiplication performed accurately deep networks. moreover additions computed relu networks exactly using single layer neurons arbitrary given terms relu summation repeating arguments function approximated bounded number operations involving additions multiplications also approximated well moderately-sized networks. formalized following theorem provides approximation error upper bound theorem ftm\u0001 family functions domain property ftm\u0001 approximable accuracy respect inﬁnity norm using operations involving weighted addition ﬁxed; multiplication intermediate computation stage bounded interval exists universal constant exponentially valuable width function target accuracy corollary suppose c∩ftm approximating accuracy norm using ﬁxed depth relu network requires width least poly whereas exists relu network depth width approximates accuracy inﬁnity norm polynomial depending solely research supported part marie curie grant israel science foundation grant intel icri-ci institute. would like thank shai shalev-shwartz illuminating discussions eran amar valuable help experiment.", "year": 2016}