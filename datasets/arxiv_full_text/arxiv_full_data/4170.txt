{"title": "Context-aware Captions from Context-agnostic Supervision", "tag": ["cs.CV", "cs.AI"], "abstract": "We introduce an inference technique to produce discriminative context-aware image captions (captions that describe differences between images or visual concepts) using only generic context-agnostic training data (captions that describe a concept or an image in isolation). For example, given images and captions of \"siamese cat\" and \"tiger cat\", we generate language that describes the \"siamese cat\" in a way that distinguishes it from \"tiger cat\". Our key novelty is that we show how to do joint inference over a language model that is context-agnostic and a listener which distinguishes closely-related concepts. We first apply our technique to a justification task, namely to describe why an image contains a particular fine-grained category as opposed to another closely-related category of the CUB-200-2011 dataset. We then study discriminative image captioning to generate language that uniquely refers to one of two semantically-similar images in the COCO dataset. Evaluations with discriminative ground truth for justification and human studies for discriminative image captioning reveal that our approach outperforms baseline generative and speaker-listener approaches for discrimination.", "text": "figure illustration tasks requiring pragmatic reasoning explored paper. justiﬁcation given image bird target class distractor class describe target image explain belongs target class distractor class. distractor class images shown illustration provided algorithm. discriminative image captioning given similar images produce sentence identify target image distractor image introspective speaker model improves context-free speaker. vein desirable endow machines pragmatic reasoning. approach would collect training data language used context example discriminative ground truth utterances people describing images context images justiﬁcations explaining image contains target class opposed distractor class unfortunately collecting data prohibitive cost since space objects possible contexts often large. furthermore cases context wish pragmatic unknown apriori. example free-form conversation agent respond context-aware discriminative fashion depending upon history conversation. scenarios also arise human-robot interaction case where robot need reason spoon person asking for. thus paper focus deriving pragmatic behavior given access generic ground truth. introduce inference technique produce discriminative context-aware image captions using generic context-agnostic training data example given images captions siamese tiger generate language describes siamese distinguishes tiger cat. novelty show joint inference language model context-agnostic listener distinguishes closely-related concepts. ﬁrst apply technique justiﬁcation task namely describe image contains particular ﬁne-grained category opposed another closely-related category cub- dataset. study discriminative image captioning generate language uniquely refers semantically-similar images coco dataset. evaluations discriminative ground truth justiﬁcation human studies discriminative image captioning reveal approach outperforms baseline generative speaker-listener approaches discrimination. language primary modality communicating representing knowledge. convey relevant information often language takes account context. example instead describing situation literal might pragmatically emphasize selected aspects order persuasive impactful effective. consider target image bottom left fig. literal description airplane ﬂying conveys semantics image would inadequate goal disambiguate image distractor image purpose pragmatic description would large passenger ﬂying blue sky. description aware context namely distractor image also airplane. people pragmatic considerations continuously effortlessly study qualitatively different real-world vision tasks require pragmatic reasoning. ﬁrst justiﬁcation model needs justify image corresponds ﬁne-grained object category opposed closely related undepicted category. justiﬁcation task important hobbyists domain experts ornithologists botanists often need explain image depicts particular species opposed closelyrelated species. another potential application justiﬁcation machine teaching algorithm instructs non-expert humans concepts. second task discriminative image captioning goal generate sentence describes image context semantically similar images. task grounded pragmatics also interesting scene understanding task check ﬁne-grained image understanding. also potential applications human robot interaction. recent work andreas klein derives pragmatic behaviour neural language models using contextfree data. motivated similar considerations algorithmic novelty work uniﬁed inference procedure leads efﬁcient search discriminative sentences approach based realization simply re-use sampling distribution generative model instead training separate model assess discriminativeness also important implications practitioners since easily adapt existing context-free captioning models context-aware captioning without additional training. furthermore applied abstract scenes dataset apply model qualitatively different real-image datasets ﬁne-grained birds dataset cub- coco dataset contains real-life scenes common objects. summary contributions paper novel inference procedure models introspective speaker allowing speaker reason pragmatic behavior without additional training. tasks studying discriminative behaviour pragmatics grounded vision justiﬁcation discriminative image captioning. dataset evaluate justiﬁcation systems ﬁne-grained bird images captions triplets. evaluations cub-justify human evaluation coco show approach outperforms baseline approaches inducing discrimination. work grice analyzed cooperative multiagent linguistic agents could model others’ behavior achieve common objective. consequently pragmatics literature studied higher-level behavior agents including conversational implicature gricean maxims works derive pragmatic behavior given minimal assumptions individual agents typically hand-tuned lexicons rules. recently exciting developments applying reinforcement learning techniques problems requiring less manual tuning. also interested deriving pragmatic behavior focus scaling context-sensitive behavior vision tasks. works model ideas pragmatics learn language games played online humanrobot collaboration similar spirit interested applying ideas pragmatics build systems provide justiﬁcations provide discriminative image captions relevant work recent work deriving pragmatic behavior abstract scenes made clipart andreas klein unlike technique proposed approach require training second listener model supports efﬁcient inference details provided sec. beyond image captioning image captioning task generating natural language description image seen quick progress recently research shifted beyond image captioning addressing tasks like visual question answering referring expression generation ﬁll-in-the-blanks similar spirit tasks introduce here justiﬁcation discriminative image captioning viewed beyond image captioning tasks. sadovnik ﬁrst studied discriminative image description task goal distinguishing image images. approach incorporates cues discriminability saliency uses hand-designed rules constructing sentences. contrast develop inference techniques induce discriminative behavior neural models. reference game also seen discriminative image captioning task abstract scenes made clipart interested domain real images. work generating referring expressions generates discriminative captions refer particular objects image given context-aware supervision. work different sense address instance pragmatic reasoning common case contextdependent data available training. rationales several works studied machines understand human rationales including enriching classiﬁcation asking explanations humans incorporating human rationales active learning contrast focus machines providing justiﬁcations humans. could potentially allow machines teach concepts humans recent work looks post-hoc explanations classiﬁcation decisions. instead explaining model thinks image particular class describes image class predicted classiﬁer. unlike task justiﬁcation task requires reasoning explicit context distractor class. further interested providing rationalizations classiﬁcation decisions explaining differences confusing concepts humans. show comparison appendix demonstrating importance context justiﬁcation. beam search modiﬁed objectives beam search approximate greedy technique inference sequential models. perform beam search modiﬁed objective introspective speaker model induce discrimination. similar spirit recent works inducing diversity beam search maximum mutual information inference sequence-to-sequence models describe approach inducing context-aware language justiﬁcation context another class discriminative image captioning context semantically similar image. clarity ﬁrst describe formulation justiﬁcation discuss modiﬁcation discriminative image captioning. justiﬁcation task wish produce sentence comprised sequence words {si} based given image target concept context distractor concept produced justiﬁcation capture aspects image discriminate target distractor concepts. note images distractor class provided algorithm. ﬁrst train generic context-agnostic image captioning model using training data reed collected captions describing bird images cub-- dataset. condition model addition image. model helps produce better sentences also cornerstone approach discrimination language models recurrent neural networks represent state-of-the-art language modeling across range popular tasks like image captioning machine translation etc. induce discrimination utterances language model natural consider using generator speaker models conjunction listener function scores discriminative utterance task pragmatic reasoning speaker then select utterances good sentences generative model discriminative controls tradeoff linguistic adequacy sentence discriminativeness. similar reasoning speaker model forms core approach implemented using multi-layer perceptrons noted selecting utterances reasoning speaker poses several challenges. first exact inference model exponentially large space sentences intractable. second general would expect discriminator function factorize across words making joint optimization reasoning speaker objective difﬁcult. thus andreas klein adopt sampling based strategy considered proposal distribution whose samples ranked linear combination importantly distribution full sentences hence effectiveness formulation depends heavily distribution captured since search space strings solely based speaker. inefﬁcient especially mismatch statistics context-free unknown context-aware sentence distributions. cases must resort drawing many samples good discriminative sentences. approach incorporating contextual behavior based simple modiﬁcation listener given generator construct listener module wants discriminate using following log-likelihood ratio listener depends generative model classes name introspector emphasize step re-uses generative model need train explicit listener model. substituting introspector induces following introspective speaker model discrimination figure emitter-suppressor beam search beam size distinguishing image black-throated blue warbler distractor class black white warbler. green language model produces caption white belly breast feeding distractor class language model since birds share attribute white belly appears image term white highly suppressed. blue picking likely words emitter unlikely suppressor yields discriminative caption blue throat note emitter suppressor share history important consider trade-off parameter affects produced sentences. model generates descriptions ignore context. extreme values likely make produced sentences different sentence training trivial assume exists wide enough range creating sentences discriminative well-formed. however results indicate range exists practice. given target image distractor dthat wish distinguish similar classes justiﬁcation task. construct speaker task training standard image captioning model. given speaker construct emitter-suppressor equation approach provide sensible results similar. expect humans describe similar concepts similar ways hence different thus introspector less likely overpower speaker cases note sufﬁciently different concepts speaker alone likely sufﬁcient discrimination. describing concept isolation likely enough discriminate different unrelated concept. careful inspection introspective speaker model reveals desirable properties previous work first introspector model need training since depends original generative model. thus existing language models readily re-used produce context-aware outputs conditioning demonstrate empirical validation sec. would help scale approach scenarios known apriori concepts need discriminated contrast approaches train separate listener module. second leads uniﬁed efﬁcient inference introspective speaker describe next. describe search algorithm implementing maximization call emitter-suppressor beam search. beam search algorithm heuristic graph-search algorithm commonly used inference recurrent neural networks ﬁrst factorize posterior log-probability terms introspective speaker equation length sentence. combine terms yielding following emitter-suppressor objective introspective speaker emitter generative model conditioned target concept deciding token select given timestep. suppressor conditioned distractor concept providing signals emitter tokens avoid. intuitive discriminative want emit words match avoid emitting words match maximize emitter-suppressor objective using beam search. vanilla beam search typically used language models prunes output space every timestep keeping top-b sentences highest log-probabilities instead provide details dataset cubjustify dataset used evaluation speakertraining setup justiﬁcation task. discuss experimental protocols discriminative image captioning. justiﬁcation dataset dataset contains images species north american birds. image dataset annotated ﬁne-grained captions reed captions mention various details bird mentioning name bird species. cub-justify dataset collect dataset ground truth justiﬁcations evaluating justiﬁcation. ﬁrst sample target distractor classes within hyper-category created based last name folk names species cub. instance rufous hummingbird ruby throated hummingbird fall hyper-category hummingbird. induce hyper-categories. largest single hypercategory warbler categories. select subset images test cub- classes form cub-justify test split. rest speaker training workers shown image rufous hummingbird instance images belonging distractor class ruby throated hummingbird form visual notion distractor class. also shown diagram morphology birds indicating various parts tarsus rump wingbars etc. instruction describe target image confused images distractor class. birds best distinguished non-visual cues call migration patterns. thus drop categories birds original list triplets labeled hard distinguish workers. process left triplets captions each. split dataset validation test examples respectively. details interface found appendix. speaker training implement model similar show attend tell modifying original model provide class input similar spirit exact details model architecture given appendix. train model cubjustify train split. recall context-agnostic captions used image captioning computed context-agnostic captions captioning model image class input reaches validation score cider-d original image-only captioning model reaches cider-d scores similar range existing captioning approaches justiﬁcation evaluation measure performance justiﬁcation captions cub-justify discriminative captions using cider-d metric. ciderweighs n-grams inverse document frequencies giving higher weights sentences content n-grams generic n-grams further cider-d captures importance ngram image. instance emphasizes beak over black belly beak used often human justiﬁcations. also report meteor scores completeness. detailed discussion metrics found appendix. discriminative image captioning dataset want test reasoning context introspective speaker help discriminate pairs similar images coco dataset. construct confusing image pairs follow strategies. first easy confusion image validation nearest neighbor space pre-trained vgg- repeat process neighbor ﬁnding randomly chosen source images. second hard confusion narrow list semantically similar confusing images speaker model nearest neighbor images compute word-level overlap generated sentences. pick pairs overlap. interestingly pairs identical captions. reﬂects issue output image captioning models lacking diversity seeming templated speaker training evaluation train generative speaker emitter-suppressor beam search using model implemented neuraltalk project train/val/test splits trained ﬁnetuned speaker model achieves performance cider-d test set. seen category information used task. evaluate approaches discriminative image captioning based often help humans select correct image pair images. table cub-justify test results cider-d meteor scores computed test cub-justify. model used best selected validation error values standard error mean semi-blind-is outperforms methods. also performs better approaches beam search select high log-probability sentences. indicates absence ground truth justiﬁcations indeed discrepancy searching discriminativeness searching highly likely context-agnostic sentence. perform comparisons baseline sweeping samples generator listener reranking using samples gets comparable cider-d scores semi-blind-is approach beam size suggests semi-blind-is approach computationally efﬁcient exploring output space emitter-suppressor beam search allows joint greedy inference speaker introspector leading meaningful local decisions. completeness also trained listener module discriminatively used ranker found gets cider-d validation lower showing bottleneck performance sampling rather discriminativeness listener. details found appendix. test performance table. details performance models test cub-justify model using best-performing validation introspective-speaker models strongly outperform baselines semi-blind-is slightly outperforming model. could performance semi-blind-is less sensitive exact choice among baselines best performing method blind-is model presumably model emitter-suppressor beam search baseline approaches rely sampling regular beam search respectively. qualitative results next showcase qualitative results demonstrate aspects pragmatics context dependence captured best-performing semiblind-is model. fig. demonstrates sentences uttered introspective speaker change sentence describes image well oblivious context sentence small sized bird long pointed bill. discriminative hummingbirds birds among hummingbirds figure cub-justify validation results cider-d cubjustify validation. introspective speaker approaches semiblind-is) models perform best followed class-only introspective speaker semi-blind-is outperforms methods wider range approaches reason pragmatics beat baseline generative approach error bars denote standard error mean score estimated across validation set. tioned image target class ignores distractor class; semi-blind-is introspective speaker listener access image speaker does; blind-is introspective speaker withaccess image conditioned classes; implementation andreas klein using language model listener models ranking samples keep things comparable). approaches beams/samples unless stated otherwise. validation performance fig. shows performance cub-justify validation function hyperparameter controlling tradeoff speaker introspector baseline stands tradeoff log-probability sentence score discriminator function sample re-ranking. interesting observations emerge. first semi-blind-is models outperform baselines range values. model better overall semi-blind-is stable performance wider range indicates conditioned image introspector highly discriminative overcome signals image since discrimination classes. second decreased methods improve sentences become discriminative worse becomes low. likely happen model explores rare tokens parts output space seen training leading badly-formed sentences effect stronger models since searches output space samples generator ranks using joint reasoning speaker objective interestingly approach samples generator figure effect context weight image rufous hummingbird context another hummingbird type. generative description describes bird long beak feature discriminative. taking account context intermediate values yield descriptions highlight rufous brown throat. model force sentences well formed. figure importance visual signal justiﬁcation ﬁnegrained categories. given image green kingﬁsher blindis model says bird chest inaccurate image long pointy beak discriminative feature context. time semi-blind-is model mentions green crown avoids uttering chest. given complicated intra-category invariances bird categories intuitive image signal important justiﬁcation. figure effect context class image tennessee warbler light green wings white eyebrow. described context mourning warbler green description highlights target bird white eyebrow. described context black white warbler description highlights target bird green color. model captures discriminative features neck white belly throat. interestingly model avoids saying long beak feature shared birds. next fig. demonstrates selected utterances change based context. limitation approach that since model never sees discriminative training data cases produces repeated words encouraged discriminative inference time. finally fig. illustrates importance visual reasoning justiﬁcation task. fine-grained species often large intra-class variances blind approach justiﬁcation would ignore. thus good justiﬁcation approach needs grounded image signal pick discriminative cues appropriate given instance. explained sec. create sets semantically similar target distractor images easy confusion based features alone hard confusion based sentences generated speaker interested understanding emitter-suppressor inference helps identify target image better generative speaker baseline. thus approaches speaker introspectable image pairs correctly discriminated humans based descriptions coco. introspective speaker better pointing target image given confusing distractor image across easy hard data splits speaker standard error precision report numbers tive speaker based results dataset. approaches beam size human studies setup annotation forced choice study show caption raters asking pick image sentence likely describing.. target distractor image pair tested generated captions. check fraction times method caused target image picked human. discriminative image captioning method considered better enables humans identify target image often. results study summarized table. approach outperforms baseline speaker easy confusion well hard confusion splits. however gains approach larger hard confusion split intuitive. qualitative results qualitative results coco experiments shown fig. target image successfully identiﬁed shown green border. show examples model identiﬁes target image better ﬁrst rows failure cases third row. notice model able modify utterances account context pragmatics going note sentences typically respect grammatical constructs despite forced discriminative. figure pairs images whose captions generated generic captioning speaker baseline identical. apply introspective speaker technique distinguish image left image right pair. target image shown green border generated sentence able identify correctly. notice introspective speaker often refers unambiguously target image. example sheep image generated sentence mentions sheep grazing lush green ﬁeld. bottom show failure examples. bottom left example interesting model calls stop sign policeman. cases distributions captured emitter supressor rnn’s identical approach produces sentence baseline discussion describing absence concepts inducing comparative language exciting directions future work justiﬁcation. instance justifying image lion tiger would useful able because stripes. because hair face. beyond pragmatics justiﬁcation task also interesting relations human learning. indeed experience learn better someone takes time justify explain point view. imagine justiﬁcations helpful machine teaching teacher provide justiﬁcations human learner explaining rationale image belonging particular ﬁne-grained category opposed different possibly mistaken confusing ﬁne-grained category. fundamental limitations inducing context-aware captions context-agnostic supervision. instance distinct concepts similar human-generated context-free descriptions identical model would fail extract discriminative signal. indeed hard address situations without context-aware ground truth. higher-order reasoning without necessarily setting policy gradient estimators reward functions. indeed inference objective also formulated training. however initial experiments yeild signiﬁcant performance improvements. introduce novel technique deriving pragmatic language recurrent neural network language models namely image-captioning model takes account context distractor class distractor image. technique used inference time better discriminate concepts without seen discriminative training data. study tasks vision language domain require pragmatic reasoning justiﬁcation explaining image belongs category opposed another discriminative image captioning describing image distinguish closely related image. experiments demonstrate strength method generative baselines well adaptations previous work setting. make code datasets available online. acknowledgements thank duerig support guidance shaping project. thank david rolnick bharadwaja ghali vahid kazemi help cub-justify dataset. thank ashwin kalyan sharing trained checkpoint discriminative image captioning experiments. also thank stefan andreas veit chris shallue. work funded part career grant n--- sloan fellowship allen distinguished investigator google faculty research award amazon academic research award organize appendix follows sec. analysis performance consider unre sec. generating visual explanations adapted sec. architectural changes show attend tell image captioning model justiﬁcation. sec. optimization details justiﬁcation speaker sec. choice metrics evaluating justiﬁcation. sec. cub-justify data collection details. sec. analysis baseline detail. sec. comparison approach baseline discriminatively trained listener used reranking model. coco qualitative results coco qualitative examples fig. shows qualitative results discriminative image captioning hard confusion split coco dataset. notice introspective speaker captions model context explicitly often discriminative helping identify target image clearly baseline speaker approach example second model generates caption delta passenger ﬂying clear blue discriminative caption baseline caption large passenger ﬂying blue applies target distractor images. effect increasing distance illustrate quality discriminative captions introspective speaker approach varies distractor image becomes less relevant target image target image left show -nearest neighbor th-nearest neighbor randomly selected distractor image. pick random image distractor generated discriminatve captions become less comprehensible losing relevance well grammatical structure. consistent understanding introspective speaker formulation sec. modeling context explicitly inference helps discrimination context relevant. context relevant randomly picked images original speaker model likely sufﬁcient discrimination. hendricks propose method explain classiﬁcation decisions user providing post-hoc rationalizations. given prediction classiﬁer work generates caption conditioned predicted class original image. hendricks provide rationale classiﬁcation focus related different problem concept justiﬁcation. namely want explain image contains target class opposed speciﬁc distractor class hendircks want explain classiﬁer thought image contains particular class. thus unlike visual explanation task intuitive justiﬁcation task requires explicit reasoning context. verify hypothesis ﬁrst adapting work justiﬁcation task using speaker augmenting speaker approach construct intropsective speakerm accounts context. interestingly introspective speaker approach helps improve performance generating visual explanations justiﬁcation. approach hendricks differs setup important ways. firstly uses stronger namely ﬁne-grained compact-bilinear pooling provides state-of-the-art performance dataset. secondly make explanations grounded class information also constraint induce captions speciﬁc class. achieved using policy gradient reward function models given sentence class thus sense approach encourages model produce sentences highly discriminative given class classes opposed particular distractor class interested justiﬁcation. finally policy gradient used conjunction standard maximum likelihood training train explanation model. inference explanation model conditioning caption generation predicted class. modify inference setup slightly condition caption generation target class justiﬁcation opposed predicted class explanation. call vis-exp approach. apply emittersuppressor beam search account context giving introspective visual explanation model given stronger image features complicated training procedure involving policy gradients vis-exp approach achieves strong cider-d score standard error cubjustify test set. note cub-justify test strict subset test results better achieved semi-blind-is model based regular image features vgg- imfigure qualitative examples discriminative image captioning denotes examples standard image captioning model generates caption images. method’s outputs shown target image shown left marked green border approach accurate well discriminative. second last example shows case model discriminative inaccurate original target image last example shows case caption neither accurate discriminative. figure show target image distractor images varying distances along generated captions. denotes distance target distractor images space. output speaker shown target image output introspective speaker considering distractor image context turn shown corresponding distractor image. caption distractor image describes target image distinguishing distractor. notice introspective speaker method often works well nearest neighbour nearest neighbor produces incomprehensible sentences distractor irrelevant. indeed random distractor baseline speaker outputs often sufﬁcient discrimination intuitive. however mentioned before approach similar baseline speaker cannot explicitly model context speciﬁc distractor class inference. approach reasons given image hummingbird talk long beak cannot reason speciﬁc distractor class presented inference. distractor class another hummingbird long beak would want avoid talking long beak justiﬁcation. hand distractor class hummingbird shorter beak exist hummingbirds long beak would important feature mention justiﬁcation. clearly non-trivial realize without explicitly modeling context. hence intuitively would expect incorporating context distractor class help justiﬁcation task. explained previously implement emittersuppressor inference vis-exp approach yielding vis-exp-is approach. sweep values validation best performance achieved plugging value evaluating test vis-exp-is approach achieves cider-d table cub-justify test results compare vis-exp emitter-suppressor beam search implemented vis-exp namely vis-exp-is. achieve gains vis-exp approach explicitly reasoning context using introspective speaker justiﬁcation task. error values standard error mean. score standard error improvement cider-d. gains vis-exp lower gains approach presumably vis-exp approach already captures context-independent discriminative signals policy gradient training. overall though results provide evidence emitter-suppressor inference scheme adapted variety context-agnostic captioning models effectively induce context awareness during inference. remaining equations lstm recurrence remain adding class information deep output layer show attend tell uses deep output layer compute output word distribution every timestep incorporating signals lstm hidden state context vector input word exp) matrices used project dimensions word embeddings output layer produces output size vocabulary. similar previous adaptations class embedding addition context vector predict output every timestep blind models implementing class-only blindis model need train model uses class produce sentence. this drop attention component model equivalent setting zero equations model using class embedding captioning network trained using rmsprop batch size learning rate decayed learning rate every epochs cycling training data. word embedding embeds words dimensional vector lstm hidden cell state sizes similar show attend tell model coco. rest design choices closely mirror original work based implementation available https// github.com/kelvinxu/arctic-captions. make tensorﬂow implementation show attend tell publicly available. section expand discussion choice metrics evaluating justiﬁcation addition metrics report main paper namely cider-d meteor also considered using recently introduced spice spice metric uses dependency parser extract scene graph representation candidate reference sentences computes f-measure scene graph representations. given metric uses dependency parser intermediate step unclear well would scale justiﬁcation task sentences model might good justiﬁcations exactly grammatical. periments cub. note explanation section coco models trained using neuraltalk package implements show tell captioning model vinyals changes understood three simple modiﬁcations aimed class information model. ﬁrst embed class label continuous vector three changes then show attend tell model follows changes initial lstm state original show attend tell model uses image annotation vectors outputs convolutional feature compute initial cell hidden states long-short term memory image annotation vector averaged across spatial locations used compute initial state follows changes lstm recurrence show attend tell computes scalar attention location feature uses compute context vector every timestep attending image annotation also embeds input word using embedding matrix uses previous hidden state compute following lstm recurrence every timestep producing outputs table cub-justify validation results spice scores computed validation cub-justify. model used best value. error values standard error mean. outperforms methods good margin spice. discriminative justiﬁcations emerge result tradeoff high-likelihood sentences discrimination note tradeoff inherent since don’t ground truth discriminative training data. thus spice problematic metric context. however sake completeness report spice numbers validation giving approach access best value table. although outperform baselines using spice metric corner cases also found spice metric scores slightly un-interpretable. example candidate sentence this bird speckled belly breast short pointy bill. reference sentences this bird yellow eyebrow grey auriculars this bird yellow supercilium white throat spice scores higher would expect reference intuitively related sentence this grey yellow bird yellow eyebrow. obtains lower spice score reference sentences. investigation revealed relation f-measure roughly measures sentences encode relations high score corner cases. hypothesize inconcsistency scores might because spice uses soft similarity wordnet computing f-measure might calibrated ﬁne-grained domain specialized words supercilium auriculars etc. result observations decided perform evaluations spice metric. provide details collection cubjustify dataset presented target image selected target class workers along distractor images belonging distractor class. distractor images chosen random validation test split dataset created justiﬁcation. non-expert workers unlikely given explicit visual model given ditractor category indigo bunting. thus distractor images shown entail concept distractor class justiﬁcation. explained sec. choice distractor classes made based hierarchy induce using folk names birds. given target class distractor class images workers asked describe target image manner sentence confusing respect distractor images. further workers instructed someone reads sentence able recognize target image distinguishing distractor images. order workers attention images told explicitly distractor images belonged other unique distractor class. helping identify minute difference images birds well enabling workers write accurate captions also showed diagram morphology bird also showed list parts examples shown diagram eyeline rump eyering etc. list words well examples morphology diagram picked based consultation ornithology hobbyist. workers also explicitly instructed describe target image accurate manner mentioning details present target image opposed providing jusitiﬁcations talk features absent. initial rounds data collection revealed interesting corner cases caused ambiguity. example workers confused whether part bird called gray white could appear gray either part white shadow part actually gray. initial rounds feedback proceeded collect entire dataset. figure diagram morphology bird labeling different parts. diagram shown workers getting justiﬁcations explaining image contains target class distractor class. comparsion also evaluate reasoning speaker approaches different rankers introspector main paper) chance ranker randomly scores class sentence. trained listener marginally better turn better thus trained listener marginal impact performance larger factor affecting performance sampling semi-blind-is approach able effectively.", "year": 2017}