{"title": "Model compression as constrained optimization, with application to  neural nets. Part II: quantization", "tag": ["cs.LG", "cs.NE", "math.OC", "stat.ML"], "abstract": "We consider the problem of deep neural net compression by quantization: given a large, reference net, we want to quantize its real-valued weights using a codebook with $K$ entries so that the training loss of the quantized net is minimal. The codebook can be optimally learned jointly with the net, or fixed, as for binarization or ternarization approaches. Previous work has quantized the weights of the reference net, or incorporated rounding operations in the backpropagation algorithm, but this has no guarantee of converging to a loss-optimal, quantized net. We describe a new approach based on the recently proposed framework of model compression as constrained optimization \\citep{Carreir17a}. This results in a simple iterative \"learning-compression\" algorithm, which alternates a step that learns a net of continuous weights with a step that quantizes (or binarizes/ternarizes) the weights, and is guaranteed to converge to local optimum of the loss for quantized nets. We develop algorithms for an adaptive codebook or a (partially) fixed codebook. The latter includes binarization, ternarization, powers-of-two and other important particular cases. We show experimentally that we can achieve much higher compression rates than previous quantization work (even using just 1 bit per weight) with negligible loss degradation.", "text": "consider problem deep neural compression quantization given large reference want quantize real-valued weights using codebook entries training loss quantized minimal. codebook optimally learned jointly ﬁxed binarization ternarization approaches. previous work quantized weights reference incorporated rounding operations backpropagation algorithm guarantee converging loss-optimal quantized net. describe approach based recently proposed framework model compression constrained optimization results simple iterative learning-compression algorithm alternates step learns continuous weights step quantizes weights guaranteed converge local optimum loss quantized nets. develop algorithms adaptive codebook ﬁxed codebook. latter includes binarization ternarization powers-of-two important particular cases. show experimentally achieve much higher compression rates previous quantization work negligible loss degradation. widespread application deep neural nets recent years seen explosive growth size training sets number parameters nets amount computing power needed train them. present deep neural nets upwards many million weights common applications computer vision speech. many applications particularly useful small devices mobile phones cameras sensors limited computation memory communication bandwidth short battery life. becomes desirable compress neural memory storage smaller and/or runtime faster consumes less energy. neural compression problem interest already early days neural nets driven example desire implement neural nets vlsi circuits. however current wave deep learning work resulted ﬂurry papers many academic particularly industrial labs proposing various ways compress deep nets various standard forms compression used another low-rank decomposition quantization binarization pruning others. paper focus quantization ordinarily unconstrained real-valued weights neural forced take values within codebook ﬁnite number entries. codebook adaptive entries learned together quantized weights ﬁxed includes speciﬁc approaches binarization ternarization powers-of-two approaches. among compression approaches quantization great interest even crudely quantizing weights trained produces considerable compression little degradation loss task hand however ignores fact quantization independent loss indeed achieving really number bits weight would incur large loss make quantized unsuitable practical use. previous work applied quantization algorithm previously trained reference incorporated ad-hoc modiﬁcations basic backpropagation algorithm training net. however none approaches guaranteed produce upon convergence quantized weights optimal loss among possible quantized nets. paper primary objectives provide mathematically principled statement quantization problem involves loss resulting provide algorithm solve problem local optima eﬃcient convenient way. starting point recently proposed formulation general problem model compression constrained optimization problem develop case constraints represent optimal weights coming codebook. results learning-compression algorithm alternates optimization loss real-valued weights quadratic regularization term quantization current real-valued weights. quantization step takes form follows necessarily problem deﬁnition without ad-hoc decisions k-means adaptive codebooks optimal assignment ﬁxed codebooks binarization ternarization powers-of-two show experimentally compress deep nets considerably previous quantization algorithms—often maximum possible compression single weight without signiﬁcant error degradation. much work exists compressing neural nets using quantization low-rank decomposition pruning techniques carreira-perpi˜n´an references therein. focus exclusively work based quantization. quantization neural weights recognized important problem early neural literature often goal eﬃcient hardware implementation received much attention recently. main approaches types. ﬁrst consists using low-precision ﬁxed-point weight representations form rounding even single-bit values. seen quantization using ﬁxed codebook second approach learns codebook form soft hard adaptive quantization. also work using low-precision arithmetic directly training focus work whose goal quantize neural real-valued non-quantized weights. work explored binarization ternarization general powers-of-two quantization quantization forms revisited recent years impressive results large neural nets trained gpus much innovation algorithmically. basic idea papers essentially same modify backpropagation encourages binarization ternarization form quantization neural weights. modiﬁcation involves evaluating gradient loss quantized weights applying update continuous weights. speciﬁc details vary quantization operator type codebook. distribution binarized drastic binarization must still done. given problems surprising techniques seem somewhat eﬀective empirically quantizing weights still achieve little loss degradation reported papers above. exactly eﬀective type nets open research question. algorithm optimization essentially happens continuous weight space minimizing well-deﬁned objective regularly corrected quantization operator algorithm gradually converges truly quantized weight vector achieving loss form steps particular quantization operator follows principled optimal constrained form problem given desired form quantization form step determined overall algorithm guaranteed converge valid solution. less restrictive incur lower loss. hardware implementation eﬃcient compute scalar product activation vector quantized weight vector require activation values centroid ﬂoating-point multiplications indeed experiments section show using adaptive codebook clearly beats using quantization adaptive codebook obviously powerful ﬁxed codebook even though store codebook itself. quantization using adaptive codebook also explored neural nets literature using approaches based soft quantization hard quantization discuss brieﬂy. given real-valued elements adaptive quantization represent element exactly entry codebook. codebook assignment values codebook entries minimize certain distortion measure squared error. learning codebook assignment done algorithm possibly approximate quantization related clustering often algorithm goal diﬀerent quantization seeks minimize distortion rather model data quantization compression purposes. case neural compression additional peculiarity complicates optimization quantization weight values jointly learned minimize loss task. types clustering exist hard soft clustering. hard clustering data point assigned exactly cluster soft clustering probability distribution points clusters likewise basic approaches exist neural quantization based hard soft quantization. review next. hard quantization weight assigned exactly codebook value. usual meaning quantization. diﬃcult problem because even loss diﬀerentiable weights assignment makes problem inherently combinatorial. previous work quantization step postprocessing step reference suboptimal learn weights codebook assignment jointly. call direct compression discuss detail section algorithm learn weights codebook assignment jointly converges local optimum problem soft quantization assignment values codebook entries based probability distribution. originally proposed nowlan hinton share weights softly neural goal improving generalization recently revisited goal compression idea penalize loss negative log-likelihood gaussian mixture model scalar weights net. advantage diﬀerentiable coadapting weights parameters however uniquely assign weight mean fact resulting distribution weights quantized; simply encourages creation gaussian clusters weights assign weights means postprocessing step suboptimal. basic problem good model noisy uncertain data here. quantizing weights compression implies constraint certain weights must take exactly value without noise uncertainty optimize loss. seek optimal assignment truly hard soft. indeed prior quantization quadratic prior sparsity quadratic prior encourages weights small encourage weights exactly zero prior encourages weights form gaussian clusters become groups identical weights. noted introduction compressing neural optimally means ﬁnding compressed lowest loss. ﬁrst goal formulate mathematically amenable nonconvex optimization techniques. following carreira-perpi˜n´an deﬁne following model compression constrained optimization problem entropy classiﬁcation task training set) constraint indicates weights must result decompressing low-dimensional parameter vector corresponds quantization described section problem equivalent unconstrained problem minθ nondiﬀerentiable quantization introducing auxiliary variable lead convenient algorithm. second goal solve problem eﬃcient algorithm. although might done diﬀerent ways particularly simple proposed carreira-perpi˜n´an achieves separability data-dependent part problem data-independent part first apply penalty method solve consider augmented lagrangian augmented lagrangian method works follows. ﬁxed optimize accurately enough. then update lagrange multiplier estimates finally increase repeat process limit iterates tend local point constrained problem simpler less eﬀective penalty method quadratic penalty method results setting throughout; describe explicitly carreira-perpi˜n´an involves optimizing regularized version loss pulls optimizer towards currently quantized weights. neural nets solved stochastic gradient descent note that throughout optimization weight vectors evolve simultaneously contains real-valued non-quantized weights contains quantized weights step projection current feasible quantized vectors. step optimizes loss pulled towards current formulation algorithm crucial advantages. ﬁrst convenient separation learning quantization allows solve step reusing existing code. data-dependent part optimization conﬁned within step. part computationally costly requiring access training neural usually implemented using sgd. data-independent part optimization i.e. compression weights conﬁned within step. needs access vector current real-valued weights second advantage form step determined choice quantization form algorithm designer need worry modifying backpropagation convergence valid solution occur. example form quantization discovered wished decompression mapping form solve compression mapping problem unlike much work neural quantization various somewhat arbitrary quantization rounding operations incorporated usual backpropagation training makes unclear problem overall algorithm optimizing optimize anything all. section solve compression mapping problem adaptive ﬁxed codebook cases. suﬃces know involve running k-means adaptive codebook form rounding ﬁxed codebook. figure schematic representation idea model compression constrained optimization general particular quantization ﬁgure adapted carreira-perpi˜n´an plots illustration uncompressed model space contour lines loss compressed models grayed areas) generic compression technique θ-space shown. optimizes infeasible direct compression feasible optimal compressed optimal compressed. plot shows local optima loss respective points plot shows several feasible sets corresponding diﬀerent compression corresponding constraints plot black line feasible corresponding constraint line quadratic-penalty method path simple case straight line segment point solution mark three points blue represents reference codebook solution form k}}. subspace deﬁnes particular assignment weights centroids ck}. assignments. knew yield unconstrained objective tunable vectors shows actual geometry case weights codebook centroid. exactly visualized assignment variables redundant eliminated minwwc s.t. compression mapping easily seen orthogonal projection onto diagonal line w-space particular case however misleading constraints involve single linear subspace rather union combinatorial number subspaces. solved simply exactly setting eliminating variables linear neural net. loss quadratic optimization problem binary quadratic problem weights np-complete however algorithm still converge local optimum sense k-means algorithm said converge local optimum step cannot improve given step vice versa. generally global optimum problem good solution loss algorithm guaranteed converge weight vector increasing penalty parameter slowly. practice order reduce computing time increase aggressively following multiplicative schedule however important small enough allows algorithm explore solution space committing speciﬁc assignments weights. figure pseudocode algorithm quantization scalar weights adaptive codebook augmented-lagrangian version. reference k-means initialized k-means++; step k-means initialized previous iteration’s codebook step compression mapping k-means codebook quantized weights result setting weight assigned codebook entry initialize i.e. reference direct compression exact solution show next section. stop algorithm solution i.e. quantized weights using codebook assignments runtime step negligible compared step. ﬁxed codebook step simple assignment weight. adaptive codebook step runs k-means iteration linear number weights number iterations k-means runs tens ﬁrst k-means subsequent steps seen experiments. runtime dominated steps i.e. optimizing loss. quadratic-penalty augmented-lagrangian methods deﬁne path iterates converges local solution beginning path special importance called direct compression carreira-perpi˜n´an taking limit assuming initial minw minθ θdc. hence corresponds training reference non-quantized quantizing regardless loss illustrated suboptimal farther reference feasible set. happen feasible small i.e. codebook size small indeed experiments show large practically identical result algorithm decreases loss becomes larger larger algorithm. variation direct compression consists iterating follows. ﬁrst optimize obtain quantize k-means θdc. next optimize initializing compress etc. called iterated direct compression carreira-perpi˜n´an improve loss optimization exact single simply would cycle forever reference weights weights optimum figure pseudocode algorithm quantization scalar weights ﬁxed codebook augmented-lagrangian version. simplicity notation represent assignments local optima might converge diﬀerent optimum quantization step however point cycling reference quantized net. practice sgd-based optimization loss large neural nets approximate; stop converged. implies iterates never fully reach keep oscillating forever somewhere fact proposed recently quantization although without context constrained optimization framework provides. gong applied k-means quantize weights reference i.e. trained quantization tries improve iterating process i.e. idc. experiments verify neither converge local optimum problem algorithm does. caused selecting quadratic penalty augmented lagrangian possible penalties quadratic penalty gives rise simpler optimization problems focus paper. describe write quantization mapping parameter space solve optimization problem problem well-known quadratic distortion problem np-complete typically solved approximately k-means using good initialization k-means++ well known k-means alternating optimization algorithm iterates following steps assignment step update assignments independently given centroids centroid step update centroids independently setting mean assigned points. iteration reduces distortion leaves unchanged. algorithm converges ﬁnite number iterations local optimum cannot improve given practice neural nets quantize scalar weight values directly i.e. real value. computationally k-means considerably faster scalar values vectors. vectors dimension data points centroids iteration k-means takes runtime assignment step scanning points accumulating mean incrementally). dimension iteration done exactly using binary search sorted centroids assignment step takes sorting assigning total fact k-means step arbitrary choice quantization algorithm necessary consequence assumptions fact want assign weights elements codebook dictates form scalar weights solved alternating optimization k-means assignment step identical centroid step uses median instead mean points assigned centroid number distortion measures developed quantization literature might used penalty perhaps convenient losses applications. ﬁxed codebook next section form step regardless penalty. topic choice penalty possible concern could raise outliers data. used clustering k-means known sensitive outliers nonconvexities data distribution. consider following situations simplicity using centroid first dataset outlier pull centroid towards away rest data compression purposes seem waste centroid data penalty centroid would insensitive outlier. second dataset consists separate groups centroid middle both data k-means penalty. again seem waste centroid. clustering algorithms proposed ensure centroids distribution mass k-modes algorithm however concerns misguided neural compression data modeling problem consider overall algorithm step isolation. step centroids coincide distortion zero outliers. course possible algorithm converge local optimum neural quantization np-complete problem happen various reasons. section experiments algorithm model whose weights contain clear outliers demonstrate solution found makes sense. fixed codebook consider quantization using ﬁxed codebook i.e. codebook entries ﬁxed learn them learn weight assignments derive algorithms compression weights based approaches binarization ternarization also explored literature neural compression implemented modiﬁcations backpropagation case low-dimensional parameters decompression zikck compression mapping also written elementwise low-dimensional parameters private codebook shared weights learned. pseudocode notation write optimally quantized weights. simpliﬁes scalar case i.e. weights quantized scalars. here write codebook array scalars sorted increasingly elementwise compression mapping mink=...k written generically since codebook deﬁnes voronoi cells intervals midpoints adjacent centroids. written compactly satisﬁes deﬁne computationally done using binary search although practice small enough linear search makes little diﬀerence. scalar weight quadratic-penalty method shifted scalar weight again quantization algorithm step arbitrary choice follows necessarily codebook-based quantization works. furthermore unlike adaptive codebook case scalar weights solution independent choice penalty order real numbers unique seeks avoid ﬂoating-point multiplications using ﬁxed-point binary arithmetic powers-of-two sums powers-of-two multipliers accelerates computation requires less hardware. note generic k-means algorithm solves problem hence particular cases exactly iteration centroid step nothing assignment step identical expressions however expressions eﬃcient especially powers-of-two case runs fixed codebook values produce large loss good weight values quite bigger quite smaller improvement rescale weights equivalently rescale codebook elements scale parameter learned. low-dimensional parameters shared parameter private. dek= zikck application binarization ternarization scale special cases solve problem exactly without need iterative algorithm. give solution binarization ternarization scale again give directly scalar quantization quantization operators equal rounding procedures used previous work neural quantization binarization taking sign weight well known formula binarization scale rastegari ternarization scale considered solution give approximate; correct optimal solution given theorem mentioned before approaches incorporate rounding backpropagation algorithm heuristic resulting algorithm solve problem framework algorithm solution step follows necessarily; need heuristics. computing optimal scale weights runtime case binarization scale case ternarization scale. ternarization sums done cumulatively total runtime dominated sort possible avoid sort using heap reduce total runtime particular cases interest input quantization real-valued weights; ternarization scale weights must sorted decreasing magnitude quantized solves step algorithm quantization. proofs appendix. evaluate learning-compression algorithm quantizing neural nets diﬀerent sizes diﬀerent compression levels several tasks datasets linear regression mnist classiﬁcation mnist cifar. compare direct compression iterated direct compression correspond previous works gong respectively. using codebook values also compare binaryconnect aims learning binary weights. summary experiments conﬁrm theoretical arguments behavior show achieves comparable loss values algorithms compression levels drastically outperforms high compression levels reach maximum possible compression without signiﬁcant error degradation networks describe used theano lasagne libraries. throughout augmented lagrangian found faster robust quadratic penalty particular setting hyperparameters. initialize algorithms reasonably well-trained reference net. initial iteration gives solution. step consists k-means till convergence initialized previous iteration’s centroids ﬁrst compression k-means++ initialization ﬁrst compression take several tens k-means iterations subsequent ones need often report loss classiﬁcation error training test. quantize multiplicative weights neural biases. biases span larger range multiplicative weights hence requiring higher precision anyway biases neural compared number multiplicative weights. bits bits p⌈log size codebook; number multiplicative weights biases respectively; codebook size; -bit ﬂoats represent real values note important quote firstly conduct simple experiment understand interplay loss model complexity compression level given classiﬁcation error number hidden units codebook size respectively. important reason compression practically useful better train large accurate model compress train smaller model compress ﬁrst place also many papers show surprisingly large compression levels possible neural nets expect large compression levels without loss degradation general? answer questions depends relation loss model complexity compression. here explore experimentally simple setting classiﬁcation neural inputs dimension sets lmax i.e. points satisfying lmax. middle plot size level sets color markers identify best operational point within level i.e. point smallest size lmax. outputs dimension hidden tanh units fully connected trained minimize average cross-entropy. algorithm quantize using codebook size size bits resulting nets follows explore space optimal nets order determine best operational point order achieve target loss lmax smallest want solve following optimization problem train reference units compress using codebook size training procedure exactly lenet neural discussed later. fig. target loss values lmax within given level points large correspond regime train large reference compress maximally; points small correspond regime train small reference compress intermediate points correspond intermediate model sizes moderate compression levels. plot shows target loss large maximal compression optimal; target loss increases optimal point moves towards intermediate compression levels. require loss degradation whatsoever might achievable without compression. therefore general clear optimal regime solving model selection problem practice involve trial error model sizes compression levels. however often case signiﬁcant compression achievable tolerate minor loss degradation. compression also simpliﬁes model selection neural designer simply overestimate size required achieve target loss compression smaller similar loss. seems clear good approximate strategy take large enough model compress much possible. experiment goals verify controlled setting without local optima exact steps identical signiﬁcantly worse test algorithm weight distribution gaussian problem simulated super-resolution task want recover high-resolution image low-resolution training linear regression pairs i.e. loss biases construct low-resolution image bicubic interpolation highresolution image ignoring border eﬀects slight nonlinearities means pixel approximately linear combination constant coeﬃcients corresponding pixels hence write mapping high resolution approximately linear mapping contains nonzero weights ground-truth recovery matrix optimizes loss similar structure roughly contains nonzeros whose values across rows. also gaussian noise generating low-resolution image spreads optimal weights around ideal values also spreads biases around zero. summary means reference model weights clustered distribution large cluster around zero small clusters positive values. construct dataset randomly selected mnist images resized added gaussian noise generate compress using codebook size reference model step single closed-form solution given linear system. algorithm increase iterations. fig. shows results codebook sizes firstly expected change past ﬁrst iteration achieves much lower loss. reference weight distribution shows large cluster zero small clusters around small clusters correspond bicubic interpolation coeﬃcients crucial preserve order achieve loss. algorithm indeed good this places centroid zero around training test sets. normalize pixel grayscales subtract mean. compress layers network layer codebook size loss average cross-entropy. train good reference nesterov’s accelerated gradient method figure regression problem using codebook size training loss method column loss iterations. column weight distribution reference model direct compression algorithm using kernel density estimate manually selected bandwidth. inset enlarges vertically distributions show small cluster structure. locations codebook centroids shown trained binaryconnect using code courbariaux deterministic rounding every minibatch iterations initialized reference trained minibatch iterations. lenet variation original lenet convolutional described lecun included caﬀe used relu activations dropout densely connected layers softmax outputs total trainable parameters step runs iterations momentum learning following ﬁgures tables show results. fig. shows learning curves error compression tradeoﬀ. runtime essentially same loss error compression levels. high compression distinctly superior. using also outperforms binaryconnect shown table note weight values found latter close equal local optimum long training times required sgd-type algorithms. since compressed nets keep training gain accuracy reference. figs. show evolution weights codebook centroids lenet. converges feasible local optimum not. argued earlier likely oscillates region half figure compression results lenet neural nets using diﬀerent algorithms diﬀerent codebook sizes corresponding compression ratio forms tabular graph report training loss training test classiﬁcation error etrain etest curves show tradeoﬀ error compression ratio. lenet neural shown diﬀerent color algorithm shown diﬀerent line type reference direct compression. oscillations noticeable weight trajectories layers weights change centroid assignment oscillate around centroid’s value. contrast weights change centroid assignment robustly converge fig. shows ﬁnal locations centroids codebook sizes lenet. although general distribution centroids similar methods subtle diﬀerences which seen translate signiﬁcantly lower error large enough centroid distribution resembles reference net. expected because enough centroids optimally quantized network similar reference algorithms give result namely quantizing reference weights directly hence centroid distribution optimal quantization reference’s weight distribution. since latter roughly gaussian lenet centroids reﬂect this seen larger however compression purposes interested small-k region ﬁnal weight distribution signiﬁcantly diﬀer reference. values distribution centroids usually symmetric around zero spread increases layer layer layer range standard deviation. although observations carry types neural nets emphasize reference weight distribution hence centroid distribution strongly depend problem indeed clustered distribution regression problem diﬀerent gaussian. therefore seems risky anticipate optimal codebook distribution finding really accurate must done figs. show reference ﬁnal weights lenet compressed algorithm using codebook size gives binary weights. many weights sign quantized weight equals sign corresponding reference weight. however weights change side optimization speciﬁcally weights layers respectively. training test sets. normalize pixel colors subtract mean. train -layer convolutional neural network inspired described courbariaux structure shown table convolutions symmetrically padded zero network million parameters time considerations report performance respect reference net. reference achieves error total neurons fully connected neurons dropout followed relu fully connected neurons dropout followed relu fully connected neurons dropout followed softmax figure evolution centroid distribution iterations algorithm layer lenet left weight distribution iterations using kernel density estimate manually selected bandwidth. locations codebook centroids shown distributions figure distribution centroids learnt algorithms layers lenet codebook sizes actual centroid locations distribution weights reference shown kernel density estimate. bottom mean neural quantization involves minimizing loss weights taking discrete values makes objective function nondiﬀerentiable. reformulated optimizing loss subject quantization constraints mixed discrete-continuous problem given iterative learning-compression algorithm solve alternates steps learning step optimizes usual loss quadratic regularization term solved sgd; compression step independent loss training quantizes current real-valued weights. compression step takes form k-means codebook adaptive optimal assignment rescaling codebook ﬁxed binarization. algorithm guaranteed converge local optimum quantization problem np-complete. experimentally algorithm beats previous approaches based quantizing reference incorporating rounding backpropagation. often reaches maximum possible compression without signiﬁcant loss degradation. prove optimal quantization results binarization ternarization powers-of-two section formulas binarization ternarization without scale follow formulas powers-of-two binarization ternarization scale given theorems below. deﬁne sign |w|. proof. sign obviously equal sign consider call solution written partition four intervals equivalently intervals optimally assigned centroids +log respectively. solutions ﬁrst second fourth intervals obvious. solution third interval follows. interval assigned centroid given midpoints centroids equivalently theorem solution problem", "year": 2017}