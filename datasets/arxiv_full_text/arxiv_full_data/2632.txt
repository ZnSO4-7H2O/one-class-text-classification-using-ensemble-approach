{"title": "Bridging the Gap Between Value and Policy Based Reinforcement Learning", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "We establish a new connection between value and policy based reinforcement learning (RL) based on a relationship between softmax temporal value consistency and policy optimality under entropy regularization. Specifically, we show that softmax consistent action values correspond to optimal entropy regularized policy probabilities along any action sequence, regardless of provenance. From this observation, we develop a new RL algorithm, Path Consistency Learning (PCL), that minimizes a notion of soft consistency error along multi-step action sequences extracted from both on- and off-policy traces. We examine the behavior of PCL in different scenarios and show that PCL can be interpreted as generalizing both actor-critic and Q-learning algorithms. We subsequently deepen the relationship by showing how a single model can be used to represent both a policy and the corresponding softmax state values, eliminating the need for a separate critic. The experimental evaluation demonstrates that PCL significantly outperforms strong actor-critic and Q-learning baselines across several benchmarks.", "text": "establish connection value policy based reinforcement learning based relationship softmax temporal value consistency policy optimality entropy regularization. speciﬁcally show softmax consistent action values correspond optimal entropy regularized policy probabilities along action sequence regardless provenance. observation develop algorithm path consistency learning minimizes notion soft consistency error along multi-step action sequences extracted onoff-policy traces. examine behavior different scenarios show interpreted generalizing actor-critic q-learning algorithms. subsequently deepen relationship showing single model used represent policy corresponding softmax state values eliminating need separate critic. experimental evaluation demonstrates signiﬁcantly outperforms strong actor-critic q-learning baselines across several benchmarks. model-free aims acquire effective behavior policy trial error interaction black environment. goal optimize quality agent’s behavior policy terms total expected discounted reward. model-free myriad applications games robotics marketing name few. recently impact model-free expanded deep neural networks promise replace manual feature engineering end-to-end learning value policy representations. unfortunately challenge remains best combine advantages value policy based approaches presence deep function approximators mitigating shortcomings. although recent progress made combining value policy based methods issue settled intricacies perspective exacerbated deep models. primary advantage policy based approaches reinforce directly optimize quantity interest remaining stable function approximation biggest drawback sample inefﬁciency since policy gradients estimated rollouts variance often extreme. although policy updates improved appropriate geometry need variance reduction remains paramount. actor-critic methods thus become popular value approximators replace rollout estimates reduce variance cost bias. nevertheless on-policy learning remains inherently sample inefﬁcient estimating quantities deﬁned current policy either on-policy data must used updating must sufﬁciently slow avoid signiﬁcant bias. naive importance correction hardly able overcome shortcomings practice contrast value based methods q-learning learn trajectory sampled environment. off-policy methods able exploit data sources experts making inherently sample efﬁcient on-policy methods drawback off-policy learning stably interact function approximation practical consequence extensive hyperparameter tuning required obtain stable behavior. despite practical success also little theoretical understanding deep q-learning might obtain near-optimal objective values. ideally would like combine unbiasedness stability on-policy training data efﬁciency off-policy approaches. desire motivated substantial recent work off-policy actor-critic methods data efﬁciency policy gradient improved training offpolicy critic although methods demonstrated improvements on-policy actor-critic approaches resolved theoretical difﬁculty associated off-policy learning function approximation. hence current methods remain potentially unstable require specialized algorithmic theoretical development well delicate tuning effective practice paper exploit relationship policy optimization entropy regularization softmax value consistency obtain form stable off-policy learning. even though entropy regularized policy optimization well studied topic fact attracting renewed interest concurrent work contribute observations study essential methods propose ﬁrst identify strong form path consistency relates optimal policy probabilities entropy regularization softmax consistent state values action sequence; second result formulate novel optimization objective allows stable form off-policy actor-critic learning; ﬁnally observe objective actor critic uniﬁed single model coherently fulﬁlls roles. notation background model agent’s behavior parametric distribution deﬁned neural network ﬁnite actions. iteration agent encounters state performs action sampled environment returns scalar reward transitions next state st+. note main results identify speciﬁc properties hold arbitrary action sequences. keep presentation clear focus attention properties provide simpliﬁed presentation main body paper assuming deterministic state dynamics. restriction necessary appendix provide full treatment concepts generalized stochastic state dynamics. desired properties continue hold general case algorithms proposed remain unaffected. simplicity assume per-step reward next state given functions speciﬁed environment. begin formulation reviewing elements q-learning uses notion hard-max bellman backup enable off-policy control. first observe expected discounted reward objective recursively expressed denote optimal state value state given maximum value policies i.e. maxπoer. accordingly denote optimal policy results i.e. argmaxπ oer. optimal policy one-hot distribution assigns probability action maximal return elsewhere. thus paper study optimal state action values softmax form temporal consistency arises augmenting standard expected reward objective discounted entropy regularizer. entropy regularization encourages exploration helps prevent early convergence sub-optimal policies conﬁrmed practice case express regularized expected reward expected reward discounted entropy term note equivalent entropy regularized objective proposed maxπoent denote soft optimal state value state denote optimal policy attains maximum oent. optimal policy longer one-hot distribution since entropy term prefers policies uncertainty. characterize optimal policy terms oent-optimal state values successor states boltzmann distribution form veriﬁed solution noting oent objective simply τ-scaled constant-shifted kl-divergence hence optimum achieved derive terms policy substituted manipulation yields intuitive deﬁnition optimal state value terms softmax backup much like q-learning consistency equation used perform one-step backups asynchronously bootstrap based appendix prove procedure tabular case converges unique ﬁxed point representing optimal values. point notion softmax q-values studied previous work concurrently work also proposed soft q-learning algorithm continuous control based similar notion softmax temporal consistency. however contribute observations lead novel training principles explore. manipulation taking sides reveals important connection optimal state value value successor state reached action taken corresponding action probability optimal log-policy theorem policy maximizes oent state values maxπoent satisfy following temporal consistency property state action important property one-step softmax consistency established extended multi-step consistency deﬁned action sequence given state. softmax optimal state values beginning action sequence related rewards optimal log-probabilities observed along trajectory. corollary optimal policy optimal state values satisfy following extended temporal consistency property state action sequence proof. proof appendix applies theorem sequence summing left right hand sides induce telescopic cancellation intermediate state values. corollary follows special case. theorem motivates one-step multi-step path-wise consistencies foundation algorithms learn parameterized policy value estimates minimizing discrepancy left right hand sides temporal consistency properties optimal policy optimal state values developed lead natural path-wise objective training policy parameterized state value function parameterized minimization soft consistency error. based ﬁrst deﬁne notion soft consistency d-length sub-trajectory sii+d function γdvφ denote value policy learning rates respectively. given consistency property must hold path algorithm applies updates trajectories sampled on-policy well trajectories sampled replay buffer. union trajectories comprise used deﬁne opcl. speciﬁcally given ﬁxed rollout parameter iteration samples batch on-policy trajectories computes corresponding parameter updates sub-trajectory length exploits off-policy trajectories maintaining replay buffer applying additional updates based batch episodes sampled buffer iteration. found beneﬁcial sample replay episodes proportionally exponentiated reward mixed uniform distribution although exhaustively experiment sampling procedure. particular sample full episode replay buffer size probability r)/z discounting rewards normalization factor hyper-parameter. pseudocode provided appendix. note stochastic settings squared inconsistency objective approximated monte carlo samples biased estimate true squared inconsistency issue arises q-learning well others proposed possible remedies also applied algorithm maintains separate model policy state value approximation. however given soft consistency state action value functions express soft consistency errors strictly terms q-values. denote model action values parameterized based estimate state values policy given uniﬁed parameterization policy value formulate alternative algorithm called uniﬁed path consistency learning optimizes objective differs combining policy value function single model. merging policy value function models signiﬁcant presents actor-critic paradigm policy distinct values note practice found beneﬁcial apply updates using different learning rates much like pcl. accordingly update rule takes form familiar advantage-actor-critic methods pcl’s update rules might appear similar. particular advantage-actor-critic on-policy method exploits expected value function reduce variance policy gradient service maximizing expected reward. models trained concurrently actor determines policy critic trained estimate ﬁxed rollout parameter chosen advantage on-policy trajectory sii+d estimated expectation esii+d|θ denotes sampling current policy updates exhibit striking similarity updates expressed fact takes omits replay buffer slight variation recovered. sense interpret generalization moreover restricted on-policy samples minimizes inconsistency measure deﬁned path hence exploit replay data enhance efﬁciency off-policy learning. also important note essential tracks non-stationary target ensure suitable variance reduction. tracking required. difference dramatic uniﬁed single model trained actor critic. necessary separate actor critic; actor serve critic. also compare hard-max temporal consistency algorithms q-learning fact setting rollout uniﬁed leads form soft q-learning degree softness determined therefore conclude path consistency-based algorithms developed paper also generalize q-learning. importantly uniﬁed restricted single step consistencies major limitation q-learning. proposed using multi-step backups hard-max q-learning approach theoretically sound since rewards received non-optimal action relate hard-max q-values therefore interpret notion temporal consistency proposed paper sound generalization one-step temporal consistency given hard-max q-values. connections softmax q-values optimal entropy-regularized policies previously noted. cases entropy regularization expressed form relative entropy cases standard entropy papers derive similar relationships stop short stating singlemulti-step consistencies action choices highlight. moreover algorithms proposed works essentially single-step q-learning variants suffer limitation using single-step backups. another recent work uses softmax relationship limit proposes augment actor-critic algorithm ofﬂine updates minimize single-step hard-max bellman errors. again methods propose differentiated multi-step path-wise consistencies allow resulting algorithms utilize multi-step trajectories off-policy samples addition on-policy samples. proposed uniﬁed algorithms bear similarity multi-step q-learning rather minimizing one-step hard-max bellman error optimizes q-value function approximator unrolling trajectory number steps using hard-max backup. method shown empirical success theoretical justiﬁcation lacking since rewards received non-optimal action longer relate hard-max q-values contrast algorithms propose incorporate log-probabilities actions multi-step rollout crucial version softmax consistency consider. notions temporal consistency similar softmax consistency discussed literature. previous work used boltzmann weighted average operator particular operator used propose iterative algorithm converging optimal maximum reward policy inspired work boltzmann weighted average brieﬂy mention softmax operator would similar theoretical properties. recently proposed mellowmax operator deﬁned log-average-exp. log-averageexp operators share similar non-expansion property proofs non-expansion related. figure results baselines. plot shows average reward across random training runs choosing best hyperparameters. also show single standard deviation clipped max. x-axis number training iterations. exhibits comparable performance tasks clearly outperforms challenging tasks. across tasks performance worse pcl. additionally possible show restricted inﬁnite horizon setting ﬁxed point mellowmax operator constant shift investigated here. cases suggested training algorithm optimizes single-step consistency unlike uniﬁed optimizes multi-step consistency. moreover papers present clear relationship action values ﬁxed point entropy regularized expected reward objective formulation algorithmic development paper. finally considerable amount work reinforcement learning using off-policy data design sample efﬁcient algorithms. broadly speaking methods understood trading bias variance previous work considered multi-step off-policy learning typically used correction truncated importance sampling bias correction eligibility traces contrast method deﬁnes unbiased consistency entire trajectory applicable onoff-policy data. empirical comparison methods remains however interesting avenue future work. evaluate proposed algorithms namely uniﬁed across several different tasks compare implementation based implementation double q-learning prioritized experience replay based consistently match beat performance baselines. also provide comparison uniﬁed single uniﬁed model values policy competitive pcl. algorithms easily amenable incorporate expert trajectories. thus difﬁcult tasks also experiment seeding replay buffer randomly sampled expert trajectories. training ensure trajectories removed replay buffer always maximal priority. details tasks experimental setup provided appendix. present results variants figure ﬁnding best hyperparameters plot average reward training iterations randomly seeded runs. synthetic tree environment protocol performed seeds instead. figure results uniﬁed pcl. overall using single model values policy detrimental training. although simpler tasks edge uniﬁed difﬁcult tasks uniﬁed preforms better. figure results augmented small number expert trajectories hardest algorithmic tasks. incorporating expert trajectories greatly improves performance. hard discern simple tasks copy reverse repeatcopy. however noticeable observed synthetic tree duplicatedinput results signiﬁcant gaps clear harder tasks including reversedaddition reversedaddition hard reversedaddition. across experiments clear prioritized performs worse pcl. results suggest competitive algorithm cases signiﬁcantly outperforms strong baselines. compare uniﬁed figure protocol performed best hyperparameters plot average reward several training iterations. using single model values policy uniﬁed slightly detrimental simpler tasks difﬁcult tasks uniﬁed competitive even better pcl. present results along augmented expert trajectories figure observe incorporation expert trajectories helps considerable amount. despite using small number expert trajectories opposed mini-batch size inclusion expert trajectories training process signiﬁcantly improves agent’s performance. performed similar experiments uniﬁed observed similar lift using expert trajectories. incorporating expert trajectories relatively trivial compared specialized methods developed policy based algorithms compare algorithms take advantage expert trajectories success shows promise using pathwise consistencies. importantly ability incorporate expert trajectories without requiring adjustment correction desirable property real-world applications robotics. study characteristics optimal policy state values maximum expected reward objective presence discounted entropy regularization. introduction entropy regularizer induces interesting softmax consistency optimal policy optimal state values expressed either single-step multi-step consistency. softmax consistency leads development path consistency learning algorithm resembles actor-critic maintains jointly learns model state values model policy similar q-learning minimizes measure temporal consistency error. also propose variant uniﬁed maintains single model policy values thus upending actor-critic paradigm separating actor critic. unlike standard policy based algorithms uniﬁed apply on-policy off-policy trajectory samples. further unlike value based algorithms uniﬁed take advantage multi-step consistencies. empirically uniﬁed exhibit signiﬁcant improvement baseline methods across several algorithmic benchmarks. antos szepesvári munos. learning near-optimal policies bellman-residual minimization based ﬁtted policy iteration single sample path. machine learning input environment learning rates discount factor rollout number steps replay buffer capacity prioritized replay hyperparameter function gradients function initialize initialize empty replay buffer sample gradients. update ηπ∆θ. update input priority |rb| remove episodes uniformly random. sample gradients. update ηπ∆θ. update ηv∆φ. initial testbed developed simple synthetic environment. environment deﬁned binary decision tree depth training reward edge sampled uniformly subsequently normalized maximal reward trajectory total reward trained using fully-parameterized model node decision tree parameters determine logits parameter determine q-learning uniﬁed implementations parameters node needed determine q-values. complex environments evaluated uniﬁed baselines algorithmic tasks openai library library provides tasks rough order difﬁculty copy duplicatedinput repeatcopy reverse reversedaddition reversedaddition. tasks agent operates grid characters digits observing character digit time. time step agent move step direction optionally write character digit output. reward received correct emission. agent’s goal task copy copy sequence characters output. duplicatedinput deduplicate sequence characters. repeatcopy copy sequence characters ﬁrst forward order reverse environments implicit curriculum associated them. observe performance algorithm without curriculum also include task hard reversedaddition goal reversedaddition utilize curriculum. environments parameterized agent recurrent neural network lstm cells hidden dimension hyperparameter search found simple parameterize critic learning rate terms actor learning rate critic weight. synthetic tree environment used batch size rollout discount replay buffer capacity ﬁxed parameter pcl’s replay buffer used dqn. optimal hyperparameters performed extensive grid search actor learning rate critic weight entropy regularizer uniﬁed pcl; replay buffer parameters. used standard gradient descent optimization. algorithmic tasks used batch size rollout replay buffer capacity using distributed training workers ﬁxed actor learning rate found work well across variants. optimal hyperparameters performed extensive grid search discount pcl’s replay buffer; critic weight entropy regularizer prioritized replay buffer; also experimented exploration rates copy frequencies target experiments used adam optimizer experiments implemented using tensorﬂow section provide general theoretical foundation work including proofs main path consistency results. ﬁrst establish basic results simple one-shot decision making setting. initial results useful proof general inﬁnite horizon setting. although main paper expresses main claims assumption deterministic dynamics assumption necessary restricted attention deterministic case main body merely clarity ease explanation. given appendix provide general foundations work consider general stochastic setting throughout later sections. particular general stochastic inﬁnite horizon setting introduce discuss entropy regularized expected return deﬁne softmax operator show existence unique ﬁxed point establishing softmax bellman operator contraction inﬁnity norm. relate optimal value entropy regularized expected reward objective oent term able show expected. subsequently present policy determined satisﬁes oent. given characterization terms establish consistency property stated theorem main text. finally show consistent solution optimal satisfying conditions constrained optimization problem easy verify note exponentiation component-wise. maps real valued vector probability vector. denote probability simplex denote entropy function lemma proof. first consider constrained optimization problem right hand side lagrangian given hence conditions optimization problems following system equations unknowns note satisfying requires unique assignment exp/τ also ensures subsequently satisfy must solved since right hand side strictly decreasing solution also unique case given therefore provide unique solution conditions since objective strictly concave must unique global maximizer establishing easy show algebraic manipulation establishes corollary must optimizer corresponding optimal value. proof. jointly satisfy must also satisfy conditions hence must unique maximizer corresponding objective value. although results elementary reveal strong connection optimal state values optimal action values optimal policies softmax operators. particular lemma states that optimal action value current state optimal state value must simply entropy regularized value optimal policy current state. corollaries make stronger observation mutual consistency optimal state value optimal action values optimal policy probabilities must hold every action expectation actions sampled furthermore achieving mutual consistency form equivalent achieving optimality. also need make following properties lemma vector although results main body paper expressed terms deterministic problems prove desired properties hold general stochastic case stochastic transition determined environment. given characterization general case application deterministic case immediate. continue assume action space ﬁnite state space discrete. policy deﬁne entropy regularized expected return expectation taken respect policy respect stochastic state transitions determined environment. convenient also work on-policy bellman operator deﬁned state action note using denote vector values choices given denote vector conditional action probabilities speciﬁed state lemma policy state satisﬁes recurrence note lemma shows ﬁxed point corresponding on-policy bellman operator next characterize quickly convergence ﬁxed point achieved repeated application ther operator. lemma states holds that thus value function converge repeated application on-policy backups also need make following monotonicity property on-policy bellman operator. lemma proof. first observe softmax bellman operator contraction inﬁnity norm. consider value functions denote state transition probability function determined environment. result follows monotonicity corollary bounded proof. consider arbitrary policy corollary corollary exists since bounded; hence conclude next given existence deﬁne speciﬁc policy follows proof. since corollary hence next observe lemma finally lemma know ﬁxed point unique hence corollary optimal state value function optimal policy satisfy γes|sa)] every state action note theorem main body stated assumption deterministic dynamics. used assumption main body merely keep presentation simple understandable. development given appendix considers general case stochastic environment. give proof general setting; result stated theorem follows special case. theorem follow special case. consider policy deﬁned corollary know theorem know hence optimizer oent state therefore must considered premise. assertion follows directly corollary proof. consider policy value function satisfy general consistency property stochastic environment γes|sa)] corollary must theorem follows special case environment deterministic.", "year": 2017}