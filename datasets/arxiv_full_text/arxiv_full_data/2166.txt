{"title": "Feature Importance Measure for Non-linear Learning Algorithms", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Complex problems may require sophisticated, non-linear learning methods such as kernel machines or deep neural networks to achieve state of the art prediction accuracies. However, high prediction accuracies are not the only objective to consider when solving problems using machine learning. Instead, particular scientific applications require some explanation of the learned prediction function. Unfortunately, most methods do not come with out of the box straight forward interpretation. Even linear prediction functions are not straight forward to explain if features exhibit complex correlation structure.  In this paper, we propose the Measure of Feature Importance (MFI). MFI is general and can be applied to any arbitrary learning machine (including kernel machines and deep learning). MFI is intrinsically non-linear and can detect features that by itself are inconspicuous and only impact the prediction function through their interaction with other features. Lastly, MFI can be used for both --- model-based feature importance and instance-based feature importance (i.e, measuring the importance of a feature for a particular data point).", "text": "complex problems require sophisticated non-linear learning methods kernel machines deep neural networks achieve state prediction accuracies. however high prediction accuracies objective consider solving problems using machine learning. instead particular scientiﬁc applications require explanation learned prediction function. unfortunately methods come straight forward interpretation. even βjxj straight forward explain features exhibit complex correlation structure. computational biology positional oligomer importance matrices address need interpretation sophisticated learning machines. poims speciﬁcally explain output kernelbased learning methods acting sequences using weighted degree string kernel kernel breaks discrete sequences length apart subsequences length counts number matching subsequences—the so-called positional oligomers following considerations alphabet random variable alphabet length poims assign length starting position importance score poimyj e|xjj+k poims allow visualization po’s signiﬁcance prediction function seminal property poims take overlaps different positions lengths account. visual inspecting poims tedious proposed motifpoims probabilistic non-convex method automatically extract biological factors underlying svm’s prediction promoter elements transcription factor binding sites –often called motifs. unfortunately poims restricted speciﬁc applications. generalization poims feature importance ranking measure assigns feaprediction function varying feature. expected value prediction function changed varying feature feature considered unimportant. unfortunately firm general intractable paper propose measure feature importance general applied arbitrary learning machine intrinsically non-linear detect features inconspicuous impact section describe proposed method measure feature importance extends concepts poim firm non-linear feature interactions instance-based feature importance attribution particularly simple apply. distinguish model-based instance-based introduce function called explanation mode maps sample respective feature space. exemplary instance-based explanation sequence would mapped itself whereas sequence would mapped poim case model-based explanation. deﬁnition random variable space furthermore prediction function real-valued feature. function arbitrary space. lastly kernel functions. deﬁne here task globally assess features given learning machine regards signiﬁcant independent examples given. case sequence data sequences length alphabet importance k-mers positions gained using explanation mode σk×l−k+ sequence mapped sparse entries indicate presence absence positional k-mers. case dimensional image data rd×d already decent visual explanation mode keeps surroundings mapping data itself. cases const neglected various case studies summarized table corresponding examples shown figure right. given speciﬁc example task hand assess example assigned speciﬁc classiﬁer score prediction. case sequence data compute feature importance positional k-mer given sequence xii+k gii+k. case images rd×d image interest expose pixel maps random samples rd×d pixel gij. cases neglect examples speciﬁc instruction table figure left. reproducing kernels cross-covariance operator. following interesting relation hsic. lemma given kernel deﬁnition φ)|f corresponding hilbert-schmidt independence criterion becomes hsic relation hsic provides practical tool assess non-linear feature importances deﬁned kernel deﬁnition order make approach practically suitable resort sampling inference method. subset containing samples. sφ{f µsµφ approximated ˆsφf z∈z{f hence number samples ˆsφf corresponding sampling scheme also available kernel mfi. section evaluate proposed method empirically regarding ability explain relevance features modelinstance-based explanation models. although method applied learning machines focus experiments support vector machines using gaussian kernel function convolutional neural networks validation follow relevant first strategy successively calculate classiﬁer performance blurring pixels descending relevance randomly. idea blurring pixels high relevance inﬂuence classiﬁer decision thus drop performance faster blurring randomly chosen pixels would following evaluate proposed method usps data using kernel following architecture convolution layer tanh-ﬁlters size max-pool layer size= dense-layer relus dense layer softmax units. experiments used sample size samples considered suitable trade-off runtime accuracy. figure results shown usps data using kernel important pixels found kernel embedded mean picture digit three. figure show classiﬁer performance loss successively blurring pixel regarding relevance found kernel compared random pixel blurring. suitable trade-off runtime accuracy evaluate runtime convergence behavior increasing numbers samples. results shown figure observe frobenius distance converges zero already small sample sizes unfortunately runtime grows fast showing boundaries method. hence good trade-off runtime accuracy would sample size experiment. following experiments used sample size model-based feature importance results shown figure observe both pixel bridge changes digit three digit eight high importance. figure classiﬁer performance increasing amount blurring pixels terms morf explained shown. compared random pixel blurring clearly observe performance drops signiﬁcantly faster blurring important pixels figure instance-based explanation decision usps test data images. highlighted pixels informative individual decisions ﬁrst images correctly classiﬁed. instance-based feature importances pixel-wise explanation experiment kernel trained usps training data set. figure observe pixels building vertical connection three eight strong discriminative evidence. positions left blank image classiﬁed three which case last three images leads misclassiﬁcations. nucleotide-wise explanation experiment kernel trained synthetic training data set. inserted motifs positive class figure observe nucleotides building patterns inserted positive sequences strong discriminative evidence. discriminative patterns noisy sequences assumed stem negative class which case false negative example leads mis-classiﬁcations. patterns inserted classiﬁer gives high evidence single pattern assigns wrong label. figure instance-based feature importances experiment. highlighted nucleotids informative decision four test sequences correctly incorrectly classiﬁed. work contributed opening black learning machines. building poims firm proposed general measure feature importance applicable arbitrary learning machines. used general explanation prediction model data instance speciﬁc explanation. nonlinear measure detect features exhibit importance interactions features. experiments artiﬁcially generated splice-site sequence data well real-world image data demonstrate properties beneﬁts approach. present work focused images sequences framework allows explain arbitrary data sources. future research would like study applications including wind turbine anomaly detection well want investigate advanced sampling techniques probabilistic machine learning lead faster convergence. mmcv supported bmbf alice grant ibb. also acknowledge support german research foundation grant kl/- thanks partial funding national research foundation korea funded ministry education science technology program. supported german ministry education research awards berlin data center bbdc", "year": 2016}