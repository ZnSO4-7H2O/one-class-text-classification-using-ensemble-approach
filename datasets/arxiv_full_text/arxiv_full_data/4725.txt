{"title": "Interaction Networks for Learning about Objects, Relations and Physics", "tag": ["cs.AI", "cs.LG"], "abstract": "Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. Here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Our model takes graphs as input, performs object- and relation-centric reasoning in a way that is analogous to a simulation, and is implemented using deep neural networks. We evaluate its ability to reason about several challenging physical domains: n-body problems, rigid-body collision, and non-rigid dynamics. Our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps, estimate abstract quantities such as energy, and generalize automatically to systems with different numbers and configurations of objects and relations. Our interaction network implementation is the first general-purpose, learnable physics engine, and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains.", "text": "reasoning objects relations physics central human intelligence goal artiﬁcial intelligence. introduce interaction network model reason objects complex systems interact supporting dynamical predictions well inferences abstract properties system. model takes graphs input performs objectrelation-centric reasoning analogous simulation implemented using deep neural networks. evaluate ability reason several challenging physical domains n-body problems rigid-body collision non-rigid dynamics. results show trained accurately simulate physical trajectories dozens objects thousands time steps estimate abstract quantities energy generalize automatically systems different numbers conﬁgurations objects relations. interaction network implementation ﬁrst general-purpose learnable physics engine powerful general framework reasoning object relations wide variety complex real-world domains. representing reasoning objects relations physics core domain human common sense knowledge among basic important aspects intelligence many everyday problems predicting happen next physical environments inferring underlying properties complex scenes challenging elements composed combinatorially many possible arrangements. people nevertheless solve problems decomposing scenario distinct objects relations reasoning consequences interactions dynamics. introduce interaction network model perform analogous form reasoning objects relations complex systems. interaction networks combine three powerful approaches structured models simulation deep learning. structured models exploit rich explicit knowledge relations among objects independent objects themselves supports general-purpose reasoning across diverse contexts. simulation effective method approximating dynamical systems predicting elements complex system inﬂuenced interactions another dynamics system. deep learning couples generic architectures efﬁcient optimization algorithms provide highly scalable learning inference challenging real-world settings. figure schematic interaction network. physical reasoning model takes objects relations input reasons interactions applies effects physical dynamics predict states. complex systems model takes input graph represents system objects relations instantiates pairwise interaction terms computes effects relational model aggregated combined external effects generate input object model predicts interactions dynamics inﬂuence objects interaction networks explicitly separate reason relations reason objects assigning task distinct models fundamentally objectrelation-centric; independent observation modality task speciﬁcation lets interaction networks automatically generalize learning across variable numbers arbitrarily ordered objects relations also recompose knowledge entities interactions novel combinatorially many ways. take relations explicit input allowing selectively process different potential interactions different input data rather forced consider every possible interaction imposed ﬁxed architecture. evaluate interaction networks testing ability make predictions inferences various physical systems including n-body problems rigid-body collision non-rigid dynamics. interaction networks learn capture complex interactions used predict future states abstract physical properties energy. show roll thousands realistic future state predictions even trained single-step predictions. also explore generalize novel systems different numbers conﬁgurations elements. though restricted physical reasoning interaction networks used represent ﬁrst general-purpose learnable physics engine even potential learn novel physical systems physics engines currently exist. related work model draws inspiration previous work reasons graphs relations using neural networks. graph neural network framework shares learning across nodes edges recursive autoencoder adapts processing architecture exploit input parse tree neural programmer-interpreter composable network mimics execution trace program spatial transformer learns dynamically modify network connectivity capture certain types interactions. others explored deep learning logical arithmetic relations relations suitable visual question-answering behavior model similar spirit physical simulation engine generates sequences states repeatedly applying rules approximate effects physical interactions dynamics objects time. interaction rules relation-centric operating objects interacting dynamics rules object-centric operating individual objects aggregated effects interactions participate similar approach developed independently chang previous work physical reasoning explored commonsense knowledge qualitative representations simulation techniques approximating physical prediction inference neuroanimator perhaps ﬁrst quantitative approach learning physical dynamics training neural networks predict control state articulated bodies. ladický recently used regression forests learn ﬂuid dynamics. recent advances convolutional neural networks efforts learn predict coarse-grained physical dynamics images notably fragkiadaki used cnns predict control moving ball image centered coordinates. mottaghi trained cnns predict trajectory object external impulse applied. used cnns parse objects images input physics engine supported prediction inference. deﬁnition describe model physical reasoning example build simple model full interaction network predict dynamics single object might object-centric function inputs object’s state time outputs future state ot+. objects governed dynamics could applied each independently predict respective future states. objects interact another insufﬁcient capture relationship. assuming objects directed relationship e.g. ﬁxed object attached spring freely moving mass ﬁrst inﬂuences second interaction. effect interaction predicted relation-centric function takes input well attributes relationship e.g. spring constant. modiﬁed input receiver’s current state enabling interaction inﬂuence future state formulation expanded larger complex systems representing graph nodes correspond objects edges relations assume attributed directed multigraph relations attributes multiple distinct relations objects system objects relations inputs {oj}j=...no rkk}k=...nr {xj}j=...no represents states object. triplet represents k-th relation system sender receiver relation attribute represents external effects active control inputs gravitational acceleration deﬁne part system applied object separately. basic deﬁned marshalling function rearranges objects relations interaction terms relation correspond interaction’s receiver sender relation attributes. relational model predicts effect interaction applying aggregation function collects effects apply receiver object merges them combines form object model inputs object. object model predicts interactions dynamics inﬂuence objects applying returning results basic predict evolution states dynamical system physical simulation equal future states objects ot+. also augmented additional component make abstract inferences system. rather serving output combined another aggregation function input abstraction model returns single output whole system. explore variant ﬁnal experiments predict potential energy. applies every respectively makes relational object reasoning able handle variable numbers arbitrarily ordered objects relations. additional constraint must satisﬁed maintain this function must commutative associative objects relations. using summation within merge elements satisﬁes this division would not. focus binary relations means interaction term relation another option interactions correspond n-th order relations combining senders interactions could even variable order includes sender objects interact receiver would require handle variable-length inputs. possibilities beyond scope work interesting future directions. implementation general deﬁnition previous section agnostic choice functions algorithms outline learnable implementation capable reasoning complex systems nonlinear relations dynamics. standard deep neural network building blocks multilayer perceptrons matrix operations etc. trained efﬁciently data using gradient-based optimization stochastic gradient descent. deﬁne matrix whose columns correspond objects’ ds-length state vectors. relations triplet binary matrices index receiver sender objects respectively matrix whose dr-length columns represent relations’ attributes. j-th column one-hot vector indicates receiver object’s index; indicates sender similarly. graph fig. matrix whose columns dx-length vectors represent external effect applied objects. marshalling function computes matrix products concatenates resulting matrix whose columns represent interaction terms relations constructs interaction terms modiﬁed described experiments section input applies column. output de-length vector distributed representation effects. concatenates effects form effect matrix input computes matrix product whose j-th column equivalent elementwise across whose corresponding relation receiver object concatenated resulting matrix whose columns represent object states external effects per-object aggregate interaction effects. input applies another columns. output -length vector concatenates form output matrix infer abstract properties system additional appended takes input. aggregation function performs elementwise across columns return -length vector input another returns da-length vector represents abstract global property system. training requires optimizing objective function learnable parameters note involve matrix operations contain learnable parameters. shared across relations objects respectively training statistically efﬁcient. similar cnns efﬁcient weight-sharing scheme. treats local neighborhood pixels related interacting entities pixel effectively receiver object neighboring pixels senders. convolution operator analogous local linear/nonlinear kernel applied neighborhood. skip connections recently popularized residual networks loosely analogous inputs though cnns relationobject-centric reasoning delineated. cnns exploit local interactions ﬁxed well-suited speciﬁc topology images capturing longer-range dependencies requires either broad insensitive convolution kernels deep stacks layers order implement sufﬁciently large receptive ﬁelds. avoids restriction able process arbitrary neighborhoods explicitly speciﬁed input. physical reasoning tasks experiments explored types physical reasoning tasks predicting future states system estimating abstract properties speciﬁcally potential energy. evaluated in’s ability learn make judgments three complex physical domains n-body systems; balls bouncing box; strings composed springs collide rigid objects. simulated trajectories elements systems physics engine recorded sequences states. supplementary material full details. n-body domain solar systems bodies exert distancemass-dependent gravitational forces other relations input model. across simulations objects’ masses varied ﬁxed attributes held constant. training scenes always included bodies testing used bodies. half systems bodies initialized velocities would cause stable orbits interactions objects; half random velocities. bouncing balls domain moving balls could collide static walls. walls represented objects whose shape attribute represented rectangle whose inverse-mass relations input model objects relations). collisions difﬁcult simulate gravitational forces data distribution much challenging ball participated collision less steps following straight-line motion times. model thus learn despite rigid relation objects meaningful collision interactions contact. also varied object attributes shape scale mass well coefﬁcient restitution relation attribute. training scenes contained balls inside variably sized walls test scenes contained either balls. string domain used types relations relation structures sparse speciﬁc all-to-all well variable external effects. scene contained string comprised masses connected springs static rigid circle positioned string. masses spring relations immediate neighbors masses rigid relations rigid object gravitational acceleration magnitude varied across simulation runs applied string always fell usually colliding static object. gravitational acceleration external input training scene contained string point masses test scenes contained either mass strings. training point masses string chosen random always held static pinned wall masses free move. test conditions also included strings ends pinned ends pinned evaluate generalization. model takes input state system decomposed objects physical relations well external effects object state could divided dynamic state component static attribute component relation attributes represented quantities coefﬁcient restitution spring constant. input represented system current time. prediction experiment’s target outputs velocities objects subsequent time step energy estimation experiment’s targets potential energies system current time step. also generated multi-step rollouts prediction experiments assess model’s effectiveness creating visually realistic simulations. output velocity time step became input velocity position updated predicted velocity data training validation test data sets generated simulating scenes time steps randomly sampling million one-step input/target pairs respectively. model trained epochs randomly shufﬂing data indices each. used mini-batches balanced data distributions targets similar per-element statistics. performance reported results measured held-out test data. explored adding small amount gaussian noise data’s input positions velocities initial phase training reduced epochs noise std. dev. std. dev. element’s values across dataset. allowed model experience physically impossible states could generated physics engine learn project back nearby possible states. error measure reﬂect clear differences without noise rollouts models trained noise slightly visually realistic static objects less subject drift many steps. model architecture mlps contained multiple hidden layers linear transforms plus biases followed rectiﬁed linear units output layer linear transform plus bias. best model architecture selected grid search layer sizes depths. inputs normalized centering median rescaling percentiles training objectives test measures used mean squared error model’s prediction ground truth target. prediction experiments used architecture parameters selected hyperparameter search. four -length hidden layers output length -length hidden layer output length targeted y-velocity. customized model invariant absolute positions objects scene. concatenated three terms difference vector dynamic states receiver sender concatenated receiver sender attribute vectors relation attribute vector. outputs velocities positions input energy estimation experiments used prediction experiments additional -length hidden layer. inputs’ columns length output length optimized parameters using adam waterfall schedule began learning rate down-scaled learning rate time validation error estimated window epochs stopped decreasing. forms regularization explored applied effects another model parameters. regularizing improved generalization different numbers objects reduced drift many rollout steps. likely incentivizes sparser communication prompting operate independently. regularizing parameters generally improved performance reduced overﬁtting. penalty factors selected grid search. competing models available literature compare model against considered several alternatives constant velocity baseline output input velocity; baseline -length hidden layers took input ﬂattened vector input data; variant component removed prediction experiments results show predict next-step dynamics task domains accurately training orders magnitude lower test error alternative models dynamics domain depended crucially interactions among objects able learn exploit relationships predictions. dynamics-only mechanism processing interactions performed similarly constant velocity model. baseline mlp’s connectivity makes possible principle learn interactions would require learning relation indices selectively process interactions. would also beneﬁt sharing learning across relations objects instead forced approximate interactive dynamics parallel objects. also generalized well systems fewer greater numbers objects domain selected best model system size trained evaluated different system size. tested smaller n-body spring systems trained performance actually exceeded model trained smaller system. model’s ability exploit greater experience objects relations behave available complex system. also found trained single-step predictions used simulate trajectories thousands steps effectively often tracking ground truth closely especially n-body string domains. rendered images videos model-generated trajectories figure prediction rollouts. column contains three panels three video frames spanning rollout steps. columns ground truth model predictions n-body systems bouncing balls strings. model column generated single model trained underlying states system size panel. middle bottom panels show generalization systems different sizes structure. n-body training bodies generalization bodies. balls training balls generalization balls. strings training masses pinned generalization masses ends pinned. urls full videos rollout table model true https//youtu.be/cxzubiwugm https//youtu.be/otigntfjwpu https//youtu.be/zotvzzijz https//youtu.be/bfrbjdtcs https//youtu.be/jgsraymqyny https//youtu.be/rljdaiiiw https//youtu.be/c-upxsjohi https//youtu.be/opdihhak https//youtu.be/oqoquovjv https//youtu.be/xizxopxc https//youtu.be/tpixfr_ekae https//youtu.be/xgogrgo https//youtu.be/svg_qpm https//youtu.be/ilon_vxvg https//youtu.be/y_sakvis https//youtu.be/pbkvm_xqb https//youtu.be/tsecevw https//youtu.be/zdczyqyrvo figure prediction experiment accuracy generalization. colored represents model’s predicted velocity ground truth physics engine’s sublots show n-body performance show balls show string. leftmost subplots domain compare constant velocity model baseline dynamics-only full panels show in’s generalization performance different numbers conﬁgurations objects indicated subplot titles. string systems numbers correspond usually visually indistinguishable ground truth physics engine given initial conditions cohere perfectly dynamics highly nonlinear imperceptible prediction errors model rapidly lead large differences systems’ states. incoherent rollouts violate people’s expectations might roughly people’s understanding domains. estimating abstract properties trained abstract-estimation variant model predict potential energies n-body string domains found much accurate baseline presumably learns gravitational spring potential energy functions applies relations respective domains combines results. introduced interaction networks ﬂexible efﬁcient model explicit reasoning objects relations complex systems. results provide surprisingly strong evidence ability learn accurate physical simulations generalize training novel systems different numbers conﬁgurations objects relations. could also learn infer abstract properties physical systems potential energy. alternative models tested performed much poorly orders magnitude greater error. simulation rich mental models thought crucial mechanism humans reason physics complex domains battaglia recently posited simulation-based intuitive physics engine model explain human physical scene understanding. interaction network implementation ﬁrst learnable physics engine scale real-world problems promising template approaches reasoning physical mechanical systems scene understanding social perception hierarchical planning analogical reasoning. future important develop techniques allow interaction networks handle large systems many interactions culling interaction computations negligible effects. interaction network also serve powerful model model-predictive control inputting active control signals external effects differentiable naturally supports gradient-based planning. also important prepend perceptual front-end infer objects relations observations provided input interaction network reason underlying structure scene. adapting interaction network recurrent neural network even accurate long-term predictions might possible though preliminary tests found little beneﬁt beyond already-strong performance. modifying interaction network probabilistic generative model also support probabilistic inference unknown object properties relations. combining three powerful tools modern machine learning toolkit relational reasoning structured knowledge simulation deep learning interaction networks offer ﬂexible accurate efﬁcient learning inference challenging domains. decomposing complex systems objects relations reasoning explicitly provides combinatorial generalization novel contexts important future challenges crucial step toward closing humans machines think.", "year": 2016}