{"title": "Deep learning from crowds", "tag": ["stat.ML", "cs.CV", "cs.HC", "cs.LG"], "abstract": "Over the last few years, deep learning has revolutionized the field of machine learning by dramatically improving the state-of-the-art in various domains. However, as the size of supervised artificial neural networks grows, typically so does the need for larger labeled datasets. Recently, crowdsourcing has established itself as an efficient and cost-effective solution for labeling large sets of data in a scalable manner, but it often requires aggregating labels from multiple noisy contributors with different levels of expertise. In this paper, we address the problem of learning deep neural networks from crowds. We begin by describing an EM algorithm for jointly learning the parameters of the network and the reliabilities of the annotators. Then, a novel general-purpose crowd layer is proposed, which allows us to train deep neural networks end-to-end, directly from the noisy labels of multiple annotators, using only backpropagation. We empirically show that the proposed approach is able to internally capture the reliability and biases of different annotators and achieve new state-of-the-art results for various crowdsourced datasets across different settings, namely classification, regression and sequence labeling.", "text": "last years deep learning revolutionized ﬁeld machine learning dramatically improving stateof-the-art various domains. however size supervised artiﬁcial neural networks grows typically need larger labeled datasets. recently crowdsourcing established efﬁcient cost-effective solution labeling large sets data scalable manner often requires aggregating labels multiple noisy contributors different levels expertise. paper address problem learning deep neural networks crowds. begin describing algorithm jointly learning parameters network reliabilities annotators. then novel general-purpose crowd layer proposed allows train deep neural networks end-to-end directly noisy labels multiple annotators using backpropagation. empirically show proposed approach able internally capture reliability biases different annotators achieve state-of-the-art results various crowdsourced datasets across different settings namely classiﬁcation regression sequence labeling. last decade deep learning made major advances solving artiﬁcial intelligence problems different domains speech recognition visual object recognition object detection machine translation success often attributed ability discover intricate structures high-dimensional data thereby making particularly well suited tackling complex tasks often regarded characteristic humans vision speech natural language understanding. however typically requirement learning deep representations complex high-dimensional data large sets labeled data. unfortunately many situations data readily available humans required manually label large collections data. hand recent years crowdsourcing established reliable solution annotate large collections data. indeed crowdsourcing platforms like amazon mechanical turk crowdﬂower proven copyright association advancement artiﬁcial intelligence rights reserved. efﬁcient cost-effective obtaining labeled data especially kind human-like tasks vision speech natural language understanding deep learning methods shown excel. even ﬁelds like medical imaging crowdsourcing used collect large sets labeled data modern data-savvy deep learning methods enjoy however crowdsourcing scalable enough allow labeling datasets would otherwise impractical single annotator handle well known noise associated labels provided various annotators compromise practical applications make type data thus surprising large body recent machine learning literature dedicated mitigating effects noise biases inherent heterogeneous sources data albarqouni guan learning deep neural networks labels multiple annotators typical approaches rely sort label aggregation mechanisms prior training. classiﬁcation settings simplest common approach majority voting naively assumes annotators equally reliable. advanced approaches proposed variants whitehill jointly model unknown biases annotators answers noisy versions latent ground truth. despite improved ground truth estimates majority voting recent works shown jointly learning classiﬁer model annotators noise model using em-style algorithms generally leads improved results paper begin describing algorithm learning deep neural networks crowds multi-class classiﬁcation settings highlighting limitations. then novel crowd layer proposed allows train neural networks end-to-end directly noisy labels multiple annotators using backpropagation. alternative approach allows avoid additional computational overhead also leads generalpurpose framework generalizes trivially beyond classitraining deep neural networks exploits information annotators. idea model multiple experts individually neural network then keeping predictions ﬁxed independently learning averaging weights combining using backpropagation. like proposed approach two-stage procedure require algorithm estimate annotators weights. however approach ability capture biases different annotators correct them approach learns combine predicted answers multiple annotators weighting differently. moreover two-stage learning procedure increases computation complexity training whereas proposed approach kept same. lastly work focuses classiﬁcation consider regression structured prediction problems well. regarding applications areas multiple-annotator learning popular ones image classiﬁcation computer-aided diagnosis/radiology object detection text classiﬁcation natural language processing speechrelated tasks paper data areas evaluate different approaches. given precisely areas seen dramatic improvements recent contributions deep learning developing novel efﬁcient algorithms learning deep neural networks crowds great importance ﬁeld. algorithm deep learning crowds yn}n dataset size input vector given vector crowdsourced labels representing label provided annotator annotators. following ideas shall assume existence latent true class whose value particular case determined softmax output layer deep neural network parameterized annotator provides noisy version according formulation corresponds keeping per-annotator confusion matrix model expertise denotes number classes. assuming annotators provide labels independently other write complete-data likelihood based formulation derive expectationmaximization algorithm jointly learning reliabilities annotators parameters neural network expected-value complete-data logﬁcation settings. empirically proposed crowd layer shown able automatically distinguish good unreliable annotators capture individual biases thus achieving state-of-the-art results real data amazon mechanical turk image classiﬁcation text regression named entity recognition. experiments show compared complex em-based approaches approaches state crowd layer able achieve comparable many cases signiﬁcantly superior results. increasing popularity crowdsourcing label large collections data inexpensive scalable manner much interest machine learning community developing methods address noise trustworthiness issues associated direction early contributions work dawid skene proposed algorithm obtain point estimates error rates patients given repeated conﬂicting responses medical questions. work basis many variants aggregating labels multiple annotators different levels expertise proposed extends dawid skene’s model also accounting item difﬁculty context image classiﬁcation. similarly ipeirotis propose using dawid skene’s approach extract single quality score worker allows prune low-quality workers. approach proposed paper contrast line work allowing neural networks trained directly noisy labels multiple annotators thereby avoiding need resort prior label aggregation schemes. despite generality label aggregation approaches described above used combination type machine learning algorithm sub-optimal compared approaches also jointly learn classiﬁer itself. prominent works direction raykar proposed algorithm jointly learning levels expertise different annotators parameters logistic regression classiﬁer modeling ground truth labels latent variables. idea later extended types models gaussian process classiﬁers supervised latent dirichlet allocation recently convolutional neural networks softmax outputs paper begin describing generalization approach multiclass settings highlighting technical difﬁculties associated then novel type neural network layer proposed allows training deep neural networks directly noisy labels multiple annotators using pure backpropagation. contrasts works literature rely complex iterative procedures based furthermore simplicity proposed approach allows straightforward extensions regression structured prediction problems. thereby avoiding aforementioned limitations em-based approaches learning crowds. intuition rather simple. crowd layer takes input would normally output layer deep neural network learns annotator-speciﬁc mapping output layer labels different annotators crowd captures annotator reliabilities biases. former output layer becomes bottleneck layer shared among different annotators. figure illustrates bottleneck structure context simple convolutional neural network classiﬁcation problems classes annotators. idea using labels given annotator propagate errors whole neural network crowd layer adjusts gradients coming labels annotator according his/her reliability scaling adjusting bias. bottleneck layer network receives adjusted gradients different annotators’ labels aggregates backpropagates rest network. turns crowd layer network able account unreliable annotators even correct systematic biases labeling. moreover done naturally within backpropagation framework. formally output deep neural network arbitrary structure. without loss generality shall assume vector correspond output softmax layer corresponds probability input instance belonging class activation crowd layer annotator deﬁned annotator-speciﬁc function output crowd layer simply softmax activations question deﬁne function mapping experiments section study different alternatives. classiﬁcation problems reasonable assumption consider matrix transformation annotator-speciﬁc matrix. given cost function expected output annotator actual label compute gradients ∂e/∂ar activation annotator indicator function takes value zero otherwise. practice since crowd annotators typically label small portion data particularly important carefully impose dirichlet priors compute estimates instead order avoid numerical issues. estimating parameters deep neural network follow approach noise-adjusted ground-truth estimates backpropagate error network using standard stochastic optimization techniques stochastic gradient descent adam kindly notice raises important question schedule steps. perform iteration mini-batch risk enough evidence estimate annotators reliabilities. hand adam convergence computational overhead becomes large. practice found that typically iteration training epoch provides good computational efﬁciency without compromising accuracy. however seems vary among different datasets thus making hard tune practice. fundamental aspect development approach probabilistic interpretation softmax output layer deep neural networks classiﬁcation. unfortunately probabilistic interpretation typically available considering example continuous output variables thereby making difﬁcult generalize approach regression problems. furthermore notice target variable sequence marginalization latent variables quickly become intractable number possible label sequences grows exponentially length sequence. dataset consisting images dogs cats goal distinguish species. classes represented respectively. since binary classiﬁcation task easily simulate annotators different levels expertise assigning individual sensitivities speciﬁcities sampling answers bernoulli distribution parameter true label bernoulli distribution parameter otherwise. using procedure simulated challenging scenario annotators following values particular problem used fairly standard architecture convolutional layers patches pooling relu activations. output convolutional layers fully-connected layer relu units ﬁnally goes output layer softmax activation. batch normalization apply dropout between output layers. proposed approach further adds crowd layer softmax output layer training. base architecture selected possible conﬁgurations using true labels optimizing accuracy validation random search. important note supposed representative typical approaches image classiﬁcation rather single best possible architecture literature particular dataset. furthermore main interest paper contribution crowd layer training neural network. proposed crowd layer compared with multi-annotator approach based supervised latent dirichlet allocation ma-slda; trained result majority voting dl-mv; trained output label aggregation approach proposed dawid skene dl-ds; using approach described earlier dl-em; using doctor approach dldn consists training predict labels multiple annotators combining predictions using majority voting; lastly using weighted doctor approach dl-wdn best performing variant according original paper. approach similar dldn additionally learns weight predictions different annotators. kindly details. reference point also compare trained true labels dl-true. consider variants proposed crowd layer different annotator-speciﬁc functions increasing number parameters vector per-class weights vector per-class biases version matrix weights experiments found approaches parameters wrσ+br identiﬁability issues start occur. gradient vector bottleneck layer naturally becomes weighted gradients according labels different annotators. moreover annotator likely mislabel class class matrix actually adjust gradients accordingly. problem missing labels annotators easily addressed setting gradient contributions zero. estimating annotator weights {wr}r since parameterize mapping output bottleneck layer annotators labels {or}r estimated using standard stochastic optimization techniques adam network trained crowd layer removed thus exposing output bottleneck layer readily used make predictions unseen instances. obvious concern approach described identiﬁability. therefore important overparametrize since adding parameters beyond necessary make output bottleneck layer lose interpretability shared estimated ground truth. another important aspect parameter initialization. experiments found best practice initialize crowd layer identities i.e. zeros additive parameters ones scalar parameters identity matrix multiplicative matrices etc. alternative solution regularization force parameters crowd layer close identities. however cases might undesirable property. example consider biased annotator wish force matrix close identity matrix. based experiments initialization alternative provides best results. lastly noted that em-based approaches implicit assumption random adversarial annotators constitute vast majority case crowd layer would perform better random predictor. particularly important aspect note framework described quite general. example straightforwardly applied sequence labeling problems without changes adapted regression problems considering univariate scalar bias parameters annotator crowd layer. proposed crowd layer implemented type layer keras using practice requires single line code. source code datasets demos experiments provided http//www.fprodrigues.com/. image classiﬁcation begin evaluating proposed crowd layer controlled setting using simulated annotators different levels expertise large image classiﬁcation dl-wdn approaches observe that although also outperform dl-mv dl-ds baselines accuracy inferior proposed dl-cl explained fact dl-dn dl-wdn unable correct annotators’ biases furthermore important recall two-stage procedure dl-wdn computational time signiﬁcantly higher dl-cl. regarding different variants proposed crowd layer verify approach gives best average accuracy. order better understand approach doing inspected weight matrices annotator figure shows relationship diagonal elements true sensitivities speciﬁcities corresponding annotators highlighting strong linear correlation two. evidences proposed crowd layer able internally represent reliabilities annotators. veriﬁed crowd layer performing well simulated annotators moved evaluating real data amazon mechanical turk purpose used image classiﬁcation dataset adapted part labelme data whose goal classify images according classes highway inside city tall building street forest coast mountain open coun images dogs cats dataset used training remaining testing different approaches. order account effect random initialization used parameters network performed executions approaches report average accuracies table immediately verify em-based crowd layer approaches signiﬁcantly outperform majority voting dawid skene baselines thus demonstrating gain learning answers multiple annotators directly rather relying aggregation schemes prior training. dltry. consists total images used obtain labels multiple annotators amazon mechanical turk. image labeled average workers mean accuracy remaining images using evaluating different approaches. since training rather small pretrained layers vgg- deep neural network apply layer output layer using dropout. last column table shows obtained results. verify dl-em dl-wdn dl-cl approaches outperform majority voting dawid skene baselines also probabilistic approaches proposed based supervised latent dirichlet allocation being proposed crowd layer approach gives best results. however unlike dogs cats dataset differences different function mappings crowd layer become evident. justiﬁed ability version able model biases annotators. indeed compare learned weight matrices respective true confusion matrices annotators notice resemble other. figure shows comparison annotators color intensity cells increases relative magnitude value thus demonstrating crowd layer able learn labeling patterns annotators. text regression previously mentioned advantages proposed crowd layer straightforward extension types target variables. section consider regression problem based dataset also introduced dataset consists movie reviews goal predict rating given movie based text review. using authors collected average answers pool workers movie reviews. remaining reviews used testing. letting output bottleneck layer denoted considered variants proposed crowd layer different annotator-speciﬁc functions per-annotator scale parameter per-annotator bias parameter version both base neural network architecture used problem consists convolutional layer features pooling convolutional layer features pooling layer hidden units. layers except output relu activations. proposed dl-cl compared with neural network trained mean answer annotators approach based supervised lda. order make baselines even competitive propose variant algorithm described earlier follows approach extension proposed regression problems. approach assumes following model annotators answers given ground truth n|zn /λr). although formulation relies probabilistic interpretation linear regression model develop algorithm learning nevertheless adapt resultant algorithm replacing linear regression model deep neural network. ﬁnal iterative procedure alternates computing adjusted ground truth re-estimating neural network annotators’ parameters finally although guan discuss extensions regression also developed variants dl-dn dl-wdn continuous output variables. dl-wdn approach considered different weighting functions combining answers multiple annotators namely single weight annotator single bias both. experimented different alternatives found using per-annotator bias combining answers multiple annotators gives best results. proposed crowd layer compared baselines considered classiﬁcation problems. previously explained approach hard generalize sequence labelling problems marginalization latent ground truth sequences order make marginalization tractable assume fully factorized distribution posterior approximation denotes length sequence. although focus paper deep learning approaches sake completeness also compare results multi-annotator approach based conditional random ﬁelds table shows obtained average results clearly demonstrate proposed approach signiﬁcantly outperforms methods provides similar results crf-ma reducing training time least order magnitude compared latter paper proposed crowd layer novel neural network layer enables train deep neural networks end-to-end directly labels multiple annotators crowds using backpropagation. despite simplicity crowd layer able capture reliabilities biases different annotators adjust error gradients backpropagated training accordingly. empirical evaluation shows proposed approach outperforms approaches rely aggregation annotators’ answers prior training well methods state-of-the-art often rely complex harder setup computationally demanding em-based approaches. furthermore unlike latter crowd layer trivial generalize beyond classiﬁcation problems empirically demonstrate using real data amazon mechanical turk text regression named entity recognition tasks. ferent approaches verify proposed crowd layer particularly variant signiﬁcantly outperforms methods. order better understand crowd layer variant doing plotted learned values comparison true biases annotators computed average difference answers ground truth. figure shows comparison verify learned values highly correlated true biases annotators thus showing crowd layer able account annotator bias learning noisy labels multiple annotators. named entity recognition lastly evaluated proposed crowd layer named entity recognition task. purpose used dataset introduced based conll shared task goal identify named entities sentence classify persons locations organizations miscellaneous. dataset consists labeled sentences using pool workers. remaining sentences original dataset used testing. neural network architecture used problem consists layer -dimensional word embeddings initialized pre-trained weights glove followed convolutional layer features whose output cell -dimensional hidden state. individual hidden states passed layer softmax activation. crowd acknowledgments research leading results received funding people programme european union’s seventh framework programme grant agreement european union?s horizon research innovation programme marie sklodowska-curie individual fellowship h-msca-if- number", "year": 2017}