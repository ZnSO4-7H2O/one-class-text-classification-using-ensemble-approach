{"title": "Value Prediction Network", "tag": ["cs.AI", "cs.LG"], "abstract": "This paper proposes a novel deep reinforcement learning (RL) architecture, called Value Prediction Network (VPN), which integrates model-free and model-based RL methods into a single neural network. In contrast to typical model-based RL methods, VPN learns a dynamics model whose abstract states are trained to make option-conditional predictions of future values (discounted sum of rewards) rather than of future observations. Our experimental results show that VPN has several advantages over both model-free and model-based baselines in a stochastic environment where careful planning is required but building an accurate observation-prediction model is difficult. Furthermore, VPN outperforms Deep Q-Network (DQN) on several Atari games even with short-lookahead planning, demonstrating its potential as a new way of learning a good state representation.", "text": "paper proposes novel deep reinforcement learning architecture called value prediction network integrates model-free model-based methods single neural network. contrast typical model-based methods learns dynamics model whose abstract states trained make option-conditional predictions future values rather future observations. experimental results show several advantages model-free model-based baselines stochastic environment careful planning required building accurate observation-prediction model difﬁcult. furthermore outperforms deep q-network several atari games even short-lookahead planning demonstrating potential learning good state representation. model-based reinforcement learning approaches attempt learn model predicts future observations conditioned actions thus used simulate real environment multi-step lookaheads planning. call models observation-prediction model distinguish another form model introduced paper. building accurate observationprediction model often challenging observation space large even difﬁcult environment stochastic. therefore natural question whether possible plan without predicting future observations. fact observations contain information unnecessary planning dynamically changing backgrounds visual observations irrelevant value/utility. starting point work premise planning truly requires ability predict rewards values future states. observation-prediction model relies predictions observations predict future rewards values. could predict future rewards values directly without predicting future observations? model could easily learnable complex domains ﬂexible dealing stochasticity. paper address problem learning planning value-prediction model directly generate/predict value/reward future states without generating future observations. main contribution novel neural network architecture call value prediction network combines model-based model-free uniﬁed framework. order train propose combination temporal-difference search n-step q-learning brief vpns learn predict values q-learning rewards supervised learning. time vpns perform lookahead planning choose actions compute bootstrapped target q-values. empirical results navigation task demonstrate advantage model-free baselines also show robust stochasticity environment observation-prediction model approach. furthermore show outperforms several atari games even short-lookahead planning suggests model-based reinforcement learning. dyna-q integrates model-free modelbased learning observation-prediction model using generate samples q-learning addition model-free samples obtained acting real environment. extended ideas continuous control problems. work similar dyna-q sense planning learning integrated architecture. however vpns perform lookahead tree search choose actions compute bootstrapped targets whereas dyna-q uses learned model generate imaginary samples. addition dyna-q learns model environment separately value function approximator. contrast dynamics model combined value function approximator single neural network indirectly learned reward value predictions backpropagation. another line work uses observation-prediction models planning improving exploration. distinction prior works method learns abstract-state dynamics predict future observations instead predict future rewards/values. continuous control problems deep learning combined model predictive control speciﬁc using observation-prediction model. cases observation-prediction model differentiable respect continuous actions backpropagation used optimal action compute value gradients contrast work focuses learning planning using lookahead discrete control problems. vpns related value iteration networks perform value iteration approximating bellman-update convolutional neural network however vins perform entire state space practice requires state space small representable vector dimension corresponding separate state states topology local transition dynamics vpns limitations thus generally applicable show empirically paper. close in-part inspired predictron recurrent neural network acts transition function abstract states. viewed grounded predictron rollout corresponds transition environment whereas rollout predictron purely abstract. addition predictrons limited uncontrolled settings thus policy evaluation whereas vpns learn optimal policy controlled settings. model-free deep reinforcement learning. mnih proposed deep q-network architecture learns estimate q-values using deep neural networks. variations proposed learning better state representation including memory-based networks handling partial observability estimating state-values advantage-values decomposition q-values learning successor state representations learning several auxiliary predictions addition main values viewed model-free architecture decomposes q-value reward discount value next state uses multi-step reward/value predictions auxiliary tasks learn good representation. difference prior work listed learns simulate future rewards/values enables planning. although straw maintain sequence future actions using external memory cannot explicitly perform planning simulating future rewards/values. monte-carlo planning. monte-carlo tree search methods used complex search problems game simulator environment already available thus learned. recently alphago introduced value network directly estimates value state order better approximate value leaf-node states tree search. takes similar approach predicting value abstract future states tree search using value function approximator. temporal-difference search combined td-learning mcts computing target values value function approximator mcts. algorithm training viewed instance search learns dynamics future rewards/values instead given simulator. figure value prediction network. learns predict immediate reward discount value next abstract-state. unrolls core module abstract-state space compute multi-step rollouts. value prediction network developed semi-markov decision processes observation history observations partially observable mdps option time option maps observations primitive actions following bellman equation holds policies γirt+i discount factor immediate reward time number time steps taken option terminating observation xt+k. learns option-value function neural network parameterized like model-free also learns dynamics rewards/values perform planning. describe architecture section section describe perform planning using vpn. section describes train q-learning-like framework outcome module predicts option-reward executing option abstract-state option takes primitive actions termination outcome module predict discounted immediate rewards scalar. outcome module also predicts option-discount induced number steps taken option. figure illustrates core module performs -step rollout composing modules core module takes abstract-state option input makes core separate option-conditional predictions option-reward option-discount value abstract-state option-termination. combining predictions estimate q-value follows γvθ). addition recursively applies core module predict sequence future abstract-states well rewards discounts given initial abstract-state sequence options illustrated figure ability simulate future plan based simulated future abstract-states. although many existing planning methods applied implement simple planning method performs rollouts using certain depth henceforth denoted planning depth aggregates intermediate value estimates described algorithm figure formally given abstract-state option maxo trans planning algorithm divided steps expansion backup. expansion step recursively simulate options depth unrolling core module. backup step compute weighted average direct value estimate maxo average possible value d-step planning) equation note maxo estimates. propose compute uniform average possible returns using weights proportional maxo uniform average expected returns along path best sequence options illustrated figure reduce computational cost simulate b-best options expansion step based also choosing best option certain depth compromise performance much analogous using default policy mcts beyond certain depth. heuristic visits reasonably good abstract states planning though principled also used balance exploration exploitation. planning method used choosing options computing target q-values training described following section. intuitively vpn’s k-step prediction abstract-state time predicted xt−k following options ot−k trajectory illustrated figure applying value outcome module compute k-step prediction value reward discount. k-step prediction loss step deﬁned trained existing valuebased algorithm value predictions combined supervised learning reward discount predictions. paper present modiﬁcation n-step q-learning search main idea generate trajectories following \u0001-greedy policy based planning method described section given n-step trajectory generated \u0001-greedy policy k-step predictions deﬁned follows value computed d-step planning method described intuitively accumulates losses -step k-step predictions values rewards discounts. applying logγ discount prediction loss helps optimization amounts computing squared loss respect number steps. learning algorithm introduces hyperparameters number prediction steps planning depth used choosing options computing bootstrapped targets. also make target network parameterized synchronized certain number steps stabilize training suggested loss accumulated n-steps ∇θlt. full algorithm model-based sense learns abstract-state transition function sufﬁcient predict rewards/discount/values. meanwhile also viewed model-free sense learns directly estimate value abstract-state. perspective exploits several auxiliary prediction tasks reward discount predictions learn good abstract-state representation. interesting property planning ability used compute bootstrapped target well choose options q-learning. therefore improves quality future predictions perform better evaluation improved planning ability also generate accurate target q-values training encourages faster convergence compared conventional q-learning. experiments investigated following questions outperform model-free baselines advantage planning observation-based planning? useful complex domains high-dimensional sensory inputs atari games? network architecture. used encoding module transition module consists option-conditional convolution layer uses different weights depending option followed convolution layers. used residual connection previous abstract-state next abstract-state transition module learns change abstract-state. outcome module similar transition module except residual connection fully-connected layers used produce reward discount. value module consists fully-connected layers. number layers hidden units vary depending domain. details described appendix. implementation details. algorithm based asynchronous n-step q-learning threads used. target network synchronized every steps. used adam optimizer best learning rate decay chosen respectively. learning rate multiplied decay every steps. implementation based tensorflow four hyperparameters number predictions steps training plan depth training plan depth evaluation branching factor indicates number options simulated expansion step planning. used dtrain dtest throughout experiment unless otherwise stated. represents model learns predict simulate d-step futures training evaluation. branching factor depth depth means simulates -best options depth best option that. plan steps plan steps figure collect domain. agent collect many figure example vpn’s plan. goals possible within time limit given additional plan best future options input. collects goals given steps current state. ﬁgures show vpn’s found optimal trajectory planning collects goals. different plans depending time limit. baseline directly estimates q-values output trained asynchronous n-step q-learning. unlike original however baseline takes option additional input applies option-conditional convolution layer last encoding convolution layer similar architecture. identical training procedure except performs -step rollout estimate q-value shown figure viewed variation predicts reward discount value next state decomposition q-value. call observation prediction network similar except directly predicts future observations. speciﬁcally train independent networks model network predicts reward discount next observation value network estimates value observation. training scheme similar algorithm except squared loss observation prediction used train model network. baseline performs d-step planning like vpn. task description. deﬁned simple challenging navigation task agent collect many goals possible within time limit illustrated figure task agent goals walls randomly placed episode. agent four options move left/right/up/down ﬁrst crossing branch corridor chosen direction. agent given steps episode receives positive reward collects goal moving time-penalty step. although easy learn sub-optimal policy collects nearby goals ﬁnding optimal trajectory episode requires careful planning optimal solution cannot computed polynomial time. observation represented tensor binary values indicating presence/absence object type. time remaining normalized concatenated convolution layer network channel. evaluated architectures ﬁrst deterministic environment investigated robustness stochastic environment separately. stochastic environment goal moves block probability step. addition option repeated multiple times probability makes difﬁcult predict plan future precisely. overall performance. result summarized figure understand quality different policies implemented greedy algorithm always collects nearest goal ﬁrst shortest-path algorithm ﬁnds optimal solution exhaustive search assuming environment deterministic. note even small terms reward qualitatively substantial indicated small greedy shortest-path algorithms. results show many architectures learned better-than-greedy policy deterministic stochastic environments except baselines perform poorly stochastic environment. addition performance improved plan depth increases implies deeper predictions reliable enough provide accurate value estimates future states. result -step planning represented ‘vpn’ performs best environments. comparison model-free baselines. vpns outperform baselines large margin shown figure figure shows example trajectories given initial state. although dqn’s behavior reasonable ended collecting less goal compared vpn. hypothesize convolution layers used expressive enough best route episode ﬁnding optimal path requires combinatorial search task. hand perform combinatorial search extent simulating future abstract-states advantages model-free approaches dealing tasks require careful planning. comparison observation-based planning. compared opns perform planning based predicted observations vpns perform slightly better equally well deterministic environment. observed opns predict future observations accurately observations task simple environment deterministic. nevertheless vpns learn faster opns cases. conjecture takes additional training steps opns learn predict future observations. contrast vpns learn predict minimal sufﬁcient information planning reward discount value future abstract-states reason vpns learn faster opns. stochastic collect domain vpns signiﬁcantly outperform opns. observed opns tend predict average possible future observations deterministic. estimating values blurry predictions leads estimating different true expected value ex]. hand trained approximate true expected value explicit constraint loss predicted abstract state. hypothesize distinction allows learn different modes possible future states ﬂexibly abstract state space. result suggests value-prediction model beneﬁcial observation-prediction model environment stochastic building accurate observation-prediction model difﬁcult. table generalization performance. number represents average reward. ‘fgs’ ‘mws’ represent unseen environments fewer goals walls respectively. bold-faced numbers represent highest rewards conﬁdence level. generalization performance. advantage model-based approach generalize well unseen environments long dynamics environment remains similar. property evaluated architectures types previously unseen environments either reduced number goals increased number walls. turns much robust unseen environments compared model-free baselines shown table model-free baselines perform worse greedy algorithm unseen environments whereas still performs well. addition generalizes well learn near-perfect model deterministic setting signiﬁcantly outperforms stochastic setting. suggests good generalization property like model-based methods robust stochasticity. effect planning depth. investigate effect planning depth measured average reward deterministic environment varying planning depth evaluation training ﬁxed number prediction steps planning depth shown figure since learn predict observations guarantee perform deeper planning evaluation planning depth used training interestingly however result figure shows dtrain achieves better performance evaluation deeper tree search also tested dtrain found planning depth achieved best performance evaluation. thus suitably large number prediction steps training able beneﬁt deeper planning evaluation relative planning depth training. figure shows examples good plans length greater found trained planning depth another observation figure performance planning depth degrades planning depth training increases. means improve value estimations long-term planning expense quality short-term planning. figure effect evaluation planning depth. curve shows average reward function planning depth dtest architecture trained ﬁxed number prediction steps. ‘vpn*’ trained make -step predictions performed -step planning training investigate deals complex visual observations evaluated several atari games unlike collect domain atari games primitive actions small value consequences difﬁcult hand-design useful extended options. nevertheless explored vpns useful atari games even short-lookahead planning using simple options repeat primitive action extended time periods using frame-skip pre-processed game screen gray-scale images. architectures take last frames input. doubled number hidden units fully-connected layer approximately match number parameters. learns predict rewards values discount trained make -option-step predictions planning means agent predicts seconds ahead real-time. summarized table figure outperforms baseline atari games learned signiﬁcantly faster seaquest qbert krull crazy climber. possible reason outperforms even -step planning indeed helpful learning better policy. figure shows example vpn’s -step planning seaquest. predicts reasonable values given different sequences actions potentially help choose better action looking short-term future. another hypothesis architecture itself several auxiliary prediction tasks multi-step future rewards values useful learning good abstract-state representation model-free agent. finally algorithm performs planning compute target q-value potentially speed learning generating accurate targets performs value backups multiple times simulated futures discussed section results show approach applicable complex visual environments without needing predict observations. much previous work atari games used frame-skip though using larger frame-skip generally makes training easier make training harder games require ﬁne-grained control figure examples vpn’s value estimates. ﬁgure shows trajectories different sequences actions initial state along vpn’s value estimates parentheses action sequences downright-downrightfire-rightfire up-up-up left-left-left up-right-right. predicts highest value agent kills enemy lowest value agent killed enemy. introduced value prediction networks deep integrating planning learning simultaneously learning dynamics abstract-states make option-conditional predictions future rewards/discount/values rather future observations. empirical evaluations showed vpns outperform model-free baselines multiple domains outperform traditional observation-based planning stochastic domain. interesting future direction would develop methods automatically learn options allow good planning vpns. references abadi agarwal barham brevdo chen citro corrado davis dean devin ghemawat goodfellow harp irving isard józefowicz kaiser kudlur levenberg mané monga moore murray olah schuster shlens steiner sutskever talwar tucker vanhoucke vasudevan viégas vinyals warden wattenberg wicke zheng. tensorﬂow large-scale machine learning heterogeneous distributed systems. arxiv preprint arxiv. browne powley whitehouse lucas cowling rohlfshagen tavener perez samothrakis colton. survey monte carlo tree search methods. computational intelligence games ieee transactions mnih kavukcuoglu silver rusu veness bellemare graves riedmiller fidjeland ostrovski petersen beattie sadik antonoglou king kumaran wierstra legg hassabis. human-level control deep reinforcement learning. nature silver huang maddison guez sifre driessche schrittwieser antonoglou panneershelvam lanctot dieleman grewe nham kalchbrenner sutskever lillicrap leach kavukcuoglu graepel hassabis. mastering game deep neural networks tree search. nature figure examples trajectories planning deterministic collect domain. ﬁrst column shows initial observations following columns show trajectories respectively. shown sometimes chooses non-optimal option ends collecting fewer goals vpn. last column visualizes vpn’s option-step planning initial state. note vpn’s initial plans always match actual trajectories re-plans every step observes state. figure example trajectory stochastic collect domain. shows trajectory given initial state. decides move collect nearby goals moves left collect goals. result collects fewer goals compared vpn. since goals move randomly outcome option stochastic agent take account many different possible futures best option highest expected outcome. though outcome noisy stochasticity environment tends make better decisions often expectation. figure examples vpn’s planning atari games. ﬁrst column shows initial states following columns show vpn’s value estimates parentheses given different sequences actions. black arrows represent movement actions without ‘fire’. correspond ‘no-operation’ ‘fire’. estimates higher values moving reﬁll oxygen tank lower values moving kill enemies agent loses life oxygen tank empty almost running out. estimates lowest value moving towards enemy also estimates value moving right already eaten yellow pellets right side. hand estimates relatively higher values moving left collects pellets avoiding enemy. estimates higher values collecting nearby gives positive reward lower values collecting estimates higher values accelerating avoiding collision lower values colliding cars. algorithm describes algorithm training value prediction network observed training outcome module additional data collected random policy slightly improves performance reduces bias towards agent’s behavior. speciﬁcally replay memory transitions random policy training sample transitions replay memory train outcome module. procedure described line lines algorithm method used collect domain experiment generating transitions random policy. algorithm asynchronous n-step q-learning k-step prediction d-step planning global parameter global target network parameter global step counter plan depth number prediction steps store transitions using random policy converged figure transition module used collect domain. ﬁrst convolution layer uses different weights depending given option. sigmoid activation function used last convolution output forms mask. mask multiplied output convolution layer. note residual connection thus transition module learns change consecutive abstract states. encoding module consists conv-conv-conv conv represents ﬁlters size stride transition module illustrated figure consists optionconv-conv-conv separate conv mask multiplied output convolution layer transition module. ‘optionconv’ uses different convolution weights depending given option. also used residual connection previous abstract state next abstract state transition module learns difference states. outcome module optionconv-conv-fc-fc represents fully-connected layer hidden units. value module consists fc-fc. exponential linear unit used activation function architectures. baseline consists encoding module followed transition module followed value module. thus overall architecture similar except outcome module. match number parameters used hidden units dqn’s value module. found architecture outperforms original architecture collect domain several atari games. model network baseline architecture except additional decoding module consists deconv-deconv-deconv. module applied predicted abstract-state predict future observations. value network architecture baseline. encoding module consists conv-conv transition module optionconv-conv mask residual connection described above. outcome module optionconv-conv-fc-fc value module consists fc-fc. baseline encoding module followed transition module value module used hidden units value module approximately match number parameters. hyperparameters ones used collect domain except discount factor used.", "year": 2017}