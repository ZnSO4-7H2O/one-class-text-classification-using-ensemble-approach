{"title": "On Clustering Time Series Using Euclidean Distance and Pearson  Correlation", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "For time series comparisons, it has often been observed that z-score normalized Euclidean distances far outperform the unnormalized variant. In this paper we show that a z-score normalized, squared Euclidean Distance is, in fact, equal to a distance based on Pearson Correlation. This has profound impact on many distance-based classification or clustering methods. In addition to this theoretically sound result we also show that the often used k-Means algorithm formally needs a mod ification to keep the interpretation as Pearson correlation strictly valid. Experimental results demonstrate that in many cases the standard k-Means algorithm generally produces the same results.", "text": "abstract. time series comparisons often observed z-score normalized euclidean distances outperform unnormalized variant. paper show z-score normalized squared euclidean distance fact equal distance based pearson correlation. profound impact many distance-based classiﬁcation clustering methods. addition theoretically sound result also show often used k-means algorithm formally needs modiﬁcation keep interpretation pearson correlation strictly valid. experimental results demonstrate many cases standard kmeans algorithm generally produces results. kd-nuggets poll july frequently analyzed type data time series data voted second place. therefore surprising large number papers algorithms cluster classify segment index time series published recent years. tasks measures compare time series needed quite number measures proposed among measures euclidean distance frequently used distance measure applications ﬁnance medicine electric power load-forecast mention few. however many authors realize normalizing euclidean distance applying standard machine learning algorithms hugely improves results. paper closer investigate popular combination clustering time series data together normalized euclidean distance. commonly used learning method clustering tasks k-means algorithm show z-score normalized squared euclidean distance actually equal distance based pearson correlation coeﬃcient. oberservation allows standard learning methods based euclidean distance pearson correlation coeﬃcient simply performing appropriate normalization input data. ﬁnding applies learning methods based solely distance metric perform subsequent operations weighted average. therefore reasonably simple methods nearest neighbor hierarchical clustering also complex learning algorithms based e.g. kernels converted using pearson correlation coeﬃcient without actually touching underlying algorithm itself. order show aﬀects algorithms perform additional e.g. averaging operations chose often used k-means algorithm requires modiﬁcation underlying clustering procedure work correctly. experiments indicate however diﬀerence standard euclideanbased k-means pearson-based k-means high impact results therefore indicating even cases simple data preprocessing turn algorithm using euclidean distance equivalent based pearson correlation coeﬃcient. paper organized follows. section brieﬂy recap distances time series. afterwards show properly normalized euclidean distance equivalent distance based pearson correlation coeﬃcient. theoretically sound result quickly review k-means required modiﬁcations operate pearson correlation coeﬃcient. section perform experiments demonstrate diﬀerence behaviour standard k-means preprocessed inputs modiﬁed version. section brieﬂy review euclidean distance frequently used data miners distance based pearson correlation often used measure strength direction linear dependency time series. suppose time series consisting samples euclidean distance metric zero distance holds. time series analysis often recommended normalize time series either globally locally tolerate vastly diﬀerent ranges order pearson correlation coeﬃcient distance measure time series desirable generate distance values positively correlated series. pearson distance therefore deﬁned section show squared euclidean distance expressed pearson coeﬃcient long euclidean distance normalized appropriately note transformation applied recommended many authors noted ﬁrst resp. third part equation reﬂect times standard deviation series resp. assuming series normalized mean µr/s assuming normalization also ensures standard deviations σr/s equal simplify equation equivalence normalized euclidean distance pearson coeﬃcient particularly interesting since many published results using euclidean distance functions time series similarities come ﬁnding normalization original time series crucial. shows authors fact simulating pearson correlation coeﬃcient multiple result applied learning algorithm relies solely distance measures perform additional computations based distance measure averaging aggregation operations. case many simple algorithms nearest neighbor hierarchical clustering also applies sophisticated learning methods kernel methods. however algorithms prominent k-means algorithm perform subsequent averaging training instances compute cluster centers. simply apply normalizing input data introduce diﬀerent underlying distance measure. however since quite often ranking patterns respect underlying distance function actually used would interesting observation used apply also learning algorithms without actually adjusting entire algorithm. following section show works well known often used k-means clustering algorithm. cluster/prototype distance record prototype classical k-means uses squared euclidean distance batch version k-means prototypes cluster memberships updated alternatingly minimize objective function step part parameters kept ﬁxed optimized. hence phases algorithm ﬁrst assume prototypes optimal determine minimize membership degrees. since allow values minimize assigning record closest prototype afterwards assuming memberships optimal optimal position prototypes. answer question obtained solving derivatives ∂j/∂pi case euclidean distance optimal position mean data objects assigned cluster k-means algorithm name prototype update readjusts prototype mean assigned data objects. reason mean delivers optimal prototypes euclidean distance measure change distance measure check carefully update equations need changed too. seen section euclidean distances becomes pearson coeﬃcient vectors normalized. distances actually identical prototype update also same? equivalence holds normalized time series avoid constantly normalizing time series pearson coeﬃcient transform series preprocessing replace ≤t≤t ≤t≤t ˆrit rit−µr k-means objective function arguments distance function always pairs data object prototype. time series data normalized beginning necessarily hold prototypes. prototypes calculated mean normalized time series prototypes also zero mean unit variance guaranteed. prototypes calculated variance longer equivalence longer holds. therefore order stick pearson coeﬃcient must include additional constraint prototypes minimizing corresponds normalization usually obtained prototypes guarantee constraint unit variance. note formally division zero possible include constant time series using pearson distance version k-means uses pearson correlation coeﬃcient underlying distance measure. diﬀerence diﬀerent update equations really matter? following show number experiments inﬂuence optimization small. goal experimental evaluation twofold. k-means algorithm used frequently z-score normalized time series literature. seen case semantically sound solution would require modiﬁcations section obtain wrong results using standard k-means? corrected version normalization yield diﬀerent results all? much resulting cluster diﬀer kind data used typically literature. data objects prototype miss factor. however factors approximately prototoypes make relative error clusters rank cluster respect distance change. since k-means assigns data object top-ranked cluster diﬀerences distances inﬂuence prototype assignment. note also error propagate further since re-compute average normalized input data iteration algorithm. presenting experimental results ﬁrst discuss circumstances diﬀerence versions k-means occur. assume sake simplicity cluster consists time series only. prototype standard k-means since mean already mentioned order inﬂuence ranking clusters prototype’s norm must diﬀer within clusters. easily happen instance amount noise clusters linearly increasing time series without noise correlates perfectly adding random noise series pearson coeﬃcient approaches amplitude noise continuosly increased. demonstrate relevance theoretical considerations construct artiﬁcial test data set. consist clusters additional time series serving probes demonstrate diﬀerent behavior algorithms. test contains cluster linearly increasing time series well cluster linearly decreasing time series series cluster. gaussian noise samples data increasing trend noisy data decreasing trend. finally another time series equally belong clusters consist linearly increasing linearly decreasing portion pearson correlation well identical therefore expectation adding gaussian noise group time series average half assigned cluster. experiment want concentrate small group probe series distributed among clusters. data group time series shown ﬁgure reduce inﬂuence random eﬀects initialize clusters linearly increasing decreasing series clustering z-score normalized data standard k-means ﬁnally arrives prototypes norm approximately. instead expected situation probing series assigned cluster representing increasing series include normalization step given semantics clustering z-score normalized time series close clustering pearson correlation example clearly indicates obtain counterintuitive partitions using standard k-means. clusters poorly correlating data greedily absorb data would justiﬁed pearson correlation. undesired bias removed properly modiﬁed version k-means. still chosen example artiﬁcial rather drastic question remains inﬂuence issue really practice. order demonstrate real world data sets time series data sets clustering tasks available online data number classes number clusters experiment times diﬀerent random selections initial prototypes k-means. vectors initial prototypes diﬀerent types algorithms avoid problems unfamously instable k-means algorithm. order compare result natural instability k-means third experiment diﬀerent random initialization. data create clustering using classical k-means algorithm create clustering using k-means algorithm using euclidean distance create clustering using k-means algorithm using euclidean distance obviously number classes necessarily correspond number clusters. sake experiments reported here exact match true underlying clustering crucial also reason make available class information judge quality clustering methods considered here. entropy measures composed weighted average entropy clusters clustering using cluster indices clustering class labels. epear therefore measures diﬀerence clustering achieved correctly modiﬁed k-means algorithm lazy version normalize input data refrain normalizing prototypes. erandom shows diﬀerence runs k-means setting diﬀerent initalizations illustrate natural internal instability k-means. hypothesis would epear zero least substantially erandom using pearson correlation coeﬃcient worse inﬂuence k-means diﬀerent initializations. table shows results. show name dataset number clusters ﬁrst columns. length number time series training data listed columns following four columns show minimum resp. maximum value experiments entropies. ﬁned clustering data exists case unstable k-means also normalization prototypes change outcome slightly never impact diﬀerent initialization algorithm case. note many case even unstable classic k-means indicate diﬀerence prototype normalized unnormalized versions however. point trace face-four among others good examples eﬀect. experiments seems safe conclude simulate kmeans based pearson correlation coeﬃcients similarity metric simply normalizing input data without changing normal algorithm all. course even easier apply discussed properties classiﬁcation clustering algorithms need subsequent averaging steps k-nearest neighbor hierarchical clustering kernel methods etc. cases distance function using pearson correlations perfectly emulated simply normalizing input data appropriately using existing implementation based euclidean distance. shown squared euclidean distance z-score normalized vectors equivalent inverse pearson correlation coeﬃcient constant factor. result especially interest number time series clustering experiments euclidean distances applied normalized data shows authors fact often using something close equivalent pearson correlation coeﬃcient. also experimentally demonstrated standard k-means clustering algorithm without proper normalization prototypes still performs similar correct version enabling standard k-means implementations without modiﬁcation compute clusterings using pearson coeﬃcients. algorithms without internal computations k-nearest neighbor hierarchical clustering make theoretical proof provided paper applied without modiﬁcations course.", "year": 2016}