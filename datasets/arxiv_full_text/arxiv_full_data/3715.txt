{"title": "Physics Informed Deep Learning (Part II): Data-driven Discovery of  Nonlinear Partial Differential Equations", "tag": ["cs.AI", "cs.LG", "math.AP", "math.NA", "stat.ML"], "abstract": "We introduce physics informed neural networks -- neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations. In this second part of our two-part treatise, we focus on the problem of data-driven discovery of partial differential equations. Depending on whether the available data is scattered in space-time or arranged in fixed temporal snapshots, we introduce two main classes of algorithms, namely continuous time and discrete time models. The effectiveness of our approach is demonstrated using a wide range of benchmark problems in mathematical physics, including conservation laws, incompressible fluid flow, and the propagation of nonlinear shallow-water waves.", "text": "introduce physics informed neural networks neural networks trained solve supervised learning tasks respecting given physics described general nonlinear partial diﬀerential equations. second part two-part treatise focus problem data-driven discovery partial diﬀerential equations. depending whether available data scattered space-time arranged ﬁxed temporal snapshots introduce main classes algorithms namely continuous time discrete time models. eﬀectiveness approach demonstrated using wide range benchmark problems mathematical physics including conservation laws incompressible ﬂuid propagation nonlinear shallow-water waves. deep learning gained unprecedented attention last years deservedly introduced transformative solutions across diverse scientiﬁc disciplines despite ongoing success exist many scientiﬁc applications failed beneﬁt emerging technology primarily high cost data acquisition. well known current state-of-the-art machine learning tools lacking robustness fail provide guarantees convergence operating small data regime i.e. regime training examples available. ﬁrst part study introduced physics informed neural networks viable solution training deep neural networks training examples cases available data known respect given physical described system partial diﬀerential equations. cases abundant study physical biological engineering systems longstanding developments mathematical physics shed tremendous insight systems structured interact dynamically evolve time. knowledge underlying physical introduce structure eﬀectively regularizes training neural networks enables generalize well even training examples available. lens diﬀerent benchmark problems highlighted features physics informed neural networks context data-driven solutions partial diﬀerential equations second part study shift attention problem data-driven discovery partial diﬀerential equations consider parametrized nonlinear partial diﬀerential equations general form denotes latent solution nonlinear operator parametrized subset setup encapsulates wide range problems mathematical physics including conservation laws diﬀusion processes advection-diﬀusion-reaction systems kinetic equations. motivating example dimensional burgers’ equation here subscripts denote partial diﬀerentiation either time space. problem data-driven discovery partial diﬀerential equations poses following question given small scattered potentially noisy observations hidden state system parameters best describe observed data? follows provide overview main approaches tackle problem namely continuous time discrete time models well series results systematic studies diverse collection benchmarks. ﬁrst approach assume availability scattered potential noisy measurements across entire spatio-temporal domain. latter infer unknown parameters data snapshots taken distinct time instants. data codes used manuscript publicly available github https//github.com/ maziarraissi/pinns. proceed approximating deep neural network. assumption along equation result physics informed neural network network derived applying chain rule diﬀerentiating compositions functions using automatic diﬀerentiation worth highlighting parameters diﬀerential operator turn parameters physics informed neural network ﬁrst example consider burgers’ equation. equation arises various areas applied mathematics including ﬂuid mechanics nonlinear acoustics dynamics traﬃc fundamental partial diﬀerential equation derived navier-stokes equations velocity ﬁeld dropping pressure gradient term. small values viscosity parameters burgers’ equation lead shock formation notoriously hard resolve classical numerical methods. space dimension equation reads proceed approximating deep neural network. result physics informed neural network shared parameters neural networks along parameters diﬀerential operator learned minimizing mean squared error loss here denote training data loss corresponds training data enforces structure imposed equation ﬁnite collocation points whose number location taken training data. illustrate eﬀectiveness approach created training data-set randomly generating points across entire spatio-temporal domain exact solution corresponding ./π. locations training points illustrated panel ﬁgure data-set used train -layer deep neural network neurons hidden layer minimizing mean square error loss using l-bfgs optimizer upon training network calibrated predict entire solution well unknown parameters deﬁne underlying dynamics. visual assessment predictive accuracy physics informed neural network given middle bottom panels ﬁgure network able identify underlying partial diﬀerential equation remarkable accuracy even case scattered training data corrupted uncorrelated noise. scrutinize performance algorithm performed systematic study respect total number training data noise corruption levels neural network architecture. results summarized tables observation proposed figure burgers equation predicted solution along training data. middle comparison predicted exact solutions corresponding three temporal snapshots depicted dashed vertical lines panel. bottom correct partial diﬀerential equation along identiﬁed obtained learning methodology appears robust respect noise levels data yields reasonable identiﬁcation accuracy even noise corruption enhanced robustness seems greatly outperform competing approaches using gaussian process regression previously reported well approaches relying sparse regression require relatively clean data accurately computing numerical gradients next example involves realistic scenario incompressible ﬂuid described ubiquitous navier-stokes equations. navier-stokes equations describe physics many phenomena scientiﬁc engineering table burgers’ equation percentage error identiﬁed parameters diﬀerent number training data corrupted diﬀerent noise levels. here neural network architecture kept ﬁxed layers neurons layer. table burgers’ equation percentage error identiﬁed parameters diﬀerent number hidden layers neurons layer. here training data considered noise-free ﬁxed interest. used model weather ocean currents water pipe around wing. navier-stokes equations full simpliﬁed forms help design aircraft cars study blood design power stations analysis dispersion pollutants many applications. consider navier-stokes equations dimensions given explicitly diﬀerent regimes reynolds number u∞d/ν. assuming non-dimensional free stream velocity cylinder diameter kinematic viscosity system exhibits periodic steady state behavior characterized asymmetrical vortex shedding pattern cylinder wake known k´arm´an vortex street generate high-resolution data problem employed spectral/hp-element solver nektar speciﬁcally solution domain discretized space tessellation consisting triangular elements within element solution approximated linear combination tenth-order hierarchical semi-orthogonal jacobi polynomial expansion assumed uniform free stream velocity proﬁle imposed left boundary zero pressure outﬂow condition imposed right boundary located diameters downstream cylinder periodicity equation using third-order stiﬄy stable scheme system reaches periodic steady state depicted ﬁgure follows small portion resulting data-set corresponding steady state solution used model training remaining data used validate predictions. simplicity chosen conﬁne sampling rectangular region downstream cylinder shown ﬁgure given scattered potentially noisy data stream-wise transverse velocity components goal identify unknown parameters well obtain qualitatively accurate reconstruction entire pressure ﬁeld cylinder wake deﬁnition identiﬁed constant. created training data-set randomly sub-sampling full high-resolution data-set. highlight ability method learn scattered scarce training data chosen corresponding mere total available data illustrated ﬁgure also plotted representative snapshots predicted velocity components model trained. neural network architecture used consists layers neurons layer. figure navier-stokes equation incompressible dynamic vortex shedding past circular cylinder spatio-temporal training data correspond depicted rectangular region cylinder wake. bottom locations training datapoints stream-wise transverse velocity components respectively. unknown parameters high accuracy even training data corrupted noise. speciﬁcally case noisefree training data error estimating respectively. predictions remain robust even training data corrupted uncorrelated gaussian noise returning error respectively. intriguing result stems network’s ability provide qualitatively accurate prediction entire pressure ﬁeld absence training data pressure itself. visual comparison exact pressure solution presented ﬁgure represenfigure navier-stokes equation predicted versus exact instantaneous pressure ﬁeld representative time instant. deﬁnition pressure recovered constant hence justifying diﬀerent magnitude plots. remarkable qualitative agreement highlights ability physics-informed neural networks identify entire pressure ﬁeld despite fact data pressure used model training. bottom correct partial diﬀerential equation along identiﬁed obtained learning tative pressure snapshot. notice diﬀerence magnitude exact predicted pressure justiﬁed nature navier-stokes system pressure ﬁeld identiﬁable constant. result inferring continuous quantity interest auxiliary measurements leveraging underlying physics great example enhanced capabilities physics informed neural networks oﬀer highlights potential solving high-dimensional inverse problems. approach assumes availability scattered data throughout entire spatio-temporal domain. however many cases practical interest able observe system distinct time instants. next section introduce diﬀerent approach tackles data-driven discovery problem using data snapshots. leveraging classical runge-kutta time-stepping schemes construct discrete time physics informed neural networks retain high predictive accuracy even temporal data snapshots given merely training data snapshots shared parameters neural networks along parameters burgers’ equation learned minimizing squared errors equation here created training data-set comprising spatial points randomly sampling exact solution time instants respectively. training data shown middle panel ﬁgure neural network architecture used consists hidden layers neurons each number runge-kutta stages empirically chosen yield temporal error accumulation order machine precision setting time-step example bottom panel ﬁgure summarizes identiﬁed parameters cases noise-free data well noisy data gaussian uncorrelated noise corruption. cases proposed algorithm able learn correct parameter values remarkable accuracy despite fact data snapshots used training apart potentially describe diﬀerent regimes underlying dynamics. sensitivity analysis performed quantify accuracy predictions respect training snapshots noise levels training data physics informed neural network architecture. shown table proposed algorithm quite robust noise corruption levels consistently returns reasonable estimates unknown parameters. robustness mainly attributed ﬂexibility underlying implicit runge-kutta scheme admit arbitrarily high number stages allowing data snapshots apart time compromising accuracy nonlinear dynamics equation resolved. highlight discrete time formulation identiﬁcation problems setting apart competing approaches lastly table presents percentage error identiﬁed parameters demonstrating robustness estimates respect underlying neural network architecture. ﬁnal example aims highlight ability proposed framework handle governing partial diﬀerential equations involving higher order derivatives. here consider mathematical model waves shallow figure burgers equation solution along temporal locations training snapshots. middle training data exact solution corresponding temporal snapshots depicted dashed vertical lines panel. bottom correct partial diﬀerential equation along identiﬁed obtained learning water surfaces; korteweg-de vries equation. equation also viewed burgers’ equation added dispersive term. equation several connections physical problems. describes evolution long one-dimensional waves many physical settings. physical settings include shallow-water waves weakly non-linear restoring forces long internal waves density-stratiﬁed ocean acoustic waves plasma acoustic waves crystal lattice. moreover equation governing equation string fermi-pasta-ulam problem continuum limit. equation reads obtain training test data simulated using conventional spectral methods. speciﬁcally starting initial condition assuming periodic boundary conditions integrated equation ﬁnal time using chebfun package spectral fourier discretization modes fourth-order explicit runge-kutta temporal integrator time-step using data-set extract solution snapshots time randomly sub-sample using generate training data-set. data train discrete time physics informed neural network minimizing squared error loss equation using l-bfgs network architecture used comprises hidden layers neurons layer output layer predicting solution runge-kutta stages i.e. un+cj computed using equation setting panel present exact solution along locations data snapshots used training. detailed overview exact solution training data given middle panel. worth noticing complex nonlinear dynamics equation causes dramatic diﬀerences form solution reported snapshots. despite diﬀerences large temporal training snapshots method able correctly identify unknown parameters regardless whether training data corrupted noise not. speciﬁcally case noise-free training data error estimating respectively case noise training data returns errors respectively. introduced physics informed neural networks class universal function approximators capable encoding underlying physical laws govern given data-set described partial diﬀerential equations. work design data-driven algorithms discovering dynamic models described parametrized nonlinear partial diﬀerential equations. inferred models allow construct computationally eﬃcient fully diﬀerentiable surrogates subsequently used diﬀerent applications including predictive forecasting control optimization. although series promising results presented reader perhaps agree two-part treatise creates questions answers. broader context along seeking understanding tools believe work advocates fruitful synergy machine learning classical computational physics potential enrich ﬁelds lead high-impact developments. figure equation solution along temporal locations training snapshots. middle training data exact solution corresponding temporal snapshots depicted dashed vertical lines panel. bottom correct partial diﬀerential equation along identiﬁed obtained learning", "year": 2017}