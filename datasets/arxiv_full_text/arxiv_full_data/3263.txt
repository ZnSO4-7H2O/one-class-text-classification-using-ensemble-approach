{"title": "Generalized Twin Gaussian Processes using Sharma-Mittal Divergence", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "There has been a growing interest in mutual information measures due to their wide range of applications in Machine Learning and Computer Vision. In this paper, we present a generalized structured regression framework based on Shama-Mittal divergence, a relative entropy measure, which is introduced to the Machine Learning community in this work. Sharma-Mittal (SM) divergence is a generalized mutual information measure for the widely used R\\'enyi, Tsallis, Bhattacharyya, and Kullback-Leibler (KL) relative entropies. Specifically, we study Sharma-Mittal divergence as a cost function in the context of the Twin Gaussian Processes (TGP)~\\citep{Bo:2010}, which generalizes over the KL-divergence without computational penalty. We show interesting properties of Sharma-Mittal TGP (SMTGP) through a theoretical analysis, which covers missing insights in the traditional TGP formulation. However, we generalize this theory based on SM-divergence instead of KL-divergence which is a special case. Experimentally, we evaluated the proposed SMTGP framework on several datasets. The results show that SMTGP reaches better predictions than KL-based TGP, since it offers a bigger class of models through its parameters that we learn from the data.", "text": "abstract growing interest mutual information measures wide range applications machine learning computer vision. paper present generalized structured regression framework based shama-mittal divergence relative entropy measure introduced machine learning community work. sharma-mittal divergence generalized mutual information measure widely used r´enyi tsallis bhattacharyya kullback-leibler relative entropies. speciﬁcally study sharma-mittal divergence cost function context twin gaussian processes generalizes kl-divergence without computational penalty. show interesting properties sharma-mittal theoretical analysis covers missing insights traditional formulation. however generalize theory based sm-divergence instead kl-divergence special case. experimentally evaluated proposed smtgp framework several datasets. results show smtgp reaches better predictions kl-based since oﬀers bigger class models parameters learn data. since work done measure information probabilistic metrics. claude shannon proposed powerful framework mathematically quantify information foundation information theory development communication networking computer science applications. many problems physics computer science require reliable measure information divergence motivated many mathematicians physicists computer scientists study diﬀerent divergence measures. instance r´enyi tsallis kullback-leibler divergences applied many computer science applications. eﬀectively used machine learning many tasks including subspace analysis facial expression recognition texture classiﬁcation image registration clustering nonnegative matrix factorization pose estimation machine learning community attempts done understand information connect uncertainty. many proposed terminologies turns diﬀerent views measure. instance bregman information statistical information csiszr-morimoto f-divergence expectations jensen’s inequality turn equivalent maximum reduction uncertainty convex functions contrast prior probability distribution work proposed order unify divergence functions cichocki ichi amari considered explicitly relationships alpha-divergence beta-divergence gamma-divergence singleparameter divergence measure. then cichocki introduced twoparameter family. however study two-parameter divergence measure investigated physics community interesting considered machine learning community. akturk physicists studied entropy measure called sharmamittal theormostatics originally introduced sharma sharma-mittal divergence parameters detailed later section akturk discussed entropy generalizes tsallis r´enyi entropy limiting cases parameters; originally showed addition shown entropy converges shannon entropy akt¨urk also suggested physical meaning entropy free energy diﬀerence equilibrium oﬀ-equilibrium distribution. entropy also investigated multidimensional harmonic oscillator systems similarly relative entropy generalizes r´enyi tsallis mutual information divergences. closed-form expression divergence gaussian distributions recently proposed motivated study measure structured regression setting. paper present generalized framework structured regression utilizing family divergence measures includes divergence r´enyi divergence tsallis divergence divergence. particular study divergence within context twin gaussian processes state-of-the-art structured-output regression method. sminchisescu proposed structured prediction approach based estimating divergence input output gaussian processes denoted kltgp. since divergence symmetric sminchisescu also studied based divergence output input data denoted ikltgp work present generalization using divergence denoted smtgp. since divergence two-parameter family study eﬀect parameters related distribution data. context show parameters could interpreted distribution bias divergence order context structured learning. also highlight probabilistic causality direction objective function. speciﬁcally contributions paper ﬁrst presentation divergence machine learning community generalized version based divergence predict structured rest paper organized follows section presents background divergence available closed-form expression multivariate gaussians. section presents optimization problem used framework derived analytic gradients. section presents theoretical analysis under framework spectral perspective. section presents experimental validation. finally section discusses concludes work. two-parameter generalized entropy measure originally introduced sharma worth mention two-parameter family divergence functions recently proposed machine learning community since tsallis entropy betadivergence r´enyi entropy related gammadivergences tsallis r´enyi relative entropies diﬀerent generalization standard boltzmann-gibbs entropy however focus divergence three reasons generalizes considerable family functions suitable structured regression problems possible future consideration measure works study entropy divergence functions divergence closed-form expression recently proposed multivariate gaussian distributions interesting study. another motivations work study parameters divergence generalized entropy measure aﬀect performance structured regression problem. show analogy physics domain motivates study. indicated masi physics domain important understand tsallis r´enyi entropies diﬀerent generalizations along diﬀerent paths. tsallis generalizes non-extensive systems r´enyi quasi-linear means. entropy generalizes non-extensive sets non-linear means tsallis r´enyi measures limiting cases. hence regression setting indicates resolving trade-oﬀ control direction bias towards distributions changing also allows higher-order divergence measure changing another motivation physics entropy entropy gives rise thermostatistics based escort mean values admitting partition function order solve optimization problems eﬃciently relative entropy critical closed-form formula optimized function relative entropy framework. prediction gaussian processes performed practically multivariate gaussian distribution. hence interested ﬁnding closed-form formula relative entropy distribution frank nielsen proposed closed form expression divergence follows positive deﬁnite matrix denotes matrix determinant. following section builds closed-form expression predict structured output leads analytic gradient smtgp cost function cubic computational complexity. present simpliﬁed expression closed-form expression equation results equivalent smtgp analytic gradient quadratic complexity. prediction problems expect similar inputs produce similar predictions. notion adopted predict structured output based divergence gaussian processes. section presents structured regression minimizing relative entropy. follow theoretical analysis tgps section begin introducing notation. joint distributions input output deﬁned follows similarity kernel similarly matrix similarity kernel applying gaussian-rbf kernel functions similarity kernels inputs outputs form ﬁrstly proposed minimizes kullbackleibler divergence marginal inputs outputs. however focusing human pose estimation problem. result estimated pose using given solution following optimization problem optimization problem solved using second order bfgs quasinewton optimizer cubic polynomial line search optimal step size selection. since divergence symmetric sminchisescu also studied inverse kl-divergence output input distribution tgp; denote model ikltgp. equations show ikltgp cost function corresponding gradient. equations hard gradients kltgp ikltgp computed quadratic complexity given precomputed training stored depends training data. quadratic complexity kltgp gradient presents benchmark compute gradient smtgp hence address benchmark framework detailed following subsections. |kx| positive constant since positive deﬁnite matrices. hence could removed optimization problem. argument holds |kx∪x| |kx| could also removed cost function. removed constants prediction function reduces minimizing following expression avoid numerical instability problems equation optimized log) instead lαβ. derived gradient log) applying matrix calculus directly logarithm equation presented below; derivation steps detailed appendix denote original closed-form expression simpliﬁed form applying simpliﬁed expression lemma measure divergence cost function becomes following form since |kx∪x| multiplicative positive constants depend dropped cost function. also additive constant ignored optimization. after ignoring multiplicative positive constants added constant improved smtgp cost function reduces contrast equation involve determinant large matrix. hence predict output directly minimizing equation since cost function factors depend follow rule constant functions interprets terms computational complexity cost function equation gradient equation quadratic test time number depend training training data. since points precomputed training time. hence hypothesis quadratic computational complexity improved smtgp prediction function gradient true since remaining computations indicates advantage using closed-form expression divergence lemma closed-form proposed cubic complexity. however expression equivalent straight forward compute gradient quadratic complexity expression. advantage previous subsection shows computational complexity smtgp prediction decreased signiﬁcantly using test time quadratic compared cubic complexity dαβ. context show another general advantage using proposed closed-form expression generally compute sm-divergence gaussian distributions times faster compute needs operations much less condition. since operations needed compute appendix proof. conclude section general form lemma equation equation achieved refactorizing exponential term using matrix identities. case times faster computing dαβ. needs operations case less since operations needed compute appendix indicates simpliﬁcations provided work could used generally speedup computation divergence gaussian distributions beyond context tgps. order understand role parameters smtgp performed eigen analysis cost function equation generally speaking basic notion prediction extend dimensionality divergence measure training examples examples involves test point unknown output hence start discussing extension general gaussian process kz∪z domain point extends kz∪z detailed subsection based discussion derive lemmas address properties smtgp prediction subsection lead probabilistic interpretation provide subsection section superscript disambiguate kernel matrix size i.e. gaussian process arbitrary domain marginalization given gaussian process training points extension marginalization points adding point kernel matrix written terms follows variance distribution direction corresponding eigen vectors. hence determinant matrix generalizes notion variance multiple dimensions volume elliptical distribution oriented eigen vectors. notion could interpret ratio variance marginalized gaussian process scaled introduced data point looking closely notice decreases data point closer points. situation makes highly correlated eigen vectors small eigen values since term maximized points smallest principal component hence uncertainty measure minimized data point produces vector maximizes certainty data could thought measurement proportional computing input space makes equivalent predictive variance gaussian process regression prediction depends input space. however discussing uncertainty extension arbitrary domain beneﬁcial smtgp analysis follows. deﬁned equation detailed section divergence involves determinant three matrices size namely kx∪x kx∪x. hence three uncertainty extensions follows might straightforward think +kx∪x within formulation kernel matrix deﬁned space equation gives interpretation constraint equation since weighted valid kernels positive weights kx∪x valid kernel matrix space. equation derived following lemmas. |−α|.η−α |αky +kx|.ηxy since |αky |kx| depend predicted output indicates smtgp optimization function inversely proportional upper-bounded lemma hence hard controls whether maximize |−α|ky |αη−α |αky |ηxy agreement function extended distributions agreement function increases weighted volume input output distributions close possible volume joint distribution kx∪x|). function reaches distributions identical justiﬁes maximizing indicated lemma another view maximizing prefers minimizing maximizes abbreviate motivated intuition subη−α gives probasection however smtgp maximizes bilistic sense cost function follow intuition hence could seen p−αpα discussed following subsection. understanding motivated plot relation test error smtgp prediction. figure shows clear correlation prediction error. hence introduces clear motivation study certainty measure could associated structured output prediction. mean? since obvious max. figure shows behavior also bounded max. according ﬁgure behaves similar |ηx−ηy| approaches zero linear approximation accurate. however gets bigger gets biased towards indicated left column ﬁgure hence hence regression predicts output maximum certainty kx∪x conditioned uncertainty extension conditioning biased towards gives best discrimination relative hence maximize certainty prediction. case diﬀerence high prediction based weighted shown point above. section evaluate smtgp examples usps dataset image reconstruction task poser dataset humaneva dataset pose estimation task. shown outperforms kernel regression gaussian process regression weighted k-nearest neighbor regression hilbert schmidt independence criterion kernel target alignment method example humaneva dataset poser dataset hence extended evaluation beyond pose estimation datasets. compared smtgp kltgp ikltgp. ikltgp stands inverse kltgp predicts output minimizing divergence output probability distribution input probability distribution main motivation behind comparison kltgp ikltgp biased distributions therefore user choose either kltgp ikltgp based problem. contrast smtgp could adapted validation prediction error minimized. point denote kltgp ikltgp smtgp tgps. presentation results starts speciﬁcation examples datasets subsection then present parameter settings training ﬁrst problem predict output variable given control consists values generated uniformly y+.sin+ǫ evaluated figure stars correspond examples regression suﬀer boundary/discontinuous eﬀects indicated tgps tested equally spaced inputs used mean prediction error measure performance example. order introduce challenging situation generate double shape; figure example constructed concatenated shapes makes overall prediction error challenging reduce. addition down-sampled points total number points example hence less evidence training data compared example similarly tgps tested equally spaced inputs used error-measure example image reconstruction problem given outer pixel values handwritten digit usps data goal predict pixel values lying center. split dataset test examples training samples range pixel values dataset error measure amounts root mean-square error averaged gray-scales center. errorpose predicted -values’ vector lying center true -colors given outer pixels values poser dataset consists training test images synthetically generated tuned unimodal predictions. image features corresponding bag-of-words representation silhouette-based shape context features. tgps requires inversion matrices training complexity solution impractical larger. hence poser human datasets applied tgps ﬁnding nearest neighbors strategy also adopted poser dataset generated using poser software package motion capture data error measured root mean square error humaneva datset contains synchronized multi-view video mocap data. consists subjects performing multiple activities. histogram oriented gradient features proposed training validations sub-sets humaneva-i utilize data color cameras total image-pose frames camera. consistent experiments half data training half testing. humaneva pose encoded joint markers deﬁned relative torso distal joint camera-centric coordinate frame error pose measured average euclidean kˆym y∗mk estimated pose distance errorpose vector true pose vector. smtgp prediction done optimizing equation gradient descend steps since proved mainly changing power cost function theoretically aﬀect prediction detailed section hence motivated consider three values actually edge cases found role practice mainly aﬀecting convergence rate purpose cross validation converges faster. found speciﬁc value gives best performance datasets. hence suggest selecting cross validation like diﬀerent purpose. performed fold cross validation parameters ranging step while selected three values practice learning parameters covers diﬀerent divergence measures select setting minimize error validation set. finally initialize prediction regarding values selected training kltgp table shows parameter setting used kltgp iktgp smtgp models. models share parameters. however smtgp additional parameters. noticed figures smtgp improved kltgp dataset. improvement achieved dataset challenging; figures results indicates advantages parameter selection table notice smtgp improved kltgp also ikltgp shows adaptation behavior smtgp tuning argued kltgp performs better ikltgp pose estimation. while reported gave almost performance refer presented draw conclusions. first kltgp always outperform ikltgp argued humaneva dataset. second smtgp could tuned cross-validation outperform kltgp ikltgp. examples interesting behavior smtgp performs least good best kltgp ikltgp datasets. kltgp ikltgp biased towards input output distributions. however smtgp learns training data bias factor towards input output distributions. results could also justiﬁed fact divergence generalization family divergence measure. powerful property smtgp controlling smtgp provides divergence functions optimize prediction. however member selected training tuning validation set. hence smtgp learns make better predictions. finally smtgp desirable generalization test set; table table also shows smtgp complexity kltgp also similar constant factor. four datasets smtgp faster ikltgp kltgp optimized matrix operations three methods possible. smtgp kltgp similar number matrix operations; justiﬁes similar computational times. conclude results reporting performance hsic-knn kta-knn w-knn datasets; table comparing table table obvious tgps outperforms hsic-knn kta-knn w-knn. baseline approaches also compared kltgp results consistent conclusion reached comparison example humaneva dataset; parameters baselines selection. indicates methods applied training data k-neighborhood testing point proposed framework structured output regression based sm-divergence. performed theoretical analysis understand properties smtgp prediction helped learn parameters sm-divergence. part analysis argued certainty measure could associated prediction. discuss main ﬁndings work. critical theoretical aspect missing kl-based formulation understanding cost function regression-perspective. cover missing theory analyzing cost function based instead providing understanding smtgp cost function covers claims supported theoretical analysis presented section main theoretical result sm-based prediction maximizes certainty measure call prediction depend theoretically. probabilistic interpretation discussed part analysis shown negative correlation test error interesting result; ﬁgure ﬁgure highlights similarity smtgp certainty measure predictive variance provided gaussian process regression single output prediction. computationally eﬃcient closed-form expression sm-divergence presented leads reducing smtgp prediction complexity makes smtgp kltgp computationally equivalent. moreover reduces number operations compute sm-divergence general gaussian distributions context; section practically achieve structured output regression tuning parameters sm-divergence cross validation smtgp cost function. performed intensive evaluation diﬀerent tasks datasets experimentally observed desirable generalization property smtgp. experiments report resultant approach smtgp outperformed kltgp ikltgp hsic wknn methods examples three datasets. conclude highlighting practical limitation smtgp requires additional time tuning cross validation. however would like indicate cross validation time short datasets using smaller grid could signiﬁcantly decrease validation time. used grid steps however found experiments enough grid size addition selecting single randomly selected validation like neural networks models could save time instead selecting entire training cross validation performed experiment. presented theoretical analysis two-parameter generalized divergence measure named sharma-mittal structured output prediction. proposed alternative equivalent formulation divergence whose computation quadratic compared cubic structured output prediction task investigated theoretical properties concluded derivation much simpler starting simpliﬁed closed-form expression sm-divergence multivariate gaussians. ignoring multiplied positive constants added constants +αky multiplied constants added constant) improved smtgp cost function reduces know eﬃcient compute equation requires operations; illustrate follows. cholesky decompistion requires computing then choseskly decompition tional ignore. hence required computations required compute lemma could computed similarly operations required compute determinants cholesky decomposition. case additional operations needed compute σq)− total times faster compute", "year": 2014}