{"title": "Protein Secondary Structure Prediction Using Cascaded Convolutional and  Recurrent Neural Networks", "tag": ["q-bio.BM", "cs.AI", "cs.LG", "cs.NE", "q-bio.QM"], "abstract": "Protein secondary structure prediction is an important problem in bioinformatics. Inspired by the recent successes of deep neural networks, in this paper, we propose an end-to-end deep network that predicts protein secondary structures from integrated local and global contextual features. Our deep architecture leverages convolutional neural networks with different kernel sizes to extract multiscale local contextual features. In addition, considering long-range dependencies existing in amino acid sequences, we set up a bidirectional neural network consisting of gated recurrent unit to capture global contextual features. Furthermore, multi-task learning is utilized to predict secondary structure labels and amino-acid solvent accessibility simultaneously. Our proposed deep network demonstrates its effectiveness by achieving state-of-the-art performance, i.e., 69.7% Q8 accuracy on the public benchmark CB513, 76.9% Q8 accuracy on CASP10 and 73.1% Q8 accuracy on CASP11. Our model and results are publicly available.", "text": "protein secondary structure prediction important problem bioinformatics. inspired recent successes deep neural networks paper propose end-to-end deep network predicts protein secondary structures integrated local global contextual features. deep architecture leverages convolutional neural networks different kernel sizes extract multiscale local contextual features. addition considering long-range dependencies existing amino acid sequences bidirectional neural network consisting gated recurrent unit capture global contextual features. furthermore multitask learning utilized predict secondary structure labels amino-acid solvent accessibility simultaneously. proposed deep network demonstrates effectiveness achieving state-of-theart performance i.e. accuracy public benchmark accuracy casp accuracy casp. model results publicly available. introduction accurately reliably predicting structures especially structures protein sequences challenging tasks computational biology great interest bioinformatics structural understanding critical protein analysis also meaningful practical applications including drug design understanding protein secondary structure vital intermediate step protein structure prediction secondary structure protein reﬂects types local structures present protein. thus accurate secondary structure prediction signiﬁcantly reduces degree freedom tertiary structure give rise precise high resolution protein structure prediction frequently used analyze probability speciﬁc amino acids appearing different secondary structure elements accuracy i.e. accuracy three-category classiﬁcation helix strand coil models lower inadequate features. signiﬁcant improvements achieved exploiting evolutionary information proteins structural family position-speciﬁc scoring matrices period accuracy exceeded taking advantage features. however progress stalled came challenging -category classiﬁcation problem needs distinguish among following categories secondary structure elements −helix α−helix π−helix β−strand β−bridge β−turn bend loop irregular century various machine learning methods especially artiﬁcial neural networks utilized improve performance e.g. svms recurrent neural networks probabilistic graphical models conditional neural ﬁelds combining crfs neural networks generative stochastic networks well known local contexts critical protein secondary structure prediction. speciﬁcally secondary structure category information neighbours amino acid effective features classifying secondary structure amino acid belongs instance fig. amino acids likely assigned secondary structure label given neighbours’ information. convolutional neural networks speciﬁc type deep neural networks using translation-invariant convolutional kernels applied extracting local contextual features proven effective many natural language processing tasks inspired success text classiﬁcation paper cnns various kernel sizes used extract multiscale local contexts protein sequence. rest paper organized follows. section introduce proposed end-to-end deep model detail. present implementation details experimental results ablation study section section concludes paper ﬁnal remarks. network architecture illustrated fig. deep convolutional recurrent neural network protein secondary structure prediction consists four parts feature embedding layer multiscale convolutional neural network layers three stacked bidirectional gated recurrent unit layers fully connected hidden layers. input deep network carries types features protein amino acid sequence sequence features proﬁle features. feature embedding layer responsible transforming sparse sequence feature vectors denser feature vectors feature space. embedded sequence features original proﬁle features multiscale layers different kernel sizes extract multiscale local contextual features. concatenated multiscale local contexts three stacked bgru layers capture global contexts. cascaded bgru layers fully connected hidden layers taking concatenated local global contexts input. output second fully connected layer softmax activation output layer performs −category secondary structure −category solvent accessibility classiﬁcation. feature embedding better understanding protein secondary structure prediction formulated follows. given amino acid sequence need predict secondary structure label every amino acid n-dimensional feature vector corresponding i-th amino acid -state secondary structure label. paper input feature sequence decomposed parts sequence -dimensional feature vectors encoding types amino acids protein sequence -dimensional proﬁle features obtained psi-blast rescaled logistic function note feature vector ﬁrst sequence sparse one-hot vector i.e. elements none-zero proﬁle feature vector dense representation. order avoid inconsistency feature representations adopt embedding operation natural language processing transform sparse sequence features denser representation embedding operation implemented feedforward neural network layer embedding matrix wemb r×demb maps sparse -dimensional vector denser demb-dimensional vector. paper empirically demb initialize embedding matrix random numbers. embedded sequence feature hand long-range interdependency among different types amino acids also holds vital evidences category secondary structure e.g. β−strand steadied hydrogen bonds formed β−strands distance instance also fig. amino acids determined share secondary structure label given disulphide bond annotated link similar cnns recurrent neural networks another speciﬁc type neural networks loop connections. designed capture dependencies across distance larger extent local contexts. previous work models could perform well protein secondary structure prediction partially difﬁculty train models. fortunately rnns gate memory structures including long short term memory gate recurrent units structure artiﬁcially learn remember forget information using speciﬁc gates control information ﬂow. paper exploit bidirectional gate recurrent units capture long-range dependencies among amino acids protein sequence. propose novel deep convolutional recurrent neural network protein secondary structure prediction. deep network consists feature embedding layer multiscale layers local context extraction stacked bidirectional layers global context extraction fully connected softmax layers ﬁnal joint secondary structure solvent accessibility classiﬁcation. experimental results dataset public benchmark recent casp casp datasets demonstrate proposed deep network outperforms existing methods achieves state-of-the-art performance. figure end-to-end deep convolutional recurrent neural network predicting protein secondary structures. input consists sequence features proﬁle features. feature embedding concatenation preprocessed features multiscale layer multiple kernel sizes used extract multiscale local features. concatenated multiscale features local contexts three stacked bgru layers capturing global contexts. stacked bgru layers fully connected hidden layers used multi-task joint classiﬁcation. vector concatenated proﬁle feature vector multiscale layers. multiscale cnns illustrated fig. second component deep network multiscale convolutional neural network layers. given amino acid sequence embedded concatenated features preprocessed feature vector i-th amino acid. model local dependencies adjacent amino acids leverage cnns sliding window rectiﬁed linear unit extract local contexts. convolutional kernel extent kernel along protein sequence feature dimensionality individual amino acids bias term ‘relu’ activation function. kernel goes full input sequence generates corresponding output sequence channels since amino acid sometimes affected residues relative large distance e.g. residues interaction distance disulphide bond labeled link fig. multiscale layers different kernel sizes used obtain multiple local contextual feature maps. paper three layers results three feature maps multiscale features concatenated together local contexts concatenate{ ˜l}. bgrus addition local dependencies long-range dependencies line fig. also widely exist amino acid sequences. multiscale cnns capture dependencies among amino acids separated distance larger maximum kernel size. capture dependencies across larger distance exploit bidirectional gate recurrent units recurrent neural networks powerful capacity deal context-dependent sequences. however training rnns used difﬁcult vanishing gradients recent years rnns gated units e.g. long short term memory gate recurrent units became practically useful. network grus used capture global contexts achieve comparable performance less parameters comparison lstm notation previous equations mechanism presented follows input respectively activations internal memory cell output number hidden units wl˜h wh˜h weight matrices; bias terms. addition sigm tanh stand element-wise multiplication sigmoid hyperbolic functions respectively. compared lstm three gates external memory cell state output state gates output state. least important gate lstm merges input gate forget gate together form update gate reset gate control information artiﬁcially remembered forgotten. total number parameters lstm ture prediction solvent accessibility prediction predicted probabilities secondary structure labels solvent accessibility labels respectively ground-truth labels secondary structure solvent accessibility respectively weight vector number residues. experimental results datasets features four publicly available datasets produced pisces cullpdb casp casp evaluate performance proposed deep neural network. large non-homologous protein sequence structure dataset proteins include proteins training proteins validation proteins testing. note testing different public benchmark dataset used testing only. since exists redundancy smaller ﬁltered version formed removing sequences similarity sequence done ﬁltered dataset proteins used training used testing set. casp casp contain domain sequences respectively. used testing performance proposed network trained ﬁltered dataset. performance secondary structure prediction measured accuracy. every protein sequence aforementioned datasets channels data residue. among channels channels used sequence feature speciﬁes category amino acid channels sequence proﬁle channels secondary structure category labels channels solvent accessibility labels although additional features could considered improve performance focus network architecture paper. note exist mask channels sequence feature sequence proﬁle secondary structure label. convenience subsequent processing implementation length protein amino acid sequences datasets normalized sequences longer truncated shorter padded zeros. sequences shorter secondary structure label amino acid depends label preceding amino acid sequence also label following amino acid. thus bidirectional grus consists forward backward output forward backward grus time concatenated together form output bidirectional time. furthermore enhance global information network three bgrus stacked together dropout improve performance. note forward hidden state stacked bgrus calculated orward gru. absolute relative accessibility judged speciﬁc thresholds original normalized solvent accessibility values computed dssp program solvent accessibility closely related secondary structure prediction. according multi-task training method l−norms added loss function regularization term. addition dropout employed stacked bgru layers penultimate layer avoid overﬁtting. joint loss function formulated follows. reference. obtained feature maps channels concatenated together local contextual feature vector. three stacked bgru layers hidden units. take local contexts input. output bgru layers regularized dropout avoid overﬁtting. local global contexts obtained multiscale layers bgru layers concatenated together fully connected layers relu activation. balancing joint learning tasks regularization term. also exploit bagging obtain ensemble models. according standard bagging algorithm weak model randomly choose proteins original training form validation remaining training samples form training set. ensemble model consists independently trained weak models. early stopping used training. speciﬁcally score validation increasing epoches decrease learning rate factor learning rate smaller predeﬁned threshold stop training test model obtained every epoch validation choose best performance validation trained model. code implemented theano publicly available deep learning software basis keras library. weights neural networks initialized using default setting keras. train layers deep network simultaneously using adam optimizer batch size entire deep network trained single nvidia geforce titan memory. takes train deep network without early stopping takes hours take advantage early stopping. testing stage protein takes average. performance evaluate overall performance deep network performing three sets experiments. ﬁrst experiments perform training testing original dataset. second perform training ﬁltered dataset testing benchmark. third still perform training ﬁltered dataset performance measured recent casp casp datasets. training testing model trained using training achieves .±.% accuracy deﬁnes state higher previous best result obtained solvent accessibility accuracy testing compare result testing used different testing deﬁned publicly available either. table compare overall performance table classiﬁcation precision recall individual secondary structure labels testing results compared gsn. frequencies secondary structure labels training given last column. boldface numbers indicate best performance. frequency well performance individual secondary structure labels previous best results achieved apparently proposed model achieves higher precision recall almost individual labels. believe better performance power neural networks also owing power integrated local global contexts. specifically four labels high frequency model achieves better performance fact model millions parameters higher representation capacity. nevertheless model also performs better previous models frequency labels likely integrated local global contexts form core contribution paper. training filtered testing also performed validation public benchmark using model trained ﬁltered dataset whose proteins include sequences similarity proteins single trained model achieves accuracy higher previous state achieved deepcnf training testing sets solvent accessibility accuracy validation ground-truth solvent accessibility labels unavailable also compare model existing methods sspro individual secondary structure labels table note that addition standard sequence feature proﬁle feature used methods comparison model trained using three extra features entire feature vector amino acid -dimensional. even though trained using features model still achieves overall higher accuracy. terms individual labels model achieves slightly lower accuracies high frequency labels signiﬁcantly higher accuracies frequency labels table comparison classiﬁcation precisions individual secondary structure labels note trained -dimensional features deepcnf model trained standard dimensional features. precision single model calculated using confusion matrix. precision ensemble model obtained voted confusion matrix. frequencies individual labels training descriptions presented last columns. boldface numbers indicate best performance. trained independently randomly sampled training validation subsets according bagging algorithm. model averaging process accuracy improved shown table addition accuracy single model higher previous state mapping -state labels -state labels follows mapped mapped -state labels mapped according addition p-value signiﬁcance test model outperforms methods using results different runs. training filtered testing casp casp verifying generalization capability model trained ﬁltered dataset also evaluated recent datasets casp casp. compare accuracy model sspro raptorx-ss deepcnf. addition accuracy model also compared sspro spinex psipred raptorxss jpred deepcnf. according results shown table accuracy model casp casp accuracy model casp casp respectively note accuracy accuracy method test obtained single model rather ensemble model fair comparison. ablation study discover vital elements success proposed network conduct ablation study removing replacing individual components network. speciﬁcally table training ﬁltered testing casp casp. accuracies reported sspro spine-x psipred jpred raptorxss deepcnf model boldface numbers indicate best performance. tested performance models without feature embedding layer multiscale cnns stacked bgrus backward passes. addition also tested model feed local contexts last fully connected layers another bgru layers replaced bidirectional simplernn layers without gate structure ﬁgure importance gate structure presence long-range dependencies. results dataset presented table bidirectional layers effective component network performance drops forward passes without backward passes. multiscale cnns also important performance drops without them. addition compared bidirectional simplernn gate structure layers necessary dealing long-range dependencies widely existing amino acid sequences. stacked bgru layers also beneﬁcial enhancing global information circulation compared single bgru layer. furthermore directly feeding local contexts fully connected layers addition global contexts also essential good performance especially prediction low-frequency secondary structure categories. last least feature embedding multi-task learning bagging applied improve accuracy robustness method. without feature embedding without multiscale cnns without backward passes replacing simplernn single bgru layer without integrated local&global contexts without multi-task learning conclusions apply recent deep neural networks protein secondary structure prediction proposed end-to-end model multiscale cnns stacked bidirectional grus extracting local global contexts. integrated local global contexts previous state protein secondary structure prediction improved. taking interactions different protein properties consideration multi-task joint feature learning exploited reﬁne performance. success proposed deep neural network secondary structure prediction models combining local global contexts potentially applied challenging structure prediction tasks protein computational biology. existing bgrus unable deal extremely long dependencies especially frequency long dependencies. powerful architectures implicit attention mechanism neural turing machines suitable solving problem improving prediction performance. acknowledgments wish thank sheng wang jian zhou discussion consultation datasets anonymous reviewers valuable comments. ﬁrst author supported shau postgraduate fellowship university hong kong. references stephen altschul thomas madden alejandro sch¨affer jinghui zhang zheng zhang webb miller david lipman. gapped blast psiblast generation protein database search programs. nucleic acids research fr´ed´eric bastien pascal lamblin razvan pascanu james bergstra goodfellow arnaud bergeron nicolas bouchard yoshua bengio. theano features speed improvements. james bergstra olivier breuleux fr´ed´eric bastien pascal lamblin razvan pascanu guillaume desjardins joseph turian david warde-farley yoshua bengio. theano math expression compiler. merri¨enboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder–decoder statistical machine translation. page kyunghyun bart merri¨enboer dzmitry bahdanau yoshua bengio. properties neural machine translation encoder–decoder approaches. syntax semantics structure statistical translation page james cuff geoffrey barton. evaluation improvement multiple sequence methods protein secondary structure prediction. proteins structure function bioinformatics eshel faraggi zhang yuedong yang lukasz kurgan yaoqi zhou. spine improving protein secondary structure prediction multistep learning coupled prediction solvent accessible surface area backbone torsion angles. journal computational chemistry wolfgang kabsch christian sander. dictionary protein secondary structure pattern recognition hydrogen-bonded geometrical features. biopolymers kryshtafovych bohdan alessandro barbato krzysztof monastyrskyy torsten schwede anna tramontano. assessment assessment evaluation model quality estimates casp. proteins structure function bioinformatics christophe magnan pierre baldi. sspro/accpro almost perfect prediction protein secondary structure relative solvent accessibility using proﬁles machine learning structural similarity. bioinformatics gr´egoire mesnil yann dauphin kaisheng yoshua bengio deng dilek hakkanitur xiaodong larry heck gokhan dong using recurrent neural networks slot ﬁlling spoken language understanding. audio speech language processing ieee/acm transactions john moult krzysztof fidelis andriy kryshtafovych torsten schwede anna tramontano. critical assessment methods protein structure prediction -round proteins structure function bioinformatics vinod nair geoffrey hinton. rectiﬁed linear units improve restricted boltzmann maproceedings international conchines. ference machine learning pages gianluca pollastri darisz przybylski burkhard rost pierre baldi. improving prediction protein secondary structure three eight classes using recurrent neural networks proﬁles. proteins structure function bioinformatics shaoqing kaiming ross girshick jian sun. faster r-cnn towards real-time object detection region proposal networks. advances neural information processing systems pages babaei sepideh geranmayeh amir seyyedsalehi seyyed ali. protein secondary structure prediction using modular reciprocal bidirectional recurrent neural networks. computer methods programs biomedicine richard simpson francis morgan. complete amino acid sequence embden goose egg-white lysozyme. biochimica biophysica acta -protein structure molecular enzymology nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research sujun zhirong. novel method protein secondary structure prediction high segment overlap measure support vecjournal molecular biology machine approach. ashraf yaseen yaohang template-based c-scorpion protein -state secondary structure prediction method using structural information context-based features. bioinformatics wen-tau kristina toutanova john platt christopher meek. learning discriminative projections text similarity measures. proceedings fifteenth conference computational natural language learning pages association computational linguistics xiang zhang junbo zhao yann lecun. character-level convolutional networks text classiﬁcation. advances neural information processing systems pages jian zhou olga troyanskaya. deep supervised convolutional generative stochastic network protein secondary structure prediction. proceedings international conference machine learning pages", "year": 2016}