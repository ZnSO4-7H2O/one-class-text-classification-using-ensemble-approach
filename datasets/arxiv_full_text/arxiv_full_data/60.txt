{"title": "Recurrent Neural Networks with External Memory for Language  Understanding", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "abstract": "Recurrent Neural Networks (RNNs) have become increasingly popular for the task of language understanding. In this task, a semantic tagger is deployed to associate a semantic label to each word in an input sequence. The success of RNN may be attributed to its ability to memorize long-term dependence that relates the current-time semantic label prediction to the observations many time instances away. However, the memory capacity of simple RNNs is limited because of the gradient vanishing and exploding problem. We propose to use an external memory to improve memorization capability of RNNs. We conducted experiments on the ATIS dataset, and observed that the proposed model was able to achieve the state-of-the-art results. We compare our proposed model with alternative models and report analysis results that may provide insights for future research.", "text": "uses e.g. elman architecture speciﬁcally long shortterm memory neural networks three gates control ﬂows error signals. recently proposed gated recurrent neural networks considered simpliﬁed lstm fewer gates. along line research developing advanced architectures paper focuses novel neural network architecture. inspired recent work extend simple elman architecture using external memory. external memory stores past hidden layer activities current sentence also past sentences. predict outputs model uses input observation together content retrieved external memory. proposed model performs strongly common language understanding dataset achieves state-of-the-art results. paper organized follows. brieﬂy describe background research sec. section presents details proposed model. experiments section relate research works sec. finally conclusions discussions sec. language understanding system predicts output sequence tags named-entity given input sequence words. often output input sequences aligned. alignments input correspond null single tag. example given table recurrent neural networks become increasingly popular task language understanding. task semantic tagger deployed associate semantic label word input sequence. success attributed ability memorize long-term dependence relates current-time semantic label prediction observations many time instances away. however memory capacity simple rnns limited gradient vanishing exploding problem. propose external memory improve memorization capability rnns. conducted experiments atis dataset observed proposed model able achieve state-of-the-art results. compare proposed model alternative models report analysis results provide insights future research. index terms recurrent neural network language understanding long short-term memory neural turing machine neural network based methods recently demonstrated promising results many natural language processing tasks speciﬁcally recurrent neural networks based methods shown strong performances example language modeling language understanding machine translation tasks. main task language understanding system associate words semantic meanings example sentence please book ticket hong kong seattle system hong kong departurecity trip seattle arrival city. widely used approaches include conditional random ﬁelds support vector machine recently rnns consists input recurrent hidden layer output layer. input layer reads word output layer produces probabilities semantic labels. success rnns attributed fact rnns successfully trained relate current prediction input words several time steps away. however rnns difﬁcult train gradient vanishing exploding problem problem also limits rnns’ memory capacity error signals able back-propagated enough. lines researches address problem. design learning algorithms avoid gradient exploding e.g. using gradient clipping and/or gradient vanishing e.g. using second-order optimization methods alternatively researchers proposed advanced model architectures contrast simple figure rnn-em model. model reads input outputs hidden layer activity depends input model’s memory content retrieved forget update gates. denote erase content vector. external memory. weight function denotes timedelay operator. diamond symbol denotes diagonal matrix multiplication. extend simple section using external memory. figure illustrates proposed model denote rnn-em. simple consists input layer hidden layer output layer. however instead feeding past hidden layer activity directly hidden layer simple input hidden layer content external memory. rnn-em uses weight vector retrieve content external memory next time instance. element weight vector proportional similarity current hidden layer activity content external memory. therefore content irrelevant current hidden layer activity small weights. describe rnn-em details following sections. equations described bias terms omit simplicity descriptions. implemented rnn-em using theano posterior probability computed using rnn. consists input layer hidden layer output layer elman architecture hidden layer activity dependent input also recurrently past hidden layer activity ht−. current hidden layer activity simple related past hidden layer activity nonlinear function non-linearity cause errors back-propagated explode vanish. phenomenon prevents simple learning patterns spanned long time dependence tackle problem long short-term memory neural network proposed introduction memory cells linearly dependent past values. lstm also introduces three gating functions namely input gate forget gate output gate. follow variant lstm external memory read rnn-em external memory rm×n. considered memory slots slot vector elements. similar external memory computers memory capacity rnn-em increased using large model generates vector search content external memory. though many possible ways generate vector choose simple linear function relates hidden layer activity follows erase vector generated notice c-th element forget gate zero read weight erase vector c-th element one. therefore memory cannot forgotten read. order compare proposed model alternative modeling techniques conducted experiments well studied language understanding dataset travel information system training part dataset consists sentences words. sentences words test. number semantic label including common null label. lexicon-only features experiments. input rnn-em window size consisting current input word neighboring words. adadelta method update gradients maximum number training iterations hyper parameters tuning included hidden layer size number memory slots dimension memory slot best performing rnn-em dimensional hidden layer memory slots dimensional memory slot. table lists performance score rnn-em together previous best results alternative models literature. since previous results grnn implementation study. results optimal respective systems. previous best result achieved using lstm. change score lstm result signiﬁcant conﬁdence level. results table show rnn-em signiﬁcantly better previous best result using lstm. results previous sections obtained models using different sizes. section compares neural network models given approximately number parameters listed table adadelta gradient update method models. figure plots rnn-em along line research uses external memory improve memory capacity neural networks. perhaps closest work neural turing machine work focuses tasks require simple inference proved effectiveness copy repeat sorting tasks. requires complex models tasks. proposed model considerably simpler considered extension simple rnn. importantly shown experiments common language understanding dataset promising results using external memory architecture. paper proposed novel neural network architecture rnn-em uses external memory improve memory capacity simple recurrent neural networks. common language understanding task rnn-em achieves state-ofthe-art results performs signiﬁcantly better previous best result using long short-term memory neural networks. conducted experiments analyze convergence memory capacity. experiments provide insights future research directions mechanisms accessing memory contents methods increase memory capacity. training entropy respect iteration numbers. better illustrate convergences converted entropy values logarithms. results show rnn-em converges lower training entropy models. rnn-em also converges faster simple lstm. repeated atis experiments times different random seeds neural network models. evaluated performances convergences. table lists averaged scores together maximum minimum scores. change signiﬁcant conﬁdence level comparing lstm result. results table show rnn-em average signiﬁcantly outperforms lstm. best performance rnn-em also signiﬁcantly better best performing lstm. size external memory proportional number memory slots ﬁxed dimension memory slots varied number slots. table lists test scores. best performing rnn-em notice rnn-em performed better simple score table explained using gate functions eqs. rnn-em absent simple rnns. rnn-em also performed similarly gated score table partly gate functions. memory capacity measured using training entropy. table shows training entropy decreased initially increased showing memory capacity rnn-em improved. however entropy increased increased. suggests memory bastien lamblin pascanu bergstra goodfellow bergeron bouchard bengio theano features speed improvements deep learning unsupervised feature learning nips workshop bergstra breuleux bastien lamblin pascanu desjardins turian warde-farley bengio theano math expression compiler proceedings python scientiﬁc computing conference jun. oral presentation. dahl bates brown fisher hunickesmith pallett rudnicky shriberg expanding scope atis task atis- corpus proceedings workshop human language technology. association computational linguistics merrienboer g¨ulc¸ehre bahdanau bougares schwenk bengio learning phrase representations using encoder-decoder statistical machine translation emnlp mesnil dauphin bengio deng hakkani-tur heck zweig using recurrent neural networks slot ﬁlling spoken language understanding ieee/acm trans. audio speech language processing vol.", "year": 2015}