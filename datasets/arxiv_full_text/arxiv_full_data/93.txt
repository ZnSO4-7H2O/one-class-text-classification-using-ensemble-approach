{"title": "Regularizing RNNs by Stabilizing Activations", "tag": ["cs.NE", "cs.CL", "cs.LG", "stat.ML"], "abstract": "We stabilize the activations of Recurrent Neural Networks (RNNs) by penalizing the squared distance between successive hidden states' norms.  This penalty term is an effective regularizer for RNNs including LSTMs and IRNNs, improving performance on character-level language modeling and phoneme recognition, and outperforming weight noise and dropout.  We achieve competitive performance (18.6\\% PER) on the TIMIT phoneme recognition task for RNNs evaluated without beam search or an RNN transducer.  With this penalty term, IRNN can achieve similar performance to LSTM on language modeling, although adding the penalty term to the LSTM results in superior performance.  Our penalty term also prevents the exponential growth of IRNN's activations outside of their training horizon, allowing them to generalize to much longer sequences.", "text": "david krueger roland memisevic department computer science operations research university montreal montreal canada {david.kruegerumontreal.ca memisevriro.umontreal.ca} stabilize activations recurrent neural networks penalizing squared distance successive hidden states’ norms. penalty term effective regularizer rnns including lstms irnns improving performance character-level language modeling phoneme recognition outperforming weight noise dropout. achieve competitive performance timit phoneme recognition task rnns evaluated without beam search transducer. penalty term irnn achieve similar performance lstm language modeling although adding penalty term lstm results superior performance. penalty term also prevents exponential growth irnn’s activations outside training horizon allowing generalize much longer sequences. overﬁtting machine learning addressed restricting space hypotheses considered. accomplished reducing number parameters using regularizer inductive bias simpler models early stopping. effective regularization achieved incorporating sophisticated prior knowledge. keeping rnn’s hidden activations reasonable path difﬁcult especially across long time-sequences. mind devise regularizer state representation learned temporal models rnns aims encourage stability path taken representation space. speciﬁcally propose following additional cost term recurrent neural networks vector hidden activations time-step hyperparameter controlling amounts regularization. call penalty norm-stabilizer successfully encourages norms hiddens stable unlike temporal coherence penalty jonschkowski brock penalty encourage state representation remain constant norm. absence inputs nonlinearities constant norm would imply orthogonality hiddento-hidden transition matrix simple rnns however case orthogonal transition matrix inputs nonlinearities still change norm hidden state resulting instability. makes targeting hidden activations directly attractive option achieving norm stability. stability becomes especially important seek generalize longer sequences test time seen training hidden state lstm usually product squashing nonlinearities hence bounded. norm memory cell however grow linearly input input modulation forget gates saturated nonetheless memory cells exhibit norm stability past training horizon suggest part makes lstm successful. activation norms simple rnns saturating nonlinearities bounded. relu nonlinearities however activations explode instead saturating. transition matrix eigenvalues absolute value greater part hidden state aligned corresponding eigenvector grow exponentially extent relu inputs fails cancel growth. simple rnns relu clipped relu nonlinearities performed competitively several tasks suggesting learn stable. show however irnns performance rapidly degrade outside training horizon norm-stabilizer prevents activations exploding outside training horizon allowing irnns generalize much longer sequences. additionally show penalty results improved validation performance irnns. somewhat surprisingly also improves performance lstms tanh-rnns. best knowledge proposal entirely novel. pascanu proposed vanishing gradient regularization encourages hidden transition preserve norm direction cost derivative. like norm-stabilizer cost depends path taken representation space norm stabilzer prioritize cost-relevant directions accounts effects inputs well. hard constraint activations lstm memory cells previously proposed hannun clipped relu also effect limiting activations. techniques operate element-wise however whereas target activations’ norms. several works used penalties difference hidden states rather norms regularizers rnns target norm stability include weight noise dropout show norm-stabilizer improves performance character-level language modeling penntreebank lstm irnns tanh-rnns. present results found values could slightly improve performance also resulted much longer training time task. scheduling increase throughout training might allow faster training. unless otherwise speciﬁed units lstm/srnn learning rate=. momentum=. gradient clipping=. train maximum epochs sequences length taken without overlap. encounter cost function divide learning rate restart previous epoch’s parameters. lstms either apply norm-stabilizer penalty memory cells hidden state although greff found output tanh essential good performance removing gave slight improvement task. compare tanh relu grid search across cost weight gradient clipping learning rate. simple rnns found zero-bias relu threshold gave best performance. best performance relu activation functions obtained penalty applied. tanh-rnns best performance obtained without regularization. results better penalty without experiment settings. compare alternatives norm-stabilizer cost penntreebank irnns without biases using setup include relative error norm absolute difference penalties don’t target successive time-steps. following penalties performed poorly included table |∆ht proposal penalizing successive states’ norms gives best performance alternatives seem promising deserve investigation. particular relative error could appropriate; unlike norm-stabilizer cost cannot reduced simply dividing hidden states constant. value chosen target norms based value found proposed cost; practice would another hyperparameter tune. success regularizers encourage norm stability indicates inductive bias favor stable norms useful. table phoneme error rate timit different experiment settings average experiments. norm-stabilized networks achieve best performance. regularization parameters norm stabilizer dropout probability standard deviation additive gaussian weight noise. show norm-stabilizer improves phoneme recognition timit dataset outperforming networks regularized weight noise and/or dropout. experiments similar setup previous state task bidirectional lstms layers hidden units train adam using learning rate=. gradient clipping=. unlike graves beam search transducer. early stop epochs without improvement development set. apply norm-stabilization hidden activations standard deviation weight noise dropout. pair-wise combinations regularization techniques. experiments settings report average phoneme error rate combining weight noise norm-stabilization gave poor performance networks failing train results omitted. adding dropout minor effect results. norm-stabilized networks best performance inspired results decided train larger networks regularization observed performance improvements also used higher patience early stopping criterion here terminating epochs without improvement. unlike previous experiments experiment settings. network hidden units gave best performance development dev/test .%/.%. competitive state results task graves evaluate without beam search transducer. although t´oth achieved .%/.% using convolutional neural networks. network hidden units achieved dev/test .%/.%. figure average timit core test different combinations regularizers. norm-stabilizer shows clear positive effect performance. weight noise also improves performance less combining weight noise norm-stabilization gives poor results. table phoneme error rate timit experiments hidden units normstabilizer regularization networks regularized weight noise network units achieved best time-steps training; inputs time-steps carry information. element input sequence consists pair chosen uniform random indicates numbers add. sequences length none models able reduce cost short-sighted baseline predicting ﬁrst indicated numbers sequence length. able solve task successfully. uniform initialization learning rate=. gradient clipping=. compare across nine random seeds without norm-stabilizer norm-stabilized networks reduced test cost cases averaging mse. unregularized networks averaged outperformed short-sighted baseline cases also failing improve constant predictor cases. comparing identical srnns trained without norm-stabilizer penalty found lstms rnns tanh activation functions continued perform well beyond training horizon. although activations lstm’s memory cells could potentially grow linearly experiments stable. applying norm-stabilizer signiﬁcantly decrease average norm variability norm however irnns hand suffered exploding activations resulting poor performance norm-stabilizer effectively controls norms maintains high level performance; ﬁgure norm-stabilized irnns’ performance norms stable longest horizon evaluated figure norm lstm memory cells hidden states different values across time-steps non-zero values dramatically reduce mean variance norms. lstm memory cells potential grow linearly instead exhibit natural stability. insight norm-stabilizer outperforms alternative costs examined hidden norms networks trained values ranging dataset length- sequences taken wikipedia penalize difference initial ﬁnal norms difference norms ﬁxed value increasing cost change shape norms; still begin explode within training horizon norm-stabilizer however increasing penalty signiﬁcantly delayed activation explosions dataset. also noticed distribution activations concentrated fewer hidden units applying norm-stabilization penntreebank. similarly found forget gates lstm networks peaked distribution average across dimensions lower finally found eigenvalues regularized irnn’s hidden transition matrices larger number large eigenvalues unregularized irnn much larger number eigenvalues closer absolute value supports hypothesis orthogonal transitions inherently desirable rnn. explicitly encouraging stability norm-stabilizer seems favor solutions maintain stability selection active units rather restricting choice transition matrix. figure average logarithm hidden norms function time-step. bottom average cost function time-step. solid blue dashed notice irnn’s activations explode exponentially within training horizon causing cost quickly inﬁnity outside training horizon figure hidden norms function time-step values norm-stabilizer penalty initial ﬁnal norms norm-stabilizer delays explosion activations changing shape curve extending region. introduced norm-based regularization rnns prevent exploding vanishing activations. compare range novel methods encouraging enforcing norm stability. best performance achieved penalizing squared difference subsequent hidden states’ norms. penalty norm-stabilizer improved performance tasks language modeling addition tasks gave state performance phoneme recognition timit dataset. exploring relationship stability generative modeling rnns applying norm-regularized irnns challenging tasks applying similar regularization techniques feedforward nets research developed funding defense advanced research projects agency force research laborotory views opinions and/or ﬁndings expressed authors interpreted representing ofﬁcial views policies department defense u.s. government. appreciate many gpus provided computecanada. authors would like thank developers theano blocks special thanks alex lamb amar shah asja fischer caglar gulcehre cesar laurent dmitriy serdyuk dzmitry bahdanau faruk ahmed harm vries jose sotelo marcin moczulski martin arjovsky mohammad pezeshki philemon brakel saizhen zhang useful discussions and/or sharing code. bastien fr´ed´eric lamblin pascal pascanu razvan bergstra james goodfellow bergeron arnaud bouchard nicolas warde-farley david bengio yoshua. theano features speed improvements. corr abs/. http//arxiv.org/abs/. graves mohamed a.-r. hinton speech recognition deep recurrent neural networks. acoustics speech signal processing ieee international conference ./icassp... graves alex fern´andez santiago gomez faustino schmidhuber j¨urgen. connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural networks. proceedings international conference machine learning icml york acm. isbn ---. ./.. http//doi.acm.org/./.. hannun case casper catanzaro diamos elsen prenger satheesh sengupta coates deep speech scaling end-to-end speech recognition. arxiv e-prints december kam-chuen giles horne bill analysis noise recurrent neural networks convergence generalization. ieee transactions neural networks ./.. http//dx.doi.org/./.. pham kermorvant christopher louradour j´erˆome. dropout improves recurrent neural networks handwriting recognition. corr abs/. http//arxiv. org/abs/.. hasim senior andrew kanishka irsoy ozan graves alex beaufays franc¸oise schalkwyk johan. learning acoustic frame labeling speech recognition recurrent neural networks. acoustics speech signal processing ieee international conference ieee t´oth l´aszl´o. combining timefrequency-domain convolution convolutional neural networkieee international conference acoustics speech signal based phone recognition. processing icassp florence italy ieee ./icassp... http//dx.doi.org/./icassp.. merri¨enboer bart bahdanau dzmitry dumoulin vincent serdyuk dmitriy warde-farley david chorowski bengio yoshua. blocks fuel frameworks deep learning. corr abs/. http//arxiv.org/abs/.. tsung-hsien gasic milica mrksic nikola pei-hao vandyke david young steve semantically conditioned lstm-based natural language generation spoken dialogue systems. corr abs/. http//arxiv.org/abs/..", "year": 2015}