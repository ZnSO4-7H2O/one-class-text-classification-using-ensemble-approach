{"title": "Recursive Neural Networks Can Learn Logical Semantics", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "Tree-structured recursive neural networks (TreeRNNs) for sentence meaning have been successful for many applications, but it remains an open question whether the fixed-length representations that they learn can support tasks as demanding as logical deduction. We pursue this question by evaluating whether two such models---plain TreeRNNs and tree-structured neural tensor networks (TreeRNTNs)---can correctly learn to identify logical relationships such as entailment and contradiction using these representations. In our first set of experiments, we generate artificial data from a logical grammar and use it to evaluate the models' ability to learn to handle basic relational reasoning, recursive structures, and quantification. We then evaluate the models on the more natural SICK challenge data. Both models perform competitively on the SICK data and generalize well in all three experiments on simulated data, suggesting that they can learn suitable representations for logical inference in natural language.", "text": "tree-structured recursive neural networks sentence meaning successful many applications remains open question whether ﬁxed-length representations learn support tasks demanding logical deduction. pursue question evaluating whether models— plain treernns tree-structured neural tensor networks —can correctly learn identify logical relationships entailment contradiction using representations. ﬁrst experiments generate artiﬁcial data logical grammar evaluate models’ ability learn handle basic relational reasoning recursive structures quantiﬁcation. evaluate models natural sick challenge data. models perform competitively sick data generalize well three experiments simulated data suggesting learn suitable representations logical inference natural language. tree-structured recursive neural network models sentence meaning successful array sophisticated language tasks including sentiment analysis image description paraphrase detection results encouraging ability models learn produce strong semantic representations sentences. however remains open question whether fully learned model achieve kind high-ﬁdelity distributed representations proposed recent algebraic work vector space modeling whether model match performance grammars based logical forms ability model core semantic phenomena like quantiﬁcation entailment contradiction recent work algebraic approach coecke yielded rich frameworks computing meanings fragments natural language compositionally vector tensor representations yielded effective methods learning representations data typical machine learning settings. past experimental work reasoning distributed representations largely conﬁned short phrases however robust natural language understanding essential model phenomena full generality complex linguistic structures. paper describes four machine learning experiments directly evaluate abilities models learn representations support speciﬁc semantic behaviors. tasks follow format natural language inference goal determine core inferential relationship sentences. introduce novel architecture natural language inference independently computes vector representations sentences using standard treernn treerntn models produces judgment pair using representations. allows gauge abilities models represent necessary semantic information sentence vectors. much theoretical work natural language inference involves natural logics formal systems deﬁne rules inference natural language words phrases sentences without need intermediate representations artiﬁcial logical language. ﬁrst three experiments test models’ ability learn foundations natural language inference training reproduce behavior natural logic maccartney manning artiﬁcial data. logic deﬁnes seven mutually-exclusive relations synonymy entailment contradiction mutual consistency summarized table provides rules semantic combination projecting relations lexicon complex phrases. formal properties system well-understood ﬁrst experiment using logic covers reasoning bare logical relations second extends reasoning statements constructed compositionally recursive functions third covers additional complexity results quantiﬁcation though performance plain treernn model somewhat poor ﬁrst experiment stronger treerntn model generalizes well every case suggesting learned simulate target logical concepts. experiments simulated data provide convincing demonstration ability neural networks learn build semantic representations complex natural language sentences reasonably-sized training sets. however also interested practical question whether learn representations naturalistic text. address question apply models sick entailment challenge data small size corpus puts datahungry models like disadvantage nonetheless able achieve competitive performance surpassing several submitted models signiﬁcant hand-engineered taskspeciﬁc features baseline. suggests representational abilities observe previous sections limited carefully circumscribed tasks. conclude treerntn models adequate typical cases figure separate treestructured networks build vector representations sentences using either layer functions. comparison layer uses resulting vectors produce features classiﬁer. limit scope experiments paper neural network models adhere linguistic principle compositionality says meanings complex expressions derived meanings parts speciﬁc composition functions distributed setting word meanings embedding vectors dimension learned composition function maps pairs single phrase vectors dimension merged represent complex phrases forming tree structure. entire sentencelevel representation derived tree serves ﬁxed-dimensional input subsequent layer function. apply recursive models task propose tree pair model architecture depicted fig. phrases compared processed separately using pair tree-structured networks share single parameters. resulting vectors separate comparison layer meant generate feature vector capturing relation phrases. output layer given softmax classiﬁer produces distribution seven relations represented table table seven relations maccartney manning logic deﬁned abstractly pairs sets drawing universe straightforwardly applied pair natural language words phrases sentences. relations deﬁned mutually exclusive. here column vector representations left right children node node’s output. treernn concatenates them multiplies matrix learned weights adds bias treerntn adds learned full rank third-order tensor dimension modeling multiplicative interactions child vectors. comparison layer uses layer function composition layers independently learned parameters separate nonlinearity function. rather tanh nonlinearity here found better results leaky rectiﬁed linear function min. strong tree-structured models proposed past work believe provide valuable case study positive results likely generalize well stronger models. model forward assemble tree-structured networks match structures provided phrase either included source data given parser. word vectors looked vocabulary embedding matrix composition comparison functions used pass information tree classiﬁer. objective table assess models’ ability learn inference pairs relations using rules represented here derived deﬁnitions relations table entry ample given column lets conclude cells containing correspond situations valid inference drawn. initialize parameters uniformly using range layer parameters embeddings train model using stochastic gradient descent learning rates computed using adadelta classiﬁer feature vector ﬁxed dimensions dimensions recursive layers tuned manually. training times cpus vary hours days across experiments. experiments artiﬁcial data report mean results fold crossvalidation variance across runs typically percentage points. addition classes necessarily balanced report accuracy macroaveraged source code generated data released review period. simplest kinds deduction natural logic involve atomic statements using relations table instance relation propositions infer relation applying deﬁnitions relations directly. also given relation conclude basic set-theoretic reasoning full sound inferences pairs premise relations depicted table though basic inferences involve compositional sentence representations successful reasoning using compositional representations rely ability perform sound inferences kind ﬁrst experiment studies well model learn perform isolation. experiments begin creating world model base statements train test sets. takes form small boolean structure terms denote sets entities small domain. fig. depicts structure form three entities eight proposition terms generate relational statement pair terms model shown fig. divide statements evenly train test sets delete test examples canproven train examples enough information even ideal system choose correct label. experimental create model terms domain elements yielding training examples test examples. trained models comparison functions data sets. cases models implemented described since items compared single terms rather full tree structures composition layer used models recursive. simply present models embedding vectors terms ensuring model information terms compared except relations appear training. table performance semantic relation experiments. results results artiﬁcial data reported mean accuracy scores runs followed mean macroaveraged scores parentheses. only entries reﬂect frequency frequent class. terms geometric relations vectors able information recover relations overtly included training data. also generalizes fairly well makes enough errors remains open question whether capable learning representations properties. possible rule possibility different optimization techniques hyperparameter tuning could lead model succeed here. successful natural language inference system must reason relations familiar atomic symbols also novel structures built recursively symbols. section shows models learn compositional semantics structures. evaluations exploit fact logical language inﬁnite testing strings longer complex seen training. experiments generate artiﬁcial data formal system replace unanalyzed symbols experiment complex formulae. formulae represent complete classical propositional logic atomic symbol variable domain operators truth-functional ones. table deﬁnes logic table gives short examples relational statements data. compute relations statements exhaustively enumerate sets assignments truth values propositional variables would satisfy statements convert set-theoretic relation assignments seven relations table result relational statement represents valid theorem propositional logic succeed models must learn reproduce behavior theorem prover. experiments randomly generate unique pairs formulae containing instances logical operators compute relation holds pair. discard pairs either statement either tautology contradiction seven relations table undeﬁned. resulting formula pairs partitioned bins according number operators larger formulae. sample heldtest set. implement constraint socher show matrix-vector treernn model somewhat similar treerntn learn boolean logic logic atomic symbols simply values learning operators logic trivial outputs operator represented accurately single bit. much demanding task presented here atomic symbols variables values sentence vectors must thus able distinguish distinct conditions valuations. statements compared similar generated data dominated statements formulae refer largely separate subsets variables means relation almost always correct. effort balance distribution relation labels without departing basic task modeling propositional logic disallow individual pairs statements referring four propositional variables. order test model’s generalization unseen structures discard training examples logical operators yielding short training examples test examples across bins. addition tree models also train summing baseline largely identical treernn except instead using learned composition function simply sums term vectors expression compose passing comparison layer. unlike tree models baseline word order guaranteed ignore information would need order succeed perfectly. results fig. shows relationship test accuracy statement size. summing baseline model performed poorly across board found recursive models able perform well unseen small test examples treernn accuracy treerntn accuracy formulae bekey test whether models capture complexity study degree able develop suitable representations semantics natural language quantiﬁers like interact negation lexical entailments. quantiﬁcation negation place natural language complex functional meanings found natural focus since formed standard case study prior formal work natural language inference experiments data consist pairs sentences generated grammar simple english-like artiﬁcial language. sentence contains quantiﬁer noun negated intransitive verb negated. quantiﬁers some most three negations not-all not-most less-than-two less-than-three also include nouns four intransitive verbs negation symbol not. order able deﬁne relations sentences differing lexical items deﬁne lexical relations noun–noun pair verb–verb pair quantiﬁer–quantiﬁer pair. grammar generates pairs sentences calculates relations them. instance models might pairs like training required label randomly partition valid single sentences grammar training test label pairs within generate training pairs test pairs. model doesn’t test sentences training time cannot directly kind reasoning described must instead infer word-level relations learn complete reasoning system logic. summing baseline highly consistent sentence structure experiment means model disadvantaged lack word order information previous experiment variable placement nonetheless introduces potential uncertainty examples contain sentence single token size four training cutoff performance gradually decays expression size tree models suggesting learned approximations accurate lossy. despite treerntn’s stronger performance short sentences performance decayed quickly treernn’s. suggests learned interpret many speciﬁc ﬁxed-size tree structures directly allowing away without learning robust generalizations compose terms general case. factors contributed learning narrower generalizations even lower dimension treerntn composition function eight times many parameters treernn treerntn worked best weaker regularization treernn however even complex test examples treerntn classiﬁes true examples every class correctly majority time performance models examples indicates learned reasonable approximations underlying theorem proving task recursive structure. seen recursive models learn approximation propositional logic. however natural languages express functional meanings considerably greater complexity this. results results show tree models able learn generalize underlying logic almost perfectly. baseline summing model largely memorize training data generalize well. consistent pattern handful errors made either tree model errors consistent across model restarts suggesting fundamental obstacle learning perfect model problem. speciﬁc model architecture novel though underlying tree structure approach validated elsewhere experiments guarantee viable model handling inference real natural language data. investigate models’ ability handle noisy labels diverse range linguistic structures seen typical natural language data sick textual entailment challenge corpus corpus consists natural language sentence pairs labeled entailment contradiction neutral. thousand distinct sentences corpus large enough train high quality learned model general natural language largest human-labeled entailment corpus aware results nonetheless show tree-structured models learn inference real world. adapting task requires make additions techniques discussed order better handle rare words initialized word embeddings using dimensional vectors trained glove data wikipedia. since dimensional vectors large practical treerntn small dataset embedding transformation layer needed. embedding used input recursive layer passed additional tanh neural network layer output dimension recursive layer. layer aggregates usable information embedding vectors compact working representation. identical layer added sumnn word vectors comparison layer. also supplemented sick training data examples entailment data denotation graph project corpus noisy automatically labeled entailment examples image captions genre text sick drawn. trained single model data sources used separate softmax parameters classifying labels source. parsed data sources stanford pcfg parser also found able train working model much quickly additional technique collapse subtrees identical across sentences pair replacing single head word. training test data report performance collapsed collapsed uncollapsed copies training data used training. finally order improve regularization noisier data used dropout input comparison layer output embedding transform layer tuned model using performance held development report performance version model trained training development data tested example sick test set. also report training accuracy small sample data source. patient helped doctor contradiction little girl playing violin beach yellow drinking water bottle contradiction woman breaking eggs bowl dough spread results despite small amount high quality training data available lack resources learning lexical relationships results show tree-structured models perform competitively textual entailment beating strong baseline. neither model reached performance winning system treerntn exceed eight submitted systems including several used sophisticated hand-engineered features lexical resources speciﬁc version entailment task hand. better understand results manually annotated fraction sick test using mutually exclusive categories passive/active alternation pairs pairs differing presence negation pairs differing single word phrase substitution pairs differing multiple edits pairs little content word overlap examples table annotated random examples judge frequency category continued selectively annotating category contained least also category short pairs neither sentence contains words. results show treerntn performs especially strongly categories pick speciﬁc syntactic conﬁgurations passive suggesting model learned encode relevant structures well. also performs fairly subst closely parallels lexical entailment inferences addressed addition none models perform dramatically better short pairs rest data suggesting performance decay observed impact models trained typical natural language text. known model perform well sick without taking advantage compositional syntactic semantic structure summing baseline model powerful enough this. tree models nonethepaper ﬁrst evaluates recursive models three natural language inference tasks clean artiﬁcial data covering core relational algebra natural logic entailment exclusion recursive structure quantiﬁcation. show models learn perform entailment task natural language. results suggest treerntns potentially also treernns learn faithfully reproduce logical inference behaviors reasonably-sized training sets. positive results promising future learned representation models applied modeling compositional semantics. questions abilities models remain open. even treerntn falls short perfection recursion experiment performance falling steadily size expressions grows. remains seen whether deﬁciencies limiting practice whether overcome stronger models learning techniques. addition interesting analytical questions remain models encode underlying logics. neither underlying logical theories straightforward parameter inspection technique provides much insight point hope experiments reveal structure learned parameters representations produce. sick experiments similarly begin reveal potential models learn perform complex semantic inferences corpora ample room develop understanding using larger sources natural language data. nonetheless rapid progress ﬁeld made models recent years provides ample reason optimistic learned representation models trained meet challenges natural language semantics. dagan glickman magnini. pascal recognising textual entailment challenge. machine learning challenges. evaluating predictive uncertainty visual object classiﬁcation recognising tectual entailment. springer. learning taskdependent distributed representations backpropproc. ieee internaagation structure. tional conference neural networks volume ieee. t.f. icard l.s. moss. complete calculus monotone antitone higher-order functions. galatos kurz tsinakis editors proc. topology algebra categories logic. marelli bentivogli baroni bernardi menini zamparelli. semeval task evaluation compositional distributional semantic models full sentences semantic relatedness textual entailment. semeval. d.h.d. warren f.c.n. pereira. efﬁcient easily adaptable system interpreting natural language queries. american journal computational linguistics. watanabe mizuno nichols okazaki inui. latent discriminative model compositional entailment relation recognition using natural logic. proc. coling. l.s. zettlemoyer collins. learning sentences logical form structured classiﬁcation probabilistic categorial grammars. proc. conference uncertainty artiﬁcial intelligence.", "year": 2014}