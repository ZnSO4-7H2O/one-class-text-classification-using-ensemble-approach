{"title": "Graph Approximation and Clustering on a Budget", "tag": ["stat.ML", "cs.AI", "cs.CV", "cs.LG"], "abstract": "We consider the problem of learning from a similarity matrix (such as spectral clustering and lowd imensional embedding), when computing pairwise similarities are costly, and only a limited number of entries can be observed. We provide a theoretical analysis using standard notions of graph approximation, significantly generalizing previous results (which focused on spectral clustering with two clusters). We also propose a new algorithmic approach based on adaptive sampling, which experimentally matches or improves on previous methods, while being considerably more general and computationally cheaper.", "text": "consider problem learning similarity matrix computing pairwise similarities costly limited number entries observed. provide theoretical analysis using standard notions graph approximation signiﬁcantly generalizing previous results also propose algorithmic approach based adaptive sampling experimentally matches improves previous methods considerably general computationally cheaper. many unsupervised learning algorithms spectral clustering low-dimensional embedding laplacian eigenmaps diﬀusion maps need input matrix pairwise similarities diﬀerent objects data. cases obtaining full matrix costly matter. example based expensive-to-compute metric based physical measurement given human annotator. cases would like good approximation matrix querying limited number entries. alternative equivalent viewpoint problem approximating unknown weighted undirected graph querying limited number edges. question received previous attention works focus task spectral clustering clusters assuming distinct clusters indeed exist work consider theoretically algorithmically question query-based graph approximation generally obtaining results relevant beyond clusters beyond spectral clustering. considering graph approximations ﬁrst question notion approximation consider. important notion approximation wish every approximated graph weight close weight original graph multiplicative factor. many machine learning algorithms based clustering energy minimization etc. based cuts notion approximation natural uses. stronger notion spectral approximation wish uniformly approximate quadratic form deﬁned laplacian multiplicative factor. approximation important algorithms theoretical analysis focuses number queries needed approximations. ﬁrst consider simple intuitive strategy sampling edges uniformly random obtain results spectral approximations various assumptions. note results considerably general theoretical analysis focuses behavior eigenvector laplacian matrix crucially rely large eigengap. consider extend results adaptive sampling strategies design generic framework well adaptive sampling algorithm clustering compared previous approaches algorithm much simpler avoids costly full eigen-decomposition iteration experimentally appears obtain equal even better performance range datasets. theoretical results build techniques graph sparsiﬁcation task sparse approximation given graph somewhat similar task important diﬀerences first foremost access full graph whereas graph sparsiﬁcation graph given full knowledge used algorithms task second goal minimize number edge sampled number edges resulting graph notice wish sparse graph always graph sparsiﬁcation technique resulting graph graph guarantees sparsity. deﬁnition weighted graph subset vertices deﬁned |∂gs| weights edges exactly endpoint deﬁnition graphs vertices. \u0001-cut approximation |∂gs| ˜gs| |∂gs| deﬁnition graph laplacian deﬁned wij. normalized graph laplacian deﬁned easily seen value deﬁned many spectral graph techniques spectral clustering seen relaxation discrete problem deﬁnition graph \u0001−spectral approximation multiplicative error additive error term. particular implies approximation eigenvectors relevant many spectral algorithms includes approximation eigenvector special case moreover implies approximation fact strictly stronger simple example approximation spectral approximation). therefore focus spectral approximation theoretical results. initial approximation strategy uniformly random sample subset edges i.e. based adaptation work author considered picking edge independently. diﬀers setting interested picking edges without replacement since case probabilities picking diﬀerent edges longer independent. seems like serious complication ﬁxed using notion negative dependence intuitively group random variables negatively dependent high value others probable lower values. pick edges uniformly edge picked lowers chances edges picked intuitively probabilities negatively dependent. edge picking probabilities shown indeed negatively dependent important application negative dependence chernoﬀ-hoeﬀding bounds hold sums independent random variables also hold negatively dependent variables. supplementary material details. theorem graph weights approximation sampling edges uniformly. deﬁne second smallest eigenvalue max{log probability \u0001-spectral approximation smaller proof sketch. proof based adaptation part theorem main diﬀerences negative dependence instead independence weighted graph instead unweighted graph. proof uses following lemma relatively small eigenvalues ideal case even unfortunately show unavoidable bound essentially optimal graphs bounded i.e. expanders. next section show reasonable assumptions allows recover non-trivial guarantees even regime small eigenvalues. since spectral approximation implies approximation simple bounds number edges needed approximations. show necessary condition approximation minimal small intuition even ﬁnding single edge original graphs minimal cannot small samples needed. comparing theorem that graphs lower clustering algorithms assume certain structure graph generally assume strongly connected components i.e. clusters weak connections scenario approximation normally means small minimal show approximation used obtain useful results. give results geared towards spectral approximation towards approximation. assumption implies well connected clusters assumption excludes sparse well-connected graphs already shown earlier hard approximate. assumption essentially requires between-cluster connections relatively weaker within-cluster connections. proof sketch. i.e. spanned eigenvectors full theorem would true sin-theta theorem using assumptions. need show theorem used sin-theta theorem states ˜lout|| ˜lout|| noise factor unnormalized second eigenvalue ˜lin signal factor. using theorem ﬁrst assumptions approximate show need show ˜lout|| done using matrix chernoﬀ inequality applying result shows adapted sampling without replacements. note result limited sampling without replacements negative dependence obvious extension random matrices. details supplementary material proof sketch. approximation clusters ˜cin cin/. using chernoﬀ bound union bound cuts clusters none greater cin/. supplementary material full proof. theorem states that uniform sampling prior assumptions graph structure need least weight smallest cut. adaptive algorithm instead uniform sampling? easy graphs lower bound holds. think graph vertices consisting cliques randomly chosen edges connecting them. let’s assume oracle told vertex clique sensible algorithm consider similar problem graph known consist connected components clique size wish clusters. uniform sampling algorithm connected components return them. many edges need sample connected components? look clique basic results random graph theory show high probability number edges added connected graph lower bounds number samples needed. improve adaptive algorithm following scheme iteration pick edge random connecting smallest connected component connected component. step least probability connect connected components. nodes wrong cluster least right cluster therefore high probability argument leads consider adaptive sampling schemes iteratively sample edges according non-uniform distribution. intuitively distribution place weight edges helpful approximating structure original graph. ﬁrst discuss incorporate arbitrary non-uniform distributions framework. propose speciﬁc non-uniform distribution motivated example above leading algorithm setting context clustering. approach incorporate non-uniform distributions unbiased sampling re-scale weights according sampling probability. means weights unbiased estimates actual weights. unfortunately re-scaling easy compute general sampling without replacement probability sampling edge marginal distribution algorithm’s possible trajectories. sampling replacement much easier since depends sampling probability current iteration. moreover long sample small part edges risk re-sampling already-sampled edge negligible. finally show whatever non-uniform distribution simple modiﬁcation suﬃces approximation hold. unfortunately found approach work poorly practice unstable oscillated good clustering long good clustering initially found. issues consider biased sampling without replacement approach nonuniform distribution uniform distribution unseen edges attempt re-scale weights. speciﬁcally consider adaptive sampling algorithm picks unseen edge step probability depends graph seen far. consider modiﬁed distribution probability picks unseen edge uniformly probability picks according biased sampling ruin approximation guarantees clustering scenarios show similar results theorem adaptive sampling scheme. theoretical guarantees adaptive sampling supplementary material. turn consider speciﬁc algorithmic instantiation context clustering. motivated example earlier consider non-uniform distribution iteratively attempts connect clusters currently-observed graph picking edges them. clusters determined clustering algorithm wish approximated graph incrementally updated iteration. inspired common practice over-segmentation clusters desired number clusters moreover discussed earlier distribution uniform distribution. resulting algorithm denote clusk appears algorithm below. setting budget-constrained clustering relevant algorithms aware algorithm algorithm algorithms somewhat similar approach interleave uniform sampling non-uniform sampling scheme. however sampling scheme diﬀerent ours focuses ﬁnding edge derivative laplacian eigenvector sensitive. drawbacks. first speciﬁcally designed spectral clustering case clusters based laplacian eigenvector. extending clusters requires either recursive partitioning another possible option pick several edges distribution step makes process parallelizable. tested clusk algorithm several datasets compared discussed earlier shown inferior). important note designed speciﬁcally spectral clustering using unnormalized laplacian badly cases performed surprisingly well clustering measured cluster purity. purity single cluster percent frequent class cluster. purity clustering weighted average single cluster purity weighted number elements cluster. purity shown averaged runs. synthetic experiments performed datasets half circles dataset dataset comprising four well separated gaussians experiments used unnormalized spectral clustering using gaussian weight matrix. half circles classic clustering dataset tested three datasets iris glass datasets using gaussian weight matrix caltech- dataset subset caltech- images datasets clusters gathered using similarity matrix suggested tested dataset using normalized unnormalized laplacian clustering. results presented ﬁgure overall experiments show clusk algorithm performs good better previous algorithms budget-constrained clustering signiﬁcantly computationally cheaper well general.", "year": 2014}