{"title": "Convolution by Evolution: Differentiable Pattern Producing Networks", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "In this work we introduce a differentiable version of the Compositional Pattern Producing Network, called the DPPN. Unlike a standard CPPN, the topology of a DPPN is evolved but the weights are learned. A Lamarckian algorithm, that combines evolution and learning, produces DPPNs to reconstruct an image. Our main result is that DPPNs can be evolved/trained to compress the weights of a denoising autoencoder from 157684 to roughly 200 parameters, while achieving a reconstruction accuracy comparable to a fully connected network with more than two orders of magnitude more parameters. The regularization ability of the DPPN allows it to rediscover (approximate) convolutional network architectures embedded within a fully connected architecture. Such convolutional architectures are the current state of the art for many computer vision applications, so it is satisfying that DPPNs are capable of discovering this structure rather than having to build it in by design. DPPNs exhibit better generalization when tested on the Omniglot dataset after being trained on MNIST, than directly encoded fully connected autoencoders. DPPNs are therefore a new framework for integrating learning and evolution.", "text": "cppns currently optimized evolving topology weights neat eﬀective dimensional output spaces process becomes ineﬃcient large parameter spaces. however train neural networks millions parameters exploiting gradient-based learning methods purpose work combine success gradient-based learning neural networks capacity optimize topologies provided evolution. call model diﬀerentiable pattern producing network dppn works lamarckian evolution inheritance acquired characteristics. show using dppns rapidly reconstruct images using fewer parameters number pixels dppns used hyperneat-like framework indirect encoding larger neural network. dppns also work darwinian/baldwinian framework learned weights inherited directly initial weights dppn inherited. however lamarckian algorithm consistantly outperforms variants. dppn data eﬃcient cppn. addition improves upon existing machine learning techniques acting strong regularizer encouraging simpler solutions compared obtained directly optimizing weights larger network. example show dppn trained using algorithm produce parameters fully connected denoising autoencoder mnist digit reconstruction generates convolutional architecture embedded within fully connected feedforward network hidden unit contains blob-like receptive ﬁelds hidden nodes. evolution also discovers crop magnify image. example dppn parameters achieved binary cross entropy mnist test set. generalization omniglot character also demonstrated superior equivalent directly encoded network. cppn feedforward network contains sigmoid gaussian functions includes wider transfer functions example periodic functions sine functions. cppns invented stanley abstraction natural development. cppns genotype coordinate phenotype parameter without work introduce diﬀerentiable version compositional pattern producing network called dppn. unlike standard cppn topology dppn evolved weights learned. lamarckian algorithm combines evolution learning produces dppns reconstruct image. main result dppns evolved/trained compress weights denoising autoencoder roughly parameters achieving reconstruction accuracy comparable fully connected network orders magnitude parameters. regularization ability dppn allows rediscover convolutional network architectures embedded within fully connected architecture. convolutional architectures current state many computer vision applications satisfying dppns capable discovering structure rather build design. dppns exhibit better generalization tested omniglot dataset trained mnist directly encoded fully connected autoencoders. dppns therefore framework integrating learning evolution. compositional pattern producing networks major advance evolutionary computation permitted evolution eﬃciently optimize model incrementally starting small number parameters. cppn eﬀective encoding high dimensional output space number parameters assuming structure output space. means evolve cppns represent images; picbreeder crowd internet users evolve images selecting cppns breed. permission make digital hard copies part work personal classroom granted without provided copies made distributed proﬁt commercial advantage copies bear notice full citation ﬁrst page. copyrights components work owned others must honored. abstracting credit permitted. copy otherwise republish post servers redistribute lists requires prior speciﬁc permission and/or fee. request permissions permissionsacm.org. gecco july denver acm. isbn ----//. http//dx.doi.org/./. local interaction phenotypic elements individual component phenotype determined independently every component. cppn eﬀect convolved coordinates generate output. example cppn used produce square image side length would input coordinate comprising datapoint composed coordinates distance center image ﬁxed bias datapoints passed cppn output image generated cppn sequentially pixel pixel. cppn’s topology weights optimized evolutionary algorithm. good example methodology picbreeder crowd internet users evolve images selecting cppns breed. topology weights cppn evolved using mutation crossover starting minimal topology grows nodes edges. neuroevolution augmented topologies algorithm used constrain crossover homologous parts cppn maintain topology diversity. hyperneat cppns used indirect compressed encodings weights larger neural network. inputs cppn coordinates presynaptic postsynaptic neuron output weight joining neurons. single cppn must encode multiple layers deeper neural network possibilities either extra input given signaling layer weights cppn outputting cppn constrained always multiple output nodes speciﬁc node outputting weight assigned layer limitation cppn approach weights evolved rather learned gradient-based methods utilize backpropagation. methods scale better evolutionary methods respect number parameters model. able optimize millions parameters once e.g. convolutional neural networks performing object classiﬁcation imagenet cppns convolutional neural networks previously studied cppns used evolve adversarial examples convolutional network classiﬁers imagenet however work cppn modiﬁed gradient descent. convolutional neural networks made great strides recent years practical performance point critical component many best systems challenging artiﬁcial intelligence tasks architectures historically inspired structure mammalian visual system namely hierarchical structure ventral stream visual processing local tiled nature receptive ﬁelds primary visual cortex engineering success convolutional neural networks relative fully connected neural networks largely strong regularization imposed convolutional structure fewer weights learn networks generalize better less data. architecture places strong prior assumptions data namely translation-invariant applications architecture decided model designer rather automatically driven data. compression neural network weights possible even removing weights ﬁlters trained convolutional neural network possible predict missing weights high accuracy allows compression weights convolutional neural networks order make computationally eﬃcient interest whether appropriate simplifying structures discovered rather designed much like evolution stumbled upon structure mammalian visual system. recent work applied cppns hyperneat framework evolve weights layer lenet- convolutional neural network mnist character recognition classiﬁcation performance hyperneat alone used optimize weights network poor generations correct classiﬁcations. hyperneat used initialize weights lenet prior several epochs gradient descent learning correct classiﬁcations increased however error rates obtained backpropagation alone also reduction number parameters required represent resultant network backpropagation applied full convolutional network cppn itself. previous work exists evolving topology neural networks learning. example bayer evolved topology cells lstm recurrent neural network sequence learning recently jozefowicz explored topology lstms grus improving dppn modiﬁed implementation cppn compute gradients functions respect weights. cppn function maps coordinate vector vector output values function deﬁned directed acyclic graph nodes edges dppn node types used previous cppn papers i.e. sigmoid tanh absolute value gaussian identity sine plus rectiﬁed linear units max. experiment kinds input node identity node fully connected linear layer mapping vector equal dimensionality. parameters node weights biases linear layer. figure initial topology dppn consists randomly chosen hidden units input output node. transfer functions gaussian relu tanh sigmoid identity. input node fully connected linear layer shown also standard identity function input node comparable results. transfer functions ﬁxed unlearnable parameters. dppn initialized topology shown figure random hidden units. also experiment complex initializations fully connected feedforward dppns ranging initial nodes. dppn encoded genetically connection matrix node list. tween noiseless data prediction network. experiments mean squared error binary cross entropy. loss function usually minimized variant gradient descent. dppn output parameters directly mapped parameters denoising autoencoder. generic evolutionary algorithm used outer loop optimization algorithm. learning takes place ﬁtness evaluation evaluating ﬁtness function; number steps gradient-based learning performed starting inherited weights. lamarckian version learned weights inherited oﬀspring. darwinian version learned weights discarded initial weights parents inherited oﬀspring. describe evolutionary algorithm embedded learning algorithm detail. diﬀerent evolutionary algorithms used. simpler evolutionary algorithm microbial genetic algorithm population size random agents chosen weights trained agent’s ﬁtness evaluated mutated copy winner overwrites loser. probability crossover case loser parent winner parent second genetic algorithm asynchronous binary tournament selection algorithm running parallel. identical except whenever workers return ﬁtness random pairs chosen undergo binary tournaments. setup used mnist omniglot experiments computationally demanding. ﬁtness dppn negative loss. three types topology mutation applied random node remove random edge random edge. node added random input node random output node chosen node connected them care taken maintain feedforward property dppn. initial weights node drawn distribution weights initial dppn. probability node addition edge addition removal typically respectively replication event. also experiment applying cauchy mutation copy winner ﬁtness evaluation multiplicative co-eﬃcient cauchy mutation preferred mutations small heavy tail allowing escape local optima. crossover operator merge hidden units parents combined input output node parent discarded input unit parent connected hidden units parent random weights hidden units parent connected output unit parent random weights. thus crossover results approximate doubling dppn. attempt made neat innovation numbers. crossover topological sort algorithm used reorder connection matrix make upper-right triangular enforce check feedforward property. gradients loss respect cppn weights computed backpropagation ﬁrst computes gradients loss respect parameters passes backwards cppn combined gradients parameters respect modifying weights dppn adam momentum-based ﬂavor adaptively computes individual learning rates parameter keeping estimate ﬁrst second moments gradients. hyper-parameters used control decay rates moving averages gradient squared gradient. moving averages bias-corrected resulting estimate moments ˆvt. algorithm well suited problems incorporate large number parameters memory computationally eﬃcient. combines advantages popular methods adagrad behaves well presence sparse gradients rmsprop able cope non-stationary objectives. indirect encoding fully connected network task reconstruct mnist digits pixels zero image. figure shows logic training. learn indirectly encode fully connected feedforward denoising autoencoder encoding layer sigmoid activation functions decoding layer sigmoid activation functions. hidden layer task reconstruct mnist digits pixels zero image. convolutional network encoding layer relu activation functions decoding layer relu activation functions total number parameters network experiment dppn output nodes used encode weights kernels biases convnet. ﬁrst four outputs dppn encode respectively weights ﬁrst encoding kernel second encoding kernel ﬁrst decoding kernel second decoding kernel.the ﬁnal outputs encode biases. input vector data point dppn length ters three orders magnitude parameters convnet performs task. dppn encodes parameters outputs encoding layer decoding layer. obtain parameters passed input vectors dppn length encode following properties autoencoder coordinates input neuron xout yout coordinates output neuron dout distances center input output neuronal grids respectively. produces parameters output ﬁrst ×××+× elements ﬁrst second elements second used encode parameters autoencoder. similar process consisting forwards pass dppn copy dppn outputs autoencoder forward backwards pass autoencoder mnist minibatch followed backpropagation gradients dppn iterated times ﬁtness evaluation. training ﬁtness dppn deﬁned negative random mnist images training set. ﬁnal loss test random mniist images. figure shows details evolutionary population dppns crossover probability initialized node dppns evolved reconstruct handwritten digit evolved full lamarckian algorithm i.e. learned weights inherited oﬀspring. figure shows setup baldwinian evolution i.e. learning takes place inheritance acquired characteristics. finally figure shows setup pure darwinian evolution cauchy mutation weights co-eﬃcient learning weights all. ﬁnal setting closest cppn. examples shown lamarckian inheritance achieves baldwinian darwinian lamarckian eﬀective baldwinian eﬀective darwinian. traditionally cppns initialized minimal networks. also eﬀective initialize dppns larger networks fully connected upper right triangle connection matrix. batch runs show mean ence. also tried hybrid variant learning rates cauchy mutation. small non-signiﬁcant beneﬁt adding cauchy noise learning rates investigated therefore later runs used cauchy mutation coeﬃcient additive bloat punishments produced improvement level produced signiﬁcantly worse greater number nodes edges dppn. figure shows setup figure without crossover. order magnitude diﬀerence crossover compared without crossover. reconstruction qualitatively worse without crossover. batch runs size show order magniwithout crossover. trivial reason beneﬁt crossover merely increases size networks allowing greater number parameters optimized gradient descent possibly reducing chance getting stuck local optimum. another factor merging dppns allows informational merging diﬀerent useful parts image reconstructed diﬀerent individuals population. figure dppn produced encoding decoding kernels convolutional denoising autoencoder mnist. left encoding decoding kernels. right digit reconstructions figure shows encoding decoding kernels evolved dppn convolutional denoising autoencoder along digit reconstructions ﬁtness graph showing tournaments suﬃcient test set. dppn discovers regular on-center oﬀ-center receptive ﬁelds resembling retinal ganglion cells image smoothing removes uncorrelated dropout noise reconstruction. without crossover producing parameters fully connected denoising autoencoder. cases dppn rediscovers convolutions learning on/oﬀ center kernels convolving fully connected network. contrast receptive ﬁelds normally learned networks much less regular figure extent eﬀective compression achieved autoencoder’s parameters dppn remarkable figure shows dppn encoded network achieve much lower directly encoded network number parameters furthermore capable generalization omniglot dataset better equivalent directly encoded network figure video supplementary material shows evolution mnist reconstructions throughout run. figure image reconstruction handwritten digit lamarckian baldwinian darwinian inheritance weights. insert target image shows character reconstruct. grids show reconstructions produced evolutionary runs tournaments sampled every tournaments starting left proceeding bottom right corner. lamarckian better baldwinian better darwinian. encoding layer weights denoising autoencoder left shows parameter network linear layer input right shows parameter network fully connected linear layer input. hidden layer representations rotated cropped inverted encoder. results demonstrate dppns associated learning algorithms described capable massively compressing parameters larger neural networks improve upon performance cppns trained dargrid structure visualize activations hidden layer digit figure shows hidden layer activations fully connected denoising autoencoder encoded dppn identity node input dppn fully connected linear node input. figure discovery convolutional ﬁlters fully connected denoising autoencoder. input digits shown left followed neuron encoding layers weight matrix sample neuron decoding layers weight matrix ﬁnally reconstructions right. note highly regular weight matrices layers compared directly encoded matrices next ﬁgure. advantages symbiosis evolutionary gradient-based learning allows optimization better avoid stuck local optima saddle points. future framework holds potential training much deeper neural networks applied learning paradigms. rodriguez campbell folsom-kovarik stanley. picbreeder case study collaborative evolutionary exploration design space. evolutionary computation stanley. compositional pattern producing stanley d’ambrosio gauci. hypercube-based encoding evolving large-scale neural networks. artiﬁcial life stanley miikkulainen. evolving neural", "year": 2016}