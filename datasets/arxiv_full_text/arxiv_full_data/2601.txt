{"title": "Structured Inference Networks for Nonlinear State Space Models", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Gaussian state space models have been used for decades as generative models of sequential data. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption. We introduce a unified algorithm to efficiently learn a broad class of linear and non-linear state space models, including variants where the emission and transition distributions are modeled by deep neural networks. Our learning algorithm simultaneously learns a compiled inference network and the generative model, leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. We apply the learning algorithm to both synthetic and real-world datasets, demonstrating its scalability and versatility. We find that using the structured approximation to the posterior results in models with significantly higher held-out likelihood.", "text": "parametric form model unknown propose learning deep markov models class generative models classic linear emission transition distributions replaced complex multi-layer perceptrons gssms retain markovian structure hmms leverage representational power deep neural networks model complex high dimensional data. augments model presented fig. edges observations latent states following time step seen similar though restrictive than stochastic rnns variational rnns learning algorithm performs stochastic gradient ascent variational lower bound likelihood. instead introducing variational parameters data point compile inference procedure time learning generative model. idea originally used wake-sleep algorithm unsupervised learning since state-of-the-art results unsupervised learning deep generative models speciﬁcally introduce family structured inference networks parameterized recurrent neural networks evaluate effectiveness three scenarios generative model known ﬁxed parameter estimation functional form model known learning deep markov models. looking structure true posterior show theoretically empirically inference latent state performed using information future opposed recent work performed inference using information past structured variational approximation outperforms mean-ﬁeld based approximations. approach easily adapted learning general generative models example models edges observations latent states. gaussian state space models used decades generative models sequential data. admit intuitive probabilistic interpretation simple functional form enjoy widespread adoption. introduce uniﬁed algorithm efﬁciently learn broad class linear non-linear state space models including variants emission transition distributions modeled deep neural networks. learning algorithm simultaneously learns compiled inference network generative model leveraging structured variational approximation parameterized recurrent neural networks mimic posterior distribution. apply learning algorithm synthetic real-world datasets demonstrating scalability versatility. using structured approximation posterior results models signiﬁcantly higher held-out likelihood. models sequence data hidden markov models recurrent neural networks widely used machine translation speech recognition computational biology. linear non-linear gaussian state space models used applications including robotic planning missile tracking. however despite huge progress last decade efﬁcient learning non-linear models complex high dimensional time-series remains major challenge. paper proposes uniﬁed learning algorithm broad class gssms introduce inference procedure scales easily high dimensional data compiling approximate inference parameters neural network. engineering control parametric form gssm model often known typically speciﬁc parameters need data. commonly used approaches types learning inference problems often computationally demanding e.g. dual extended kalman ﬁlter expectation maximization particle ﬁlters compiled inference algorithm easily deal high-dimensions observed copyright association advancement artiﬁcial intelligence rights reserved. figure generative models sequential data hidden markov model deep markov model denotes neural networks used dmms emission transition functions. recurrent neural network denotes deterministic intermediate representation. code learning dmms reproducing results found github.com/clinicalml/structuredinference learned health records queries what would happened patients received treatment show model correctly identiﬁes certain medications affect patient’s health. related work learning gssms mlps transition distribution considered approximate posterior non-linear dynamic factor analysis scales quadratically observed dimension impractical large-scale learning. recent work considered variational learning timeseries data using structured inference recognition networks. archer propose using gaussian approximation posterior distribution block-tridiagonal inverse covariance. johnson conditional random ﬁeld inference network time-series models. concurrent work fraccaro also learn sequential generative models using structured inference networks parameterized recurrent neural networks. bayer osendorfer fabius amersfoort create stochastic variant rnns making hidden state every time step function independently sampled latent variables. chung apply similar model speech data sharing parameters rnns generative model inference network. learn model discrete random variables using structured inference network considers information past similar chung gregor al.’s models. contrast works information future within structured inference network show preferable theoretically practically. additionally systematically evaluate impact different variational approximations learning. watter construct ﬁrst-order markov model using inference networks. however learning algorithm based data tuples consecutive time steps. makes strong assumption posterior distribution recovered based observations current next time-step. show generative models like fig. posterior distribution time step function gaussian state space models consider inference learning class latent variable models given denote vector valued latent variable vector valued observation. sequence latent variables observations denoted respectively. assume distribution latent states multivariate gaussian mean covariance differentiable functions previous latent state multivariate observations distributed according distribution whose parameters function corresponding latent state collectively denote parameters generative model. subsumes large family linear non-linear gaussian state space models. example setting gtzt− ftzt matrices obtain linear state space models. functional forms initial parameters pre-speciﬁed. variational learning using recent advances variational inference optimize variational lower bound data log-likelihood. technical innovation introduction inference network recognition network neural network approximates intractable posterior. parametric conditional distribution optimized perform inference. throughout paper denote parameters generative model denote parameters inference network. remainder section consider learning bayesian network whose joint distribution factorizes pθpθ. posterior distribution typically intractable. using well-known variational principle posit approximate posterior distribution obtain following lower bound marginal likelihood kl||pθ inequality jensen’s inequality. kingma welling; rezende mohamed wierstra neural parameterize challenge resulting optimization problem lower bound includes expectation w.r.t. implicitly depends network parameters using gaussian variational approximation parametric functions observation difﬁculty overcome using stochastic backpropagation simple transformation allows obtain unbiased monte carlo estimates gradients lower bound analytic form term simplest transition models could estimate gradient term sampling variational model results high variance estimates gradients. different factorization term leading variational lower bound objective function point resulting objective function stable analytic gradients. without factorization divergence would estimate klz|x)||pz)) monte-carlo sampling since analytic form. contrast individual terms analytic forms. detailed derivation bound factorization divergence detailed supplemental material. learning gradient descent objective differentiable parameters model generative model ﬁxed perform gradient ascent otherwise perform gradient ascent stochastic backpropagation estimating gradient w.r.t. note expectations taken respect variables sufﬁcient statistics markov model. terms fact prior variational approximation posterior qφx) normally distributed hence divergence estimated analytically. algorithm learning stochastic gradient descent single sample recognition network learning evaluate expectations bound. aggregate gradients across mini-batches. sample datapoint estimate posterior parameters sample qφz|x) estimate conditional likelihood pθx|ˆz) evaluate estimate approx. estimate approx. update using adam leverage stochastic backpropagation learn generative models given corresponding graphical model fig. insight purpose inference markov properties generative model guide deriving structured approximation posterior. speciﬁcally posterior factorizes functions parameterized neural nets. although option condition information across time suggests fact sufﬁces condition information future previous latent state. previous latent state serves summary statistic information past. exact inference match factorization true posterior using inference network using gaussian variational approximation approximate posterior latent variable limits expressivity inferential model except case linear dynamical systems posterior distribution normally distributed. however could augment proposed inference network recent innovations improve variational approximation allow multi-modality modiﬁcations could yield black-box methods exact inference timeseries models leave future work. deriving variational lower bound generative model inference network interested maxθ pθx). ease exposition instantiate derivation variational bound single data point though learn corpus. figure structured inference networks mf-lr st-lr variational approximations sequence length using bidirectional recurrent neural brnn takes input sequence series non-linearities denoted blue arrows forms sequence hidden states summarizing information left right respectively. sequence non-linearities call combiner function above) denoted arrows outputs vectors parameterizing mean diagonal covariance qφx) samples drawn qφx) indicated black dashed arrows. structured variational models st-lr samples computation indicated arrows label mean-ﬁeld model arrows therefore computes qφx). inference network structured like st-lr except without past. algorithm depicts overview learning algorithm. outline algorithm mini-batch size practice gradients averaged across stochastically sampled mini-batches training set. take gradient step typically adaptive learning rate detail construct variational approximation speciﬁcally model mean diagonal covariance functions using recurrent neural networks since implementation models diagonal covariance matrix denote rather parameterization cannot general expected equal pθz|x) many cases reasonable approximation. rnns ability scale well large datasets. table details different choices inference networks evaluate. deep kalman smoother corresponds exactly functional form suggested proposed variational approximation. smoothes information past future form approximate posterior distribution. also evaluate possibilities variational models mean-ﬁeld models structured models distinguished whether information past future fig. illustration methods. conditions different subset observations summarize information input sequence corresponds st-r. hidden states parameterize variational distribution call combiner function. obtain mean diagonal covariance approximate posterior time-step manner akin gaussian belief propagation. speciﬁcally interpret hidden states forward backward rnns parameterizing mean variance gaussian-distributed messages summarizing observations past future respectively. multiply gaussians performing variance-weighted average means. operations understood performed element-wise corresponding vectors. hleft hidden states rnns past future respectively must encoded completely stochastic latent state. achieve goal create gated transition function. would like model ﬂexibility choose linear transition dimensions non-linear transitions others. adopt following parameteridatasets evaluate three datasets. synthetic consider simple linear non-linear gssms. train inference networks datapoints length consider dimensional systems inference parameter estimation. compare results using training value variational bound rmse correspond polyphonic music train dmms polyphonic music data instance sequence comprises -dimensional binary vector corresponding notes piano. learn epochs report results based early stopping using validation set. report held-out negative loglikelihood format {c}. importance sampling based estimate −lx; length sequence upper bound facilitates comparison rnns; tsbn report ilx; compute facilitate comparison work. found annealing divergence variational bound parameter updates better results. electronic health records dataset comprises diabetic patients using data major health insurance provider. observations interest level glucose glucose quantiles clinically meaningful bins. observations also include gender icd- relationship related work archer al.; difference approach parameterization account information future relevant approximate posterior distribution johnson interleave predicting local variational parameters graphical model steps message passing inference. difference approach rely structured inference network predict optimal local variational parameters directly. contrast johnson suboptimalities initial local variational parameters overcome subsequent steps optimization albeit additional computational cost. chung propose variational gaussian noise introduced time-step rnn. chung inference network shares parameters generative model uses information past. views noise variables hidden state time-step together factorization similar shown hold although term would longer analytic form since would normally distributed. nonetheless structured inference networks could used improve tightness variational lower bound empirical results suggest would result better learned models. following apply ideas deep learning non-linear continuous state space models. transition emission function unknown functional form parameterize deep neural networks. fig. illustration graphical model. emission function parameterize emission function using two-layer denotes non-linearities relu sigmoid tanh units applied element-wise input vector. modeling binary data sigmoid bemission) parameterizes mean probabilities independent bernoullis. gated transition function parameterize transition function using gated transition function inspired gated recurrent units instead mlp. gated recurrent units neural architecture parameterizes recurrence equation gating units control information hidden state next conditioned observation. unlike grus transition function conditional observations. information diagnosis codes co-morbidities diabetes congestive heart failure chronic kidney disease obesity. binary observations patient every time-step. group patient’s data three month intervals yielding sequence length synthetic data compiling exact inference seek understand whether inference networks accurately compile exact posterior inference network parameters linear gssms exact inference feasible. experiment optimize ﬁxed synthetic distribution given one-dimensional gssm. compare results obtained various approximations propose obtained implementation kalman smoothing performs exact inference. fig. depicts results. proposed st-lr outperform mean-ﬁeld based variational method mf-l looks information past. mf-lr however often able catch comes rmse highlighting role information future plays performing posterior inference evident posterior factorization st-lr converge rmse exact smoothed moreover lower bound likelihood becomes tight. approximate inference parameter estimation here experiment applying inference networks synthetic non-linear generative models well using learning subset parameters within ﬁxed generative model. synthetic non-linear datasets similarly structured variational approximations capable matching performance inference using smoothed unscented kalman filter held-out data. finally fig. illustrates instance successfully perform parameter estimation synthetic two-dimensional non-linear gssm. polyphonic music mean-field structured inference networks table shows results learning polyphonic music dataset using mf-lr st-l st-lr. st-l structured variational approximation considers information past implementation details comparable used comparing negative log-likelihoods learned models looseness variational bound signiﬁcantly affects ability learn. st-lr substantially outperform mf-lr st-l. adds credence idea taking consideration factorization posterior perform better inference consequently learning real-world high dimensional settings. note network half parameters st-lr mf-lr networks. figure synthetic evaluation compiled inference ﬁxed linear gssm training comprised onedimensional observations sequence length rmse respect true generated data. variational bound training. results held-out data similar visualizing inference sequences left panels show latent space variables right panels show observations observations generated application emission function posterior shown latent space. shading denotes standard deviations. figure parameter estimation learning parameters two-dimensional non-linear gssm. denotes vector denotes concatena tion superscript denotes indexing. table comparing inference networks test negative loglikelihood polyphonic music different inference networks trained ﬁxed structure numbers inside parentheses variational bound. outperforms rnns hmsbn everywhere outperforms storn nottingham outperform tsbn datasets except piano. compared lvrnn dmmaug obtains better results datasets except jsb. showcases ﬂexible structured inference network’s ability learn powerful generative models compare favourably state models. provide audio ﬁles samples learned models code repository. patient data learning models large observational health datasets promising approach advancing precision medicine could used example understand medications work best whom. section show used precisely application. working data poses technical challenges data noisy high dimensional difﬁcult characterize easily. patient data rarely contiguous large parts dataset often missing learn data showing handle aforementioned technical challenges model based counterfactual prediction. graphical model fig. represents generative model model captures idea underlying time-evolving latent state patient solely responsible diagnosis codes values observe. addition patient state modulated drugs prescribed doctor. assume drugs prescribed point time depend patient’s entire medical history though practice dotted edges bayesian network never need modeled since always assumed observed. natural line follow work would consider learning missing latent. make time-varying drug prescription patient augmenting additional edge every time step. speciﬁcally dmm’s transition function data indicator vector eight anti-diabetic drugs including metformin insulin metformin commonly prescribed ﬁrst-line anti-diabetic drug. figure medical data augmented external actions representing medications presented patient. latent state patient. observations model. since always assumed observed conditional distribution ignored learning. emission transition functionthe choice emission transition function data well understood. fig. experiment variants dmms using mlps emission transition function yield best generative models terms held-out likelihood. experiments hidden dimension emission transition functions. used size latent dimension size inference network learning. learning missing data dataset subset observations frequently missing data. marginalize learning straightforward within probabilistic semantics bayesian network. subnetwork original graph concerned emission function since missingness affects ability evaluate missing random variables leaves bayesian sub-network consider simple example modeling observations time namely log-likelihood data conditioned latent introduce general algorithm scalable learning rich family latent variable models time-series data. underlying methodological principle propose build inference network mimic posterior distribution space complexity learning algorithm depends neither sequence length training size offering massive savings compared classical variational inference methods. propose evaluate building variational inference networks mimic structure true posterior distribution. structured variational approximations also possible. example could instead past conditioned summary statistic future learning inference. since rnns inference network possible continue increase capacity condition different modalities might relevant approximate posterior inference without worry overﬁtting data. furthermore confers ability easily model presence missing data since semantics render easy marginalize unobserved data. contrast much difﬁcult marginalize unobserved data dependence intermediate hidden states previous input. indeed allowed develop principled application learning algorithm modeling longitudinal patient data data inferring treatment effect. tesla used research donated nvidia corporation. authors gratefully acknowledge support darpa probabilistic programming advancing machine learning program afrl prime contract fa--c- career award independence blue cross. thank david albers kyunghyun yacine jernite eduardo sontag anonymous reviewers valuable feedback comments.", "year": 2016}