{"title": "Submodular meets Structured: Finding Diverse Subsets in  Exponentially-Large Structured Item Sets", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.IR", "stat.ML"], "abstract": "To cope with the high level of ambiguity faced in domains such as Computer Vision or Natural Language processing, robust prediction methods often search for a diverse set of high-quality candidate solutions or proposals. In structured prediction problems, this becomes a daunting task, as the solution space (image labelings, sentence parses, etc.) is exponentially large. We study greedy algorithms for finding a diverse subset of solutions in structured-output spaces by drawing new connections between submodular functions over combinatorial item sets and High-Order Potentials (HOPs) studied for graphical models. Specifically, we show via examples that when marginal gains of submodular diversity functions allow structured representations, this enables efficient (sub-linear time) approximate maximization by reducing the greedy augmentation step to inference in a factor graph with appropriately constructed HOPs. We discuss benefits, tradeoffs, and show that our constructions lead to significantly better proposals.", "text": "cope high level ambiguity faced domains computer vision natural language processing robust prediction methods often search diverse high-quality candidate solutions proposals. structured prediction problems becomes daunting task solution space exponentially large. study greedy algorithms ﬁnding diverse subset solutions structured-output spaces drawing connections submodular functions combinatorial item sets high-order potentials studied graphical models. speciﬁcally show examples marginal gains submodular diversity functions allow structured representations enables efﬁcient approximate maximization reducing greedy augmentation step inference factor graph appropriately constructed hops. discuss beneﬁts tradeoffs show constructions lead signiﬁcantly better proposals. many problems computer vision natural language processing computational biology involve mappings input space exponentially large space structured outputs. instance space segmentations image pixels take labels formulations conditional random fields max-margin markov networks structured support vector machines successfully provided principled ways scoring solutions predicting single highest scoring maximum posteriori conﬁguration exploiting factorization structured output constituent parts. number scenarios posterior several modes ambiguities seek single best prediction good predictions interactive machine learning. systems like google translate photoshop solve structured prediction problems often ambiguous generating small relevant candidate solutions user select greatly improve results. m-best hypotheses cascades. machine learning algorithms often cascaded output model another hence initial stages necessary make single perfect prediction. rather seek plausible predictions subsequently re-ranked combined processed sophisticated mechanism. scenarios ideally want small plausible non-redundant structured-outputs hedge bets. submodular maximization diversity. task searching diverse high-quality subset items ground well-studied information retrieval sensor placement document summarization viral marketing robotics across domains submodularity emerged fundamental practical concept property figure input image; space possible object segmentations labelings convert problem ﬁnding item highest marginal gain inference problem factor graph base variables appropriately deﬁned hop. addition monotone i.e. simple greedy algorithm achieves approximation factor result signiﬁcant practical impact unfortunately number items exponentially large even single linear scan greedy augmentation infeasible. work study conditions feasible greedily maximize submodular function exponentially large ground whose elements combinatorial objects i.e. labelings base variables yn}. instance image segmentation base variables pixel labels item particular labeling pixels. base variable indicates presence absence edge graph item represent spanning tree maximal matching. goal plausible diverse conﬁgurations efﬁciently i.e. time sub-linear assume monotone submodular nonnegative normalized base study greedy algorithm. running example focus pixel labeling base variable takes values labels. contributions. principal contribution conceptual one. observe marginal gains number submodular functions allow structured representations enables efﬁcient greedy maximization exponentially large ground sets reducing greedy augmentation step inference query discrete factor graph augmented suitably constructed highorder potential thus work draws connections seemingly disparate highly related areas machine learning submodular maximization inference graphical models structured hops. speciﬁc examples construct submodular functions three different task-dependent deﬁnitions diversity provide reductions three different hops efﬁcient inference techniques already developed. moreover present generic recipe constructing submodular functions plugged efﬁcient hops discovered future work. empirical contribution efﬁcient algorithm producing image segmentations signiﬁcantly higher oracle accuracy previous works. algorithm general enough transfer applications. fig. shows overview approach. related work generating multiple solutions. determinental point processesare elegant probabilistic model sets items preference diversity. generalization structured setting assumes tree-structured model assumption make. guzman-rivera learn models producing solution form solutions. approach requires access learning sub-routine repeated re-training models always possible expensive proprietary. assume given single model must generate multiple diverse good solutions. perhaps closest setting recent techniques ﬁnding diverse m-best solutions modes graphical models. inapplicable since restricted chain tree graphs compare baselines section preliminaries notation select ground items. item labeling base variables. clarity non-bold letters items boldface letters base conﬁgurations. uppercase letters refer functions ground items lowercase letters functions base variables formally bijection maps items representation base variable labelings notational simplicity often mean i.e. item corresponding labeling present write label used i.e. s.t. denote tuple goal ordered list items maximizes scoring function lists generalize notation sets allow reasoning item order repetitions. details list prediction found modular nonnegative relevance function aggregates quality items list; monotone normalized submodular function measure diversity items trade-off parameter. similar objective functions used e.g. reminiscent general paradigm machine learning combining loss function measures quality regularization term encourages desirable properties submodular maximization. list maximizes subject cardinality constraint monotone submodular done greedy algorithm starts iteratively adds next best item ﬁnal solution within factor optimal solution computational bottleneck iteration must item largest marginal gain. clearly exponential size cannot touch item even once. instead propose augmentation sub-routines exploit structure maximize marginal gain solving optimization problem base variables. marginal gains conﬁguration space solve greedy augmentation step optimization transfer marginal gain world items world base variables derive functions maximizing means maximizing hard combinatorial optimization problem general. however broad class useful functions inherits exploitable structure argmaxy solved efﬁciently exactly least approximately. relevance function. structured relevance function score factor graph deﬁned base variables graph deﬁned i.e. log-potential functions cliques. quality item instance node edge factors θpq. model ﬁnding single highest quality item corresponds maximum posteriori inference factor graph. although refer terms probabilistic interpretations treat relevance function output energy-based model structured instance parameters feature vector moreover assume relevance function nonnegative. assumption ensures monotone. non-monotone algorithms greedy needed leave generalization future work. application domains relevance function learned data thus positivity assumption restrictive simply learn positive relevance function. instance ssvms relevance weights learnt maximize margin correct labeling incorrect ones. show supplement ssvm parameters assign nonnegative scores labelings achieve exactly hinge loss without nonnegativity constraint. figure diversity groups groups deﬁned presence labels groups deﬁned hamming balls around item/labeling case diversity measured many groups covered item. text details. structured diversity functions next discuss general recipe constructing monotone submodular diversity functions reducing marginal gains structured representations base variables groups deﬁned task-dependent characteristics instance image segmentation segmentations contain label groups overlapping. instance segmentation contains pixels labeled grass ggrass gcow. group coverage count diversity. given groups {gi} measure diversity list terms group coverage i.e. number groups covered jointly items deﬁne intersection unique items easy show function monotone submodular. group segmentations contain label diversity measure list segmentations number object labels appear marginal gain number groups covered thus greedy algorithm item/segmentation belongs many unused groups possible. group coverage general diversity. generally instead simply counting number groups covered reﬁned decay nonnegative nondecreasing concave scalar function. submodular functions hence submodular. eqn. special case eqn. min{ possibilities log. general deﬁnition diversity marginal gain marginal gain item proportional rare group list step greedy algorithm maximize already established structured representation factor graph next subsections specify three example deﬁnitions groups instantiate three diversity functions show marginal gains expressed speciﬁc high-order potential factor graph hops known efﬁciently optimizable hence solve augmentation step efﬁciently. table summarizes connections. diversity parsimony. groups overlapping belong many groups simultaneously. offer immediate large gain diversity many applications natural seek small list complementary labelings rather labels occur instance image segmentation groups deﬁned label presence natural scenes unlikely contain many labels time. instead labels spread across selected labelings hence include parsimony factor biases towards simpler labelings term modular function affect diversity functions directly. next outline example instantiations functions diversity labels ﬁrst example labelings containing label i.e. diversity function arises multi-class image segmentation highest scoring segmentation contains grass would like complementary segmentations contain unused class label sheep cow. structured representation marginal gains. marginal gain diversity function turns called label cost penalizes label occurs previous segmentation. lcounts) number segmentations contain label simplest case coverage diversity marginal gain provides constant reward every unseen label thus rewards presence label amount proportional rare simplest case i.e. charged constant every label used type diversity greedy augmentation step equivalent performing inference factor graph augmented label reward hops argmaxy delong show perform approximate inference label costs extension standard α-expansion algorithm. label transitions. label diversity extended reward presence previously unseen labels also presence previously unseen label transitions formally deﬁne group label pair contains adjacent variables labels diversity function rewards presence label pair amount proportional rare pair segmentations part functions marginal gain becomes called cooperative cuts inference algorithm gives fully polynomial-time approximation scheme nondecreasing nonnegative exact gain maximizer count function min{ details found supplement. diversity hamming balls label diversity function simply rewarded presence label irrespective many variables assigned label. next diversity function rewards large hamming dis]] conﬁgurations iverson bracket.) denote k-radius hamming ball centered i.e. previous section constructed group label construct group conﬁguration k-radius hamming ball centered i.e. structured representation marginal gains. diversity marginal gain becomes called cardinality potential count group coverage becomes i.e. marginal gain adding number conﬁgurations covered hamming ball centered since size intersection union hamming balls straightforward structured representation maximize lower bound instead lower bound overcounts intersection eqn. summing intersections separately. also interpret lower bound clipping series arising inclusion-exclusion principle ﬁrst-order terms. importantly depends hamming distance cardinality potential depends number variables assigned particular label. speciﬁcally ignoring constant terms lower bound written summation cardinality factors constant number points intersection k-radius hamming balls centered approximation greedy step means performing inference factor graph augmented cardinality potentials argmaxy λdlb. solved messagepassing outgoing messages cardinality factors computed time algorithm offer approximation guarantees performs well practice. subtle point note always decreasing w.r.t. become negative over-counting. clamping greater experiments unnecessary greedy algorithm never chose negative. comparison divmbest. greedy algorithm hamming diversity similar spirit recent work batra also proposed greedy algorithm ﬁnding diverse solutions graphical models. provide justiﬁcation greedy formulation sheds light work. similar approach greedy step divmbest however yi]]. linear diversity rewards robust tend over-reward diversity. formulation uses robust diversity function experiments make saturation behavior smoothly tunable parameter larger corresponds hamming balls smaller radius optimize performance validation data. found work better directly tuning radius experiments apply greedy maximization algorithms image segmentation problems interactive binary segmentation category-level object segmentation pascal dataset compare methods respective oracle accuracies i.e. accuracy accurate segmentation diverse segmentations returned method. small value high oracle accuracy indicates algorithm achieved high recall identiﬁed good pool candidate solutions processing cascaded pipeline. experiments label background typically expected appear somewhere image thus play role label cost/transition diversity functions. furthermore binary segmentation non-background label. thus report results hamming diversity multi-class segmentation experiments report experiments three. baselines. compare proposed methods divmbest greedily produces diverse segmentation explicitly adding linear hamming distance term factor graph. hamming term decomposable along variables simply modiﬁes node potentials y∈s= yi]]. divmbest shown outperform techniques mbest-map produce high scoring solutions without focus diversity samplingbased techniques produce diverse solutions without focus relevance term hence include methods here. also report results combining different diversity functions operators generate solutions diversity functions concatenate lists; linearly combine diversity functions generate solutions using combined diversity. interactive foreground-background segmentation user provides partial labels scribbles. minimize interactions system provide candidate segmentations user choose from. replicate experimental setup curated images pascal dataset manually provided scribbles objects contained them. image relevance model -label pairwise node term superpixel image edge term adjacent pair superpixels. superpixel extract colour texture features. train transductive partial supervision provided user scribbles. node potentials derived scores tsvms. edge potentials contrast-sensitive potts. fifty images used tuning diversity parameters reporting oracle accuracies. -label contrast-sensitive potts model results supermodular relevance function efﬁciently maximized graph cuts hamming ball diversity collection cardinality factors optimize cyborg implementation results. test images dataset generated single best additional solutions using method. table shows average oracle accuracies divmbest hamming ball diversity combinations. combinations slightly outperform approaches. category-level object segmentation label pixel object categories background. construct multi-label pairwise superpixels. node potentials outputs category-speciﬁc regressors trained edge potentials multi-label potts. inference presence diversity terms performed implementations delong label costs tarlow hamming ball diversity boykov label transitions. figure qualitative results shows original image ground-truth segmentation pascal singlebest segmentation oracle segmentation segmentations different deﬁnitions diversity. hamming typically performs best. certain situations label transitions help since single-best segmentation included rare pair labels results. evaluate methods pascal data consisting train test partitions images each. train regressors train report oracle accuracies different methods diversity parameters chosen performing cross-val val. standard pascal accuracy corpus-level intersection-over-union measure averaged categories. label cost transition different concave sity performs best followed divmbest label cost/transitions worse here. found worst average label transition diversity helps interesting scenario ﬁrst best segmentation includes pair rare mutually confusing labels fig. shows example illustrations provided supplement. cases searching different label transition produces better segmentation. finally note lists produced combined diversity signiﬁcantly outperform single method discussion conclusion paper study greedy algorithms maximizing scoring functions promote diverse sets combinatorial conﬁgurations. problem arises naturally domains computer vision natural language processing computational biology want search diverse high-quality solutions structured output space. diversity functions propose monotone submodular functions construction. thus entire scoring function monotone submodular. showed simply learned positive. greedy algorithm maximizing monotone submodular functions proved useful moderately-sized unstructured spaces. best knowledge ﬁrst generalization exponentially large structured output spaces. particular contribution lies reducing greedy augmentation step inference structured efﬁciently solvable hops. insight makes connections submodular optimization work inference graphical models. address questions. sample? question posed random sampling would perform large ground sets unfortunately expected value random sample elements much worse optimal value especially large. lemma proved supplement. lemma sample size taken uniformly random. exist monotone submodular functions guarantees? nonnegative monotone submodular using exact inference algorithm clearly result approximation factor many inference procedures approximate. lemma formalizes approximate inference affects approximation bounds. lemma monotone submodular. step greedy algorithm uses approximate marginal gain maximizer maxa∈v show combination supplement. monotone nonnegative lemma extended relative error bound −fmin refers fmin mins optimal solution stating results additive approximation losses occur approximation bound inference computed shifted reﬂected function pose theoretical improvements open question future work. said experiments convincingly show algorithms perform well practice even guarantees generalization. addition three speciﬁc examples section constructions generalize broad class upper-envelope potentials details provided supplement. acknowledgements. thank xiao help. majority work done intern virginia tech. partially supported national science foundation grant iis- iis- army research ofﬁce award wnf--- ofﬁce naval research award n--- awarded supported gifts amazon services google thomas stacey siebel foundation apple cenergy cisco cloudera ericsson facebook gameontalis guavus huawei intel microsoft netapp pivotal splunk virdata vmware wandisco yahoo. references batra. efﬁcient message-passing algorithm m-best problem. carbonell goldstein. diversity-based reranking reordering documents producing summaries. proc. annual international sigir conference research development information retrieval sigir pages krause jegelka. submodularity machine learning directions. icml tutorial krause singh guestrin. near-optimal sensor placements gaussian processes theory kulesza taskar. structured determinantal point processes. proc. nips lafferty mccallum pereira. conditional random ﬁelds probabilistic models streeter golovin. online algorithm maximizing submodular functions. nips tarlow givoni zemel. hop-map efﬁcient message passing high order potentials. section show ssvms natural origin parameters learnt non-negativity constraints achieve exactly hinge loss achieved without nonnegativity constraint. training instances sample space label space structured minimizes following regularized risk function. function extracting feature vector given sample label. regularized risk function non-differentiable often reformulated terms quadratic program introducing slack variable sample representing value maximum. standard structured primal formulation given follows. claim proof. adding bias feature doesn’t affect objective function every constraint invariant hence problems equivalent. next consider formulation extend enforce non-negativity scores. therefore even adding nonnegativity constraints solutions achieve values regularised risk function hence expected generalization guarantees. practice non-negativity constraints added cutting-plane procedure call. groups motivating scenario. section generalize label cost diversity function reward presence certain labels presence certain label transitions. instance highest scoring segmentation contains grass diversity function reward segmentations containing novel label transitions sheep-grass cowground sheep-sky. formally deﬁne group label pair item belongs contains adjacent variables labels structured representation marginal gains. diversity label transitions marginal gain becomes called cooperative cuts cuty speciﬁc label transition further cuts count number items contain least label transition cuts cutφ marginal gains diversity function similar single label groups gain label pair decreases cuts grows. thus rewards presence pair amount proportional rare segmentations analogously label costs parsimony factor setting encouraging individual small number label transitions. speciﬁcally using count coverage parsimony term eventually maximize supermodular function edges label transition. thus multilabel cooperative inference algorithm kohli applies. construction general looks similar degrading cost front min. sake completeness show difference sets solutions generated different diversity functions show sample sets solutions generated given image results help understanding behavior different diversity functions. monotone nonnegative function shift function obtain relative bound lemma arbitrary monotone submodular function fmin mins⊆v apply greedy algorithm obtain solution satisﬁes deﬁne shifted monotone non-negative version fmin solution bounded relative approximation error fmin addition three speciﬁc examples given main document also generalize constructions broad class hops called upper envelope potentials disjoint groups polynomial size number base variables consider group count diversity. iteration greedy algorithm assume without loss generalization covered iteration marginal gain region consistency diversity. consider region image whose superpixels want bias towards homogenous uniform labeling. thus search diverse labelings want encourage entire superpixels change together. case group corresponds segmentations assign label region diversity function deﬁne pairwise function maximised using standard message passing algorithms however cases region consistency diversity deﬁned above pairwise function supermodular graph cuts used.", "year": 2014}