{"title": "Long Short-Term Memory-Networks for Machine Reading", "tag": ["cs.CL", "cs.NE"], "abstract": "In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.", "text": "paper address question render sequence-level networks better handling structured input. propose machine reading simulator processes text incrementally left right performs shallow reasoning memory attention. reader extends long short-term memory architecture memory network place single memory cell. enables adaptive memory usage recurrence neural attention offering weakly induce relations among tokens. system initially designed process single sequence also demonstrate integrate encoder-decoder architecture. experiments language modeling sentiment analysis natural language inference show model matches outperforms state art. sequence-level network induce relations presumed latent text processing? recurrent network attentively memorize longer sequences humans paper design machine reader automatically learns understand text. term machine reading related wide range tasks answering reading comprehension questions fact relation extraction ontology learning textual entailment rather focusing speciﬁc task develop general-purpose reading simulator drawing inspiration human language processing fact language comprehension incremental readers continuously extracting meaning utterances word-by-word basis. order understand texts machine reader provide facilities extracting representing meaning natural language text storing meanings internally working stored meanings derive consequences. ideally system robust open-domain degrade gracefully presence semantic representations incomplete inaccurate incomprehensible. would also desirable simulate behavior english speakers process text sequentially left right ﬁxating nearly every word read creating partial representations sentence preﬁxes language modeling tools recurrent neural networks bode well human reading behavior rnns treat sentence sequence words recursively compose word previous memory meaning whole sentence derived. practice however sequence-level networks least three challenges. ﬁrst concerns model training problems associated vanishing exploding gradients partially ameliorated gated activation functions long shortterm memory gradient clipping second issue relates memory compression problems. input sequence gets compressed blended single dense vector suffbi chasing criminal thethe chasing criminal thethe fbifbi chasing criminal thethe fbifbi thethe fbifbi thethe fbifbi thethe fbifbi thethe fbifbi thethe fbifbi thethe fbifbi figure illustration model reading sentence chasing criminal run. color represents current word ﬁxated blue represents memories. shading indicates degree memory activation. ﬁciently large memory capacity required store past information. result network generalizes poorly long sequences wasting memory shorter ones. finally acknowledged sequence-level networks lack mechanism handling structure input. imposes inductive bias odds fact language inherent structure. paper develop text processing system addresses limitations maintaining incremental generative property recurrent language model. recent attempts render neural networks structure aware seen incorporation external memories context recurrent neural networks idea multiple memory slots outside recurrence piece-wise store representations input; read write operations slot modeled attention mechanism recurrent controller. also leverage memory attention empower recurrent network stronger memorization capability importantly ability discover relations among tokens. realized inserting memory network module update recurrent network together attention memory addressing. attention acts weak inductive module discovering relations input tokens trained without direct supervision. point departure previous work memory network employ internal recurrence thus strengthening interaction leading representation learner able reafigure illustrates reading behavior lstmn. model processes text incrementally learning past tokens memory extent relate current token processed. result model induces undirected relations among tokens intermediate step learning representations. validate performance lstmn language modeling sentiment analysis natural language inference. cases train lstmn models end-to-end task-speciﬁc supervision signals achieving performance comparable better state-of-the-art models superior vanilla lstms. machine reader recurrent neural network exhibiting important properties incremental simulating human behavior performs shallow structure reasoning input streams. recurrent neural network successfully applied various sequence modeling sequence-to-sequence transduction tasks. latter assumed several guises literature machine translation sentence compression reading comprehension contributing factor success ability handle well-known problems exploding vanishing gradients leading models gated activation functions advanced architectures enhance information within network remaining practical bottleneck rnns memory compression since inputs recursively combined single memory representation typically small terms parameters becomes difﬁcult accurately memorize sequences problem sidestepped attention mechanism learns soft alignments decoding states encoded memories unit extended memory tape explicitly simulates human memory span. model performs implicit relation analysis tokens attention-based memory addressing mechanism every time step. following ﬁrst review standard long short-term memory describe model. long short-term memory recurrent neural network processes variable-length sequence incrementally adding content single memory slot gates controlling extent content memorized content erased current content exposed. time step memory hidden state updated following equations gate activations. compared standard lstm uses additive memory updates separates memory hidden state interacts environment making predictions. ﬁrst question arises lstms extent able memorize sequences recursive compression. lstms produce list state representations composition however next state always computed current state. given current state next state conditionally independent states h···ht− tokens x···xt. recursive state update performed markov manner assumed lstms maintain unbounded memory assumption fail practice example sequence long model memory attention added within sequence encoder allowing network uncover lexical relations tokens. idea introducing structural bias neural models means new. example reﬂected work socher apply recursive neural networks learning natural language representations. context recurrent neural networks efforts build modular structured neural models date back connect recurrent neural network external memory stack learning context free grammars. recently weston propose memory networks explicitly segregate memory storage computation neural networks general. model trained end-to-end memory addressing mechanism closely related soft attention applied machine translation grefenstette deﬁne differentiable data structures memories controlled recurrent neural network. tran combine lstm external memory block component interacts hidden state. kumar employ structured neural network episodic memory modules natural language also visual question answering similar work leverage memory attention recurrent neural network inducing relations tokens module larger network responsible representation learning. property soft attention intermediate relations capture soft differentiable. contrast shift-reduce type neural models intermediate decisions hard induction difﬁcult. finally note model captures undirected lexical relations thus distinct work dependency grammar induction learned head-modiﬁer relations directed. section present machine reader designed process structured input retaining incrementality recurrent neural network. core model long short-term memenvironment memory tape used represent actually stored memory. therefore token associated hidden vector memory vector. denote current input; denotes current memory tape previous hidden tape. time step model computes relation x···xt− h···ht− attention layer yields probability distribution hidden state vectors previous tokens. compute adaptive summary vector previous hidden tape memory tape denoted respectively idea behind lstmn attention inducing relations tokens. relations soft differentiable components larger representation learning network. although appealing provide direct supervision attention layer e.g. evidence collected dependency treebank treat submodule optimized within larger network downstream task. also possible structured relational reasoning module stacking multiple memory hidden layers alternating fashion resembling stacked lstm resulting long short-term memory-network stores contextual representation input token unique memory slot size memory grows time upper bound memory span reached. design enables lstm reason relations tokens neural attention layer perform non-markov state updates. although feasible apply write read operations memories attention concentrate latter. conceptualize read operation attentively linking current token previous memories selecting useful content processing although focus work signiﬁcance write operation analogously justiﬁed incrementally updating previous memories e.g. correct wrong interpretations processing garden path sentences shown equations figure major change deep fusion lies recurrent storage inter-alignment vector target memory network help target network review source information. section present experiments evaluating performance lstmn machine reader. start language modeling natural testbed model. assess model’s ability extract meaning representations generic sentence classiﬁcation tasks sentiment analysis. finally examine whether lstmn recognize semantic relationship sentences applying natural language inference task. code available https//github.com/cheng/ snli-attention. natural language processing tasks machine translation textual entailment concerned modeling sequences rather single one. standard tool modeling sequences recurrent networks encoder-decoder architecture second sequence processed conditioned ﬁrst section explain combine lstmn applies attention intra-relation reasoning encoder-decoder network whose attention module learns inter-alignment sequences. figures illustrate types combination. describe models formally below. shallow attention fusion shallow fusion simply treats lstmn separate module readily used encoder-decoder architecture lieu standard lstm. shown figure encoder decoder modeled lstmns intra-attention. meanwhile interattention triggered decoder reads target token similar inter-attention introduced bahdanau deep attention fusion deep fusion combines interintra-attention computing state updates. different notation represent sets attention. following section denote target memory tape hidden tape store representations target symbols processed far. computation intra-attention follows equations additionally represent source memory tape hidden tape length source sequence conditioned upon. compute figure lstmns sequence-to-sequence modeling. encoder uses intra-attention decoder incorporates intrainter-attention. ﬁgures present ways combine intrainter-attention decoder. language modeling experiments conducted english penn treebank dataset. following common practice trained sections used sections validation sections dataset contains approximately million tokens vocabulary size average sentence length perplexity evaluation metric denotes negative likelihood entire test corresponding number tokens. used stochastic gradient descent optimization initial learning rate decays factor epoch signiﬁcant improvement observed validation set. renormalize gradient norm greater mini-batch size dimensions suite experiments compared lstmn variety baselines. ﬁrst kneser-ney -gram language model generally serves non-neural baseline language modeling task. also present perplexity results standard lstm models. also implemented sophisticated lstm architectures stacked lstm gated-feedback lstm depth-gated lstm gated-feedback lstm feedback gates connecting hidden states across multiple time steps adaptive control information ﬂow. depth-gated lstm uses depth gate connect memory cells vertically adjacent layers. general glstm dlstm able capture long-term dependencies degree explicitly keep past memories. number layers experiment mainly agree language modeling experiments chung also note single-layer variants glstm dlstm; implemented multi-layer systems. hidden unit size lstmn comparison models experimented -layer lstmns. latter model predict sentiment label sentence based averaged hidden vector passed -layer neural network classiﬁer relu activation function. memory size lstmn models compatible previous lstm models applied task. used pretrained glove vectors initialize word embeddings. gradient words glove embeddings scaled ﬁrst epoch word embeddings updated normally. used adam optimization momentum parameters respectively. initial learning rate regularization constant mini-batch size dropout rate applied neural network classiﬁer. compared model wide range topperforming systems. models lstm variants recursive neural networks convolutional neural networks recursive models assume input sentences represented parse trees take advantage annotations phrase level. lstm-type models cnns trained sequential input figure examples intra-attention bold lines indicate higher attention scores. arrows denote word focused attention computed direction relation. baselines lstm signiﬁcant margin. amongst deep architectures three-layer lstmn also performs best. study memory activation mechanism machine reader visualizing attention scores. figure shows four sentences sampled penn treebank validation set. although explicitly encourage reader attend memory slot much attention focuses recent memories. agrees linguistic intuition long-term dependencies relatively rare. illustrated figure model captures valid lexical relations note arcs undirected different directed arcs denoting head-modiﬁer relations dependency graphs. sentiment analysis second task concerns prediction sentiment labels sentences. used stanford sentiment treebank contains ﬁne-grained sentiment labels sentences. following previous work dataset used sentences training validation testing. average sentence length addition also performed binary classiﬁcation task removing neutral label. resulted senrecent approaches sequential lstms encode premise hypothesis respectively apply neural attention reason logical relationship furthermore rockt¨aschel show non-standard encoder-decoder architecture processes hypothesis conditioned premise results signiﬁcantly boosts performance. similar approach tackle task lstmns. speciﬁcally lstmns read premise hypothesis match comparing hidden state tapes. perform average pooling hidden state tape lstmn concatenate averages form input -layer neural network classiﬁer relu activation function. used pre-trained glove vectors initialize word embeddings. out-of-vocabulary words initialized randomly gaussian samples updated vectors ﬁrst epoch word embeddings updated normally. dropout rate selected used adam optimization momentum parameters respectively initial learning rate mini-batch size fair comparison previous work report results different hidden/memory dimensions compared variants model different types lstms speciﬁcally include model encodes premise hypothesis independently lstms shared lstm word-by-word attention model matching lstm model sequentially processes hypothesis position tries match current word attention-weighted representation premise also compared models bag-of-words baseline averages pre-trained embeddings words sentence concatenates create features logistic regression classiﬁer lstmns achieve better performance compared exception ct-lstm operates tree-structured network topologies constituent trees. comparison also report performance paragraph vector model table second block) neither operates trees sequences learns distributed document representations parameterized directly. results table show -layer lstmns outperform lstm baselines achieving numbers comparable state art. number layers models comparable previously published results. ﬁne-grained binary classiﬁcation tasks -layer lstmn performs close best system t-cnn figure shows examples intra-attention sentiment words. interestingly network learns associate sentiment important words though fantastic good. ability reason semantic relationship sentences integral part text understanding. therefore evaluate model recognizing textual entailment i.e. whether premise-hypothesis pairs entailing contradictory neutral. task used stanford natural language inference dataset contains premisehypothesis pairs target labels indicating relation. removing sentences unknown labels pairs training development testing. vocabulary size average sentence length performed lower-casing tokenization entire dataset. models test concatenation lstm lstm-att mlstm lstmn lstmn shallow fusion lstmn deep fusion lstmn shallow fusion lstmn deep fusion lstmn shallow fusion lstmn deep fusion table parameter counts |θ|m size hidden unit model accuracy natural language inference task. lstms also observe fusion generally beneﬁcial deep fusion slightly improves shallow fusion. explanation deep fusion inter-attention vectors recurrently memorized decoder gating operation also improves information network. standard training deep fusion yields state-of-the-art performance task. although encouraging result interpreted caution since model substantially parameters compared related systems. could compare different models using number total parameters. however would inevitably introduce biases e.g. number hyper-parameters would become different. paper proposed machine reading simulator address limitations recurrent neural networks processing inherently structured input. model based long short-term memory architecture embedded memory network explicitly storing contextual representations input tokens without recursively compressing them. importantly intra-attention mechanism employed memory addressing induce undirected relations among tokens. attention layer optimized direct supervision signal entire network downstream tasks. experimental results across three tasks show model yields performance comparable although experiments focused lstms idea building structure aware neural models general applied types networks. direct supervision provided similar architectures adapted tasks dependency parsing relation extraction. future hope develop linguistically plausible neural architectures able reason nested structures neural models learn discover compositionality weak indirect supervision. thank members ilcc school informatics anonymous reviewers helpful comments. support european research council award number translating multiple modalities text gratefully acknowledged.", "year": 2016}