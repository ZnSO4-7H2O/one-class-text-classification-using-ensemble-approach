{"title": "Adversarial Machine Learning at Scale", "tag": ["cs.CV", "cs.CR", "cs.LG", "stat.ML"], "abstract": "Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a \"label leaking\" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.", "text": "adversarial examples malicious inputs designed fool machine learning models. often transfer model another allowing attackers mount black attacks without knowledge target model’s parameters. adversarial training process explicitly training model adversarial examples order make robust attack reduce test error clean inputs. adversarial training primarily applied small problems. research apply adversarial training imagenet contributions include recommendations succesfully scale adversarial training large models datasets observation adversarial training confers robustness single-step attack methods ﬁnding multi-step attack methods somewhat less transferable singlestep attack methods single-step attacks best mounting black-box attacks resolution label leaking effect causes adversarially trained models perform better adversarial examples clean examples adversarial example construction process uses true label model learn exploit regularities construction process. shown machine learning models often vulnerable adversarial manipulation input intended cause incorrect classiﬁcation particular neural networks many categories machine learning models highly vulnerable attacks based small modiﬁcations input model test time problem summarized follows. let’s machine learning system input sample call clean example. let’s assume sample correctly classiﬁed machine learning system i.e. ytrue. it’s possible construct adversarial example perceptually indistinguishable classiﬁed incorrectly i.e. ytrue. adversarial examples misclassiﬁed often examples perturbed noise even magnitude noise much larger magnitude adversarial perturbation adversarial examples pose potential security threats practical machine learning applications. particular szegedy showed adversarial example designed misclassiﬁed model often also misclassiﬁed model adversarial example transferability property means possible generate adversarial examples perform misclassiﬁcation attack machine learning system without access underlying model. papernot papernot demonstrated attacks realistic scenarios. shown injecting adversarial examples training could increase robustness neural networks adversarial examples. another existing approach defensive distillation train network however prior work studies defense measures relatively small datasets like mnist cifar. concurrent work studies attack mechanisms imagenet focusing question well adversarial examples transfer different types models focus defenses studying well different types adversarial example generation procedures transfer relatively similar models. successfully used adversarial training train inception model imagenet dataset signiﬁcantly increase robustness adversarial examples generated fast gradient sign method well one-step methods. demonstrated different types adversarial examples tend different transferability properties models. particular observed adversarial examples harder resist using adversarial training less likely transferrable models. showed models higher capacity tend robust adversarial examples compared lower capacity model architecture. provides additional could help building robust models. also observed interesting property call label leaking. adversarial examples constructed single-step method making true labels easier classify clean adversarial examples adversarially trained model learn exploit regularities adversarial example construction process. suggests using adversarial example construction processes make true label. rest paper structured follows section review different methods generate adversarial examples. section describes details adversarial training algorithm. finally section describes experiments results adversarial training. clean image unmodiﬁed image dataset adversarial image output procedure intended produce approximate worst-case modiﬁcation clean image. sometimes call candidate adversarial image emphasize adversarial image necessarily misclassiﬁed neural network. misclassiﬁed adversarial image candidate adversarial image misclassiﬁed neural network. addition typically interested misclassiﬁed adversarial images corresponding clean image correctly classiﬁed. size adversarial perturbation. cases require norm perturbation less done goodfellow always specify terms pixel values range note work adversarial examples minimizes size perturbation rather imposing constraint size perturbation one-step methods adversarial example generation generate candidate adversarial image computing gradient. often based ﬁnding optimal perturbation linear approximation cost model. iterative methods apply many gradient updates. typically rely approximation model typically produce harmful adversarial examples iterations. method simple computationally efﬁcient compared complex methods like lbfgs however usually lower success rate. imagenet top- error rate candidate adversarial images fgsm one-step target class methods fgsm ﬁnds adversarial perturbations increase value loss function. alternative approach maximize probability speciﬁc target class ytarget unlikely true class given image. neural network cross-entropy loss lead following formula one-step target class method basic idea adversarial training inject adversarial examples training continually generating adversarial examples every step training adversarial training originally developed small models batch normalization. scale adversarial training imagenet recommend using batch normalization successfully found important examples grouped batches containing normal adversarial examples taking training step described algorithm loss single example true class total number training examples minibatch; number adversarial examples minibatch parameter controls relative weight adversarial examples loss. used read minibatch training generate adversarial examples clean examples using current state network make minibatch training step network using minibatch fraction weight adversarial examples used minibatch differs huang authors replaced entire minibatch adversarial examples. however experiments done smaller datasets case adversarial training lead decrease accuracy clean images. found approach works better imagenet models observed training networks become robust speciﬁc value therefore recommend choosing randomly independently training example. experiments achieved best results magnitudes drawn truncated normal distribution deﬁned interval underlying normal distribution adversarially trained inception model imagenet. experiments done using synchronous distributed training machines minibatch examples machine. observed network tends reach maximum accuracy around iterations. continue training beyond iterations eventually accuracy might decrease fraction percent. thus experiments around iterations used obtained accuracy ﬁnal result experiment. looked interaction adversarial training forms regularization default training inception model uses three them. noticed disabling label smoothing and/or dropout leads small decrease accuracy clean examples small increase accuracy adversarial examples hand reducing weight decay leads decrease accuracy clean adversarial examples. experimented delaying adversarial training iterations. case used clean examples ﬁrst training iterations iterations included clean adversarial examples minibatch. noticed delaying adversarial training almost effect accuracy clean examples sufﬁcient number training iterations time noticed larger delays adversarial training might cause decline accuracy adversarial examples high magnitude adversarial perturbations. small delay changes accuracy statistically signiﬁcant recommend used delay allowed reuse partially trained model starting point many different experiments. experimented adversarial training using several types one-step methods. found adversarial training using type one-step method increases robustness types one-step adversarial examples tested. however still accuracy clean adversarial examples could vary depending combination methods used training evaluation. adversarial training caused slight decrease accuracy clean examples imagenet experiments. differs results adversarial training reported previously adversarial training increased accuracy test possible explanation adversarial training acts regularizer. datasets labeled examples overﬁtting primary concern adversarial training reduces test error. datasets like imagenet state-of-the-art models typically high training error adding regularizer like adversarial training increase training error decreases training test error. results suggest adversarial training employed scenarios model overﬁtting regularizer required. security adversarial examples concern. case adversarial training method provides security known defense losing small amount accuracy. comparing different one-step methods adversarial training observed best results terms accuracy test achieved using step l.l. step rnd. method. moreover using methods helped model become robust adversarial examples generated one-step methods. thus ﬁnal experiments used step l.l. adversarial method. table accuracies adversarially trained network clean images adversarial images various test-time training evaluation done using step l.l. method. adversarially training caused baseline model become robust adversarial examples lost accuracy clean examples. therefore also trained deeper model additional inception blocks. deeper model beneﬁts adversarial training terms robustness adversarial perturbation loses less accuracy clean examples smaller model does. results adversarial training using step l.l. method provided table seen table able signiﬁcantly increase top- top- accuracy adversarial examples make accuracy clean images. however lost accuracy clean examples. able slightly reduce accuracy clean images slightly increasing size model. done adding additional inception blocks model. speciﬁc details inception blocks refer szegedy table accuracy adversarially trained network iterative adversarial examples. adversarial training done using step l.l. method. results computed iterations training. overall training one-step adversarial examples confer resistance iterative adversarial examples. also tried iterative adversarial examples training however unable gain beneﬁts computationally costly able obtain robustness adversarial examples prevent procedure reducing accuracy clean examples signiﬁcantly. possible much larger models necessary achieve robustness large class inputs. discovered label leaking effect model trained fgsm adversarial examples evaluated using fgsm adversarial examples accuracy adversarial images becomes much higher accuracy clean images effect also occurs using one-step methods require true label input. label speciﬁc example leaked model classiﬁes adversarial example correctly adversarial example generated using true label misclassiﬁes corresponding adversarial example created without using true label. many labels leaked accuracy adversarial examples might become bigger accuracy clean examples observed imagenet dataset. believe effect occurs one-step methods true label perform simple predictable transformation model learn recognize. adversarial example construction process thus inadvertently leaks information true label input. found effect vanishes adversarial example construction processes true label. effect also vanishes iterative method used presumably output iterative process diverse less predictable output one-step process. overall label leaking effect recommend fgsm methods deﬁned respect true class label evaluate robustness adversarial examples; recommend one-step methods directly access label instead. recommend replace true label likely label predicted model. alternately maximize cross-entropy full distribution predicted labels given clean input distribution predicted labels given perturbed input revisited adversarially trained mnist classiﬁer goodfellow found leaks labels. labels leaked mnist data model leaks labels test examples. however amount label leaking small compared amount error caused adversarial examples. error rate adversarial examples exceeds error rate clean examples explains label leaking effect noticed earlier. figure inﬂuence size model classiﬁcation accuracy various adversarial examples. left column base model without adversarial training right column model adversarial training using step l.l. method. results step l.l. adversarial images middle results iter. l.l. adversarial images bottom results basic iter. adversarial images. text section explanation meaning horizontal vertical axes. table effect label leaking adversarial examples. training evaluation done using fgsm accuracy adversarial examples higher clean examples. effect happening training evaluation done using step l.l. method. experiments training done iterations initial learning rate experiment picked scale factor multiplied number ﬁlters convolution words means unchanged inception means inception half usual number ﬁlters convolutions chosen trained independent models adversarial training another without. evaluated accuracy clean adversarial examples trained models. experiments earlier experiments found deeper models beneﬁt adversarial training. increased depth changed many aspects model architecture. experiments varying examine effect controlled setting architecture remains constant except number feature maps layer. experiments observed accuracy clean images kept increasing increase though increase slowed became bigger. thus measure robustness used ratio accuracy adversarial images accuracy clean images increase ratio means accuracy adversarial clean images becomes smaller. ratio reaches accuracy adversarial images clean ones. successful adversarial example construction technique would never expect ratio exceed since would imply adversary actually helpful. defective adversarial example construction techniques suffering label leaking inadvertently produce ratio greater models without adversarial training observed optimal value yielding best robustness. models large small perform worse. indicate models become robust adversarial examples become large enough overﬁt respect. adversarially trained models found robustness consistently increases increases model size. able train large enough models process ends models twice normal size accuracy ratio approaching one-step adversarial examples. evaluated iterative adversarial examples trend toward increasing robustness increasing size remains exceptions. also none models large enough approach accuracy ratio regime. security perspective important property adversarial examples tend transfer model another enabling attacker black-box scenario create adversarial table transfer rate adversarial examples generated using different adversarial methods perturbation size equivalent error rate attack scenario attacker preﬁlters adversarial examples ensuring misclassiﬁed source model deploying target. transfer rates rounded nearest percent order table page. following models used comparison inception models different random initializations inception model activations instead relu inception model. also table absolute error rate attack preﬁltered rather transfer rate adversarial examples. figure inﬂuence size adversarial perturbation transfer rate adversarial examples. transfer rate computed using inception models different random intializations. could seen plots increase leads increase transfer rate. noted transfer rate ratio number transferred adversarial examples number successful adversarial examples source network. numerator denominator ratio increasing increase however observed numerator increasing much faster compared increase denominator. example increases relative increase denominator less considered methods time relative increase numerator studied transferability adversarial examples following models copies normal inception inception inception uses activation instead relu. models independently trained scratch achieved maximum accuracy. experiment ﬁxed source target networks constructed adversarial examples randomly sampled clean images test using source network performed classiﬁcation using source target networks. experiments done independently different adversarial methods. measured transferability using following criteria. among images picked misclassiﬁed adversarial example source model measured fraction misclassiﬁed target model. seen results fgsm adversarial examples transferable iter l.l. least. hand iter l.l. method able fool network cases fgsm least likely fool network. suggests might inverse relationship transferability speciﬁc method ability method fool network. haven’t studied phenomenon further possible explanation could fact iterative methods tend overﬁt speciﬁc network parameters. addition observed considered methods transfer rate increasing increase thus potential adversary performing black-box attack incentive higher increase chance success attack. paper studied increase robustness adversarial examples large models trained large dataset showed adversarial training provides robustness adversarial examples generated using one-step methods. adversarial training didn’t help much iterative methods observed adversarial examples generated iterative methods less likely transferred networks provides indirect robustness black adversarial attacks. addition observed increase model capacity could also help increase robustness adversarial examples especially used conjunction adversarial training. finally discovered effect label leaking resulted higher accuracy fgsm adversarial examples compared clean examples network adversarially trained. references battista biggio igino corona davide maiorca blaine nelson nedim ˇsrndi´c pavel laskov giorjoint giacinto fabio roli. evasion attacks machine learning test time. european conference machine learning knowledge discovery databases springer nilesh dalvi pedro domingos sumit sanghai deepak verma adversarial classiﬁcation. proceedings tenth sigkdd international conference knowledge discovery data mining takeru miyato shin-ichi maeda masanori koyama nakae shin ishii. distributional smoothing virtual adversarial training. international conference learning representations april papernot mcdaniel goodfellow. transferability machine learning phenomena black-box attacks using adversarial samples. arxiv e-prints http//arxiv.org/abs/.. nicolas papernot patrick drew mcdaniel somesh ananthram swami. distillation defense adversarial perturbations deep neural networks. corr abs/. http//arxiv.org/abs/.. nicolas papernot patrick drew mcdaniel goodfellow somesh berkay celik ananthram swami. practical black-box attacks deep learning systems using adversarial examples. corr abs/. http//arxiv.org/abs/.. olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein imagenet large scale visual recognition challenge. arxiv preprint arxiv. christian szegedy wojciech zaremba ilya sutskever joan bruna dumitru erhan goodfellow fergus. intriguing properties neural networks. iclr abs/. http//arxiv.org/abs/.. christian szegedy vincent vanhoucke sergey ioffe jonathon shlens zbigniew wojna. rethinking inception architecture computer vision. corr abs/. http//arxiv.org/abs/.. addition fgsm step l.l. methods explored several one-step adversarial methods training evaluation. generally methods separated large categories. methods maximize loss ﬁrst category. second category contains methods maximize probability speciﬁc target class also tried different types random noise instead adversarial images random noise didn’t help robustness adversarial examples. random normal variable zero mean identity covariance matrix. random truncated normal perturbation zero mean standard deviation deﬁned uncorrelated pixels leads following formula perturbed images overall observed using single step methods adversarial training sufﬁcient gain robustness them. fig. shows accuracy various one-step adversarial examples network trained using step l.l. method. time observed one-step methods equally good adversarial training shown table best results obtained adversarial training done using step l.l. step rnd. methods. figure comparison different one-step adversarial methods eval. adversarial training done using step l.l. method. evaluation methods show increasing accuracy increasing part curve label leaking effect. section contains details regarding inﬂuence size model robustness adversarial examples. provide additional figure shows robustness calculated using accuracy. generally exhibits properties corresponding plots accuracy. section contains results transfer rate various adversarial examples models. addition transfer rate computed misclassiﬁed adversarial examples also interesting observe error rate candidate adversarial examples generated model classiﬁed model. result might interesting models following attack. instead trying pick good adversarial images adversary tries modify available images order much misclassiﬁed images possible. compute error rate randomly generated adversarial images using source model classiﬁed using target model. results various models adversarial methods table comparison different one-step adversarial methods adversarial training. evaluation training steps. cases except fast grad fast grad evaluation done using fgsm. fast grad fast grad evaluation done using step l.l. method. case training testing done fgsm performance adversarial examples artiﬁcially high label leaking effect. based table recommend using step rnd. step l.l. method generating adversarial examples training time order obtain good accuracy clean adversarial examples. computed conﬁdence intervals based standard error mean around test error using fact test error evaluated samples. within column indicate methods statistically tied best using bold face. adversarial training fgsm fast predicted class fast entropy step rnd. step l.l. fast grad. fast grad. sign random perturbation random normal perturbation table error rates adversarial examples transferred models rounded nearest percent. results provided adversarial images generated using different adversarial methods ﬁxed perturbation size following models used comparison inception models different random initializations inception model activations instead relu inception model. also table transfer rate adversarial examples rather absolute error rate. evaluated robustness adversarial examples network trained using various nonlinear activation functions instead standard relu activation used adversarial training step l.l. adversarial images. tried following activation functions instead relu training converged using activations however test performance necessarily relu. tanh reludecayβ=. lose accuracy clean examples step l.l. adversarial examples. relu reludecayβ=. reludecayβ=. demonstrated similar accuracy relu clean images percent loss accuracy step l.l. images. time non-linear activation functions increased classiﬁcation accuracy iterative adversarial images. detailed results provided table figure inﬂuence size adversarial perturbation error rate adversarial examples generated model classiﬁed using another model. source target models inception networks different random intializations. table activation functions robustness adversarial examples. activation function adversarially trained network step l.l. adversarial images classiﬁcation clean images adversarial images generated using various adversarial methods relu relu reludecay. reludecay. reludecay. tanh relu relu reludecay. reludecay. reludecay. tanh relu relu reludecay. reludecay. reludecay. tanh overall noticed increase lead increase accuracy adversarial examples decrease accuracy clean examples. half adversarial examples minibatch provide signiﬁcant improvement accuracy adversarial images however lead additional decrease accuracy table results adversarial training depending number adversarial examples minibatch. adversarial examples training evaluation generated using step l.l. method. adv‘ baseline result without adversarial training rows ‘adv results adversarial training adversarial examples minibatch. total minibatch size thus correspond minibatch without clean examples.", "year": 2016}