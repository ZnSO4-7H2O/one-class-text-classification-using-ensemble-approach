{"title": "A Benchmarking Environment for Reinforcement Learning Based Task  Oriented Dialogue Management", "tag": ["stat.ML", "cs.CL", "cs.NE"], "abstract": "Dialogue assistants are rapidly becoming an indispensable daily aid. To avoid the significant effort needed to hand-craft the required dialogue flow, the Dialogue Management (DM) module can be cast as a continuous Markov Decision Process (MDP) and trained through Reinforcement Learning (RL). Several RL models have been investigated over recent years. However, the lack of a common benchmarking framework makes it difficult to perform a fair comparison between different models and their capability to generalise to different environments. Therefore, this paper proposes a set of challenging simulated environments for dialogue model development and evaluation. To provide some baselines, we investigate a number of representative parametric algorithms, namely deep reinforcement learning algorithms - DQN, A2C and Natural Actor-Critic and compare them to a non-parametric model, GP-SARSA. Both the environments and policy models are implemented using the publicly available PyDial toolkit and released on-line, in order to establish a testbed framework for further experiments and to facilitate experimental reproducibility.", "text": "dialogue assistants rapidly becoming indispensable daily aid. avoid signiﬁcant effort needed hand-craft required dialogue dialogue management module cast continuous markov decision process trained reinforcement learning several models investigated recent years. however lack common benchmarking framework makes difﬁcult perform fair comparison different models capability generalise different environments. therefore paper proposes challenging simulated environments dialogue model development evaluation. provide baselines investigate number representative parametric algorithms namely deep reinforcement learning algorithms natural actor-critic compare non-parametric model gp-sarsa. environments policy models implemented using publicly available pydial toolkit released on-line order establish testbed framework experiments facilitate experimental reproducibility. recent years large improvements achieved automatic speech recognition natural language understanding machine learning techniques dialogue systems gained much attention academia industry. directions intensively researched open-domain chat-based systems task-oriented dialogue systems former cover non-goal driven dialogues general topics. latter assist users achieve speciﬁc goals natural language making attractive interface small electronic devices. speech-driven scenario spoken dialogue systems typically based modular architecture consisting input processing modules dialogue management modules output processing modules speech synthesis). domain deﬁned ontology structured representation database system deﬁning requestable slots informable slots database entries part dialogue architecture explained schematically figure appendix module core component modular controlling conversational dialogue. traditional approaches mostly based handcrafted decision trees covering possible dialogues outcomes. however approach scale larger domains resilient noisy inputs resulting errors. therefore data-driven methods proposed learn policy automatically either corpus dialogues direct interaction human users supervised learning used learn dialogue policy training policy model \"mimic\" responses observed training corpora approach however several shortcomings. spoken dialogue scenario training corpora guaranteed represent optimal behaviour. effect selecting action future course dialogue considered result sub-optimal behaviour. addition large size dialogue state space training dataset lack sufﬁcient coverage. tackle issues mentioned above task frequently formulated planning problem solved using reinforcement learning framework system learns trial-and-error process governed potentially delayed reward signal. therefore module learns plan actions order maximise ﬁnal outcome. recent advances gaussian process based deep methods signiﬁcant progress data-driven dialogue modelling showing general algorithms policy gradients q-learning achieve good performance challenging dialogue scenarios however contrast domains lack common testbed spoken dialogue made difﬁcult compare different algorithms. recent advancements largely inﬂuenced release benchmarking environments allow fair comparison made different algorithms operating similar conditions. spirit based recently released pydial multi-domain tool-kit paper aims provide testbed environments developing evaluating dialogue models. account large variability different scenarios environments span different size domains different user behaviours different input channel performance. provide baselines evaluation representative reinforcement learning algorithms presented. benchmark environment implementations available on-line allowing development implementation evaluation algorithms tasks. last decade several reinforcement learning algorithms applied task dialogue policy optimization however evaluations algorithms hard compare mostly lack common benchmark environment. addition usually evaluated environments making hard assess potential generalise different environments. ﬁelds video game playing continuous control release common benchmarking environments great stimulus research area leading achievements human level game playing beating world champion game historically common testbed dialogue policy optimisation task. several reasons this. first unlike supervised learning tasks using corpus dialogues train algorithm used bootstrapping phase. however corpus used evaluate ﬁnal outcome dialogue learning agent involves sequential observation feedback generated operating world. feedback conditioned action agent itself. therefore different policies generate different sequences observations. training testing policies directly interacting real users proposed however system complexity time high cost make approach infeasible large part research community. addition hard control extraneous factors modify behaviour users mood tiredness making fair assessment difﬁcult. cope problems simulated users proposed. models approximate behaviour real users along input channel noise introduced errors. however development processing modules needed create simulated dialogue environment requires effort. even though simulated environments publicly available cover small dialogue domains lack consistency across prohibits wide-scale testing. need common testbed dialogue task known issue dialogue community initiatives dialogue state tracking challenges prominent challenges possible thanks clear evaluation metric. recently babi dialogue tasks dstc create testbed end-to-end text based dialogue management. however tasks focused either end-to-end supervised learning based question answering tasks reward signal delayed steps time. dialogue management cast continuous composed continuous multivariate belief state space ﬁnite actions reward function belief state probability distribution possible states. given time agent observes belief state executes action agent receives reward drawn policy deﬁned function probability takes action state policy value function corresponding deﬁned discount factor one-step reward. objective reinforcement learning optimal policy i.e. policy maximizes value function belief state. equivalently estimate unique optimal value function corresponds optimal policy. cases goal optimal policy maximises discounted total return framework environment encompasses every part system outside control agent. modular environment every part system except policy itself. classical approaches policy module acts agent rest modules constitute environment however various ways proposed train policy jointly modules using example state tracker policy trained jointly approaches train policy module jointly learn query database policy together train models system jointly paper focus classical approach policy optimised reinforcement learning. design features impact environment. example reduce action space full actions clustered summary actions addition often desirable sdss constraint actions system take turn usually done deﬁning action masks i.e. heuristics reduce number actions take dialogue state action masks also speeds learning. however heuristics must carefully deﬁned system designer since poor design summary actions masks lead suboptimal policies. addition domain determines state space size action size well inﬂuencing several modules. appendix schematic example summary action mapping action mask deﬁnition slot based ontology. summary dialogue environment several sources variability domain user behaviour input channel output channel action masks summary actions database access mechanism. robust dialogue policy able generalise conditions. section algorithms used benchmarking described. detailed explanations methods adapted dialogue management found general algorithms divided classes value-based policy gradient methods. value-based methods. value-based methods usually estimate q-value function approximation given belief state action form one-step reward time policy deﬁned greedily action maximizes policy gradient methods. value-based models often suffer divergence problems using function approximation. happens optimize value space following greedy policy. therefore slight change value function estimate lead large change policy space however directly parametrize policy adjust parameters maximize expected reward expectation taken respect possible dialogue trajectories start initial belief state provide baselines investigate representative parametric algorithms namely deep reinforcement learning algorithms episodic natural actor-critic models compare non-parametric algorithm gp-sarsa table presents main characteristics four algorithms. pydial pydial open-source statistical spoken dialogue system toolkit provides domainindependent implementations dialogue system modules shown figure well simulated users simulated error models. therefore toolkit potential create benchmark environments compare different algorithms conditions. main focus pydial task-oriented dialogue user matching entity based number constraints. example system needs provide user description laptop store meets speciﬁc user requirements. work pydial used deﬁne different environments conﬁguration ﬁles specify environments provided paper. benchmarking tasks rl-based research typically evaluated single small environments. tests reveal much capability algorithms generalise different settings prone overﬁtting speciﬁc cases. test capability algorithms different environments tasks deﬁned spans wide range environments across number dimensions domain. ﬁrst dimension variability environments application domain. three ontologies databases differing sizes deﬁned representing information seeking tasks restaurants cambridge francisco generic shopping task laptops slot-based ontologies dialogue state factorised slots table provides summary characteristics domain. table description domains. third column represents number database search constraints user deﬁne fourth number information slots user request given database entry ﬁfth number values requestable slot. input error. second dimension variability comes channel simulation modelling. pydial modelled semantic level whereby true user corrupted noise generate n-best-list associated conﬁdence scores semantic error rate three different values simulating different noise levels speech understanding input channel. user model. third dimension variability comes user model. even parameters model sampled beginning dialogue distribution parameters sampled different. addition standard parameter sampling distribution deﬁne unfriendly distribution users barely provide extra information system. masking mechanism. finally order test learning capability algorithms action masking mechanism provided pydial disabled tasks. total user model/error model/action mask environments deﬁned representing environments masks deactivated them. moreover parameters user behaviour model sampled beginning dialogue simulating situation every interaction conducted unique user. parameter sampling distributions deﬁned standard unfriendly. thus summarised table total different tasks deﬁned evaluating algorithm. simulated user input channel user behaviour modelled agenda-based simulator provides semantic-level interactions actions taken dialogue conditioned parameters sampled user model. re-sampled beginning dialogue ensure unique proﬁle every dialogue. user model parameters range parameters sampled provided pydial conﬁguration ﬁle. semantic error rate introduced noisy speech channel simulated error model parameters learned real data model parameters tasks rule-based dialogue state tracker. factorises dialogue state distribution different slots deﬁned ontology plus several general slots track dialogue meta-data e.g. whether user presented entity. slot |vs| values also deﬁned ontology. detailed description state tracker refer summary actions action masks action deﬁned summary actions consists slot independent actions slot dependent actions making total actions number slots requestable slots mapping summary master actions based simple heuristics dependent belief state case action masks similar heuristics used. slot dependent actions heuristics depend distribution values slot slot independent actions masks depend general method slot tracks user conducting database search. masks slot independent actions dependent value slot model hyperparameters gp-sarsa uses linear kernel state space delta kernel action space. scale responsible degree exploration remaining parameters futher improvements overall performance obtained gaussian kernel optimized hyperparameters however explored here. unlike gpsarsa trade-off exploration exploitation handled automatically deep-rl models dependent number training dialogues. exploration schedule often critical factor obtaining good learning performance. \u0001-greedy policy used follows linear scheduling starting annealed dialogues optimal initial value found grid search values deep-rl policy models composed hidden feedforward layers. objective paper models generalise across environments hyper-parameters models across tasks kept same. hyperparameters exception size hidden layers initial tuned grid search. table presents hyperparameters best models across domain deep-rl algorithms selected grid search combinations hyperparameters. adam optimiser used train deep-rl models initial learning rate detailed description hyperparameters every implemented model speciﬁed pydial conﬁguration ﬁles provided task. handcrafted policy addition algorithms described table performance classic handcrafted policy interacting environment also evaluated. actions taken policy based carefully designed heuristics dependent belief state reward function performance metrics maximum dialogue length turns discount factor metrics presented next section average success rate average reward evaluated policy model. success rate deﬁned percentage dialogues completed successfully i.e. whether dialogue manager able fulﬁll user goal not. final reward deﬁned success indicator dialogue length turns. table reward success rates training dialogues policy models considered benchmark. represents different tasks. highest reward obtained data driven model highlighted. evaluation results tasks presented table task every model trained different random seeds evaluated training dialogues. models evaluated test dialogues results shown averaged seeds. addition evaluation results training dialogues shown appendix learning curves task shown appendix results clearly show domain complexity plays crucial role overall performance. value-based methods achieve best performance domain across environmental settings. value-based methods known higher learning rate. might lead overﬁtting larger domains domains small action state spaces higher learning rate helps achieve good policy faster policy gradient based methods. hand enac provides best performance tasks suggesting policy-gradient methods scales robustly larger state action spaces. action masks signiﬁcantly reduce size action space thus increase policy learning rate. however environments action masks deactivated policygradient methods learn much slower. contrast value-based approaches still maintain reasonable performance indicating sample-efﬁcient policy-based methods. however worth noting highly unstable especially larger domains thanks non-parametric approach pattern observed gp-sarsa. noted earlier mainly optimisation performed value space rather directly policy space. addition dialogues performance enac decreases environments. might hyperparameters optimised dialogues. extensive grid search could solve problem. performance every model drops substantially noise introduced semantic input. results tasks show however enac robust partially observed environments thus degrades less methods. reason that contrary deep-rl methods natural gradient points directly desired goal less prone getting stuck local plateaus thereby learning better policies noisy environments. could expected interacting unfriendly users task degrades performance. however performance drop smaller enac rest models. suggests policy ability learn faster guide dialogue user less prone provide information goal. gpsarsa consistently performs well showing stable performance fast learning rate overall best model across tasks domains terms learning rate ﬁnal performance followed closely enac. shows worse results contrary applications ability perform asynchronous learning less useful signiﬁcantly raises training costs real users. also observed deep-rl models prone overﬁtting. furthermore algorithms sensitive hyper-parameter values. lastly worth noting handcrafted policy model outperforms rl-based policies almost tasks larger domains showing rl-based models difﬁculties learn large state spaces. mitigate issue state space abstraction hierarchical reinforcement learning approaches used. cross-tasks evaluation examine generalisation capabilities various algorithms performed crosstask evaluations. chose three tasks namely test algorithms trained noisy environment perform zero noise set-up vice versa. table presents results gpsarsa clarity omit results since algorithm performed substantially worse. results show enac strongest generalisation capabilities best performance cross-task environments. value-based models good performance trained noisy data tested clean data getting close performance enac. however trained clean data tested noisy data performance greatly decreases especially larger domains. decrease performance severe gpsarsa. knowledge ﬁrst work present extensive simulated dialogue management environments along comparison several algorithms using open-domain toolkit. results show large amount improvement still necessary data driven models match performance handcrafted policies especially larger domains. environments presented paper however still constrained compared real world tasks future plan include multi-domain environments word-level user simulations would enable dialogue managers trained realistic environments. also environments implemented open domain toolkit offering possibility research community algorithms tasks. research partly funded epsrc grant ep/m/ open domain statistical spoken dialogue systems. paweł budzianowski supported epsrc council toshiba research europe cambridge research laboratory. pei-hao supported cambridge trust ministry education taiwan. benchmark available on-line http//www.camdial.org/ pydial/benchmarks/. paweł budzianowski stefan ultes pei-hao nikola mrkši´c tsung-hsien inigo casanueva lina rojas barahona milica gaši´c. sub-domain modelling dialogue management hierarchical reinforcement learning. proc sigdial inigo casanueva heidi christensen thomas hain phil green. adaptive speech recognition dialogue management users speech disorders. fifteenth annual conference international speech communication association inigo casanueva thomas hain heidi christensen ricard marxer phil green. knowledge transfer speakers personalised dialogue management. proceedings annual meeting special interest group discourse dialogue pages chen pei-hao milica gaši´c. hyper-parameter optimisation gaussian process reinforcement learning statistical dialogue management. sigdial conference pages duan chen rein houthooft john schulman pieter abbeel. benchmarking deep reinforcement learning continuous control. proceedings international conference machine learning pages gaši´c fabrice lefevre jurˇcíˇcek simon keizer francois mairesse blaise thomson steve young. back-off action selection summary space-based pomdp dialogue systems. automatic speech recognition understanding asru ieee workshop pages ieee milica gasic steve young. gaussian processes pomdp-based dialogue manager optimization. ieee/acm transactions audio speech language processing milica gaši´c filip jurcicek blaise. thomson steve young. on-line policy optimisation spoken dialogue systems live interaction human subjects. ieee asru james henderson oliver lemon kallirroi georgila. hybrid reinforcement/supervised learning dialogue policies communicator data. ijcai workshop knowledge reasoning practical dialogue systems pages simon keizer milica gaši´c filip jurˇcíˇcek françois mairesse blaise thomson steve young. parameter estimation agenda-based user simulation. proceedings annual meeting special interest group discourse dialogue pages association computational linguistics esther levin roberto pieraccini wieland eckert. using markov decision process learning dialogue strategies. acoustics speech signal processing proceedings ieee international conference volume pages ieee lison kennington. opendial toolkit developing spoken dialogue systems probabilistic rules. proceedings annual meeting association computational linguistics pages berlin germany association computational linguistics. volodymyr mnih koray kavukcuoglu david silver alex graves ioannis antonoglou daan wierstra martin riedmiller. playing atari deep reinforcement learning. arxiv preprint arxiv. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature nikola mrkši´c diarmuid séaghdha blaise thomson milica gaši´c pei-hao david vandyke tsung-hsien steve young. multi-domain dialog state tracking using recurrent neural networks. proceedings olivier pietquin thierry dutoit. probabilistic framework dialog simulation optimal strategy learning. ieee transactions audio speech language processing olivier pietquin steve renals. system modeling automatic evaluation optimization dialogue systems. acoustics speech signal processing ieee international conference volume pages ieee olivier pietquin matthieu geist senthilkumar chandramohan hervé frezza-buet. sampleefﬁcient batch reinforcement learning dialogue management optimization. transactions speech language processing jost schatzmann blaise thomson karl weilhammer steve young. agenda-based user simulation bootstrapping pomdp dialogue system. human language technologies conference north american chapter association computational linguistics; companion volume short papers pages association computational linguistics iulian vlad serban alessandro sordoni yoshua bengio aaron courville joelle pineau. building end-to-end dialogue systems using generative hierarchical neural network models. aaai pages david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature pei-hao pawel budzianowski stefan ultes milica gasic steve young. sampleefﬁcient actor-critic reinforcement learning supervised data dialogue management. proceedings sigdial richard sutton david mcallester satinder singh yishay mansour policy gradient methods reinforcement learning function approximation. proceedings nips volume blaise thomson simon keizer françois mairesse steve young. natural beliefcritic reinforcement algorithm parameter estimation statistical spoken dialogue systems. interspeech blaise thomson milica gasic matthew henderson pirros tsiakoulis steve young. nbest error simulation training spoken dialogue systems. spoken language technology workshop ieee pages ieee stefan ultes lina rojas-barahona pei-hao david vandyke dongho iñigo casanueva paweł budzianowski nikola mrkši´c tsung-hsien milica gaši´c steve young. pydial multi-domain statistical dialogue system toolkit. demo. association computational linguistics oriol vinyals timo ewalds sergey bartunov petko georgiev alexander sasha vezhnevets michelle alireza makhzani heinrich küttler john agapiou julian schrittwieser starcraft challenge reinforcement learning. arxiv preprint arxiv. zhuoran wang tsung-hsien pei-hao yannis stylianou. learning domainindependent dialogue policies ontology parameterisation. sigdial conference pages ziyu wang victor bapst nicolas heess volodymyr mnih remi munos koray kavukcuoglu nando freitas. sample efﬁcient actor-critic experience replay. arxiv preprint arxiv. tsung-hsien david vandyke nikola mrksic milica gasic lina rojas-barahona pei-hao stefan ultes steve young. network-based end-to-end trainable task-oriented dialogue system. eacl jason williams iker arizmendi alistair conkie. demonstration at&t let’s production-grade statistical spoken dialog system. spoken language technology workshop ieee pages ieee jason williams kavosh asadi geoffrey zweig. hybrid code networks practical efﬁcient end-to-end dialog control supervised reinforcement learning. association computational linguistics xuesong yang yun-nung chen dilek hakkani-tür paul crook xiujun jianfeng deng. end-to-end joint learning natural language understanding dialogue manager. acoustics speech signal processing ieee international conference pages ieee steve young milica gaši´c simon keizer françois mairesse jost schatzmann blaise thomson hidden information state model practical framework pomdp-based spoken dialogue management. computer speech language tiancheng zhao maxine eskenazi. towards end-to-end learning dialog state tracking management using deep reinforcement learning. arxiv preprint arxiv. figure dialogue single turn modular sds. turn begins user action processed input channel converted n-best list conﬁdence scores. n-best list used state tracker update dialogue state. dialogue state used several modules system decision process first action masks deﬁned based dialogue state. then policy choses optimal summary action based dialogue state action masks. finally summary action converted master action using heuristics based dialogue state. then system outputs action cycle begins architecture details figure performance benchmarked algorithms function number dialogues three different domains; shaded area depicts mean standard deviation different random seeds. table reward obtained three best performing algorithms cross-tasks evaluation. rows represent environment model trained columns environment evaluated. rows represent model columns domain model trained tested. reward best performing model cross-environment setup domain combination highlighted.", "year": 2017}