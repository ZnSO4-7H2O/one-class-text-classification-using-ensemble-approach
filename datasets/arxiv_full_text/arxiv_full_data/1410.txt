{"title": "Learning Important Features Through Propagating Activation Differences", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "The purported \"black box\"' nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. A detailed video tutorial on the method is at http://goo.gl/qKb7pL and code is at http://goo.gl/RM8jvH.", "text": "tions nonlinearities deeplift reveal dependencies missed approaches. deeplift scores computed using backpropagation-like algorithm obtained efﬁciently single backward pass network prediction made. approaches make perturbations individual inputs neurons observe impact later neurons network. zeiler fergus occluded different segments input image visualized change activations later layers. in-silico mutagenesis introduced virtual mutations individual positions genomic sequence quantiﬁed impact output. zintgraf proposed clever strategy analyzing difference prediction marginalizing input patch. however methods computationally inefﬁcient perturbation requires separate forward propagation network. also underestimate importance features saturated contribution output figure perturbation-based approaches gradient-based approaches fail model saturation. illustrated simple network exhibiting saturation signal inputs. point perturbing either produce change output. note gradient output w.r.t inputs also zero purported black nature neural networks barrier adoption applications interpretability essential. present deeplift method decomposing output prediction neural network speciﬁc input backpropagating contributions neurons network every feature input. deeplift compares activation neuron ‘reference activation’ assigns contribution scores according difference. optionally giving separate consideration positive negative contributions deeplift also reveal dependencies missed approaches. scores computed efﬁciently single backward pass. apply deeplift models trained mnist simulated genomic data show signiﬁcant advantages gradient-based methods. detailed video tutorial method http//goo.gl/qkbpl code http//goo.gl/rmjvh. neural networks become increasingly popular black reputation barrier adoption interpretability paramount. here present deeplift novel algorithm assign importance score inputs given output. approach unique regards ﬁrst frames question importance terms differences ‘reference’ state ‘reference’ chosen user according appropriate problem hand. contrast gradient-based methods using difference-from-reference allows deeplift propagate importance signal even situations gradient zero avoids artifacts caused discontinuities gradient. second optionally giving separate consideration effects positive negative contribucontrast perturbation methods backpropagation approaches computationally efﬁcient propagate importance signal output neuron backwards layers towards input single pass. deeplift belongs family approaches. simonyan proposed using gradient output w.r.t. pixels input image compute saliency image context image classiﬁcation tasks. authors showed similar deconvolutional networks except handling nonlinearity rectiﬁed linear units backpropagating importance using gradients gradient coming relu backward pass zero’d input relu forward pass negative. contrast backpropagating importance signal deconvolutional networks importance signal coming relu during backward pass zero’d negative regard sign input relu during forward pass. springenberg combined approaches guided backpropagation zero’s importance signal relu either input relu forward pass negative importance signal backward pass negative. guided backpropagation thought equivalent computing gradients caveat gradients become negative backward pass discarded relus. zero-ing negative gradients guided backpropagation deconvolutional networks fail highlight inputs contribute negatively output. additionally none three approaches would address saturation problem illustrated fig. gradient w.r.t. negative gradient w.r.t zero discontinuities gradients also cause undesirable artifacts bach proposed approach propagating importance scores called layerwise relevance propagation shrikumar kindermans showed absent modiﬁcations deal numerical stability original rules equivalent within scaling factor elementwise product saliency maps simonyan input experiments compare deeplift gradient input latter easily implemented whereas currently implementations available knowledge. gradient input often preferable gradients alone leverages sign strength input still address saturation problem fig. thresholding artifact fig. instead computing gradients current value input integrate gradients inputs scaled starting value current value addressess saturation thresholding problems fig. fig. numerically obtaining high-quality integrals adds computational overhead. further approach still give highly misleading results grad-cam computes coarsegrained feature-importance associating feature maps ﬁnal convolutional layer particular classes based gradients class w.r.t. feature using weighted activations feature maps indication inputs important. obtain ﬁne-grained feature importance authors proposed performing elementwise product between scores obtained grad-cam scores obtained guided backpropagation termed guided grad-cam. however strategy inherits limitations guided backpropagation caused zero-ing negative gradients backpropagation. also speciﬁc convolutional neural networks. deeplift explains difference output ‘reference’ output terms difference input ‘reference’ input. ‘reference’ input represents default ‘neutral’ input chosen according appropriate problem hand formally represent target output neuron interest represent neurons intermediate layer layers necessary sufﬁcient compute represent reference activation deﬁne quantity difference-from-reference deeplift assigns contribution scores c∆xi∆t s.t. idea partial derivatives partial derivative inﬁnitesimal change caused inﬁnitesimal change divided inﬁnitesimal change multiplier similar spirit partial derivative ﬁnite differences instead inﬁnitesimal ones. assume input layer neurons hidden layer neurons target output neuron given values m∆xi∆yj m∆yj following deﬁnition m∆xi∆z consistent summation-to-delta property refer chain rule multipliers. given multipliers neuron immediate successors compute multipliers neuron given target neuron efﬁciently backpropagation analogous chain rule partial derivatives allows compute gradient w.r.t. output backpropagation. formulating deeplift rules described section assume reference neuron activation reference input. formally neuron inputs given reference activations inputs calculate reference activation output choice reference input critical obtaining insightful results deeplift. practice choosing good reference would rely domain-speciﬁc knowledge cases best compute deeplift scores multiple different references. guiding principle what interested measuring differences against?. mnist reference input all-zeros background images. binary classiﬁcation tasks sequence inputs obtain sensible results using either reference input containing expected frequencies acgt background averaging results multiple reference inputs sequence generated shufﬂing original sequence call summation-to-delta property. c∆xi∆t thought amount difference-fromreference attributed ‘blamed’ difference-from-reference note neuron’s transfer function well-behaved output locally linear inputs providing additional motivation zero. alc∆xi∆t non-zero even lows deeplift address fundamental limitation gradients because illustrated fig. neuron signaling meaningful information even regime gradient zero. another drawback gradients addressed deeplift illustrated fig. discontinuous nature gradients causes sudden jumps importance score inﬁnitesimal changes input. contrast difference-from-reference continuous allowing deeplift avoid discontinuities caused bias terms. figure discontinuous gradients produce misleading importance scores. response single rectiﬁed linear unit bias gradient gradient×input discontinuity gradient×input assigns contribution bias term contributions bias term contrast difference-from-reference gives continuous increase contribution score. multipliers using deﬁnition section {wi∆xi gives m∆x+ m∆x+ setting multipliers case would consistent summation-to-delta possible nonzero case setting multiplier would fail propagate importance them. avoid this m∆x+ appendix compute multipliers using standard neural network ops. rule applies nonlinear transformations take single input relu tanh sigmoid operations. neuron nonlinear transformation input input summation-to-delta c∆x∆y consequently m∆x∆y rescale rule proportional follows case deﬁnition multiplier approaches derivative i.e. m∆x∆y evaluated thus gradient instead multiplier close reference avoid numerical instability issues caused small denominator. note rescale rule addresses saturation thresholding problems illustrated fig. fig. giving m∆h∆y case fig. assuming c∆x∆y m∆x∆y giving m∆x∆y contrast gradient×input assigns contribution bias term important note gradient×input implicitly uses reference all-zeros similary integrated gradients requires user specify starting point integral conceptually similar specifying reference deeplift. guided backprop pure gradients don’t reference argue limitation methods describe local behaviour output speciﬁc input value without considering output behaves range inputs. section that situations essential treat positive negative contributions differently. this every neuron introduce represent positive negative components that present rules assigning contribution scores neuron immediate inputs. conjunction chain rule multipliers rules used contributions input target output backpropagation. words average impact terms added added average impact terms added added. thought shapely values contributing considering impact positive terms absence negative terms impact negative terms absence positive terms alleviate issues arise positive negative terms canceling out. case fig. revealcancel would assign contribution inputs revealcancel rule also avoids saturation thresholding pitfalls illustrated fig. fig. circumstances might prefer rescale rule. speciﬁcally consider thresholded relu merely indicates noise would want assign contributions mitigate noise. revealcancel assign nonzero contributions considering absence vice versa. using backpropagation approaches described section would result importance assigned either exclusively revealcancel rule assigns importance inputs. case softmax sigmoid outputs prefer compute contributions linear layer preceding ﬁnal nonlinearity rather ﬁnal nonlinearity itself. would avoid attentuation caused summation-to-delta property described section example consider sigmoid output logit sigmoid function. assume output saturates close contributions respectively. however output still close revealed previous work connection deeplift shapely values. brieﬂy shapely values measure average marginal effect including input possible orderings inputs included. deﬁne including input setting actual value instead reference value deeplift thought fast approximation shapely values. time lundberg cited preprint deeplift described linear rescale rules separate treatment positive negative contributions. rescale rule improves upon simply using gradients still situations provide misleading results. consider operation depicted fig. reference values using rescale rule importance would assigned either obscure fact inputs relevant operation. understand occurs consider case linear rule calculate c∆i∆h c∆i∆h rescale thus rule c∆i∆h m∆h∆h c∆i∆h c∆i∆h m∆h∆h c∆i∆h total contribution output becomes total contribution −c∆i∆h calculation misleading discounts fact c∆i∆h would words ignores dependency induced comes canceling nonlinear neuron similar failure occurs rescale rule results c∆i∆o c∆i∆o note gradients gradient×input guided backpropagation integrated gradients would also assign importance either given input gradient zero address treating positive negative contributions separately. consider nonlinear neuron instead assuming proportional m∆x+∆y+ m∆x−∆y− m∆x∆y deﬁne follows contributions misleading comparing scores across different inputs stronger contribution logit would always translate higher deeplift score. avoid this compute contributions rather adjustments softmax layers compute contributions linear layer preceding softmax rather softmax output issue could arise ﬁnal softmax output involves normalization classes linear layer softmax not. address this normalize contributions linear layer subtracting mean contribution classes. formally number classes c∆x∆ci represents unnormalized contribution class linear layer represents normalized contribution have train convolutional neural network mnist using keras perform digit classiﬁcation obtain test-set accuracy. architecture consists convolutional layers followed fully connected layer followed softmax output layer used convolutions stride instead pooling layers result drop performance consistent previous work deeplift integrated gradients used reference input zeros. evaluate importance scores obtained different methods design following task given image originally belongs class identify pixels erase convert image target class ﬁnding sxidiff sxico sxict erasing pixels ranked descending order sxidiff sxidiff evaluate change log-odds score classes original image image pixels erased. shown fig. deeplift revealcancel rule outperformed backpropagation-based methods. integrated gradients computed numerically either intervals produced results comparable figure deeplift revealcancel rule better identiﬁes pixels convert digit another. result masking pixels ranked important original class relative target class importance scores class also shown. selected image highest change log-odds scores conversion using gradient*input integrated gradients rank pixels. bottom boxplots increase log-odds scores target original class mask applied images belonging original class testing set. integrated gradients-n refers numerically integrating gradients evenly-spaced intervals using midpoint rule. other suggesting adding intervals would change result. integrated gradients also performed comparably gradient*input suggesting saturation thresholding failure modes common mnist data. guided backprop discards negative gradients backpropagation perhaps explaining poor performance discriminating classes. also explored using rescale rule instead revealcancel various layers found degraded performance next compared importance scoring methods applied classiﬁcation tasks sequence inputs human genome millions sequence elements containing speciﬁc combinations short functional words regulatory proteins bind regulate gene activity. binding figure deeplift revealcancel gives qualitatively desirable behavior tal-gata simulation. scatter plots importance score strength motif match different tasks methods region motif matches plotted. x-axes log-odds score motif match background. y-axes total importance assigned match speciﬁed task. dots regions gata motifs inserted simulation; blue gata-only tal-only black motifs inserted. deeplift-fc-rc-conv-rs refers using revealcancel rule fully-connected layer rescale rule convolutional layers appears reduce noise relative using revealcancel layers. proportion strong matches motif regions containing gata total score task guided backprop×inp deeplift revealcancel false negatives guided backprop false positives task named gata instances given motif inserted random non-overlapping positions sequences. trained multi-task neural network convolutional layers global average pooling fullyconnected layer binary classiﬁcation tasks. positive labeled sequences task represented both gata present task represented gata present sequences task represented present. gata motifs gata motifs details simulation network architecture predictive performance given appendix deeplift integrated gradients afﬁnity speciﬁc collections short words problem computational genomics discovery motifs regulatory elements give rise distinct molecular signatures measured experimentally. here order benchmark deeplift competing methods uncover predictive patterns sequences design simple simulation captures essence motif discovery problem described above. background sequences length generated sampling letters acgt position probabilities respectively. motif instances randomly sampled previously known probabilistic motif models used reference input expected frequencies acgt position fair comparison reference also used gradient×input guided backprop×input sequence inputs found guided backprop×input performed better vanilla guided backprop; thus used former. given particular subsequence possible compute log-odds score subsequence sampled particular motif originating background distribution acgt. evaluate different importancescoring methods found matches motif sequence test well total importance allocated match different importance-scoring methods task. results shown fig. appendix ideally expect importance scoring method show following properties high scores motifs task scores task higher scores corresponding stronger log-odds matches; analogous pattern gata motifs high scores gata motifs task higher scores sequences containing kinds motifs sequences containing kind observe guided backprop×input fails assigning positive importance task fails property failing identify cooperativity task guided backprop×input gradient×input show suboptimal behavior regarding property sudden increase importance log-odds score around little differentiation higher logodds scores result guided backprop×input gradient×input assign unduly high importance weak motif matches practical consequence thresholding problem fig. large discontinuous jumps gradient also result inﬂated scores relative methods. explored three versions deeplift rescale rule nonlinearities revealcancel nonlinearities rescale rule convolutional layers revealcancel fully connected layer contrast results mnist found deeplift-fcrc-conv-rs reduced noise relative pure revealcancel. think noise-suppression property discussed section convolutional layers like motif detectors input convolutional neurons represent noise importance propagated gradient×inp integrated gradients deeplift-rescale occasionally miss relevance task corrected using revealcancel fully connected layer note revealcancel scores seem tiered. illustrated appendix related multiple instances given motif sequence figure revealcancel highlights gata motifs task representations gata motif motif used simulation scores example sequence containing gata motifs. letter height reﬂects score. blue location embedded gata motif green location embedded motif. underline chance occurrence weak match gata motifs highlighted task note using revealcancel fully-connected layer reduces noise compared using revealcancel layers. presented deeplift novel approach computing importance scores based explaining difference output ‘reference’ output terms differences inputs ‘reference’ inputs. using difference-from-reference allows information propagate even gradient zero could prove especially useful recurrent neural networks saturating activations like sigmoid tanh popular. deeplift avoids placing potentially misleading importance bias terms allowing separate treatment positive negative contributions deeplift-revealcancel rule identify dependencies missed methods open questions include apply deeplift rnns compute good reference empirically data best propagate importance ‘max’ operations beyond simply using gradients. references bach sebastian binder alexander montavon gr´egoire klauschen frederick m¨uller klaus-robert samek wojciech. pixel-wise explanations non-linear classiﬁer decisions layer-wise relevance propagation. plos july kindermans pieter-jan schtt kristof mller klausrobert dhne sven. investigating inﬂuence noise distractors interpretation neural networks. corr abs/. https //arxiv.org/abs/..", "year": 2017}