{"title": "Are all training examples equally valuable?", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "When learning a new concept, not all training examples may prove equally useful for training: some may have higher or lower training value than others. The goal of this paper is to bring to the attention of the vision community the following considerations: (1) some examples are better than others for training detectors or classifiers, and (2) in the presence of better examples, some examples may negatively impact performance and removing them may be beneficial. In this paper, we propose an approach for measuring the training value of an example, and use it for ranking and greedily sorting examples. We test our methods on different vision tasks, models, datasets and classifiers. Our experiments show that the performance of current state-of-the-art detectors and classifiers can be improved when training on a subset, rather than the whole training set.", "text": "learning concept training examples prove equally useful training higher lower training value others. goal paper bring attention vision community following considerations examples better others training detectors classiﬁers presence better examples examples negatively impact performance removing beneﬁcial. paper propose approach measuring training value example ranking greedily sorting examples. test methods different vision tasks models datasets classiﬁers. experiments show performance current state-of-the-art detectors classiﬁers improved training subset rather whole training set. developing object detection system ﬁrst challenge involves choosing training dataset instance currently popular datasets standard practice treat training examples equally feeding full training learning algorithm. indeed training data becomes available detector performance tend increase. illustrated fig. average performance plotted function number training examples. however learning concept training examples prove equally useful training. instance mislabeled inaccurately demarcated examples actually hurt performance classiﬁer thus less valuable training. interestingly even examples correctly segmented labeled figure training examples train class pascal take larger subsets training performance test grow train classiﬁer increasing sizes consider examples ranked greater lower training value observe different trend performance ﬁrst grows reaches maximum drops converging performance achieved training full training set. depict examples higher training value lower training value. equally valuable training model given concept instance model handle occlusions matching algorithm based model could hurt partially occluded truncated training examples. case adding training examples large amounts occlusion actually hurt rather improve performance ﬁnal classiﬁer. curve fig. illustrates performance changes examples used black curve sorted according training value. performance plot demonstrates training examples sorting ﬁrst performance increases faster increasing training size training examples random order. interestingly performance achieved using subset training actually higher using entire training set. seen plot maximal performance achieved best subset climbs performance achieved full training set. phenomenon example rather show paper often occurs training models classiﬁers different state-of-the-art datasets. goal bring issues attention vision community. goal propose paper deﬁnition measuring training value example deﬁnition ranking greedily sorting examples. testing methods different vision tasks models datasets classiﬁers experiments show performance current state-of-the-art detectors classiﬁers improved training subset rather whole training set. although training full training common practice notable exceptions. angelova propose method pruning training show effectiveness removing outliers hard-to-learn examples. within context face recognition task introduce outliers training test algorithm. train multiple classiﬁers using different partitions training identify examples create disagreement across classiﬁers labeling troublesome. experiments introduce artiﬁcial noise show current datasets contain large number hard-to-learn examples current recognition models prunning beneﬁcial. past approaches tried select training examples based effectiveness training model. common setting active learning whereby data unlabeled algorithm selects training examples label step highest gains performance. thus principled ordering examples reduce cost labeling lead faster increases performance function amount data available active learning approaches focus learning hardest examples ﬁrst curriculum learning instead advocates opposite strategy learning ﬁrst easy examples. curriculum learning however requires manual ranking examples always clear obtain robust learning algorithms provide alternative differentially treating training examples assigning different weights different training examples learning ignore outliers example felzenszwalb shown treating bounding boxes noisy observations provide signiﬁcant improvements perforfigure different performance different subsets training data. cars subsampled pascal train+validation sets. train instances test pascal test achieve black curve however train cars orange instead full training higher corresponding orange curve. testing cars blue produces worst curve methods pruning training examples designed work speciﬁc learning techniques. boosting vezhnevets barinova proposed algorithm eliminates training examples correctly classiﬁed training considering confusing examples. showed approach effective improving classiﬁcation performance classes large degree overlap. svms loss function changed become robust outliers training examples recently found off-the-shelf detectors trained clean subset training achieve higher performance full set. similarly razavi showed excluding small clusters training could improve performance. however best clean subset open problem. pavlopoulou used human performance metric weighting examples terms difﬁculty improve performance scene recognition task. robust learning algorithms designed remove noisy examples. however different problem address here. work current vision datasets highly curated contain reduced amount noise. goal paper show that context ﬁxed model examples valuable others learning concept. paper argue important issue determining differentiate training examples help confuse classiﬁer. training examples equal. experiment fig. demonstrates points make throughout paper. consider small dataset cars subsampled train+validation pascal fig. training detector using linear features. following standard protocol train detector full dataset test pascal test images containing cars curve black notice however train cars demarcated orange boxes obtain orange curve higher peformance alternatively train cars demarcated blue boxes curve lowest performance blue. thus detectors learn different subsets training data vary performance certain subsets produce detector performs better trained data. notice cars orange prototypical class seems intuitive would able learn better model them. directly measure value example training detector measure rank training examples? given particular task measuring value training example task treating separate classiﬁer. demonstration trained separate model example train+validation pascal category evaluated dataset. similar concept exemplarsvms performance exemplar detector assign training value example. examples high scores likely representative positive examples training set. ﬁrst column fig. shows training examples used train lda-based exemplar detector. next column depicts models detector followed detections training models. rows ﬁgure correspond training examples produce best detectors evaluated training set. next rows correspond training examples sampled uniformly best worst. notice examples produce best detectors evaluated training last ones generalize well. idea formally deﬁne training value example. consider binary classiﬁcation task training dataset particular type classiﬁcation functions positive training sample. task deﬁne training value according training average performance obtained ˆfxi training classiﬁer learned example negative examples thus measure absolute training value figure shows example category templates lda-based exemplar detector trained example windows highest detection scores training models. examples organized bottom according detecting buses. examples correspond examples produce best detectors rest examples sampled uniformly best worst. selecting training examples features relevant others learning instances informative others. section discuss relationship subset selection feature selection propose strategies subset selection inspired algorithms feature selection. experiments presented next section demonstrate proposed methodologies produce subsamplings whole training data show improved performance detectors classiﬁers training full training set. large body work feature selection classiﬁcation prediction. focus usually improving accuracy running time classiﬁer well providing better understanding data selecting best subset features. since interested selecting best subset examples problem seen dual feature selection problem. instance case linear decision function given data point rows training examples vector weights corresponding examples feature selection remove non-valuable columns example selection interested removing non-valuable rows relationship motivates adopting feature selection algorithms example selection. feature selection approaches classiﬁed embedded ﬁlter wrapper methods embedded methods perform feature selection training thus constrained model learning algorithm. filter methods evaluate feature independently order figure sort training data according methods evaluate performance model using incrementally larger subsets data. plotted classes largest improvements using greedy obtain subset training data testing object detection object detection linear scene classiﬁcation linear scene classiﬁcation kernel svm. summarize rest classes plotted absolute improvement performance achieved pascal classes scenes. also portion training used reach peak performance object detection scene classiﬁcation. choose best subset. approach simple beneﬁt independent model training algorithm; however enforce diversity feature set. wrapper methods utilize learning algorithm black score subsets features based predictive discriminative score. subset features greedily grown shrunk according deﬁnition training value introduced section obtain ranking examples. type sorting seen dual version ﬁlter method feature selection. starting sorting propose greedy forward methodology selecting training examples. greedy algorithm seen dual wrapper methods feature selection. case grow subset valuable examples assessing power model trained them. method treats detector classiﬁer black provides general tool used many situations. main disadvantage greedy approach computational cost. however goal paper demonstrate improvement performance gained dataset selection irrespective subsampling algorithm used. details greedy strategy found algorithm methodologies described section show achieve gains performance taking subsets whole dataset. experiments carried object detection scene classiﬁcation tasks. test state-of-the-art datasets features classiﬁers input full training output greedy sorting training initialization sort training examples decreasing order according training value split sorted training batches equal size batches remain figure conﬁdent chair bird detections pascal test detector trained full training pascal subset selected greedy forward method correct detections false alarms training+validation test testing set. detection frameworks respectively. fig. .a-f summarizes results object detection. fig. .a-c shows results obtained fig. .d-f obtained fig. .a.c show test changes train data. incrementally batches data sorted decreasing training value plotted green. furthermore also plot results obtained sorting data using greedy method baseline include random sorting examples black peak performance achieved training data added. fact adding data past certain point actually causes performance drop. plot classes show improvement using entire training set. fig. .b.e summarize absolute improvement performance classes sorted decreasing order improvement. fig. .c.f plot percent training data used reach maximum improvement performance classes. observe random sorting peak usually reached training instances added. however informative sortings training instances used peak performance obtained subset whole data. notice peak always occurs using less data. however size subsample always larger suggests robust model able handle difﬁcult data. fact performance drops data fig. contains qualitative results. conﬁdent detections obtained test training model best subset chair bird classes much intuitive conﬁdent detections obtained training model full training method training value random sorting) linear kernel respectively. again include curves classes show improvement using entire training set. also show .h.k absolute improvement performance achieved class sorting classes decreasing order. fig. .i.l depicts percentage examples included best subset. observe that average subset training performs better training data. however differences smaller obtained object detection main reasons using stronger classiﬁer able learn larger variety appearances scene databases cleaner contain proportion fewer hard examples object detection databases. despite this selected examples look like prototypical members category worst examples chosen greedy ranking address question whether particular attributes make examples better others relative. show scenes prototypicality correlates training value. case objects truncated occluded examples tend harder learn from. cases visually cleanest examples easier classiﬁers detectors. however show harder examples objectively bad. difﬁculty example depends examples available training also model used complexity. prototypicality qualitative results hypothesis prototypicality feature instances high training value. test hypothesis sorted scene exemplars using human typicality rankings collected ehinger compared sorting ones proposed paper. plot prototypicality examples sorted according training value. compute training value models hogx kernel gist linear curves depict mean prototypicality sorted examples across classes scenes used. cases observe that general examples higher training value rated prototypical. .b.c improvement achieved hogx kernel gist linear respectively sorting samples randomly training value prototypicality. plots sorting training value leads larger gains performance sorting prototypicality. however result expected since training value deﬁned speciﬁcally capture quality training example classiﬁer. fact according training value evaluates intances independently best instances representative class. particular training examples fig. tend prototypical non-truncated non-occluded bottom training examples tend highly occluded truncated. hand greedy forward selection approach distinction high ranked examples intuitive diversity enters picture. example ranked greedy method handled model; diversity model. latter case otherwise clean training instances added would redundancy model. instance includes instances birds present pascal training+validation set. three included best subset obtained greedy method. suggests others likely cause redundancy. natural setting consider dining hall chairs ﬂock birds. likelihood instances similar appearance features viewpoint high scene recognition experiments used classes database training examples categories. image extract visual words hogx descriptors build image descriptor using spatial pyramid train linear kernel using histogram matching. provides state results database. train classiﬁer discriminate class versus negative examples fig. .g-l shows results scene recognition. order apply greedy method evaluate partition validation avoid overﬁtting happen kernel svms. plots follow structure case object detection. fig. .g.j show function number training examples training examples assigned highest typicality scores humans occur initial batches training data sorted training value. also plot absolute improvement performance achieved class taking subsets training sorted according different metrics training performed using hogx kernel gist linear svm. vary signiﬁcantly class next. hand observe across classes truncation correlated sorting instances using training value sorting. particular number truncated examples higher average later batches data ones added beginning. occlusion part visibility behave similarly. however look examples plotted ranking order speciﬁed greedy approach correlation features clean. words image features longer highly correlated training value. matches previous observations since greedy approach also considers diversity important feature training have. examples training value better whether examples affect classiﬁer negatively depends examples available training well model used. discussed section complex model harder hurt moreover absence better examples examples would otherwise excluded still useful training detector. illustrate this fig. shows happens reverse ranking provided greedy forward method train object detector examples lowest training value ﬁrst. peformance lower achieve training best subset even lower using subset randomly chosen training examples. however keep adding training examples performance creeps thus case examples used learn concept merely argue would make efﬁcient best learn concept thus informative sorting examples help obtain higher performance fewer training examples. figure scene recognition. three scene categories strongest effect show best worst examples selected greedy algorithm using kernel hogx. also show best worst examples scene category smallest improvement performance. number shown panel average prototypicality index provided humans database hoiem performed analysis pascal bounding annotations including truncation bounding area aspect ratio well manual annotations. focused understanding detector fails. attributes look different problem interested understanding makes good training example. respect detector trained full dataset. observe fig. experiment classes common pascal datasets. interestingly maximum performance observed subset pascal training set. supports idea classiﬁer trained subset data generalizes better. paper show examples better others training models classiﬁers. deﬁne measure training value example propose methods ranking examples based measure. experimentally show methods able subsets data perform better whole training terms testing set. particular show improve approach performance number state-of-the-art classiﬁers detectors features popular vision datasets observe examples negatively impact performance classiﬁer detector removing beneﬁcial training. although standard practice entire dataset training conclude study sample selection important issue taken account general vision community.", "year": 2013}