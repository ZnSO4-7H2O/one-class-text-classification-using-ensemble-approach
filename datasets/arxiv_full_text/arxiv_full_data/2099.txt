{"title": "Bayesian Optimization With Censored Response Data", "tag": ["cs.AI", "cs.LG", "stat.ML", "G.3; G.1.6"], "abstract": "Bayesian optimization (BO) aims to minimize a given blackbox function using a model that is updated whenever new evidence about the function becomes available. Here, we address the problem of BO under partially right-censored response data, where in some evaluations we only obtain a lower bound on the function value. The ability to handle such response data allows us to adaptively censor costly function evaluations in minimization problems where the cost of a function evaluation corresponds to the function value. One important application giving rise to such censored data is the runtime-minimizing variant of the algorithm configuration problem: finding settings of a given parametric algorithm that minimize the runtime required for solving problem instances from a given distribution. We demonstrate that terminating slow algorithm runs prematurely and handling the resulting right-censored observations can substantially improve the state of the art in model-based algorithm configuration.", "text": "bayesian optimization aims minimize given blackbox function using model updated whenever evidence function becomes available. here address problem partially right-censored response data evaluations obtain lower bound function value. ability handle response data allows adaptively censor costly function evaluations minimization problems cost function evaluation corresponds function value. important application giving rise censored data runtime-minimizing variant algorithm conﬁguration problem ﬁnding settings given parametric algorithm minimize runtime required solving problem instances given distribution. demonstrate terminating slow algorithm runs prematurely handling resulting rightcensored observations substantially improve state model-based algorithm conﬁguration. right-censored data—data lower bound measurement available—occurs several applications. example patient drops clinical study know lower bound survival time. cases actively decide censor certain data points order save time resources; example drug variant unsuccessful curing disease time known drug successful decide stop trial instead invest resources test variant here describe integrate censored observations bayesian optimization aims minimum blackbox function potentially noisy function available closed form queried arbitrary input values. proceeds phases constructing model using observed function values; using model select input next query. extend standard formulation blackbox function minimization include cost function measures cost obtaining function value given input. budget minimizing given limit cumulative cost function evaluations call resulting blackbox function minimization variant cost-varying. paper focus problems following cost monotonicity property. example function describe quickly different drug variants cure disease quickly plants reach desired size given different fertilizer variants; examples takes exactly time determine i.e. terminating function evaluation prematurely censoring threshold cost resulting censored data point also less informative obtain lower bound cost monotonicity also applies minimization objectives time energy consumption communication overhead strictly monotonic functions these. application domain motivating research following algorithm conﬁguration problem. given parameterized algorithm distribution problem instances performance metric capturing performance parameter settings instances eπ∼d] denote expected performance setting given samples distribution problem parameter setting solves minθ automated procedures solving recently substantial improvements state wide variety problem domains including sat-based formal veriﬁcation mixed integer programming automated planning traditional methods based heuristic search racing algorithms recently method smac shown compare favourably approaches. particularly important performance metric domain algorithm runtime solving problem instance given settings minimizing metric within given time budget cost monotonic problem algorithm runs also terminated prematurely yielding cheaper lower bound shown following exploiting cost monotonicity substantially improve state model-based algorithm conﬁguration. parameter setting observation training data censoring indicator various types models handle censored data. gaussian processes —the widely used tool bayesian optimization—one could approximations handle resulting non-gaussian observation likelihoods; example described laplace approximation handling right-censored data. here random forests shown yield better predictive performance high-dimensional predominantly discrete inputs typical algorithm conﬁguration following deﬁne predictive distribution model input across trees previously adapted handle censored data classical methods yield non-parametric kaplan-meier estimators lend bayesian optimization since undeﬁned beyond largest uncensored data point. introduce simple em-type algorithm ﬁlling censored values. denote probability density function cumulative density function standard normal distribution respectively. input observed censored value given gaussian predictive distribution deﬁned probability density function precisely draw required samples censored data point once using stratifying sampling. constructing random forest take bootstrap samples data points resample data point i-th data point tree resampled point total number times data point resampled tree resample data point index resampled data point tree tree regardless censoring status; leads zero multiple copies censored data point tree. keep track combined number copies data point obtain samples quantiles cumulative distribution. algorithm summarizes process pseudocode. lines assign bootstrap sample original data tree line initializes random forest uncensored data. then algorithm iterates imputing values censored data points re-ﬁtting trees uncensored data points individual trees’ imputed values censored data points implementation detail avoid potentially large outlying predictions known maximal runtime κmax ensure mean imputed value exceed κmax. compared imputing mean straight-forward adaptation schmee hahn’s algorithm modiﬁed version takes prior uncertainty account computing posterior predictive distribution thereby avoiding overly conﬁdent predictions. also emulate drawing joint samples censored data points order preserve predictive uncertainty mean multiple censored values. lines algorithm done assigning lower quantiles predictive distribution trees lower index higher quantiles ones higher index using mechanism preserves predictive uncertainty even mean imputed samples drawing sample independently would reduce uncertainty factor predictive distributions sampling-based algorithm visualized simple function figure note predictive variance censored data points collapse zero shown software engineering reasons actual implementation ﬁrst random forest using bootstrap )≥zi schmee hahn’s algorithm simply means imputing min{κmax mean. circles x-symbols denote uncensored right-censored function evaluations respectively. dotted line denotes mean prediction random forest model trees grey area denotes uncertainty. true function shown solid line expected improvement dashed line. figure schmee hahn’s procedure sampling version yield substantially lower error either dropping censored data points treating uncensored. preserving predictive uncertainty censored data points sampling method yields highest likelihoods. algorithm bayesian minimization blackbox functions right censoring input output input θinc minimal objective function value found within budget approach building regression trees node select interval select split point greedily minimize weighted within-node variance node’s children. instead selecting point sample uniformly random yields linear interpolation limit.) order minimize blackbox function bayesian optimization iteratively evaluates query point updates model uses model decide query point evaluate next. standard method trading exploration exploitation bayesian optimization select next query point maximize expected positive improvement minimal function value fmin seen far. denote mean variance predicted model input deﬁne fmin−µθ then obtain closed-form expression denote probability density function cumulative distribution function standard normal distribution respectively. usual maximize criterion across input space select next setting evaluate. bayesian optimization process change allow evaluations right-censored; difference model able handle censored data. algorithm gives pseudocode bayesian optimization case partial right censoring. variant problem face also pick censoring threshold query point willing evaluate function accept censored sample. obvious best choice increasing yields informative also costly data. here heuristically multiplicative factor times fmin. figure visualizes ﬁrst steps resulting bayesian optimization procedure minimization given blackbox function starting initial latin hypercube design. return algorithm conﬁguration problem motivating research. differs standard problems attacked bayesian optimization important ways importantly categorical input dimensions common inputs tend high dimensional; optimization objective marginal instances objective varies exponentially overhead ﬁtting using models taken account since part time budget available model-based method smac addresses issues including several modiﬁcations standard methods achieve state-of-the-art performance here improve smac setting censoring thresholds described building models resulting censored data described section compared modiﬁed version smac original version range challenging real-world conﬁguration scenarios optimizing parameters commercial mixed integer solver cplex different sets problem instances parameters industrial solver spear sets problem instances formal veriﬁcation smac allowed days maximum censoring time cplex/spear seconds. algorithm conﬁguration scenarios high maximum runtimes identiﬁed challenge smac demonstrate adaptive censoring technique substantially improved performance scenarios. performed conﬁguration runs problem domains versions smac conﬁguration recorded smac’s best found conﬁguration computed run’s test performance conﬁguration’s mean runtime test instances disjoint training sampled distribution. figure table show modiﬁed version smac censoring substantially outperformed original smac version without capping. modiﬁed version gave better results cases improvements median test performance reaching factor approach inspired adaptive capping method used algorithm conﬁguration procedure paramils indeed recover method slack factor allow slack factors greater improve model albeit expense costly data acquisition. algorithm sequential model-based optimization keeps track target algorithm runs performed performances on)}) smbo’s model θnew list promising conﬁgurations tselect runtimes required model select conﬁgurations respectively. input output optimized parameter conﬁguration θinc figure visual comparison smac’s performance without censoring; note y-axis runtime scale avoid clutter show capping slack factor performed independent runs conﬁguration procedure show boxplots test performances resulting ﬁnal conﬁgurations demonstrated censored data integrated effectively bayesian optimization proposed simple algorithm handling censored data random forests adaptively selected censoring thresholds data points small multiples best seen function values. application problem algorithm conﬁguration achieved substantial speedups state-of-the-art procedure smac. future work would like apply censoring gaussian processes actively select censoring threshold yield information time spent evaluate effectiveness censoring domains. table comparison smac without censoring conﬁgurator scenario report median test performance bold-faced entries conﬁgurators signiﬁcantly worse best conﬁgurator respective scenario based mann-whitney test would like thank summer coop student jonathan shen many useful discussions assistance cleaning matlab source code smac used experiments. thanks also steve ramage porting smac java afterwards improving usability well feedback earlier version report. fawcett helmert hoos karpas r¨oger seipp. fd-autotune domain-speciﬁc conﬁguration using fast-downward. proc. icaps workshop planning learning pages hutter hoos leyton-brown. bayesian optimization censored response data. nips workshop bayesian optimization sequential experimental design bandits published online.", "year": 2013}