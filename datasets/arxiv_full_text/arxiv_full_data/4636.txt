{"title": "The Local Optimality of Reinforcement Learning by Value Gradients, and  its Relationship to Policy Gradient Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "In this theoretical paper we are concerned with the problem of learning a value function by a smooth general function approximator, to solve a deterministic episodic control problem in a large continuous state space. It is shown that learning the gradient of the value-function at every point along a trajectory generated by a greedy policy is a sufficient condition for the trajectory to be locally extremal, and often locally optimal, and we argue that this brings greater efficiency to value-function learning. This contrasts to traditional value-function learning in which the value-function must be learnt over the whole of state space.  It is also proven that policy-gradient learning applied to a greedy policy on a value-function produces a weight update equivalent to a value-gradient weight update, which provides a surprising connection between these two alternative paradigms of reinforcement learning, and a convergence proof for control problems with a value function represented by a general smooth function approximator.", "text": "theoretical paper concerned problem learning value function smooth general function approximator solve deterministic episodic control problem large continuous state space. shown learning gradient value-function every point along trajectory generated greedy policy suﬃcient condition trajectory locally extremal often locally optimal argue brings greater eﬃciency value-function learning. contrasts traditional value-function learning value-function must learnt whole state space. also proven policy-gradient learning applied greedy policy value-function produces weight update equivalent value-gradient weight update provides surprising connection alternative paradigms reinforcement learning convergence proof control problems value function represented general smooth function approximator. reinforcement learning study agent learn actions maximise given reward function. example typical scenario robot wandering around environment time position vector ~xt. robot moves continuous space time discretize time enable modelling motion computer. thus time robot chooses action takes next state according environment’s model function ~xt+ gives immediate scalar reward given reward function robot keeps moving reaches designated terminal states. problem robot learn choose actions maximise total reward received σtrt. speciﬁcally problem policy function calculates action take given state total reward maximised. approach tackle problem assign score every point state space gives best possible total reward attainable starting state. scoring function called optimal value function function perfectly known would easy robot behave optimally instant could consider possible actions available always choose leads best valued state whilst also taking account immediate short-term reward getting there. acting called greedy policy learn represent optimal value function greedy policy approximated function. however optimal value function known start learning. given policy deﬁne value function real valued total reward would encountered robot started state followed policy termination. bellman’s optimality time slow since bellman’s condition still needs meeting entire state space optimality. even bellman’s condition perfectly satisﬁed along single trajectory performance extremely optimal bellman’s condition satisﬁed neighbouring trajectories too. hence well known community constant exploration environment must applied. exploration could provided stochastic model functions stochastic policy stochastic start point trajectory. ability algorithms work stochastic environments virtue also necessity reason goal paper deﬁne value-function learning methods work deterministic environment. although value function learning methods produced successes robot control value function learning methods problematic theoretical convergence guarantees function approximators limited. proven converge provided function approxfunction approximator used represent value function greedy policy used required robot problem. divergence examples exist non-linear function approximator bellman condition depends turn depends making progress learning undo progress learning other. second issue highly relevant control problems since ultimate objective learn value function ﬁxed policy improve policy becomes optimal thus successful convergence analysis value function address issues following method dual heuristic programming werbos tries explicitly learn gradient value function respect state vector i.e. instead directly. call method value gradient learning distinguish usual direct updates values value function refer value learning methods extend werbos’ method include bootstrapping parameter give algorithm call vgl. method addresses issue bellman equation needing solved whole state space turns necessary learn value gradient along single trajectory locally optimal. contrasts strongly methods need learn value function immediately neighbouring trajectories local optimality signiﬁcant eﬃciency gain method. optimality almost-immediate consequence pontryagin’s maximum principle proven section address diﬃculty analysing interdependence simultaneously updating showing dependency greedy policy value function primarily value-gradient. hence value-gradient analysis necessary level provide theoretical gateway analysing convergence properties value-function weight update uses greedy policy; weight update weight update. alternative paradigm called policy gradient learning rely learning value function all. deﬁne algorithms gradient ascent total reward deﬁnition includes methods methods natural convergence guarantees since hill climbing strategies function upper bound proved successful robot control continuous spaces section show weight update identical weight update makes theoretical connection provide theoretical insight convergence result longstanding problem proving convergence value-function learning methods function approximator providing theoretical link value-function learning pgl. method model based method requires model functions known. knowledge model functions delivers many beneﬁts method. many researchers deﬁne speciﬁcally case unknown model functions. answer would supplement method separate learning system speciﬁcally learn model functions prior trying learn policy. commonly used strategy successful methods example recent success maintaining inverted ﬂight helicopter model functions learned separately prior learning policy. also suggest model learning relatively straightforward part task since supervised learning problem immediate answers given. also model functions control problem often simple known laws physics change much point point continuous nature environment. exploits successfully learn model functions real time entirely robot travelling along single trajectory. however acknowledge ideally full learning system would concurrently learn model policy value function still ensuring convergence. paper successfully analyse items three list case expense assuming ﬁrst ﬁxed known. note third paradigm exists called actor-critic architecture. architecture function approximator represent second function approximator represent policy. greedy policy used. successful theoretical results exist concurrent uppaper organised follows. section deﬁnes algorithm gives deﬁnitions necessary this. concepts deﬁned approximated value function target values; approximated value gradient target. next sections give main theoretical results section prove local optimality objective single trajectory discuss eﬃciency method. section demonstrate connection hence give convergence proof certain conditions. short discussions follow main theoretical results. section presents conclusions work. section deﬁnes algorithm. preliminary deﬁnitions made section describe target values used deﬁne deﬁnition target values done concise diﬀers conventional literature allows deﬁne targets algorithm concepts readers experienced technical diﬃculty needs dealing saturated actions deﬁned section state space trajectories model functions. state space subset state state space denoted column vector state space large continuous function approximator necessary represent learned policy. trajectory list states state space starting given point trajectory parameterised actions chosen action space time steps according model. model comprised known smooth deterministic functions ﬁrst model function links state trajectory next given action markovian rule ~xt+ second model function gives immediate real-valued reward arriving next state ~xt+. states designated terminal states. assume trajectory guaranteed reach terminal state ﬁnite time example scenario like could aircraft limited fuel trying land; could navigation problem imposed time limit. particular trajectory label ﬁnal time step terminal state trajectory. note general trajectory dependent start point actions taken global constant. action vectors. action vectors real scalars several real components control dimensions agent. example action components might accelerator pedal position steering wheel angle brake pedal position. monorail train might scalar needed. assume action component real number that problems constrained constraints imposed action space policy. policy function parameterised vector generates actions function state. thus given trajectory generated given policy since policy pure function policy memoryless. vector holds parameters smooth function approximator example could concatenation column vector form weights neural network. value function. trajectory starts state follows policy reaching terminal state value function dependency distinguishes notate greedy policy general policy extend deﬁnition value function also apply greedy policy write greedy trajectory. greedy trajectory generated greedy policy. since greedy policy depends upon weight greedy policy tightly coupled; weight vector used greedy policy trajectory shorthand notation. given trajectory states actions function deﬁned state space appears numerator diﬀerential. upper indices indicate component vector matrix. example ∂~xt matrix column; column; column; column; element equal ∂~xi equal ∂~at ∂~at egt+ ~wit example product ∂~xij ~w∂~xij example second derivative scalar approximate value function. since quantity right-hand side greedy policy comes often deﬁne function speciﬁcally approximate value function deﬁned ﬁxed global constant calculate particular point state space necessary cache whole trajectory starting greedy policy work backwards along applying recursion; thus deﬁned points state space. make equal +evt+ coupled greedy policy satisﬁes bellman’s optipoint since dependent actions simple matter attain objective since changing move values slowly towards targets. greatly simpliﬁes analysis value functions valuegradients. deﬁned recursive form allows easily diﬀerentiate form target value-gradient hence deﬁne algorithms. would straightforward deﬁne algorithms using traditional formulation λ-return described appendix extra complication arises actions bounded e.g. constraints present action components. need handling carefully able diﬀerentiate policy function later paper. example action component represents steering wheel action component saturated steering wheel rotated full limit either direction pressure applied limit. formally greedy policy constraints present action component action component either conditions sometimes want refer unsaturated components action vector notation denote vector unsaturated components i.e. vector often lower dimension saturated components removed. saturated components follows deﬁnition saturated action components above imagine steering wheel fully turned right pressure applied. pressure applied inﬁnitesimal changes circumstances change position steering wheel. note that exist example multiple joint maxima respect inﬁnitesimal change function could cause ∂~ait ∂~ait ﬁrst implications true since saturated action ∂~ait deﬁnition recursive formula takes known target value-gradient works backwards along trajectory point trajectory alternatively using derivation similar lemma shown problems environment model functions make exist even though model functions designed diﬀerentiable. example agent boundary terminal state non-terminal state velocity zero depending goes next determine whether trajectory terminates not. hence total reward could vastly diﬀerent cases function diﬀerentiable respect point. bifurcation points hopefully rare state space problems. rare occurrences non-existence aﬀect main theoretical results paper since results talk consequences target value gradient exist. however certain problems would suitable method without reformulation example total reward deﬁned equal integer number time-steps trajectory. total reward function step function give useful derivative learning. even though time needs discretized simulation purposes calculation actual continuous value time would needed make useful diﬀerentiable total reward function. problem modiﬁed model functions accurately reﬂect underlying continuous time process method work. rule thumb problem suitable methods suitable work methods. along greedy trajectory. proven section objective suﬃcient ensure trajectory locally extremal often locally optimal. objective learning targets noted objective straightforward achieve since targets moving ones highly dependent hence must weight update slowly move approximated gradients towards targets seems inspired choice werbos included matrix since section spontaneously appears weight update giving explicit formula specify case suﬃces positive semi-deﬁnite. ﬁrst deﬁne total reward given trajectory irrespective policy used trajectory starting state following actions reaching terminal state given model total reward encountered given function ∂~at locally optimal trajectories. deﬁne trajectory parameterised values locally optimal local maximum respect parameters subject constraints action component locally extremal trajectories deﬁne trajectory parameterised values locally extremal action components possibility bound actions introduces extra complication saturated actions. second condition understood steering wheel analogy given action component saturated steering wheel fully turned right pressure implying would like turn even direction possible deﬁnition provides suﬃcient condition locally optimal trajectory. proof. proven induction. first note since deﬁned next gives ∂~at egt+ ∂~xt ∂~at ∂~xt ∂~xt ∂~at ∂~xt ∂~xt egt+ ∂~xt egt+ ∂~xt application lemma used fact that remark according bang-bang principle bang-bang control often arises situations model functions linear respect bound action vectors problem solved time minimisation problem. hence often case lets found method locally optimal. since weight update special case weight update speculated still open question time objective instance could always produce locally optimal trajectories. greedy policy forms maximum condition completes pontryagin’s conditions let. however still needs supplementing lemma applicable explicitly identiﬁes lets formulated proof allows derive corollary’s extra conclusion bang-bang control producing locally optimal trajectories. state space. implies methods much lesser requirement exploration methods since local part exploration comes free methods. mean provided learning algorithm makes progress towards achieving along greedy trajectory trajectory make progress bending towards locally optimal shape happen without need stochastic exploration. argue leads greater eﬃciency comparison failure without exploration deterministic environment dramatic common even valuefunction perfectly learned along single trajectory; examples given section requirement optimality proof episodic problems could relaxed introducing discount factor details) proof extended altering terminal step induction however lemma instead boundary condition limt→∞ γtg′ clear extend proof undiscounted non-episodic problem. model functions known backpropagation time follow here well suited deterministic systems. however studies literature designed stochastic environments unknown model functions form bptt merely eﬃcient implementation formula often cases policy provided neural-network also deﬁned general diﬀerentiable policy. note although equation looks quite diﬀerent regularly used equation mathematically equivalent since proven weight update equal equivalence show holds actions unbound. bound actions required could implemented indirectly applying exponential cost function model function penalise components close desired limits prohibiting greedy policy choosing actions beyond range. initially consider case where jectory also hence ∂~at reinterpreted greedy policy lemma greedy policy implies unbound action ∂~at proof. substitute gence guarantees apply conditions. certain simple problems always true signiﬁcantly always true continuous time setting value-gradient policy deﬁned used. conveniently policy also ensures actions always completely unsaturated condition equivalence. gt)t ωt). neither weight update true gradient descent unless policy ﬁxed i.e. non-greedy. example equations fewer terms would found diﬀerentiating error functions fully using chain rule describes missing terms ﬁxed policy even terms missing tightly-coupled greedy policy used even therefore learning progress measured either error function monotonic. pgl-vgl equivalence proof surprising shows anomalies weight update actually gradient ascent chosen carefully neatly solves monotonic progress problem tightly-coupled greedy policy. also surprising learn value-function weight updates fundamentally diﬀerent ﬁrst thought. known weight update applied greedy policy approximate value function would thing value function weight update; even course usually same unless particular choice chosen. equivalence creates diﬃculty distinguishing particular however describe weight update update; form deﬁned i.e. prior equivalence realised. used good choice since ensure monotonic progress respect conditions. however means sometimes small could slow learning down. alternative choices hence produce aggressive speed learning expense monotonic progress. less aggressive speed method gradient ascent conjugate gradients could used instead using identity matrix analysis successful tightly-coupled greedy policy value function unusual since analyses value-function updates literature applicable ﬁxed policy. interestingly using tightly-coupled value-function greedy policy necessary equivalence hold. architecture since greedy policy avoid need considering lemma result compared disadvantaged valid possible advantages method follows result thought apply trained completion state space every time policy changes requirement nested inside loop updates policy; whole thing quite computationally expensive. finally order proven local optimality learning target value gradients along greedy trajectory argued eﬃciency beneﬁts approach demonstrated equivalence pgl. research interested genuine theoretical challenges understanding value-function learning greedy policy regardless whether vgl; particularly greedy policy aﬀected work would like extend optimality proof section undiscounted non-episodic problems course somehow work extend convergence proof include unlikely weight update current form since divergence examples exist thus identical however since uses recursive notation easier diﬀerentiate enabling deﬁne λ-return provides equivalent formulation algorithm known forwards view proves equivalent weight update. give implementations algorithm produce identical weight update. algorithm faster requires storage whole trajectory batch-mode only. algorithm used on-line memory eﬃcient since store whole trajectory slower since requires manipulation eligibility trace matrix. algorithm makes direct implementation making foward pass trajectory storing states actions followed backward pass trajectory accumulating recursion implementation second order derivatives approximate value function required inner product vector. means neural network used function approximator methods analogous used matrix-vector products evaluated operations. means takes operations algorithm whole trajectory. algorithm accumulates weight updates single forward pass trajectory. requires storage trajectory memory eﬃcient requires time carry matrix multiplications. algorithm requires full matrix which neural network would take operations evaluate. hence slowest steps algorithm would matrix-matrix multiplications lines taking onw) operations. hence total time algorithm process full trajectory onw) operations. neither implementations section attempts learn value gradient ﬁnal time-step trajectory since prior knowledge target value gradient always zero terminal state. hence", "year": 2011}