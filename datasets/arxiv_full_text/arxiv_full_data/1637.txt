{"title": "Tree-Structured Neural Machine for Linguistics-Aware Sentence Generation", "tag": ["cs.AI", "cs.CL", "cs.LG"], "abstract": "Different from other sequential data, sentences in natural language are structured by linguistic grammars. Previous generative conversational models with chain-structured decoder ignore this structure in human language and might generate plausible responses with less satisfactory relevance and fluency. In this study, we aim to incorporate the results from linguistic analysis into the process of sentence generation for high-quality conversation generation. Specifically, we use a dependency parser to transform each response sentence into a dependency tree and construct a training corpus of sentence-tree pairs. A tree-structured decoder is developed to learn the mapping from a sentence to its tree, where different types of hidden states are used to depict the local dependencies from an internal tree node to its children. For training acceleration, we propose a tree canonicalization method, which transforms trees into equivalent ternary trees. Then, with a proposed tree-structured search method, the model is able to generate the most probable responses in the form of dependency trees, which are finally flattened into sequences as the system output. Experimental results demonstrate that the proposed X2Tree framework outperforms baseline methods over 11.15% increase of acceptance ratio.", "text": "work improves upon studies incorporating results linguistic analysis decoder. specifically leverage dependency parser transform response sentence dependency tree containing local dependency information. proposed model learns sentence canonicalized tree ﬂattened ﬁnal output. consider intermediate task automatic conversation generation. instead generating response given input post directly generate dependency parse tree corresponding response top-down fashion. additionally tree canonicalization method proposed aiming transforming trees different numbers children nodes equivalent form namely full ternary trees order accelerate training simplify model implementation gpu. then postprocessing step converts dependency tree sequence ﬁnal response. also theoretically prove ternary tree best choice model complexity. models also process trees bottom-up fashion. socher proposed max-margin structure prediction architecture based recursive neural networks demonstrated successfully parses sentences understands scene images. extended chain-structured lstm tree-structured lstm shown effective representing tree structure latent vector. models process trees bottom-up fashion children nodes recursively merged parent nodes root generated. however bottom-up models require leaf nodes predicted tree given advance. example generate constituency parse tree sentence tokens appeared given sentence used leaf nodes tree. similarly parse natural scene images image ﬁrst divided segments corresponds leaf node output tree. given leaves bottom-up process recursively processes internal nodes root built. different sequential data sentences natural language structured linguistic grammars. previous generative conversational models chain-structured decoder ignore structure human language might generate plausible responses less satisfactory relevance ﬂuency. study incorporate results linguistic analysis process sentence generation high-quality conversation generation. speciﬁcally dependency parser transform response sentence dependency tree construct training corpus sentencetree pairs. tree-structured decoder developed learn mapping sentence tree different types hidden states used depict local dependencies internal tree node children. training acceleration propose tree canonicalization method transforms trees equivalent ternary trees. then proposed tree-structured search method model able generate probable responses form dependency trees ﬁnally ﬂattened sequences system output. experimental results demonstrate proposed xtree framework outperforms baseline methods increase acceptance ratio. many natural language processing tasks formulated sequence sequence problems. given sequence tokens task generate another sequence tokens equal non-equal length. example machine translation models sequence words target language expressing identical meaning source sentence; conversational models respond post utterance semantically coherent grammatically correct sentence. neural models applied tasks achieved stateof-the-art performances recent years neural models essence chain-structured decoder sequentially generate tokens given context vector encoded input sequence. notice decoding process mostly linear meaning tokens obtained order appearances. basically considers dependency word preceding copyright association advancement artiﬁcial intelligence rights reserved. alleviate long-distance sequence generation. words higher levels dependency tree usually inﬂuential sentence. generating important words earlier stages decoding process essentially free decoder burden store important semantic information many time steps. also believe process tree-structured sentence generation consistent human construct sentences. although people speak sentence sequential order keep keywords verbs nouns mind ﬁlling descriptive adjectives adverbs generate full sentence. paper develop tree-structured decoder framework tree learning represents structure encoding post latent vector. since tokens response explicitly given input post appropriate generate dependency bottom. need address following challenges need carefully model different dependencies tree node children. children different positions different meanings generation child node depends parent ancestors also siblings. thus need fully consider memory inherited ancestors siblings tree node could obtain number children. non-trivial automatically determine number children. furthermore gpu-based parallel computing difﬁcult children number different node. therefore need tree canonicalization process outputs equivalent standard tree internal node ﬁxed number child nodes model inference required develop algorithm searching probable trees instead sequences. since beam search utilized previous studies handles chain structures general search algorithm tree structures needs developed challenges addressed main contributions twofold propose generative neural machine tree structures apply conversational model. speciﬁcally introduce tree canonicalization method standardize generative process greedy search method tree structure inference. empirically demonstrate proposed method successfully predict dependency trees conversational responses input post. speciﬁcally task automatic conversation proposed xtree framework achieves increase acceptance ratio. also worth mentioning need perfect dependency parser. task sequential sentence ﬁnal output dependency tree immediate result. parsed tree contains errors similar patterns model learn patterns. convert generated tree sequence sequence still correct also demonstrated experiments. prediction. consider task fig. intermediate task automatic conversation generation. instead generating response given input post directly generate dependency parse tree corresponding response. then postprocessing step converts dependency tree sequence ﬁnal response. compared seqseq solution conversation generation argue tree-structured modeling method effective shorter average decoding length extra structure information provided parse tree. task clearly seen that since tokens response explicitly given input post appropriate generate dependency bottom top. previous works tree-structured lstm show incorporating syntactic structures encoder decoder results sentence embedding improved performances tasks like sentiment analysis semantic relatedness. paper propose inject tree structures decoding process following motivations dependency tree parsing extracts short-distance dependencies local area sentence. utilizing linguistic results reduces difﬁculty sequential learning thus helps decoders generate grammatically semantically correct utterances. response sentence input dependency tree then average length node root sentence length |y|. thus tree transformation furthermore argue children different positions obtain different underlying meanings. hence different types hidden states designed children node worth mentioning order explicitly notify tree generation need special token leaf nodes children. hence leaf nodes tree training dataset nodes. tree canonicalization aforementioned proposed xtree model requires tree k-ary full tree. whereas dialogue generation task response sentence parsed dependency tree number child nodes level. training generating difﬁcult determine section introduce xtree learning framework. training dataset given response post corresponding tree e.g. dependency tree. task learn mapping tree structure speciﬁcally adopts encoder-decoder framework. assume already aforementioned developed decoder adopts topgenerative process. atom step generating children given node. atom step performed node cannot generate valid nodes. thus decoder modeling parent-children dependency. note also model parameters parentchildren dependency shared atom steps tree generation. ﬁrst assume tree k-ary full tree every internal node exactly children model type tree section generative model k-ary full tree. then propose canonicalization method transforms tree k-ary full tree discuss different applications section tree canonicalization. finally introduce algorithm tree inference section tree generation. generative model k-ary full tree here propose generative model k-ary full tree. simplicity also represents latent vector encoded input post. within probabilistic learning framework main task express conditional probability pair ﬁrst reformulate denotes root non-root nodes respectively. ﬁrst term equ. eled follows nonlinear potentially multi-layered function vocabulary containing possible values discrete random variables. child-node number word. additionally variable-length data tricky acceleration. hence original dependency tree canonicalized k-ary full tree training. tree equivalent original one. words must exist algorithm support bi-directional transformation tree k-ary full counterpart. considering number linear number model parameters reduce model complexity usually hope small possible. given tree simple method transform full tree empty positions nodes. method every tree node obtains children maximal number immediate children tree nodes. however large tree nodes sparse redundant nodes signiﬁcantly increase learning complexity. hence ideally before ﬁlling step want transform tree binary ternary tree. here mainly consider scenarios. ordered tree ordering speciﬁed children node transform left-child right-sibling binary tree transformation reversible one-to-one mapping ordered tree lcrs counterpart. furthermore conversational generation tasks need ﬂatten predicted tree sequence. therefore need store position information dependency tree. purpose ﬁrst give following deﬁnition sequence-preserved tree deﬁnition tree ordered tree node tagged integer number children node dered left right) right part contains remaining nodes. in-order traversal ﬁrst visit nodes left part current node ﬁnally right part. fig. shows three trees corresponding sequences. obviously dependency tree sentence tree number attached node obtained checking position relationship node children original sentence shown figure example node says obtains number means child node left part original sequence. discussed earlier tree canonicalization step needed transform original dependency tree k-ary full tree. preserve sequence order transform dependency ternary tree. present algorithm discuss ternary tree best choice. alg. details canonicalization process illustration shown fig.. ternary tree node three children namely left middle right nodes. node attached number alg. ﬁrst determines left middle child ternary tree. speciﬁcally left child ﬁrst child original tree; middle child ci+. child right child recursively. ternary tree simple in-order traversal order left child parent middle child right child restore sequence. next prove resulting ternary tree equivalent original tree sense transformed other. theorem given tree transformed ternary tree transformed back original tree proof. using alg. transform ternary tree show transform back node right child denote right child denote right child denote right child obtains right child. original tree {rj}n simplicity denote parent. additionally prove ternary tree best choice model complexity. theoretically dependency tree equivalent k-ary tree since number linear parameter size xtree model prefer simpler models smaller values theorem formally shows trees equivalent binary trees. therefore ternary tree best choice. thus training perform preprocessing step converts response corresponding dependency tree canonicalize ternary trees. theorem given tree algorithm exists transforms lcrs tree re-converts proof. respectively denote sequence-preserved trees ordered trees lcrs trees nodes. since ordered trees lcrs trees obtain oneto-one correspondence inferred element number |on| |ln|. node ordered tree children obtain speciﬁed ordering namely deﬁned converts tree. furthermore different trees different. thus |on| |ln|. moreover suppose algorithm exists transforms lcrs tree re-converts infers |ln|. contradictory |ln|. note generated tree full tree perfect tree. sentence words transformed k-ary full tree contains exactly nodes extra nodes tokens. thus nodes induce computing waste. minimize waste expect small possible. theorems tell minimal number transformed tree equivalent original dependency tree. beam search traditionally adopted sequence structure generation step keeps best candidates maximal probabilities far. then candidates expanded next. candidate beam grows node current sequence. process repeats recursively candidates nodes. since sequence special case trees searching tree generation challenges address. first arbitrary tree multiple leaves could potentially generate children. second growing children leaf node need generate children whole since correlate multiple groups children need generated best candidates. example fig. describe tree generation method. original tree leaves nodes leaves generate children. speciﬁcally node generates groups children shown fig. since children ordered local step children generation actually task sequence generation thus conventional beam search used. here specify number candidate sequences generated leaf. child generation leaves compare candidate trees retain top-g trees next round generation. process recursively continues leaves tree nodes. note proposed method generalized beam search. beam search sequence generation special case since sequence equivalent -ary tree. method detailed algorithm figure examples step generalized beam search. fig. shows original tree. fig. show searching results. note words double quotes expanded. here denotes special token eob. dataset details experiments focus dialogue generation task. million post-response pairs obtained tencent weibo. removing spams advertisements pairs left among training model validation. benchmark methods implemented following four popular neural-based dialogue models comparison seqseq model utilizes last hidden state encoder initial hidden state decoder; models sequences sequences directly differ summarize encoder hidden states latent vector. thus proposed tree decoder applied models potentially improve response quality different perspective. here stress tree-decoder easily applied model summarizes multiple rounds dialogues latent vector. future tree decoder multi-round dialog evaluated. implementation details sentences experiments segmented ltp. vocabulary frequent chinese words corpus used training contains words. out-of-vocabulary words replaced unk. implementations based theano library nvidia gpu. applied one-layer -dimensional hidden states {fk}k baseline models. suggested word embeddings encoders decoders learned separately whose dimensions models. parameters initialized using uniform distribution training mini-batch size used adadelta optimization. training stops perplexity validation increases consecutive epochs. models best perplexities selected evaluation. generating responses xtree generalized beam search global beam size local beam size xseq baseline models conventional beam search beam size used. high diversity nature dialogs practically impossible construct data adequately covers responses given post. hence apply human judgment experiments. detail labelers invited evaluate quality responses randomly sampled posts. post model generated top- different responses fair comparison create single post followed responses shufﬂed avoid labelers knowing model response generated following three levels level response ungrammatical. level response basically grammatical irrele level response grammatical relevant input post. response level acceptable dialog system. labeling results average percentages responses different levels calculated. additionally labeling agreement evaluated fleiss’ kappa measure inter-rater consistency. furthermore also report bleu- scores posts. since researchers indicate bleu good measure dialog evaluation consider human judgment major measure experiments. experimental results analysis experimental results summarized table seqseq xtree agreement value range interpreted substantial agreement. meanwhile encdec obtain relatively higher kappa value almost perfect agreement. hence believe labeling standard considered clear leads high agreement among labelers. nodes worst case depth triple dependency tree average number ancestors nodes average size y<t. fig. shows average number steps hidden states need remember different sequence lengths data set. level- xtree visibly outperforms models. best baseline method achieves level- ratio xtree reaches increase percentage improvement mainly less irrelevant responses generated indicating xtree outputs acceptable responses. notice table percentage ungrammatical responses xtree less baselines bleu score greater baselines experiments. shows responses generated tree-structured decoder grammatical chain-structured decoders demonstrate xtree’s robustness parser errors. additionally xtree encdec achieve best grammatical ratio encdec fails generating relevant responses. hence tree decoder improve response relevance experiments. conjecture reason xtree ﬁrstly generate core verb responses. ﬁrst generated relevant post makes whole response relevant post. easiness learning table discover percentage grammatical responses xtree visibly surpasses models experiments. conjecture tree-structured decoder easier learn hidden states need store less information counterparts chainstructured decoder. detail given response utterance length hidden state position chain-structured decoder needs store information previous words aver. contrast size tree-structured decoder needs store information ancestors transforming response triple dependency tree structure average depth overall hidden states tree-structured decoder need store less information chain-structured decoder’s. makes xtree potentially capable handle complex semantic structures response utterances. statistical machine translation. neural-based encoder-decoder framework generative conversation models follows line statistical machine translation. sutskever used multi-layered lstm encoder decoder machine translation. later proposed encoder-decoder framework context vector every unit decoder. bahdana extended encoder-decoder framework attention mechanism model alignment source target sequences. conversation models. inspired neural recent studies showed models also successfully applied dialogue systems. speciﬁcally short conversation shang proposed neural responding machine extended attention mechanism global local schemes. zhou proposed marm generate diverse responses upon multiple mechanisms. recently researchers focused multi-round conversation. serban built endto-end dialogue system using hierarchical neural network. sordoni proposed related model hierarchical recurrent encoder-decoder framework query suggestion. proposed model also applied multi-round conversation models potentially improve performances. tree-structured neural network. recently studies tree-structured neural network instead conventional chain-structured neural network improve quality semantic representation. socher proposed recursive neural tensor network. phrase represented word vectors parse tree. vectors higher level nodes computed using child phrase vectors. extended chainstructured lstm tree structures. models tree structures summarize sentence context vector propose decode context vector generate sentences root-to-leaf direction. additionally zhang proposed tree lstm activation function top-down fashion. here important points differentiate work theirs. first zhang mainly estimate generation probability dependency tree apply model sentence completion dependency parsing reranking tasks xtree handles dialogue modeling encoder-decoder framework. second canonicalization method xtree model process ﬁxed number children step acceleration zhang need process children sequentially. thus proposed tree canonicalization method helps reduce training time. works also generating different structure types. rabinovich proposed abstract syntax networks transform card image game hearthstone well-formed executable outputs. cheng utilized predicate-argument structures store natural language utterances intermediate domain-general representations. study proposed tree-structured decoder improve response quality dialogue systems. incorporating linguistic knowledge modeling process proposed xtree framework outperforms baseline methods increase acceptance ratio response generation. future study incorporating tree-structured encoder promising enhance sentence generation quality. work also supported wechat tencent. thank leyu lixin zhang cheng xiaohu cheng constructive advices. also thank anonymous aaai reviewers helpful feedback.", "year": 2017}