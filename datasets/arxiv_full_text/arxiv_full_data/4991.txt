{"title": "A Survey on Multi-Task Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL. First, we classify different MTL algorithms into several categories: feature learning approach, low-rank approach, task clustering approach, task relation learning approach, dirty approach, multi-level approach and deep learning approach. In order to compare different approaches, we discuss the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, batch MTL models are difficult to handle this situation and online, parallel and distributed MTL models as well as feature hashing are reviewed to reveal the computational and storage advantages. Many real-world applications use MTL to boost their performance and we introduce some representative works. Finally, we present theoretical analyses and discuss several future directions for MTL.", "text": "abstract—multi-task learning learning paradigm machine learning leverage useful information contained multiple related tasks help improve generalization performance tasks. paper give survey mtl. first classify different algorithms several categories feature learning approach low-rank approach task clustering approach task relation learning approach dirty approach multi-level approach deep learning approach. order compare different approaches discuss characteristics approach. order improve performance learning tasks further combined learning paradigms including semi-supervised learning active learning reinforcement learning multi-view learning graphical models. number tasks large data dimensionality high batch models difﬁcult handle situation online parallel distributed models well feature hashing reviewed reveal computational storage advantages. many real-world applications boost performance introduce representative works. finally present theoretical analyses discuss several future directions mtl. training samples learn accurate learner. example deep learning models build neural networks usually need millions labeled samples train neural networks tens even hundreds layers contain huge number model parameters. however applications medical image analysis requirement cannot fulﬁlled since samples hard collect. case limited training samples enough learn shallow models alone deep models. data insufﬁcient problem multi-task learning good solution multiple related tasks limited training samples. multiple learning tasks general learning task supervised tasks unsupervised tasks semi-supervised tasks reinforcement learning tasks multiview learning tasks graphical models. among learning tasks least subset assumed related other. case found learning tasks jointly lead much performance improvement compared learning individually. observation leads birth mtl. hence aims improve generalization performance multiple tasks related. inspired human learning activities people often apply knowledge learned previous tasks help learn task. example person learns ride bicycle tricycle together experience learning ride bicycle utilized riding tricycle vice versa. similar human learning useful multiple learning tasks learned jointly since knowledge contained task leveraged tasks. setting similar transfer learning also difference. distinction among different tasks objective improve performance tasks. however transfer learning improve performance target task help source tasks target task plays important role source tasks. hence treats tasks equally transfer learning target task gets attention among tasks. setting called asymmetric multitask learning investigated setting considers different scenario task arrived multiple tasks learned jointly method. simple solution learn tasks together scratch computationally demanding. instead asymmetric multi-task learning learns task help tasks hence core problem transfer knowledge contained tasks task. sense setting similar transfer learning hence include setting here. paper give overview mtl. giving deﬁnition classify different algorithms several categories feature learning approach further categorized feature transformation feature selection approaches low-rank approach task clustering approach task relation learning approach dirty approach multi-level approach deep learning approach. discuss characteristics approach. combined learning paradigms improve performance learning tasks hence discuss combinations learning paradigms including semi-supervised learning active learning reinforcement learning multi-view learning graphical models. number tasks large number training data tasks large makes online parallel computation models necessary. case training data different tasks locate different machines hence distributed models good solution. moreover feature hashing vital tool reducing dimension facing highdimensional data mtl. hence introduce online parallel distributed models well feature hashing helpful handling data multiple tasks. general learning paradigm many applications various areas brieﬂy review applications computer vision bioinformatics health informatics speech natural language processing applications ubiquitous computing. besides algorithmic development real-world applications review theoretical analyses discuss several future directions mtl. remainder paper organized follows. section introduces several categories models. section combinations learning paradigms reviewed. section overviews online parallel distributed models handling data multiple tasks. section presents applications various areas. section gives overview theoretical analyses ﬁnally make conclusions section discussions future directions mtl. algorithms order fully characterize ﬁrst give deﬁnition mtl. deﬁnition given learning tasks {ti}m tasks subset related multi-task learning aims help improve learning model using knowledge contained tasks. according deﬁnition focus supervised tasks since studies fall setting. types tasks review next section. case usually task accompanied training dataset consisting training samples i.e. label. denote training data matrix i.e. different tasks share training reduces multilabel learning multi-output regression. consider general setting least xi’s different general setting xi’s different other. different tasks feature space implying equals setting homogeneous-feature otherwise corresponds heterogeneous-feature mtl. without special explanation default setting homogeneous-feature studied works. need distinguish heterogeneous-feature heterogeneous mtl. heterogeneous considered consist different types supervised tasks including classiﬁcation regression problems generalize general setting heterogeneous consists tasks different types including supervised learning unsupervised learning semisupervised learning reinforcement learning multi-view learning graphical models. opposite heterogeneous homogeneous consist tasks type. word homogeneous heterogeneous focus type learning tasks homogeneous-feature heterogeneous-feature differ original feature representations. similarly without special explanation default setting homogeneous focus studies. parameter. feature-based aims learn common features among different tasks share knowledge. instancebased wants identify useful data instances task tasks shares knowledge identiﬁed instances. parameter-based uses model parameters task help learn model parameters tasks ways example regularization. existing studies mainly focus feature-based parameter-based methods works belong instance-based method. representative instance-based method multi-task distribution matching method proposed ﬁrst estimates density ratios probabilities instance well label belongs task mixture tasks uses weighted training data tasks based estimated density ratios learn model parameters task. since studies instance-based mainly review feature-based parameter-based models. determining ‘what share’ ‘how share’ speciﬁes concrete ways share knowledge among tasks. feature-based primary approaches feature learning approach deep learning approach. feature learning approach consisting shallow models focuses learning common feature representations multiple tasks learned common feature representation subset transformation original feature representation. deep learning approach learns common representations multiple tasks based deep neural networks. different feature learning approach based shallow models including regularized methods neural networks hidden layer deep learning approach relies deep models. parameter-based main approaches low-rank approach task clustering approach task relation learning approach dirty approach multi-level approach. low-rank approach interprets relatedness multiple tasks low-rank parameter matrix tasks. task clustering approach assumes tasks form clusters tasks cluster related other. task relation learning approach aims learn relations tasks data automatically task relation take form covariance. dirty approach decomposes model parameters tasks components components capture different forms sparsity. multi-level approach extension dirty approach models parameters multiple components instead treating components independently dirty approach multi-level approach relate multiple components model complex structure. total seven approaches feature-based parameterbased mtl. following sections review seven approaches chronological order reveal relations evolutions among them. feature learning approach since tasks related intuitive assume different tasks share common feature representation based original features. reason learn common feature representations instead directly using original ones original representation enough expressive power multiple tasks. training data tasks powerful representation learned tasks representation bring improvement performance. denotes loss function hinge loss square loss vector offsets tasks rd×d orthogonal transformation matrix column contains model parameters task transformation norm matrix denoted equals norm rows denotes identity matrix appropriate size positive regularization parameter. ﬁrst term objective function problem measures empirical loss training sets tasks second enforce row-sparse norm equivalent selecting features transformation. different multi-layer feedforward neural network whose hidden representation redundant orthogonality prevent mtfl method interesting problem equivalent following formulation denotes trace square matrix model parameter denotes zero vector matrix appropriate size means positive semideﬁnite. based formulation mtfl method learn feature covariance tasks interpreted section probabilistic perspective. given learning different tasks decoupled facilitate parallel computing. given analytical solution plugging solution problem regularizer squared trace norm. argyriou extend problem general formulation second term objective function becomes λtrw) function operating spectrum keeping eigenvectors discuss condition make whole problem convex. category sub-categories. ﬁrst sub-category feature transformation approach learned representation linear nonlinear transformation original representation approach feature learned representation different original features. different approach feature selection approach second subcategory selects subset original features learned representation hence learned representation similar original eliminating useless features based different criteria. following introduce approaches. multi-layer feedforward neural network belongs feature transformation approach earliest model multi-task learning. order multilayer feedforward neural network constructed figure example. neural network shown figure input layer hidden layer output layer. input layer receives training instances tasks output layer output units task. hidden layer viewed common feature representation learned tasks transformation original representation learned depends weights connecting input layer hidden layer well activation function adopted hidden units. hence activation function hidden layer linear transformation linear function otherwise nonlinear. compared multi-layer feedforward neural networks used single-task learning difference network architecture lies output layers single-task learning output unit ones. radial basis function network hidden layer extended greedily determining structure hidden layer. different neural network models silver propose context-sensitive multi-task neural network output unit shared different tasks task-speciﬁc context additional input. column integer denotes integers denotes norm vector matrix equals absolute value entries denotes norm vector. rd×d also called dictionary sparse coding shared tasks. compared mtfl method problem orthogonal matrix problem overcomplete implies larger column bounded norm. another difference problem enforced row-sparse problem sparse ﬁrst constraint. similar idea multi-task sparse coding method propose multi-task inﬁnite support vector machine indian buffet process difference dictionary sparse model parameters non-sparse. spike slab prior used learn sparse model parameters multi-output regression problems transformed features induced gaussian processes shared different outputs. hence feature selected model. sense controls global sparsity feature among tasks. moreover ˆwji becomes also only implying feature useful task local indicator sparsity task based observations ˆwji expected sparse leading objective function formulated nonnegative constraint keep model identiﬁability. proven problem leads regularizer norm regularization. moreover wang extend problem general situp ation regularizer becomes utilizing priori information describing task relations hierarchical structure propose multi-component product based decomposition number components decomposition arbitrary instead similar jebara proposes learn binary indicator vector multi-task feature selection based maximum entropy discrimination formalism. λvwigv denotes nodes given tree structure denotes leaf nodes sub-tree rooted node wigv denotes subvector indexed regularizer enforces sparse norm problem also induces sparsity subsets based tree structure. different conventional multi-task feature selection methods assume different tasks share original features zhou consider different scenario useful features different tasks overlapping. order achieve this exclusive lasso model proposed objective function formulated another select important features sparse priors design probabilistic bayesian models. p-regularized multi-task feature selection zhang propose probabilistic interpretation regularizer corresponds following prior denotes generalized normal distribution. based interpretation zhang propose probabilistic framework multi-task feature selection task relations outlier tasks identiﬁed based matrix-variate generalized normal prior. feature selection approach feature selection group sparsity based norm denoted equal wdp)q denotes denotes norm vector. obozinski among ﬁrst study multi-task feature selection problem based norm objective function formulated regularizer problem enforce rowsparse turn helps select important features. path-following algorithm proposed solve problem employ optimal ﬁrst-order optimization method solve compared problem problem similar mtfl method without learning transformation propose weighted norm multi-task feature selection weights learned well problem extended general case feature groups overlap other. order make problem robust outliers square-root loss function investigated moreover order make speedup safe screening method proposed ﬁlter useless features corresponding zero rows optimizing problem propose norm select features denotes given threshold capped-p penalty focuses rows smaller norms likely sparse. becomes large enough second term problem becomes hence problem degenerates problem equals orthonormal constraint problem makes subspace non-redundant. large value optimal become zero matrix hence problem similar problem except regularization problem smaller number rows columns problem corresponding variable square matrix. chen generalize problem advantage problem problem global optimum convex problem much easier obtained non-convex problem compare alternative objective function mtfl method problem similar formulation models feature covariance tasks. problem extended general case different wi’s manifold instead subspace. moreover latent variable model proposed decomposition provide framework modeling cases problem task clustering tasks sharing sparse representation duplicate tasks evolving tasks. well known using trace norm regularizer make matrix rank hence regularization suitable mtl. speciﬁcally objective function trace norm regularization proposed matrix entries respectively hyperparameters. shared tasks denotes feature correlation matrix learned data encodes assumption different tasks share identical feature correlations. becomes identity matrix means features independent prior degenerates horseshoe prior induce sparse estimations. point probability mass zero density speciﬁes distribution coefﬁcients nonzero. within prior indicates whether feature outlier indicates whether task outlier indicates whether non-outlier feature relevant prediction tasks outliers. indicates whether given outlier task feature task relevant irrelevant prediction. indicates whether feature relevant prediction given outlier feature. based deﬁnitions ﬁrst term righthand side speciﬁes prior feature outlier second term corresponds prior feature outlier outlier task last term rest situation. model also handle outlier tasks different comparison sub-categories sub-categories different characteristics feature transformation approach learns transformation original features representation feature selection approach selects subset original features representation tasks. based characteristics approaches feature selection approach special case feature transformation approach transformation becomes matrix. according this feature transformation approach usually training data better feature selection approach since capacity hence overﬁtting using feature transformation approach generalization performance certain probability better feature selection approach. hand selecting subset original features representation feature selection approach better interpretability. word application needs better performance feature transformation approach preferred application needs decision support feature selection approach ﬁrst choice. denotes mean model parameters corresponding tasks cluster number tasks cluster cluster indicator matrix whose entry equal task belongs cluster otherwise equals e−et combining three aspects regularizer proposed. easy third aspect important cluster structure required larger since unknown minimize training loss well regularizer since jointly convex respect resulting objective function also non-convex. convex relaxation technique convex objective function formulated kang extend mtfl method treats tasks whole group case multiple task clusters minimize squared trace norm cluster. diagonal matrix rm×m deﬁned cluster indicator matrix cluster. diagonal entry equal task lies cluster otherwise since task belong cluster easy based considerations objective previous studies assume task belong task assumption seems restrictive. gomtl method relaxes assumption allowing task belong cluster deﬁnes rd×r denotes latent basis rr×m contains weights linear combination task. assumed sparse since task generated task clustering approach task clustering approach assumes different tasks form several clusters consists similar tasks. indicated name approach close connection clustering algorithms viewed extension clustering algorithms task level conventional clustering methods data level. task clustering algorithm weighted nearest neighbor classiﬁer task weights deﬁne weighted euclidean distance learned minimizing pairwise within-class distances maximizing pairwise between-class distances simultaneously. deﬁne task transfer matrix whose entry records generalization accuracy obtained task using task tj’s distance metric cross validation. based tasks grouped clusters {ci}r maximizing denotes cardinality set. obtaining cluster structure among tasks training data tasks cluster pooled together learn weighted nearest neighbor classiﬁer. bakker heskes propose multi-task bayesian neural network model network structure similar fig. input-to-hidden weights shared tasks hiddento-output weights task-speciﬁc. deﬁning vector hidden-to-output weights task multi-task bayesian neural network assigns mixture gaussian prior specify prior mean covariance cluster. tasks cluster share gaussian distribution. equals model degenerates case model parameters different tasks share prior similar several bayesian models based gaussian processes processes. deploy dirichlet process clustering denotes dirichlet process positive scaling parameter base distribution. clustering effect integrating conditional distribution given model parameters tasks {··· wi+···} denotes distribution concentrated single point equal probability m−+α corresponds case tasks cluster sample value probability m−+α case task forms task cluster. large chance form task cluster large affect number task clusters. model extended case different tasks task cluster share similar useful features matrix stick-breaking process betabernoulli hierarchical prior respectively task compressive sensing task. moreover nested dirichlet process proposed dirichlet processes learn task clusters state structure inﬁnite hidden markov model handles sequential data task. among aforementioned methods method ﬁrst identiﬁes cluster structure learns model parameters tasks separately preferred since cluster structure learned optimal model parameters hence follow-up works learn model parameters cluster structure together. important problem clustering determine number clusters also important approach. methods methods automatically determine number task clusters method depends capacity dirichlet process method relies structurally sparse regularizer. mentioned methods belong bayesian learning i.e. rest models regularized models. among regularized methods objective function proposed convex others originally non-convex. task clustering approach related low-rank approach. that suppose task clusters tasks cluster share model parameters making parameter matrix low-rank rank moreover perspective modeling setting zero vector decomposition becomes similar aspect demonstrates relations approaches. task clustering approach visualize cluster structure advantage low-rank approach. task relation learning approach tasks related task relatedness quantitated task similarity task correlation task covariance task relations include quantitative relatedness. earlier studies task relations assumed known priori information. task assumed similar task model parameter task enforced approach average model parameters tasks. task similarities pair tasks given studies utilize task similarities design regularizers guide learning multiple tasks principle similar tasks closer corresponding model parameters expected moreover given tree structure describing relations among tasks model parameter task corresponding node tree enforced similar parent node however applications task relations available. case learning task relations data automatically good option. bonilla propose multi-task gaussian process deﬁning prior denotes kernel function describes covariance tasks order keep positive deﬁnite covariance matrix matrix containing entry also required positive compared objective function multi-task sparse coding i.e. problem regularization parameters take appropriate values problems almost equivalent except multi-task sparse coding dictionary overcomplete implying number columns larger rows number columns smaller rows. regularizer enforces pair columns chance identical solving problem cluster structure discovered comparing columns advantage structurally sparse regularizer convex problem automatically determine number task clusters. similar barzilai crammer propose task clustering method deﬁning rd×r }r×m. assumption task belongs cluster objective function formulated using hinge loss logistic loss non-convex problem relaxed min-max problem global optimum utilizing dual problem respect discarding non-convex constraints. zhou zhao cluster tasks identifying representative tasks subset given tasks. task selected task representative task expected model parameter similar deﬁned probability task selects task representative task. based {zij} objective function formulated second term objective function problem penalize complexity third term enforce closeness pair tasks based last term employs norm enforce sparsity implies number representative tasks limited. constraints problem guarantees entries deﬁne valid probabilities. problem related problem since regularizer problem reformulated different aforementioned methods investigate global learning models zhang aims learn task relations local learning methods knearest-neighbor classiﬁer deﬁning learning function weighted voting neighbors denotes task indices instance indices i.e. meaning nearest neighbors represents deﬁnes similarity contribution task data points neighbors data point viewed similarity reduces decision function classiﬁer hence viewed decision function multi-task classiﬁer. objective function learn matrix entry formulated ﬁrst regularizer problem enforces nearly symmetric matrix depending regularization parameter implies similarity nearly symmetric second penalize complexity constraints problem make sure similarity task positive also largest. similarly multi-task kernel regression proposed regression problems. aforementioned methods whose task relations symmetric except focus learning asymmetric task relations. since different tasks assumed related space spanned i.e. hence matrix viewed asymmetric task relations pairs tasks. assuming sparse objective function formulated denotes deleting aii. term training loss task i.e. λˆai enforces sparse also allows asymmetric information transfer easier tasks difﬁcult ones. regularizer problem make approach closeness depends connection problems rewrite regularizer problem deﬁnite makes task covariance describe similarities tasks. based gaussian likelihood labels given analytically marginal likelihood integrating used learn data. learning curve generalization bound multi-task gaussian process studied. since point estimation lead overﬁtting based proposed weight-space view zhang yeung propose multi-task generalized process placing inverse-wishart prior denotes degree freedom base covariance generating since models covariance pairs tasks inspired maximum mean discrepancy takes form centering matrix element formulated kernel function different bayesian models zhang yeung propose regularized multi-task model called multi-task relationship learning placing matrixvariate normal prior denotes matrix-variate normal distribution mean covariance column covariance. based prior well likelihood function objective function maximum posterior solution formulation second term objective function penalize complexity last term matrix-variate normal prior constraints control complexity positive deﬁnite covariance matrix proved last term objective function problem convex respect making whole problem convex. problem extended multi-task boosting multilabel learning learning label correlations. problem also interpreted perspective reproducing kernel hilbert spaces vector-valued functions moreover problem extended learn sparse task relations regularization number tasks large. model similar problem proposed matrix-variate normal prior inverses assumed sparse. mtrl model extended symmetric matrixvariate generalized hyperbolic distribution learn block sparse structure matrix generalized inverse gaussian prior learn low-rank moreover mtrl model generalized multi-task feature selection problem learning task relations matrix-variate generalized normal distribution. since prior deﬁned implies follows denotes wishart distribution zhang yeung generalize positive integer model high-order task relationships. induce prior generalization matrix-variate normal distribution based prior regularized method devised learn task relations special case multi-output regression problems output treated task tasks share data matrix investigated based reformulation regularizer problem special case problem assuming nonnegative matrix. even though asymmetric perspective regularizer task relations symmetric task precision matrix restrictive form. second term penalize complexities increases model ﬂexibility tasks exhibit clear cluster structure feature level. works methods proposed bring robustness multi-task feature selection methods terms method makes feature-level clustering ﬂexible improve trace norm regularization. general multi-task methods parameter-based additional variable introduced improve robustness ﬂexibility. expressive power dirty approach hence model complex task structure. given tasks possible non-empty task clusters individual task form cluster jawanpuria nath helpful task clusters assuming among possible clusters useful helping improve performance. task clusters represented nodes tree root node represents dummy node tasks nodes second level represents groups single task parent-child relations subset relation. another assumption used subset tasks dissimilar supersets also dissimilar means node selected descendants selected neither. levels corresponds node tree hence index used denote level corresponding node tree. node subset tasks belonging denotes tasks hence denotes column component contained node matrix based assumptions objective function formulated column takes value reﬂects relations among tasks node based regularizer problem trends prune subtree rooted node based norm. adopts functional form assumes tasks similar hence component parameters level approaching average deﬁne regularizers constraints respectively column help understand problem following introduce instantiations functions proposed respectively. positive regularization parameters. similar problem likely zero based hence help select important features. norm regularization makes sparse. characteristics regularizers parameter matrix eliminate unimportant features tasks corresponding rows sparse identify features tasks useful features outliers tasks. hence model viewed ‘robust’ version problem extended real-value function deﬁnes convex constraint similar problem makes low-rank. sparse regularizer makes entire model matrix robust outlier tasks similar regularization parameter large enough optimal become zero matrix hence problem similarly trace norm regularization problem different works assume sparse enforces column-sparse. related tasks corresponding columns correlated trace norm regularization corresponding columns zero. outlier tasks unrelated tasks corresponding columns take arbitrary values hence model parameters low-rank structure even though have. functions i.e. deﬁned methods cross-stitch network proposed learn task relations terms hidden feature representation similar task relation learning approach. speciﬁcally given tasks identical network architecture denotes hidden feature unit hidden layer task deﬁne cross-stitch operation ˜xij hidden features learning tasks jointly. equal training networks jointly equivalent training independently. network architecture shown fig. matrix encodes task relations tasks learned backpropagation method. different task relation learning approach whose task relations deﬁned based model parameters based hidden features. comparison among different approaches based introduction different approaches exhibit different characteristics. speciﬁcally feature learning approach learn common features generic transferable tasks hand even tasks tasks. exist outlier tasks unrelated tasks learned features inﬂuenced outlier tasks signiﬁcantly cause performance deterioration making approach robust outlier tasks. deep learning approach shares similar properties feature learning approach. moreover optimization deep learning approach difﬁcult feature learning approach since involves hidden layers also parameters. assuming parameter matrix low-rank lowrank approach explicitly learn subspace parameter matrix implicitly achieve convex non-convex regularizer. approach powerful seems applicable linear models making nonlinear extensions non-trivial devised. task clustering approach performs clustering task level terms model parameters identify task clusters consists similar tasks. major limitation task clustering approach capture positive correlations among tasks cluster ignore negative correlations among tasks different clusters. moreover even though methods category automatically determine number clusters still need model selection method cross validation determine bring computational cost. task relation makes pair columns chance identical based equivalence among columns cluster structure level exploited. increases regularization parameter component matrix level becomes smaller making number clusters larger. based multi-level structure discover latent cluster structure level. moreover level problem reduces problem compared level corresponds cluster involving subset tasks level method aims cluster structure tasks. methods model parameters different levels direct connection. dependency among levels approach model complex structure hierarchical/tree structure among tasks. objective function proposed formulated denote operators elementwise absolute value elementwise smaller than’ relation vectors matrices. objective function problem similar problem difference lies constraint problem relates component matrices different levels. note regularizer problem makes chance become identical happens based constraint problem always value obtaining optimal {wi} construct hierarchical structure leaf nodes represents tasks. becomes identical ﬁrst time level path leaf node level shares internal node path leaf node conducting operation levels construct hierarchical structure. multi-level approach viewed ‘deep’ approach terms model parameters previous approaches shallow ones hence approach powerful capacity shallow approaches. compared deep learning approach presented next section approach deep respect model parameters deep learning approach terms feature representation. deep learning approach deep learning becomes popular capacity learn nonlinear features many applications deep models used basic models task mtl. similar multi-task neural network shown fig. methods category assume different tasks share ﬁrst several hidden layers parameters generate outputs. different deep task relation learning methods including directly learn covariance matrix constraining trace sparsity trace norm regularization formulation similar problem based alternative form novel settings works aforementioned assume different tasks share feature representation task binary classiﬁcation problem label space discrete. moreover training data task stored data matrix. however cases assumptions hold following introduce works whose problem settings violate assumptions. instead assuming different tasks share identical feature representation zhang yeung consider multidatabase face recognition problem face recognition database treated task. since different face databases different image sizes naturally tasks feature space application heterogeneousfeature problem. multi-task discriminant analysis proposed tackle heterogeneous-feature problem. mtda ﬁrst projects different tasks common subspace subspace common projection learned tasks discriminate different classes different tasks respectively. latent probit model generative model proposed generate data different tasks different feature spaces sparse transformation shared latent space generate labels based latent space. classiﬁcation problems task assumed binary classiﬁcation problem hence model parameter task represented vector assembled parameter matrix column represents task many matrix techniques used. however applications number classes task larger making many methods fail directly applicable scenario. three main approaches tackle problem. ﬁrst method transform multi-class classiﬁcation problem task binary classiﬁcation problem. example multitask metric learning transforms imbalanced binary classiﬁcation problem pair data points class treated positive different classes negative. second recipe utilize characteristics learners. example linear discriminant analysis handle binary multi-class classiﬁcation problems uniﬁed formulation hence mtda naturally handle without changing formulation. last approach directly learn label correspondence among different tasks. learning approach learn model parameters pairwise task relations simultaneously. learned task relations give insight relationships tasks improve interpretability. dirty multi-level approaches viewed extensions parameter-based approaches increasing number levels model complex task structure i.e. tree structure. number levels multi-level approach important performance needs carefully determined. another taxonomy regularized methods regularized methods form main approach mtl. many regularized multi-task algorithms classiﬁed main categories learning feature covariance learning task relations learning feature covariance viewed representative formulation feature-based learning task relations parameter-based mtl. based probabilistic prior models covariance between features since covariance matrix corresponds feature different tasks share feature covariance. methods ﬁrst category differ choice function example methods restrict trace shown problems moreover multi-task feature selection methods based norm reformulated instances problem using alternative form model task relations since column covariance column corresponds task. perspective regularizers problems different meanings even though formulations seem similar. methods second category different functions learn different functionalities. example methods utilize priori information task relations directly learn deﬁning task clustering methods identify task clusters assuming block structure several methods assume training data task stored data matrix. case training data exhibit multi-modal structure task hence represented tensor instead matrix. multilinear multi-task methods proposed handle situation employing tensor norms generalization trace norm. learning paradigms previous section review different approaches supervised tasks. section present works combination learning paradigms machine learning including unsupervised learning clustering semisupervised learning active learning reinforcement learning multiview learning graphical models improve performance supervised information unlabeled data help improve performance learning paradigms. applications labeled data expensive collect unlabeled data abundant. applications training task consists labeled unlabeled data hence hope exploit useful information contained unlabeled data improve performance mtl. machine learning semi-supervised learning active learning ways utilize unlabeled data different ways. semi-supervised learning aims exploit geometrical information contained unlabeled data active learning selects representative unlabeled data query oracle hope reduce labeling cost much possible. hence semi-supervised learning active learning combined leading three learning paradigms including semi-supervised multitask learning multi-task active learning semi-supervised multi-task active learning speciﬁcally semi-supervised multi-task classiﬁcation model proposed random walk exploit unlabeled data task cluster multiple tasks relaxed dirichlet process. semi-supervised multi-task gaussian process regression tasks different tasks related hyperprior kernel parameters gaussian processes tasks proposed incorporate unlabeled data design kernel function task achieve smoothness corresponding functional spaces. different semi-supervised multi-task methods multi-task active learning adaptively selects informative unlabeled data multitask learners hence selection criterion core research issue. reichart believe data instances selected informative possible tasks instead task hence propose protocols multi-task active learning. expected error reduction used criterion task modeled supervised latent dirichlet allocation model. inspired multi-armed bandits balance trade-off exploitation exploration selection strategy proposed consider risk multi-task learner based trace norm regularization corresponding conﬁdence bound. moreover combine semi-supervised learning active learning utilize unlabeled data using fisher information criterion select unlabeled data acquire labels supervised tasks also unsupervised tasks clustering shown multi-task bregman clustering method proposed based single-task bregman clustering using earth mover distance minimize distances pair tasks terms cluster centers improved version kernel extension proposed avoid negative effect caused regularizer choice better result single-task multi-task bregman clustering. multi-task kernel k-means method proposed learning kernel matrix maximum mean discrepancy minimize distances pair tasks laplacian regularization helps identify smooth kernel space. proposed multi-task clustering methods extensions mtfl mtrl methods treating labels cluster indicators variables learned. principle incorporated subspace clustering capturing correlations data instances. multi-task clustering method belonging instance-based proposed share data instances among different tasks. different works derived generalization bound used select subset multiple unlabeled tasks acquire labels improve generalization performance tasks. reinforcement learning promising area machine learning shown superior performance many applications game playing including atari help boost performance reinforcement learning leading multi-task reinforcement learning agent needs solve sequence markov decision processes hierarchical bayesian inﬁnite mixture model used model distribution mdps previously learned distributions used informative prior. regionalized policy representation introduced characterize behavior agent task dirichlet process placed regionalized policy representations across multiple tasks cluster tasks. gaussian process temporal-difference value function model used task hierarchical bayesian approach model distribution value functions different tasks. calandriello assume parameter vectors value functions different tasks jointly sparse extend mtfs method regularization well mtfl method learn value functions multiple tasks together. proposed actor-mimic method combines deep reinforcement learning model compression techniques train policy network learn multiple tasks. multi-view learning assumes data point associated multiple sets features corresponds view usually exploits information contained multiple views supervised unsupervised tasks. multi-task multi-view learning extends multi-view learning multi-task setting task multi-view learning problem. speciﬁcally graph-based method proposed multi-task multi-view classiﬁcation problems. task view enforced consistent views labels different tasks expected similar predictions views share making views bridge construct task relatedness. regularized method mtrl method applied view different tasks different views task expected achieve agreement unlabeled data. different study multi-task multi-view classiﬁcation problem multi-task multi-view clustering methods proposed methods consider three factors within-view-task clustering clustering view task view relation learning minimizes disagreement among views task task relation learning aims learn shared subspace different tasks common view. difference methods ﬁrst method uses bipartite graph coclustering method nonnegative data another adopts semi-nonnegative matrix tri-factorization cluster general data. moreover multi-task multi-view learning task usually supplied labeled unlabeled data hence paradigm also viewed another utilize unlabeled information except semi-supervised multi-task learning multi-task active learning semi-supervised multi-task active learning. moreover help learn accurate structure graphical models shown algorithm proposed learn bayes network structures assuming different tasks/networks share similar structures common prior heuristic search used structures high scores tasks. multiple gaussian graphical models jointly learned assuming joint sparsity among precision matrices regularization. domain knowledge task relations incorporated learning multiple bayesian networks different tasks. handling data usually task limited number training data data. however number tasks large total number training data tasks hence ‘big’ aspect number tasks. another ‘big’ aspect data dimensionality high. number tasks employ online devise parallel distributed methods make speedup data high-dimensional space accelerate learning feature selection dimensionality reduction feature hashing reduce dimension without losing much information. section review relevant works. number tasks devise parallel methods speedup learning multi-cpu multi-gpu computers. representative formulation featurebased problem easy parallelize since given feature covariance matrix learning different tasks decoupled. however problem parameter-based situation totally different since even given task covariance matrix different tasks still coupled making direct parallelization fail. order parallelize problem zhang uses fista algorithm design surrogate function problem given surrogate function decomposable respect leading parallel design based different loss functions including hinge \u0001-insensitive square losses. moreover online multi-task learning also capable handling situation. setting tasks contribute toward common goal relation tasks measured global loss function deﬁned combination individual losses task evaluates quality multiple predictions several online algorithms proposed absolute norms global loss function. online algorithms devised relatedness tasks modeled constraining m-tuple actions tasks needs satisfy hard constraints. perceptron-based online algorithms proposed multi-task binary classiﬁcation problems similarities among tasks measured based geometric closeness task reference vectors dimension spanned subspace. recursive bayesian online algorithm based gaussian processes devised update estimations conﬁdence intervals data points arrive sequentially. online version mtrl method proposed updates model parameters task covariance sequential way. moreover training data distribute different places making distributed become important communication-efﬁcient distributed algorithm machine learns task based debiased lasso proposed learn jointly sparse features high-dimensional setting. high-dimensional data multi-task feature selection methods reduce dimension extend singletask dimension reduction techniques multi-task setting another option feature hashing. multiple hashing functions proposed accelerate joint learning multiple tasks. applications many applications various areas including computer vision bioinformatics health informatics speech natural language processing applications ubiquitous computing following introduce related works chronological order. computer vision hierarchical kernel stick-breaking process proposed multiple image segmentation segmentation image treated task modeled kernel stick-breaking process dirichlet process used cluster tasks. boosted multi-task method proposed face veriﬁcation different tasks share base learners boosting method. multi-task warped gaussian process proposed personalized estimation task corresponds person different tasks share kernel parameters warped function warped gaussian process different noise levels. multi-task feature selection model based norm proposed multi-cue face recognition object categorization. multi-task feature selection method based norms proposed identify brain imaging predictors memory performance. multi-task low-rank subspace clustering different tasks related structural sparsity among spanning matrices subspace clustering proposed image segmentation similar model used saliency detection. multi-task dictionary learned visual tracking sparsity. multi-task dirty dictionary learning method follows idea applied multi-view tracking. multitask dirty model component matrix regularized priori information organized graph proposed head pose classiﬁcation uncontrolled environment. multitask sparse model based beta process proposed learn dictionaries action recognition. tasks-constrained deep convolutional network proposed facial landmark detection sharing hidden layers auxiliary tasks including head pose estimation gender classiﬁcation estimation facial expression recognition facial attribute inference. multi-task model proposed learn low-dimensional feature transformation scene classiﬁcation. multi-task convolutional neural network individual cnns task fuses different cnns common layer sparse transformation proposed image-based multi-label attribute prediction. similar dirty model component modeling low-rank trace norm another capturing sparsity norm proposed multi-camera person re-identiﬁcation. multi-task deep model proposed rotate facial images target pose auxiliary task aims generated image reconstruct original image. order select thumbnails videos deep model proposed utilize available sematic information titles descriptions align embeddings video thumbnails sematic information auxiliary task click-through image data. recurrent neural network used immediacy prediction output layer multiple units estimate interaction distance stand orientation relative orientation pose estimation. deep convolutional neural network proposed pose estimation sharing hidden layers auxiliary tasks including body-part joint-point detection tasks. go-mtl method generalized multilinear multi-task setting person-speciﬁc facial action unit prediction facial action unit treated task task function learned person. bioinformatics health informatics several regularized multi-task models proposed utilize hierarchical structure among tasks model organisms. sparse multi-task regressor based norm regularization proposed identify common mechanism responses therapeutic targets. multi-task model detect commonly useful features based signiﬁcant tests proposed cross-platform sirna efﬁcacy prediction. multi-task feature selection method used detect causal genetic markers joint association analysis multiple populations. multi-task model sharing gaussian prior model parameters different tasks construct personalized brain-computer interfaces. multi-task multi-kernel methods proposed mhc-i binding prediction splice-site prediction. multi-task methods proposed used protein subcellular location prediction. multi-task method based temporal group lasso norm regularization proposed mini mental state examination alzheimers disease assessment scale cognitive subscale. prodige method proposed share information known disease genes across diseases learning positive unlabeled examples prioritization disease genes. sparse bayesian model learns correlations features tasks based automatic relevance determination proposed predict cognitive outcomes neuroimaging measures alzheimers disease. identiﬁcation longitudinal phenotypic markers alzheimers disease progression prediction formulated multi-task time-series problem time point learner associated parameters organized matrix parameter tensor consisting parameter matrices time points assumed group sparse low-rank. natural images example imagenet dataset biological images jointly trained neural networks hope transferring useful information abundant natural images limited biological images improve performance. survival analysis problem formulated multitask classiﬁcation problem assumption event occurs occur again multi-task feature selection extended solve resultant problem empirically veriﬁed various real-world gene expression cancer survival benchmark datasets. several multi-task models corresponding problems employed multiple genetic trait prediction. speech natural language processing multi-task stacked deep neural network consists multiple neural networks former neural network feeds top-most hidden layer input next neural network proposed speech synthesis neural network output units main task secondary task sharing hidden layers tasks. multi-task deep neural network used model cepstra amplitudes primary secondary tasks sinusoidal speech synthesis. multi-task time-decay neural network proposed jointly learn tasks including part-of-speech tagging chunking named entity recognition semantic role labeling language modeling identiﬁcation semantically related words unlabeled data used help train language model. multi-task model consisting general sparse learner tasks task-speciﬁc sparse learners regularized pre-computed task similarities proposed multi-domain sentiment classiﬁcation. multi-task shared hidden layers among tasks used multi-domain dialog state tracking. three multi-task encoder-decoder architectures proposed several applications. ﬁrst architecture encoder shared tasks decoders task-speciﬁc used machine translation syntactic parsing. second task encoder decoder shared tasks proposed machine translation image caption generation. last multiple encoders decoders shared among tasks machine translation. variants multi-task feature selection method proposed analyze contents microblogs different locations event forecasting civil unrest inﬂuenza outbreak location treated classiﬁcation task. applications multi-task boosting method different tasks share feature representation proposed learning rank search. multi-task boosting task common classiﬁer task-speciﬁc classiﬁer similar proposed search ranking. multi-domain collaborative ﬁltering method based matrix factorization proposed utilize rating matrices multiple related domains help similar matrix-variate normal prior placed latent user features learn correlations pair domains. multi-task feature selection additional regularization used behavioral targeting. mtrl method extended consider hierarchical structure well structural sparsity conversion maximization display advertising. ubiquitous computing multi-task neural network shared hidden layers different tasks used predict yearly returns stocks. multi-task model learns low-rank transformation enforces model parameters different tasks similar transformed space proposed multi-device localization. inverse dynamics problem robotics solved perspective mtl. multitask trajectory regression method encodes spatial temporal information laplacian regularization fused lasso proposed estimate road travel costs road networks. multi-task dirty trajectory regression method captures spatial temporal information laplacian regularization identiﬁes outliers regularization proposed predicting travel time arbitrary trajectories road networks. multi-task model tree-structured regularization regularization proposed recognition trafﬁc signs. generalization bound upper-bound generalization performance terms training loss model complexity conﬁdence core learning theory since identify learnability induce sample complexity. order derive generalization bound single-task learning usually four main tools including vapnik-chervonenkis dimension covering number stability rademacher complexity. four tools also helpful deriving generalization bounds. ﬁrst generalization bound presented based dimension covering number characterize relation generalization performance empirical performance. ben-david study generalization bound based dimension assuming data distributions different tasks transformed known family functions. ando zhang covering number analyze generalization performance low-rank model formulated problem given pool hypotheses algorithm proposed task hypothesis pool dimension used derive generalization bound. pentina ben-david present generalization bound learning multiple kernels multi-task large-margin classiﬁers based covering number. zhang extends conventional stability setting proving generalized mcdiarmids inequality uses proposed multi-task stability analyze trace norm regularization low-rank approach dirty model different tools rademacher complexity provide datadependent generalization bounds make estimation generalization performance terms training data possible. maurer analyzes generalization bound method learns common feature transformation tasks special case without problem analyzes generalization bounds regularized models based given task relations schatten-norm-regularized models generalizations trace norm regularization. kakade analyze batch online multi-task models sparse regularizations trace norm regularization based famous inequality originally derived online learning. multitask sparse coding method whose objective function shown problem maurer analyze generalization bound. dimension-independent generalization bound method based trace norm regularization derived based recent advances random matrices. gaussian average related rademacher complexity used derive generalization bound general model learn common feature transformation tasks. moreover well four tools upper-bound generalization performance kolmogorov complexity analysis tool information theory used bound generalization performance. besides generalization bounds works discuss theoretical problems mtl. example argyriou discuss conditions regularized algorithms representer theorems hold. several studies investigate conditions well recover true features multi-task feature selection methods. moreover solnon show element optimal calibration covariance matrix noise different tasks based analysis present algorithm estimate covariance matrix based minimal penalty. paper survey different aspects mtl. first give classiﬁcation models seven main approaches including feature learning approach low-rank approach task clustering approach task relation learning approach dirty approach multi-level approach deep learning approach discuss characteristics. review combinations learning paradigms including unsupervised learning semi-supervised learning active learning reinforcement learning multi-view learning graphical models help improve performance. online parallel distributed models well feature hashing discussed speedup large number tasks data high-dimensional space. applications various areas introduced show usefulness mtl. theoretical aspects discussed. future several issues addressed. ﬁrst issue existing studies mainly focus supervised tasks ones tasks unsupervised learning semi-supervised learning active learning multi-view learning reinforcement learning tasks. natural adapt extend seven approaches introduced section non-supervised tasks think adaptation extension require effort design appropriate models. moreover worth trying secondly outlier tasks unrelated tasks investigation well known hamper performance tasks learning jointly. methods handle outlier tasks alleviating negative effects bring. however lacks principled ways theoretical analyses study resulting negative effects. order make safe used human important issue needs studies. recently deep learning become dominant approach many areas several multi-task deep models proposed reviewed sections however multitask deep models share hidden layers crossstitch network exception. approach powerful tasks related vulnerable noisy outlier tasks deteriorate performance dramatically. believe desirable design ﬂexible robust multi-task deep models. caruana multitask learning machine learning vol. yang survey transfer learning ieee transactions knowledge data engineering vol. liao carin krishnapuram multi-task learning classiﬁcation dirichlet process priors journal machine learning research vol. zhang d.-y. yeung convex formulation learning task relationships multi-task learning proceedings conference uncertainty artiﬁcial intelligence zhang d.-y. yeung regularization approach learning task relationships multitask learning transactions knowledge discovery data vol. article yang xing heterogeneous multitask learning joint sparsity constraints advances neural information processing systems bickel bogojeska lengauer scheffer multi-task learning therapy screening proceedings twenty-fifth international conference machine learning liao carin radial basis function network multi-task learning advances neural information processing systems silver poirier currie inductive transfer contextsensitive neural networks machine learning vol. argyriou micchelli pontil ying spectral regularization framework multi-task structure learning advances neural information processing systems maurer pontil romera-paredes sparse coding multitask transfer learning proceedings international conference machine learning chen xing inﬁnite latent classiﬁcation multi-task learning advances neural information processing systems gong zhou efﬁcient multi-task feature learning calibration proceedings sigkdd international conference knowledge discovery data mining wang safe screening multi-task feature learning multiple data matrices proceedings international conference machine learning palatucci zhang blockwise coordinate descent procedures multi-task lasso applications neural semantic basis discovery proceedings international conference machine learning vogt roth complete analysis group-lasso proceedings international conference machine learning zhang song encoding tree sparsity multi-task learning probabilistic framework proceedings aaai conference artiﬁcial intelligence jebara multi-task feature kernel selection svms proceedings international conference machine learning hern´andez-lobato hern´andez-lobato learning feature selection dependencies multi-task learning advances neural information processing systems hern´andez-lobato hern´andez-lobato ghahramani probabilistic model dirty multi-task feature selection proceedings international conference machine learning ando zhang framework learning predictive structures multiple tasks unlabeled data journal machine learning research vol. chen tang convex formulation learning shared structures multiple tasks proceedings international conference machine learning agarwal gerber learning multiple tasks using manifold regularization advances neural information processing systems zhang ghahramani yang learning multiple related tasks using latent independent component analysis advances neural information processing systems zhang ghahramani yang flexible latent variable models multi-task learning machine learning vol. dunson carin matrix stick-breaking process ﬂexible multi-task learning proceedings international conference machine learning liao carin nonparametric bayesian feature selection multi-task learning proceedings ieee international conference acoustics speech signal processing zhang d.-y. yeung multi-task boosting exploiting task relationships proceedings european conference machine learning principles practice knowledge discovery dtabases dinuzzo gehler pillonetto learning output kernels block coordinate descent proceedings international conference machine learning ciliberto mroueh poggio rosasco convex learning multiple tasks structure proceedings international conference machine learning zhang d.-y. yeung learning high-order task relationships multi-task learning proceedings international joint conference artiﬁcial intelligence rakitsch lippert borgwardt stegle noise efﬁcient multi-task gaussian process inference structured residuals advances neural information processing systems jalali ravikumar sanghavi ruan dirty model multi-task learning advances neural information processing systems chen learning incoherent sparse low-rank patterns multiple tasks proceedings sigkdd international conference knowledge discovery data mining chen zhou integrating low-rank group-sparse structures robust multi-task learning proceedings sigkdd international conference knowledge discovery data mining zhang zeng kumar deep model based transfer multi-task learning biological image analysis proceedings sigkdd international conference knowledge discovery data mining mrksic s´eaghdha thomson gasic vandyke young multi-domain dialog state tracking using recurrent neural networks proceedings annual meeting association computational linguistics quadrianto smola caetano vishwanathan petterson multitask learning without label correspondences advances neural information processing systems wimalawarne sugiyama tomioka multitask learning meets tensor factorization task imputation convex optimization advances neural information processing systems reichart tomanek hahn rappoport multi-task active learning linguistic annotations proceedings annual meeting association computational linguistics wang wipf ling chen wassell multitask learning subspace segmentation proceedings international conference machine learning zhang zhang self-adapted multi-task clustering proceedings international joint conference artiﬁcial intelligence wilson fern tadepalli multi-task reinforcement learning hierarchical bayesian approach proceedings twenty-fourth international conference machine learning parisotto salakhutdinov actor-mimic deep multitask transfer reinforcement learning proceedings international conference learning representations niculescu-mizil caruana inductive transfer bayesian network structure learning proceedings international conference artiﬁcial intelligence statistics honorio samaras multi-task learning gaussian graphical models proceedings international conference machine learning pillonetto dinuzzo nicolao bayesian online multitask learning gaussian processes ieee transactions pattern analysis machine intelligence vol. saha venkatasubramanian online learning multiple tasks relationships proceedings fourteenth international conference artiﬁcial intelligence statistics weinberger dasgupta langford smola attenberg feature hashing large scale multitask learning proceedings annual international conference machine learning wang shterev wang carin dunson hierarchical kernel stick-breaking process multi-task image analysis proceedings international conference machine learning wang zhang zhang boosted multi-task learning face veriﬁcation applications image video search proceedings ieee conference computer vision pattern recognition wang huang risacher ding saykin shen sparse multi-task regression feature selection identify brain imaging predictors memory performance proceedings ieee international conference computer vision hong prokhorov tracking robust multi-task multi-view joint sparse representation proceedings ieee international conference computer vision ricci ramanathan lanz sebe matter flexible graph-guided multi-task learning multi-view head pose classiﬁcation target motion proceedings ieee international conference computer vision yuan tian yang wang multi-task sparse learning beta process prior action recognition proceedings ieee conference computer vision pattern recognition lapin schiele hein scalable multitask representation learning scene classiﬁcation proceedings ieee conference computer vision pattern recognition yang zhang tian davis multi-task learning rank attribute embedding person re-identiﬁcation proceedings ieee international conference computer vision almaev mart´ınez valstar learning transfer transferring latent task structures application person-speciﬁc facial action unit detection proceedings ieee international conference computer vision widmer leiva altun r¨atsch leveraging sequence classiﬁcation taxonomy-based multitask learning proceedings annual international conference research computational molecular biology alamgir grosse-wentrup altun multitask learning brain-computer interfaces proceedings international conference artiﬁcial intelligence statistics zhou yuan multi-task learning formulation predicting disease progression proceedings sigkdd international conference knowledge discovery data mining zhang fang risacher saykin shen sparse bayesian multi-task learning predicting cognitive outcomes neuroimaging measures alzheimer’s disease proceedings ieee conference computer vision pattern recognition wang huang risacher saykin shen high-order multi-task feature learning identify longitudinal phenotypic markers alzheimer’s disease progression prediction advances neural information processing systems valentini-botinhao watts king deep neural networks employing multi-task learning stacked bottleneck features speech synthesis proceedings ieee international conference acoustics speech signal processing richmond yamagishi stylianou maia fusion multiple parameterisations dnn-based sinusoidal speech synthesis multi-task learning proceedings annual conference international speech communication association collobert weston uniﬁed architecture natural language processing deep neural networks multitask learning proceedings international conference machine learning zhao chen ramakrishnan feature constrained multi-task learning models spatiotemporal event forecasting ieee transactions knowledge data engineering vol. chapelle shivaswamy vadrevu weinberger zhang tseng multi-task learning boosting application search ranking proceedings sigkdd international conference knowledge discovery data mining ahmed smola anastasakos webscale multi-task feature selection behavioral targeting proceedings international conference information knowledge management ahmed smola scalable hierarchical multitask learning algorithms conversion optimization display advertising proceedings international conference search data mining chai williams klanke vijayakumar multi-task gaussian process learning robot inverse dynamics advances neural information processing systems december huang chen robust dynamic trajectory regression road networks multi-task learning framework proceedings ieee international conference data mining wang zhou zhang ling trafﬁc sign recognition multi-modal tree-structure embedded multi-task learning ieee transactions intelligent transportation systems vol. ben-david gehrke schuller theoretical framework learning pool disparate data sources proceedings sigkdd international conference knowledge discovery data mining", "year": 2017}