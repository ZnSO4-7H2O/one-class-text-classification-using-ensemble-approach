{"title": "Prediction with Expert Advice by Following the Perturbed Leader for  General Weights", "tag": ["cs.LG", "cs.AI", "I.2.6; G.3"], "abstract": "When applying aggregating strategies to Prediction with Expert Advice, the learning rate must be adaptively tuned. The natural choice of sqrt(complexity/current loss) renders the analysis of Weighted Majority derivatives quite complicated. In particular, for arbitrary weights there have been no results proven so far. The analysis of the alternative \"Follow the Perturbed Leader\" (FPL) algorithm from Kalai (2003} (based on Hannan's algorithm) is easier. We derive loss bounds for adaptive learning rate and both finite expert classes with uniform weights and countable expert classes with arbitrary weights. For the former setup, our loss bounds match the best known results so far, while for the latter our results are (to our knowledge) new.", "text": "tives quite complicated. particular arbitrary weights results proven far. analysis alternative follow perturbed leader algorithm easier. derive loss bounds adaptive learning rate ﬁnite expert classes uniform weights countable expert classes arbitrary weights. former setup loss bounds match best known results latter results new. prediction expert advice follow perturbed leader general weights adaptive learning rate hierarchy experts expected high probability bounds general alphabet loss online sequential prediction theory prediction expert advice rapidly developed recent past. starting weighted majority algorithm littlestone warmuth aggregating strategy vovk vast variety diﬀerent algorithms variants published. parameter algorithms learning rate. parameter ﬁxed early algorithms established so-called doubling trick make learning rate coarsely adaptive. little later incrementally adaptive algorithms developed unfortunately loss bound proofs incrementally adaptive variants quite complex technical despite typically simple elegant proofs static learning rate. complex growing proof techniques also another consequence original algorithm assertions proven countable classes experts arbitrary weights modern variants usually restrict ﬁnite classes uniform weights discussion section). might suﬃcient many practical purposes prevents application general classes predictors. examples extrapolating data points help polynomial degree d=... –or– class computable predictors. furthermore authors concentrated predicting binary sequences often losses less common. nevertheless easy abstract completely predictions consider resulting losses only. instead predicting according weighted majority time step chooses single expert probability depending past cumulated loss. done e.g. elegant variant hedge algorithm analyzed. diﬀerent general approach achieve similar results follow perturbed leader principle dates back early called hannan’s algorithm kalai vempala published simpler proof main result hannan also succeeded improve bound modifying distribution perturbation resulting algorithm performance guarantees wm-type algorithms work study algorithm pea. problems algorithms mentioned addressed consider countable expert classes arbitrary weights adaptive learning rate arbitrary losses. regarding adaptive learning rate obtain proofs simpler elegant corresponding algorithms. further prove ﬁrst loss bounds arbitrary weights adaptive learning rate. result even seems ﬁrst equal weights arbitrary losses however proof technique likely carry case. paper structured follows. section give basic deﬁnitions. sections derive main analysis tools following lines important extensions. applied order prove various upper bounds section section proposes hierarchical procedure improve bounds non-uniform weights. section lower bound established. section treats additional issues. finally section discuss results compare references state open problems. sequential predictions yt∈y times .... time step access predictions ≤i≤n experts {e...en}. made prediction make observation loss revealed expert’s deﬁnes weight means e−ki vice versa. following talk rather complexities weights. ﬁnite usually sets case uniform complexities/weights. experts countably inﬁnite uniform complexities possible. vector complexities denoted ≤i≤n. time expert suﬀers loss t)≤i≤n vector losses time +...+st− total past loss vector smin loss best expert hindsight usually know advance time forward identiﬁed space unit vectors ={ei since decision consists selecting single expert states identiﬁed require minima attained. main focus however linear function always attained follow perturbed leader. given time immediate idea solve expert problem follow leader i.e. selecting expert performed best past predict according expert approach fails reasons. first minimum wrong prediction solve ﬁrst problem penalizing expert complexity i.e. predicting according expert approach solves second problem adding expert’s loss random perturbation. choose perturbation negative exponentially distributed either independent time step beginning time possibilities equivalent respect expected losses since expectation linear. former choice preferable order protect adaptive adversary generates order bounds high probability main analysis however latter convenient. henceforth assume without loss generality initial perturbation dealing expected loss. depends learning rate give choices section established main tools analysis. expected loss time ◦st]. idea analysis intermediate predictor ifpl ifpl predicts according thus knowledge ◦st] denote expected loss ifpl time losses ifpl upper bounded section lower bounded section note also expert implement highly complicated strategy depending past outcomes despite trivializing identiﬁcation constant unit vector. complex expert’s behavior summarized hidden state vector =loss≤i≤n. results therefore apply arbitrary prediction observation spaces arbitrary bounded loss functions. contrast major part work developed binary alphabet absolute loss only. finally note setup allows losses generated adversary tries maximize regret knows algorithm experts’ past predictions/losses. adversary also access fpl’s past decisions must independent randomization time step order achieve good regret bounds. section provide tools comparing loss ifpl loss best expert hindsight. ﬁrst result bounds expected error induced exponentially distributed perturbation. proof. s<t+ past cumulative penalized state vector vector exponential distributions i.e. e−qi deﬁne random variables argmini{si− furthermore ﬁxed vector ﬁxed deﬁne mini=j{si− xi}≤ xi}=m′. notation using independence mini=j{si+si distinguish static dynamic bounds. static bounds refer constant εt≡ε. since value chosen advance static choice requires certain prior information therefore practical many cases. however static bounds easy derive provide good means compare diﬀerent algorithms. hand algorithm shall applied without appropriate prior knowledge dynamic choice depending and/or past observations necessary. proofs results similar loss several pages next result establishes similar bound instead using expected value best loss smin used. computational advantages since smin immediately available needs evaluated adding them. hence bounds written ≤qsi ≤qsi √loss-regrets bounded remark. analysis theorems applies general using eεtnrt instead eεtrt leading additional factor regret. derived. case likely improved ﬁnite case hold. able derive improved bounds modiﬁcation. consider two-level hierarchy experts. first consider subclass experts form fpl. class meta experts contains complexity expert allows derive good bounds. following quantities referring complexity class superscripted initial versus independent randomization. assumed perturbations sampled time already indicated expectation equivalent generating perturbation time step former favorable analysis latter advantages. first losses generated adaptive adversary time ﬁgure random perturbation force large loss. second repeated sampling perturbations guarantees better bounds high probability. bounds high probability. derived several bounds expected loss fpl. actual loss time ◦st. simple markov inequality shows total actual loss exceeds total expected loss factor probability randomizing independently described previous paragraph actual loss expected loss before. advantage independent randomization much better high-probability bound. exploit chernoﬀ-hoeﬀding bound need compute expectations explicitly. given need compute order update note actual prediction fpl. s<t+k/εt following representation computed time overall time compute time numerically compute integral. inﬁnite last approximated dominant contributions. expectation also approximated sampling several times. recall approximating avoided using smin instead. deterministic prediction absolute loss. another last paragraph following decision space make deterministic decision d=wt∈∆ time bounds holding sure instead selecting probability |xt−yi observation predictions master algorithm predicting general weights. several dynamic bounds uniform weights result non-uniform weights know gives dynamic bound p-norm algorithm absolute loss weights rapidly decaying. hierarchical bound theorem generalizes arbitrary weights losses strengthens since both asymptotic order leading constant smaller. also analysis gets complicated also hold prediction. {νi} class probability distributions sequences assume true sequence sampled µ∈{νi} complexity known bayes-optimal predictor based −kνi -weighted mixture νi’s expected total loss lµ+√lµkµ+kµ expected total loss bayes-optimal predictor based using obtained bound except leading order constant sequence independently assumption generated another indication bound leading constant could hold. detailed comparison bayes bounds bounds.", "year": 2004}