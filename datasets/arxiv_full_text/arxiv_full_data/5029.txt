{"title": "Learning Solving Procedure for Artificial Neural Network", "tag": ["cs.AI", "cs.LG"], "abstract": "It is expected that progress toward true artificial intelligence will be achieved through the emergence of a system that integrates representation learning and complex reasoning (LeCun et al. 2015). In response to this prediction, research has been conducted on implementing the symbolic reasoning of a von Neumann computer in an artificial neural network (Graves et al. 2016; Graves et al. 2014; Reed et al. 2015). However, these studies have many limitations in realizing neural-symbolic integration (Jaeger. 2016). Here, we present a new learning paradigm: a learning solving procedure (LSP) that learns the procedure for solving complex problems. This is not accomplished merely by learning input-output data, but by learning algorithms through a solving procedure that obtains the output as a sequence of tasks for a given input problem. The LSP neural network system not only learns simple problems of addition and multiplication, but also the algorithms of complicated problems, such as complex arithmetic expression, sorting, and Hanoi Tower. To realize this, the LSP neural network structure consists of a deep neural network and long short-term memory, which are recursively combined. Through experimentation, we demonstrate the efficiency and scalability of LSP and its validity as a mechanism of complex reasoning.", "text": "abstract expected progress toward true artiﬁcial intelligence achieved emergence system integrates representation learning complex reasoning response prediction research conducted implementing symbolic reasoning neumann computer artiﬁcial neural network here present learning paradigm learning solving procedure learns procedure solving complex problems. accomplished merely learning input-output data learning algorithms solving procedure obtains output sequence tasks given input problem. neural network system learns simple problems addition multiplication also algorithms complicated problems complex arithmetic expression sorting hanoi tower. realize this neural network structure consists deep neural network long short-term memory recursively combined. experimentation demonstrate efﬁciency scalability validity mechanism complex reasoning. introduction attempt implement human reasoning capabilities machine expert system knowledge base inference engine emerged. however proven insufﬁcient transplanting human complex reasoning machine which top-down approach reached limit. approach called connectionism uses artiﬁcial neural network emerged. using back-propagation algorithm multi-layer architecture applied many recognition areas. however notable achievements made algorithm improvements parallel computing using multiple cpus gpus enable many layers steps feed-forward recurrent neural networks. thus problems overﬁtting vanishing gradient resolved. remarkable achievements also realized areas image recognition speech recognition natural language processing however achievements represent learning input-output dataprocessing falls short intent implement complex reasoning. neumann computers course possess solid history dependable symbolic-reasoning machines. however simply faithfully execute prewritten reasoning algorithms rather demonstrate complex ability reason creatively like humans. therefore classical artiﬁcial intelligence expert systems expose weakness autonomous learning lack adaptable capability. alternatively advanced anns deep learning networks demonstrate excellent learning capabilities input-output patterns. however anns experience difﬁculty learning complex computational logical reasoning problems otherwise easily handled neumann computers. therefore several attempts made implement symbolic reasoning neumann computer architecture leveraging idea neural-symbolic integration needed complex reasoning. modeling study biological neuron performance working memory using continuous ﬁring solve explicit tasks conducted ﬁeld neuroscience. graves introduced neural turing machine implements concepts anns. neural turing machine allows read-write operations given external memory space enabling weighted addressing long short-term memory turing machine general computer. extension differentiable neural computer means external memory treated variable differentiable read-write controls activated. similar associative long-term potentiation mammalian hippocampus effect creating adaptability transplanting symbolic reasoning mechanism neumann computer processes memory separated. however experiment consisting synthetic questions answers graphs block puzzles showed ability solve complex reasoning problems like human beings learning learning sequence procedures sub-problems instead learning data transitions transformations given complex problem. learning input-output data relying generalization regularization learning method constitute logical reasoning process. learning input-output data precisely solve problem rule possibility accidently solving different algorithm. take example instance inputoutput pattern coincidentally identical given sample dataset. assume algorithm problem training sampled input space algorithma trained using training sample sure learned answer generally generalization learning often described complement completeness algorithmic learning. input-output data-learning complete learning algorithm impossible? generally exists another problem algorithm satisﬁes following condition output sample coincide. however output different remaining parts except algorithm learn revealed testing. neither therefore size input space extremely large approximately learn algorithm given problem input-output; impossible learn completely. regard research high-level programs learned series low-level programs conducted neural programmer-interpreters programs environments input-output parameters etc. used train given programs series subprograms. advantage learning smaller amount sample data input-output pattern dnc. terms implementing entire program logic rather neural simulation external memory system make program object learning. sort functions learned decomposed series low-level sub-functions. somewhat inadequate programs studied limited execution sub-program required learning process independent neural system. owing lack iterative recursive expansion methods limits expanding algorithmic learning complex problems. here propose learning paradigm learning solving procedure learns problem-solving capability main factor complex reasoning. provides learn complex algorithms sequential combinations simpler problems. additionally unlike uses symbolic representation freely expressed numbers symbols rather rigid form-of-function learning theme. composed repetitively recursively usable components. learns executes algorithm complex problem combining deep neural network lstm. experiments demonstrate solving algorithms complex arithmetic expressions addition multiplication sorting hanoi tower successfully learned solved lsp. learning solving procedure goal study implementing learn algorithms given problems execute algorithms directly. algorithm learning propose learning solving procedure learning paradigm learns procedure solving given problem design architecture implements lsp. ﬁnal architecture learn execute well-known complex algorithms. neural network learn problem-solving proceanswer question propose formal method called task. types tasks simple complex. simple task immediate answer complex task consists series additional simple complex tasks. tasks required execute complex task called subtasks. neural network architecture question propose architecture composed lstm. distinguishes whether input task simple complex. simple tasks learns input-output data recalls output given input. case complex task consisting series multiple subtasks sends complex task lstm creates series subtasks. subtasks recursively entered obtain answers. therefore pure recursively composed lstm. tasks decomposed series subtasks learn generalized algorithm learning procedure solve problem. suggests provide fundamental starting point complex reasoning neural networks. illustrate working principle demonstrate learn simplest forms addition multiplication algorithms. numbers addition multiplication purely symbolic data quantitative. design neural network learns executes procedures solve arithmetic addition multiplication similar small child entering elementary school. students learn concept single-digit addition multiplication memorize results tables. addition multiplication numbers greater digits memorized tables used carry sent next digit left. execution performed order subtasks mul. performed order subtasks mul. here converted using memorized multiplication tables. performed addition example described above. regarded simple task result requires computation. simple value etc. considered complex tasks require recursive calculations involving additional subtasks. architecture addition multiplication propose architecture learn perform addition multiplication algorithms. training neural network addition multiplication solving procedures neural network learns addition multiplication algorithms. recursive structure composed lstm. input tasks input identiﬁes whether task simple complex. simple task outputs answer immediately. trained memorize addition multiplication tables. given complex task passes lstm outputs subtasks order solving procedure trained memorized complex task. subtasks recursively input execution obtain answers. architecture addition multiplication design extended architecture solve complicated problems adding many lsp-xxxs effectively compose complex problems solve them. architecture learn execute algobasic rithms complex problems data structure-processing ability needed algorithm performance. propose architecture dynamic memory support module support list stack minimum basic algorithmic data structures. consists many lsp-xxxs characterized respective functions. distinguishes input task function transfers corresponding lsp-xxx. lsp-xxx function recursive architecture composed lstm. receives input task input data distinguishes whether simple complex task. simple task outputs memorized answer. complex task sends corresponding lstm generate subtasks learned solving procedure. subtask enters recursively arrive answer. output answer enters lstm input next step thereby controlling generation execution subtask next step. supports memory functions required lsplist lsp-stack read write rightshift leftshift newlist newstack isempty etc. lsp-list supports basic list-speciﬁc tasks insertlist head tail using basic functions dms. lsp-stack supports basic stack-speciﬁc tasks push pop. case multiplication simple task singledigit multiplication without carry. cases including single-digit multiplication carry complex tasks. complex tasks lstm sequentially generates subtasks multiplication-solving procedure processes recursively lower lsp. example following subtasks generated solving procedure. mul→mul mulmul mul. again following subtasks generated solving procedure. mul→mul mulmuland mul. digit multiplication carry transformed using multiplication tables memorized dnn. then passed lstm processed addition. extended architecture addition multiplication learn solving procedure compute primitive addition multiplication problems. shown model learn algorithm speciﬁc problems addition multiplication. however addition multiplication perform simple addition multiplication operations limited numbers digits insufﬁcient learn task general problem-solving. therefore extend experiment experiment environment experiments conducted intel xeon dram nvidia intel dram nvidia experimental program implemented using python tensorﬂow. experimental goal purpose experiment proving learn solving procedures complex tasks correctly execute them. selected three problems lsp-eval lsp-sort lsp-hanoi. tasks supported lsp-xxxs successful execution. thus consists total nine lsp-xxxs running tasks. solving procedure eval task lsp-eval eval task ifpf subtask converts inﬁx expression postﬁx expression calcpf subtask calculates postﬁx expression outputting result. string obtained image analysis third complex arithmetic expressions consisting combination additions multiplications parentheticals recognized computed fourth solving procedure sort task lsp-sort sort task consists solving procedures. first inputlist containing unsorted data empty terminates. second inputlist empty separated head tail sorted tail. inserts head solving procedure hanoi task lsp-hanoi hanoi task ﬁnds order moving disks queue queue queue displays result moves. moving disks necessary satisfy constraint large disk cannot placed atop small disk. initially disklist contains largest disk number head smaller disks tail decreasing order. isempty task controls creation next subtask similar sorting task described above. small disks tail ﬁrst moved largest bottom disk moved small disks tail moved hanoi subtask moves disks tail recursively executed whole process moving disks performed output hanoioutput subtask. regarded variables separated independent dependent types. dependent variable determined value independent variable affect generation training data. thus training/testing data generated assigning possible values independent variables. call method values independent variables however size training/test data becomes large. resolve this assign values selected independent variable randomly assign values independent variables. select independent variables one-by-one generate training/test data. call method values variable random values others exists subtasklsp-output subtask lsp-output return. suppose independent variables; dependent variables; number values assigned respectively. training data generated method training data totals however method used total training data generated. table shows experimental results comparison. training/test data generation tasks must training test accuracies nearly produce accurate output input task. therefore architecture lstm constituting carefully adjusted training/test data carefully selected accuracies almost total tasks. uses average layers. average number nodes layer lstm uses oneto-two layers nodes layer. training/test data tasks generated method trained tensorﬂow. average training accuracy average test accuracy thus trained tested almost accuracy. integration representation learning complex reasoning paper demonstrates utility complex reasoning showing learning algorithms possible. also necessary show possibility representation learning image recognition complex reasoning naturally integrated. accomplish this input three types tasks images showed anns solve them. whereas image input contribution paper meaningful attempt show smooth integration representation learning complex reasoning. modiﬁed national institute science dataset images handwritten characters various styles. using images created task images used training. visual attention-based optical characterreader model used image analysis recognition error rate generated task images close images caused recognition errors excluded. result task image analysis string. lsp-string converts string task input. results experiment eval task output values correct. nine cases show small calculation error digit ﬁnal calculation incorrect. example correct output error appears hundreds position possibility small errors accumulate task repeatedly executes multiple subtasks. future research discuss possibility future academic progress warranted based following ideas hypotheses. deﬁne algorithm sequence action rules limit discussion types. assume construct algorithms assembling parts called action rules. answer question make following assumptions. first contains many lsp-xxxs welltrained tasks. task every lsp-xxx training input task speciﬁc input output pair training output task action rule lsp-al trained inputs outputs. test hypothesis problem whose algorithm well known already know solving procedures. test successful problem whose algorithm unknown solving procedures empirically known input cases conjecture proposed paper guess algorithm problem conclusion attempt realize complex reasoning efforts implement symbolic reasoning mechanism neumann computers neural networks input-output data-learning. instead paper proposed learn solving procedure given problem. architecture recursive structure consisting lstm learns solving procedure given problem evaluates answer learned procedure. successfully demonstrated problem images complex arithmetic expressions sorting hanoi tower tasks recognized processed lsp. thus shown rudimentary attempt successfully combine representation learning complex reasoning. contend human process problem-solving simulated. different previous work advantages ﬂexibility scalability. expect future work learn solving procedure unknown-algorithm problem autonomously fully exploiting scalability lsp. bordes chopra weston question answering subgraph embeddings. proc. empirical methods natural language processing arxiv. dahl improving dnns lvcsr using rectiﬁed linear units dropout. icassp. dayan simple substrates complex cognition. frontiers neuroscience eliasmith build brain neural architecture biological cognition. oxford university press. farabet couprie najman lecun ieee learning hierarchical features scene labeling. trans. pattern anal. mach. intell. graves hybrid computing using neural network dynamic external memory nature graves wayne danihelka neural turing machine. arxiv.. hazy frank oreilly banishing homunculus making working memory work. neuroscience jaeger deep neural reasoning. nature vol. hinton deep neural networks acoustic ieee signal processing modeling speech recognition. magazine hinton osindero fast learning algorithm deep belief nets. neural computation. hochreiter bengio frasconi schmidhuber gradient recurrent nets difﬁculty learning long-term dependencies. kremer kolen editors field guide dynamical recurrent neural networks. ieee press. hochreiter schmidhuber long short-term memory. neural computation hochreiter untersuchungen dynamischen neuronalen netzen. diploma thesis institut informatik technische univ. munich. hahnloser sarpeshkar mahowald douglas seung h.s. nature.. imkrizhevsky sutskever hinton agenet classiﬁcation deep convolutional neural networks. proc. advances neural information processing systems lecun lenet- convolutional neural networks. http//yann.lecun.com/exdb/lenet/ lecun bengio hinton deep learning. nature vol. mikolov deoras povey burget cernocky strategies training large scale neural network language models. proc. automatic speech recognition understanding sainath mohamed a.-r.; kingsbury ramabhadran deep convolutional neural networks lvcsr. proc. acoustics speech signal processing wang robust scene text recognition automatic rectiﬁcation. cvpr. srivastava hinton krizhevsky sutskever salakhutdinov dropout simple prevent journal machine neural networks overﬁtting. learning research szegedy going deeper convolutions. ieee conference computer vision pattern recognition reed freitas neural programmerinterpreters. arxiv.. tetko livingstone luik neural network studies. comparison overﬁtting overtraining. chem. inf. comput. sci. tompson jain lecun bregler joint training convolutional network graphical model human pose estimation. proc. advances neural information processing systems twerbos beyond regression tools prediction analysis behavioral sciences. thesis harvard university.", "year": 2017}