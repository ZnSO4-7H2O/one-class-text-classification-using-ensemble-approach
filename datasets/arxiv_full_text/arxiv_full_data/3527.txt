{"title": "Interpreting Convolutional Neural Networks Through Compression", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Convolutional neural networks (CNNs) achieve state-of-the-art performance in a wide variety of tasks in computer vision. However, interpreting CNNs still remains a challenge. This is mainly due to the large number of parameters in these networks. Here, we investigate the role of compression and particularly pruning filters in the interpretation of CNNs. We exploit our recently-proposed greedy structural compression scheme that prunes filters in a trained CNN. In our compression, the filter importance index is defined as the classification accuracy reduction (CAR) of the network after pruning that filter. The filters are then iteratively pruned based on the CAR index. We demonstrate the interpretability of CAR-compressed CNNs by showing that our algorithm prunes filters with visually redundant pattern selectivity. Specifically, we show the importance of shape-selective filters for object recognition, as opposed to color-selective filters. Out of top 20 CAR-pruned filters in AlexNet, 17 of them in the first layer and 14 of them in the second layer are color-selective filters. Finally, we introduce a variant of our CAR importance index that quantifies the importance of each image class to each CNN filter. We show that the most and the least important class labels present a meaningful interpretation of each filter that is consistent with the visualized pattern selectivity of that filter.", "text": "convolutional neural networks achieve state-of-the-art performance wide variety tasks computer vision. however interpreting cnns still remains challenge. mainly large number parameters networks. here investigate role compression particularly pruning ﬁlters interpretation cnns. exploit recently-proposed greedy structural compression scheme prunes ﬁlters trained cnn. compression ﬁlter importance index deﬁned classiﬁcation accuracy reduction network pruning ﬁlter. ﬁlters iteratively pruned based index. demonstrate interpretability car-compressed cnns showing algorithm prunes ﬁlters visually redundant pattern selectivity. speciﬁcally show importance shape-selective ﬁlters object recognition opposed color-selective ﬁlters. car-pruned ﬁlters alexnet ﬁrst layer second layer color-selective ﬁlters. finally introduce variant importance index quantiﬁes importance image class ﬁlter. show least important class labels present meaningful interpretation ﬁlter consistent visualized pattern selectivity ﬁlter. artiﬁcial deep neural networks achieved cutting-edge performance many tasks artiﬁcial intelligence machine learning computer vision autonomous driving natural language speech processing. however substantial number weights dnns limited applications devices smart phones robots limited memory computational power. beside signiﬁcant save resources compressed networks less number parameters easier investigated interpreted humans possible domain knowledge gain. achieve goal compression scheme requires least properties. first performance compressed network close original uncompressed network. second compression method take account structure network. example considering convolutional neural network convolutional ﬁlters smallest meaningful components cnn. therefore uncover redundant information network build interpretable model natural compress cnns based removing \"less important\" ﬁlters. call schemes \"structural compression\" schemes. many works compressing deep neural networks. mostly focus reducing number size weights parameters pruning quantizing without considering functionality ﬁlters layer. optimal brain damage optimal brain surgeon deep compression recently squeezenet examples. studied structural compression based removing ﬁlters introduced importance indexes based average incoming outgoing weights ﬁlter. however importance measures typically yield satisfactory compressions cnns substantially reduced classiﬁcation accuracy result. recently introduced structural compression rules convolutional neural networks proposed importance index ﬁlters cnns. index based classiﬁcation accuracy reduction greedy structural compression scheme cnns proposed based car. structural compression algorithm ﬁlter least effect classiﬁcation accuracy gets pruned iteration. network retrained iteration pruning ﬁlter. process regarded tuning. algorithm achieves state-of-the-art classiﬁcation accuracy compression rate among structural compression schemes. pruning half ﬁlters either individual convolutional layers alexnet algorithm achieves higher classiﬁcation accuracies compared best benchmark ﬁlter pruning scheme retrain pruned network achieves classiﬁcation accuracy similarly close-to-original classiﬁcation accuracy benchmark schemes algorithm achieve compression ratio around higher benchmark methods. here concentrate interpretability compressed cnn. show considering network structure compression algorithm unique advantage accessible human interpreters. knowledge report ﬁrst extensive result connections compression interpretability cnns. study ability compression method identify redundant ﬁlters cnns take closer look compressed networks. section focus second layer alexnet. ﬁlters visually diverse functionalities considerably numbers ﬁlters lenet ﬁrst layer alexnet. also easier visualize ﬁlters layer compared higher layers alexnet. however similar results hold layers alexnet lenet. ﬁrst perform structural compression prune ﬁlters second layer alexnet continued iterate algorithm classiﬁcation accuracy within relative accuracy uncompressed network. pruning ﬁlter ﬁlters layer. subset removed remaining ﬁlters visualized figure visualize pattern selectivity ﬁlter million image patch network showed image patch activate ﬁlter. approach previously used study functionality ﬁlters deep cnns manually grouped ﬁlters visually similar pattern selectivity hand crafted image shown beside group demonstrate function ﬁlters group. algorithm tends keep least ﬁlter group suggesting greedy ﬁlter pruning process able identify redundant ﬁlters. indicates pruned ﬁlters based importance index fact redundant functionality network. looking deeper indexes pruned ﬁlters ﬁrst layer second layer correspond color ﬁlters respectively. ﬁnding points fact shape often ﬁrst-order important object recognition. investigate effect compression convolutional layers shown scatter plots classiﬁcation accuracy classes imagenet figure although total classiﬁcation accuracy relative lower compressed network accuracies many categories comparable compressed uncompressed networks. fact categories accuracies larger uncompressed network. figure compression removes ﬁlters visually redundant functionality second layer alexnet. ﬁlter visualized image patches highest responses ﬁlter. manually clustered ﬁlters second layer alexnet groups pattern selectivity group illustrated left corner using manually designed patch. continue iterate car-based algorithm classiﬁcation accuracy range relative accuracy uncompressed network. leads pruning ﬁlter ﬁlters layer. pruned ﬁlters speciﬁed circle. figure classiﬁcation accuracy class image alexnet individual layer compressed compared uncompressed network. point plots corresponds categories images test set. slight modiﬁcation deﬁnition importance index build index enables interpret ﬁlters cnns respect image classes. deﬁne carc classiﬁcation accuracy reduction class ﬁlter layer pruned. carc identiﬁes classes classiﬁcation accuracy highly depends existence ﬁlter. classes figure interpretation based carc consistent visualized pattern selectivity ﬁlter layer alexnet. panel shows image patches activate ﬁlter panel show bottom classes highest lowest carc respectively. besides class label sample image class also visualized. panel shows scatter plot classiﬁcation accuracy classes imagenet. three bottom classes highest lowest carc pointed green arrows. corresponds ﬁlter layer alexnet. ones highest carc among classes. similarly ﬁlter performance classes smallest carc less dependency ﬁlter. note carc indexes could negative numbers pruned network higher classiﬁcation accuracy compared original network. labels sets classes highest lowest carc present verbal interpretation ﬁlter network. carc-based interpretation better higher layers ﬁlters layers abstract therefore explainable class labels. figure show interpretation based carc consistent visualized pattern selectivity ﬁlter layer alexnet. similar previous section visualization based image patch activating ﬁlter. selected three ﬁlters layer among important ﬁlters layer based original pruning. similar visualization figure panel illustrates image patches activate ﬁlter. panels show bottom classes highest lowest carc respectively. besides class label sample image class also visualized. classes pointed green arrows scatter plot classiﬁcation accuracy classes imagenet classes highest carc share similar patterns visible patch activating ﬁlter. ﬁlter smooth elliptic curvature consistently appears classes steep arch bridge soup bowel visible activating patches hand less elliptic curvature patterns expected classes mailbag altar. filter higher carc classes contains patterns insect bird’s head. filter mostly selected classes contain images single long tool particularly musical instruments oboe banjo. structural compression cnns dual purposes saving memory computational cost small devices resulted cnns humanly interpretable. showed car-compression prunes ﬁlters visually redundant functionalities. also proposed variant index compare classiﬁcation accuracies original pruned cnns image class. general similar comparison carried networks importance index. fruitful direction pursue particularly given recent wave various cnns different structures. finally expect structural compression algorithm cnns related interpretations adapted fully-connected networks modiﬁcations. work supported national science foundation grant dms- center science information science technology center grant agreement ccf. gratefully acknowledge support nvidia corporation donation tesla used research.", "year": 2017}