{"title": "Graph based manifold regularized deep neural networks for automatic  speech recognition", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "Deep neural networks (DNNs) have been successfully applied to a wide variety of acoustic modeling tasks in recent years. These include the applications of DNNs either in a discriminative feature extraction or in a hybrid acoustic modeling scenario. Despite the rapid progress in this area, a number of challenges remain in training DNNs. This paper presents an effective way of training DNNs using a manifold learning based regularization framework. In this framework, the parameters of the network are optimized to preserve underlying manifold based relationships between speech feature vectors while minimizing a measure of loss between network outputs and targets. This is achieved by incorporating manifold based locality constraints in the objective criterion of DNNs. Empirical evidence is provided to demonstrate that training a network with manifold constraints preserves structural compactness in the hidden layers of the network. Manifold regularization is applied to train bottleneck DNNs for feature extraction in hidden Markov model (HMM) based speech recognition. The experiments in this work are conducted on the Aurora-2 spoken digits and the Aurora-4 read news large vocabulary continuous speech recognition tasks. The performance is measured in terms of word error rate (WER) on these tasks. It is shown that the manifold regularized DNNs result in up to 37% reduction in WER relative to standard DNNs.", "text": "deep neural networks successfully applied wide variety acoustic modeling tasks recent years. include applications dnns either discriminative feature extraction hybrid acoustic modeling scenario. despite rapid progress area number challenges remain training dnns. paper presents effective training dnns using manifold learning based regularization framework. framework parameters network optimized preserve underlying manifold based relationships speech feature vectors minimizing measure loss network outputs targets. achieved incorporating manifold based locality constraints objective criterion dnns. empirical evidence provided demonstrate training network manifold constraints preserves structural compactness hidden layers network. manifold regularization applied train bottleneck dnns feature extraction hidden markov model based speech recognition. experiments work conducted aurora spoken digits aurora- read news large vocabulary continuous speech recognition tasks. performance measured terms word error rate tasks. shown manifold regularized dnns result reduction relative standard dnns. recently resurgence research area deep neural networks acoustic modeling automatic speech recognition much research concentrated techniques regularization algorithms used parameter estimation time also great deal research graph based techniques facilitate preservation local neighborhood relationships among feature vectors parameter estimation number application areas algorithms preserve local relationships often referred effect applying manifold based constraints. subspace manifolds generally deﬁned low-dimensional perhaps nonlinear surfaces locally linear relationships exist among points along surface. hence preservation local neighborhood relationships often thought approximation effect preserving structure manifold. extent assumption correct suggests manifold shape preserved without knowledge overall manifold structure. paper presents approach applying manifold based constraints problem regularizing training algorithms based acoustic models asr. approach involves redeﬁning optimization criterion training incorporate local neighborhood relationships among acoustic feature vectors. work uses discriminative manifold learning based constraints. resultant training mechanism referred manifold regularized training described section results presented demonstrating impact mrdnn training word error rates obtained mrdnn trained models behavior associated nonlinear feature space mapping. previous work optimizing deep network training includes approaches pre-training network parameters restricted boltzmann machine based generative pre-training layer-by-layer discriminative pretraining stacked auto-encoder based pre-training however recent studies shown that enough observation vectors available training models network pre-training little impact performance techniques like dropout rectiﬁed linear units nonlinear elements place sigmoid units hidden layers network also thought effect regularization training work locality sensitive hashing techniques speeding neighborhood search methods. section brieﬂy describes manifold regularization framework example implementations. simply feed-forward artiﬁcial neural network multiple hidden layers input output layers. typically network produces mapping fdnn inputs output activations achieved ﬁnding optimal weights minimize global error loss measure deﬁned outputs network targets work lnorm regularized dnns used baseline models. objective criterion model deﬁned error signal layer form depends error function activation function. typically soft-max nonlinearity used output layer along cross-entropy objective criterion. assuming input unit ﬁnal layer calculated error signal case given netl manifold learning techniques work assumption high-dimensional data considered geometrically related points lying close surface smooth dimensional manifold typically techniques used dimensionality reducing feature space transformations goal preserving manifold domain relationships among data vectors original space discriminative features obtained bottleneck layer input gaussian mixture model based speech recognizer work extends presenting improved mrdnn training algorithm results reduced computational complexity better performance. performance proposed technique evaluated well known speech-in-noise corpora. ﬁrst aurora- speech corpus connected digit task second aurora- speech corpus read news large vocabulary continuous speech recognition task speech corpora systems conﬁgured corpora described section important impact proposed technique well-behaved internal feature representations associated mrdnn trained networks. observed modiﬁed objective criterion results feature space internal network layers local neighborhood relationships feature vectors preserved. input vectors within local neighborhood input space corresponding mappings internal layers also neighbors. property characterized means objective measure referred contraction ratio describes relationship sizes local neighborhoods input mapped feature spaces. performance mrdnn training producing mappings preserve local neighborhood relationships presented terms measured contraction ratio section locality preservation constraints associated mrdnn training also shown section lead robust gradient estimation error back propagation training. rest paper structured follows. second provides review basic principals associated training manifold learning manifold regularization. section describes mrdnn algorithm formulation provides discussion contraction ratio measure locality preservation dnn. section describes task domains system conﬁgurations presents performance results. section discusses computational complexity proposed mrdnn training effect noise manifold learning methods. section anway applying manifold regularization training discussed manifold regularization used ﬁrst training epochs. conclusions suggestions future work presented section section provides brief review dnns manifold learning principals context mrdnn approach presented section section provides summary dnns applications asr. introduction manifold learning related techniques provided section includes discriminative manifold learning framec refers class label associated vector gaussian kernel heat parameter. function indicates whether neighborhood neighborhood relationships vectors determined k-nearest neighborhood search. order design objective criterion minimizes manifold domain distances feature vectors belonging speech class maximizing distances feature vectors belonging different speech classes graph scatter measure deﬁned. measure represents spread graph average distance feature vectors target space respect original space. generic graph measure graph’s scatter mapping deﬁned application manifold learning methods supported argument speech produced movements loosely constrained articulators motivated this manifold learning used acoustic modeling feature space transformation techniques preserve local relationships feature vectors. realized embedding feature vectors partially connected graphs optimality criterion formulated based preservation manifold based geometrical relationships. example locality preserving projections feature space transformations objective function formulated feature vectors close original space also close target space. based dimensionality reducing feature space transformation techniques reported provide signiﬁcant gains conventional techniques linear discriminant analysis unsupervised manifold learning based motivated based constraints regularizing dnns work. challenge applying manifold learning based methods large computational complexity requirements methods directly scale datasets. complexity originates need calculate pair-wise similarity measure feature vectors construct neighborhood graphs. work uses locality sensitive hashing based methods fast construction neighborhood graphs described creates hashed signatures vectors order distribute number discrete buckets vectors close likely fall bucket manner efﬁciently perform similarity searches exploring data-points falling adjacent buckets. manifold learning techniques unsupervised non-discriminative example lpp. result techniques generally exploit underlying inter-class discriminative structure speech features. reason features belonging different classes might well separated target space. discriminative manifold learning framework feature space transformations proposed techniques incorporate discriminative training manifold based nonlinear locality preservation. dimensionality reducing mapping techniques attempt preserve within class manifold based relationships original space maximizing criterion related separability classes output space. manifold based relationships feature vectors characterized separate partially connected graphs namely intrinsic penalty graphs intrinsic graph gint nodes graphs ωint} characterizes within class within manifold relationships feature vectors. penalty graph gpen ωpen} characterizes relationships feature vectors belonging different speech classes. ωint ωpen known afﬁnity matrices intrinsic penalty graphs respectively represent manifold based distances weights edges connecting nodes graphs. elements afﬁnity matrices deﬁned terms gaussian kernels manifold regularization data dependent regularization framework capable exploiting existing manifold based structure data distribution. introduced network weights; helps maintaining smoothness assumed continuity source space. effect regularizer controlled multiplier third term represents manifold learning based locality preservation constraints deﬁned section note constraints modeled intrinsic graph gint ωint} included constraints modeled penalty graph gpen ωpen} ignored. discussed section scalar denotes number nearest neighbors connected feature refers weights edges vector ωint intrinsic graph deﬁned relative importance mapping along manifold controlled regularization coefﬁcient framework assumes data distribution supported dimensional manifold corresponds data-dependent regularization exploits underlying manifold domain geometry input distribution. imposing manifold based locality preserving constraints network outputs procedure encourages mapping relationships along manifold preserved different speech classes well separated. manifold regularization term penalizes objective criterion vectors belong neighborhood input space become separated output space projection. computation gradient depends input vector also neighbors belong class. therefore mrdnn training broken components. ﬁrst standard component minimizes cross-entropy based error respect given targets. second manifold regularization based component focuses penalizing criterion related preservation neighborhood relationships. authors also presented manifold extended versions regularized least squares support vector machines algorithms number text image classiﬁcation tasks. several recent efforts incorporate form manifold based learning variety machine learning tasks. authors investigated manifold regularization single hidden layer multilayer perceptrons phone classiﬁcation task. manifold learning based semisupervised embedding applied deep learning hand-written character recognition task manifold regularized single-layer neural networks used image classiﬁcation spite efforts applying manifold regularization various application domains similar efforts known training deep models asr. multi-layered nonlinear structure dnns makes capable learning highly nonlinear relationships between speech feature vectors. section proposes extension based training incorporating based constraints discussed section regularization term. algorithm formulation provided section proposed training procedure emphasizes local relationships among speech feature vectors along dimensional manifold optimizing network parameters. support claim empirical evidence provided section network trained manifold constraints higher capability preserving local relationships feature vectors trained without constraints. work incorporates locality geometrical relationships preserving manifold constraints regularization term objective criterion deep network. constraints derived graph characterizing underlying manifold speech feature vectors input space. objective criterion mrdnn network producing mapping fmrdnn given follows loss target vector output vector given input vector taken cross-entropy loss work. matrix representing weights network. second term l-norm regularization penalty architecture manifold regularized training shown figure input feature vector nearest neighbors belonging class selected. vectors forward propagated network. visualized making separate copies input vector remaining neighbors. corresponding input vector trained minimize cross-entropy error respect given target copy-dnn corresponding selected neighbors trained minimize function distance output corresponding input vector note weights copies shared average gradient used weight-update. algorithm extended mini-batch training. nating classes speech feature vectors might always impact performance dnns since dnns inherently powerful discriminative models. therefore penalty graph based term included experiments presented work manifold learning based expression given consists intrinsic component only. section describes study conducted characterize effect applying manifold based constraints behavior deep network. attempt quantify neighborhood relationships feature vectors preserved within hidden layers manifold regularized dnn. measure referred contraction ratio investigated. form measure presented work contraction ratio deﬁned ratio distance feature vectors output ﬁrst hidden layer input network. average contraction ratio feature vector neighbors seen measure locality preservation compactness neighborhood. thus evolution average contraction ratio vectors function distances used characterize overlocality preserving behavior network. subset feature vectors seen training selected. distribution pair-wise distances selected vectors used identify number bins. edges bins treated range radii around feature vectors. average contraction ratio computed function radius range selected vectors neighbors falling range given vector represents vectors number vectors. represents output ﬁrst layer corresponding vector input. follows smaller contraction ratio represents compact neighborhood. figure displays contraction ratio output input neighborhood sizes relative radius neighborhood input space mrdnn systems. average contraction ratios input ﬁrst layer’s output features plotted functions median radius bins. seen plots features obtained mrdnn represent compact neighborhood obtained dnn. therefore concluded hidden layers mrdnn able learn manifold based local geometrical representation feature space. also noted contraction ratio increases radius indicating effect fig. illustration mrdnn architecture. input vector neighbors belonging class selected forward propagated network. neighbors target output vector corresponding scalar ωint represents intrinsic afﬁnity weights deﬁned clear discussion relationships given vector neighbors play important role weight update using ebp. given assumption smooth manifold graph based manifold regularization equivalent penalizing rapid changes classiﬁcation function. results smoothing gradient learning curve leads robust computation weight updates. applications framework feature space transformations inclusion intrinsic penalty graphs terms shown found important reason experiments previous work included terms however initial experiments performed corpora described section gains achieved including penalty graph found inconsistent across datasets different noise conditions. adding additional term discrimiing standard conﬁguration speciﬁed corresponds using word-based continuous density hmms digits states word-model additional models states silence state short-pause. total cdhmm states modeled gaussians. test phase different subsets used corresponding uncorrupted clean utterances utterances corrupted four different noise types namely subway babble exhibition hall signal-to-noise ratios ranging utterances subset. performance obtained gmm-hmm system conﬁguration agrees reported elsewhere second dataset used work aurora- read newspaper speech-in-noise corpus. corpus created adding noise wall street journal corpus aurora represents mvcsr task vocabulary size words. work uses standard mixed-conditions training aurora- consists noisy utterances total male female speakers corresponding hours speech. half utterances mixed-conditions training recorded primary sennheiser microphone half secondary microphone enables effect transmission channel. halves contain mixture uncorrupted clean utterances noise corrupted utterances levels varying different noise conditions bi-gram language model used perplexity context-depedent cross-word triphone cdhmm models used conﬁguring system. total senones state modeled gaussian components. similar training test recorded primary secondary microphones. subset divided seven subsets subset clean speech data remaining obtained randomly adding noise types training levels ranging thus total subsets. subset contains utterances speakers. noted corpora used work represent simulated speech-in-noise tasks. created adding noises multiple sources speech utterances spoken quite environment. reason careful generalizing results speech-in-noise tasks. corpora baseline gmm-hmm systems conﬁgured using -dimensional static mel-frequency cepstrum coefﬁcient features augmented normalized energy difference cepstrum second difference cepstrum resulting -dimensional vectors. performance reported terms wers. gmm-hmm systems also used generating context-dependent state alignments. alignments used target labels deep networks training described below. section presents experimental study conducted evaluate effectiveness proposed mrdnns terms wer. performance mrdnn presented compared dnns without manifold regularization traditional gmm-hmm systems. experiments work done separate speechin-noise tasks namely aurora- connected digit task aurora- read news lvcsr task. speech tasks system setup described section results experiments aurora- task presented section results comparative performance proposed technique aurora- task presented section addition experiments clean condition training sets aurora- aurora- also performed. results experiments provided section ﬁrst dataset used work aurora- connected digit speech noise corpus. experiments aurora- mixed-condition training used training experiments also used clean training set. clean mixed-conditions training sets contain total utterances male female speakers. mixed-conditions utterances corrupted adding four different noise types clean utterances. conventional gmm-hmm system conﬁgured uscollectively term deep network used refer mrdnn conﬁgurations. conﬁgurations include l-norm regularization applied weights networks. performance models evaluated tandem setup bottleneck deep network used feature extractor gmm-hmm system. deep networks take -dimensional input vectors created concatenating context frames -dimensional mfcc features augmented energy ﬁrst second order difference. number units output layer equal number cdhmm states. class labels targets output cdhmm states obtained single pass force-alignment using baseline gmm-hmm system. regularization coefﬁcient weight decay mrdnn setup number nearest neighbors manifold regularization techniques evaluated hybrid dnn-hmm systems would expect presented results generalize models. experimental study performed work limited tandem dnn-hmm scenarios. expected results reported also generalize training hybrid dnn-hmm conﬁgurations. topic future work. aurora- experiments deep networks hidden layers. ﬁrst four hidden layers hidden units each ﬁfth layer bottleneck layer hidden units. aurora- experiments larger networks used. seven hidden layers. ﬁrst hidden layers hidden units each seventh layer bottleneck layer hidden units. larger network aurora- in-line recent work hidden units relus activation functions. soft-max nonlinearity used output layer cross-entropy loss outputs targets network error training -dimensional output features taken bottleneck layer decorrelated using principal component analysis features corresponding components kept during transformation match dimensionality baseline system. resultant features used train gmm-hmm system using maximum-likelihood criterion. although might argue necessary compressed bottleneck output work performance gain absolute observed bottleneck features different noise channel conditions. difference performance observed clean data. set. test results presented four separate tables corresponding different noisy subset aurora- test set. noisy subset divided four subsets corresponding noise corrupted utterances snrs. noise condition results features obtained three different techniques compared. ﬁrst table labeled ‘gmm-hmm’ contains baseline gmm-hmm system trained using mfcc features appended ﬁrst second order differences. second labeled ‘dnn’ displays results bottleneck features taken described section ﬁnal labeled ‘mrdnn’ presents results bottleneck features obtained mrdnn described section cases gmm-hmm conﬁgurations described section used. initial learning rates systems decreased exponentially epoch. system trained epochs training set. main observations made results presented table first mrdnn provide signiﬁcant reductions gmm-hmm system. second observation made comparing performance mrdnn systems. evident results presented features derived manifold regularized network provide consistent gains derived network regularized weight-decay. maximum relative gain obtained using mrdnn aurora- mixed-conditions set. table lists wers four sets namely clean additive noise channel distortion additive noise combined channel distortion sets obtained grouping together fourteen subsets described section ﬁrst table labeled ‘gmm-hmm’ provides results traditional gmm-hmm system trained using mfcc features appended ﬁrst second order differences. second labeled ‘dnn’ presents wers corresponding features derived baseline regularized system. performances baseline systems reported work agree reported elsewhere baseline set-up consistent speciﬁed aurora task order able make comparisons results obtained literature task. third labeled ‘mrdnn’ displays wers features obtained mrdnn. similar case aurora- regularization coefﬁcient used training mrdnn. initial learning rate reduced exponentially epochs training stopped. similar results aurora- corpus main observations made comparing performance presented systems. first mrdnn provide large reductions wers gmm-hmm conditions. second trend reductions using mrdnn visible well. maximum relative reduction obtained using mrdnn relative gain performance aurora corpus less aurora- corpus. might fact performance manifold learning based algorithms highly susceptible presence noise. increased sensitivity linked gaussian kernel scale factor deﬁned work equal experiments. value taken previous work empirically derived subset aurora- different task easy analyze effect noise performance discussed models level aurora- corpus difﬁcult aurora- corpus corpus organized. aurora- corpus provides mixture utterances different levels noise type. therefore average given aurora- corpus might affected dependence performance choice levels better tuned model expected provide improved performance. note techniques reported provide gains dnns also investigated work. particular based generative pre-training investigated initialize weights system. surprisingly however generative pre-training mechanism lead reductions performance corpora. agreement recent studies literature suggested relu hidden units relatively well-behaved corpus enough training data obviates need pre-training mechanism addition experiments mixed-conditions noisy datasets discussed sections experiments also conducted evaluate performance mrdnn matched clean training testing scenarios. clean training sets aurora- aurora- corpora used training deep networks well building manifold based neighborhood graphs results presented table observed results presented table mrdnn provides relative improvements aurora- relative improvement aurora- corpus. table wers clean training clean testing aurora- aurora- speech corpora gmm-hmm mrdnn models. last column lists improvements mrdnn relative dnn. performance results presented aurora- aurora- corpora sections demonstrate effectiveness proposed manifold regularized training deep networks. conventional pretraining regularization approaches lead performance gains mrdnn training provided consistent reductions wers. primary reason performance gain ability mrdnns learn preserve underlying low-dimensional manifold based relationships feature vectors. demonstrated empirical evidence section therefore proposed mrdnn technique provides compelling regularization scheme training deep networks. though inclusion manifold regularization factor training leads encouraging gains cost additional computational complexity parameter estimation. additional cost mrdnn training two-fold. ﬁrst cost associated calculating pair-wise distances populating intrinsic afﬁnity matrix ωint. discussed section computation complexity managed using locality sensitive hashing approximate nearest neighbor search without sacriﬁcing performance. second source additional complexity inclusion neighbors feature vector forward back propagation. results increase computational cost factor cost managed various parallelization techniques becomes negligible light massively parallel architectures modern processing units. added computational cost relvent training networks impact test phase data transformed using trained network. networks work training nvidia graphics boards using tools developed python based frameworks numpy gnumpy cudamat training epoch aurora- took seconds epoch aurora- dataset took seconds. comparison mrdnn training took seconds epoch aurora- seconds epoch aurora- set. previous work authors demonstrated manifold learning techniques sensitive presence noise sensitivity traced gaussian kernel scale factor used deﬁning local afﬁnity matrices argument might apply mrdnn training well. therefore performance mrdnn might affected presence noise. visible extent results presented table gains using mrdnn vary level. hand aurora- test corpus contains mixture noise corrupted utterances different levels noise type. therefore average levels noise type presented. need conduct extensive experiments investigate effect done however models affected presence noise similar manner additional gains could derived designing method building intrinsic graphs separately different noise conditions. training automated algorithm could select graph best matches estimated noise conditions associated given utterance. training could result mrdnn able provide gains performance various noise conditions. section demonstrated reductions forcing output feature vectors conform local neighborhood relationships present input data. achieved applying underlying input manifold based constraints outputs throughout training. also studies literature manifold regularization used ﬁrst iterations model training. instance authors applied manifold regularization multi-task learning. authors argued optimizing deep networks alternating training without manifold based constraints increase generalization capacity model. motivated efforts section investigates scenario manifold regularization based constraints used ﬁrst epochs training. layers networks randomly initialized trained manifold based constraints ﬁrst epochs. resultant network trained epochs using standard without manifold based regularization. note contrary pre-training approaches deep learning greedy layer-by-layer training. results experiments given table aurora- dataset. brevity table presents wers average four noise types described table level. addition average wers mrdnn labeled ‘mrdnn given. refers case manifold regularization used ﬁrst training epochs. number observations made results presented table first seen results table mrdnn mrdnn training scenarios improve wers compared standard training. second mrdnn training provides reductions wers aurora- set. experiments clean training clean testing aurora- reductions achieved experiements encouraging. furthermore approach manifold constraints applied ﬁrst epochs might lead efﬁcient manifold regularized training procedure manifold regularized training higher computational complexity however unlike work applied cycle manifold constrained-unconstrained training. noted preliminary experiments. similar gains seen techniques applied aurora dataset. therefore investigation required making substantial conclusions. paper presented framework regularizing training dnns using discriminative manifold learning based locality preserving constraints. manifold based constraints derived graph characterizing underlying manifold speech feature vectors. empirical evidence also provided showing hidden layers mrdnn able learn local neighborhood relationships feature vectors. also conjectured inclusion manifold regularization term objective criterion dnns results robust computation error gradient weight updates. shown experimental results mrdnns result consistent reductions wers range relative standard dnns aurora corpus. aurora- corpus gains range relative mixed-noise training clean training task. therefore proposed manifold regularization based training seen compelling regularization scheme. studies presented open number interesting possibilities. would expect mrdnn training similar impact hybrid dnn-hmm systems. application techniques semi-supervised scenarios also topic interest. manifold regularization based techniques could used learning large unlabeled dataset followed ﬁne-tuning smaller labeled set. navdeep jaitly patrick nguyen andrew senior vincent vanhoucke application pretrained deep neural networks large vocabulary conversational speech recognition interspeech number tara sainath brian kingsbury bhuvana ramabhadran petr fousek petr novak abdel-rahman mohamed making deep belief networks effective large ieee vocabulary continuous speech recognition work. autom. speech recognit. underst. dec. vikrant singh tomar richard rose family discriminative manifold learning algorithms application speech recognition ieee/acm trans. audio speech lang. process. vol. mikhail belkin partha niyogi vikas sindhwani manifold regularization geometric framework learning labeled unlabeled examples mach. learn. vol. dahl dong deng acero contextdependent pre-trained deep neural networks large ieee trans. audio vocabulary speech recognition speech lang. process. frank seide gang chen dong feature engineering context-dependent deep neural networks conversational speech transcription ieee work. autom. speech recognit. underst. dec. pascal vincent hugo larochelle isabelle lajoie stacked denoising autoencoders learning useful representations deep network local denoising criterion mach. learn. res. vol. hirsch david pearce aurora experimental framework performance evaluation speech recognition systems noisy conditions autom. speech recognit. challenges next millenium salah rifai pascal vincent xavier muller glorot xavier yoshua bengio contractive auto-encoders explicit invariance feature extraction int. conf. mach. learn. vol. dunne campbell pairing softmax activation cross-entropy penalty functions derivation softmax activation function proc. aust. conf. neural networks shuicheng dong benyu zhang hong-jiang zhang qiang yang stephen graph embedding extensions general framework dimensionality reduction. ieee trans. pattern anal. mach. intell. vol. jan. tang richard rose study using locality preserving projections feature extraction speech recognition ieee int. conf. acoust. speech signal process. vegas mar. ieee. vikrant singh tomar richard rose application locality preserving discriminant analysis approach int. conf. inf. sci. signal process. appl. montreal canada july ieee. vikrant singh tomar richard rose efﬁcient manifold learning speech recognition using locality sensitive hashing icassp ieee int. conf. acoust. speech signal process. vancouver canada vikrant singh tomar richard rose locality sensitive hashing fast computation correlational manifold learning based feature space transformations interspeech lyon france mayur datar nicole immorlica piotr indyk vahab mirrokni locality-sensitive hashing scheme based p-stable distributions proc. twent. annu. symp. comput. geom. fr´ed´eric ratle gustavo camps-valls senior member jason weston semi-supervised neural networks efﬁcient hyperspectral image classiﬁcation ieee trans. geosci. remote sens. vikrant singh tomar richard rose noise aware manifold learning robust speech recognition icassp ieee int. conf. acoust. speech signal process.", "year": 2016}