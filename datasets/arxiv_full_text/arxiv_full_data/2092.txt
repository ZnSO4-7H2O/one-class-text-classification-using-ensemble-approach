{"title": "Learning Representations by Stochastic Meta-Gradient Descent in Neural  Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Representations are fundamental to artificial intelligence. The performance of a learning system depends on the type of representation used for representing the data. Typically, these representations are hand-engineered using domain knowledge. More recently, the trend is to learn these representations through stochastic gradient descent in multi-layer neural networks, which is called backprop. Learning the representations directly from the incoming data stream reduces the human labour involved in designing a learning system. More importantly, this allows in scaling of a learning system for difficult tasks. In this paper, we introduce a new incremental learning algorithm called crossprop, which learns incoming weights of hidden units based on the meta-gradient descent approach, that was previously introduced by Sutton (1992) and Schraudolph (1999) for learning step-sizes. The final update equation introduces an additional memory parameter for each of these weights and generalizes the backprop update equation. From our experiments, we show that crossprop learns and reuses its feature representation while tackling new and unseen tasks whereas backprop relearns a new feature representation.", "text": "abstract. representations fundamental artiﬁcial intelligence. performance learning system depends type representation used representing data. typically representations hand-engineered using domain knowledge. recently trend learn representations stochastic gradient descent multi-layer neural networks called backprop. learning representations directly incoming data stream reduces human labour involved designing learning system. importantly allows scaling learning system difﬁcult tasks. paper introduce incremental learning algorithm called crossprop learns incoming weights hidden units based meta-gradient descent approach previously introduced sutton schraudolph learning step-sizes. ﬁnal update equation introduces additional memory parameter weights generalizes backprop update equation. experiments show crossprop learns reuses feature representation tackling unseen tasks whereas backprop relearns feature representation. type representation used presenting data learning system plays role artiﬁcial intelligence machine learning. typically performance learning system speed learning error rate directly depends data represented internally learning system. hand-engineering representations using special domain knowledge norm designing learning systems. recently representations learned hierarchically directly data stochastic gradient descent. learning representations significantly improves performance learning system reduces human effort involved designing learning system. importantly allows scaling learning systems bigger harder problems. tackle incredibly hard problems classifying recognizing objects natural scene images automatically translating text speeches achieving surpassing human-level baseline atari achieving superhuman performance poker improving robot control learning experiences important note many problems difﬁcult hand-engineer data representation inadequate representation generally limits performance scalability learning system. algorithm behind training deep neural networks called backprop introduced rumelhart hinton williams extended stochastic gradient descent approach chain rule learning weights hidden layers neural network. though backprop produced many successful results suffers fundamental issues makes slow learning useful representation solves many tasks. speciﬁcally backprop tends interfere previously learned representations units found useful ones likely changed reasons weights hidden layer assumed independent other this parameters neural network race minimize error given example. order overcome issue neural network needs trained multiple sweeps data algorithm settle representation encompasses data seen far. paper introduce meta-gradient descent approach learning weights connecting hidden units neural network. previously meta-gradient descent approach introduced sutton schraudolph learning parameter-speciﬁc step-sizes adapted learning incoming weights connect hidden units. proposed method called crossprop. speciﬁcally addresses racing problem observed backprop. furthermore continual learning experiments learning system experiences sequence related tasks observed crossprop tends features best generalize across multiple tasks. backprop hand tends unlearn relearn features task experiences. continual learning perspective learning system experiences sequence tasks related other desirable learning system leverage learning past experiences solving unseen difﬁcult tasks experiences future. ﬁrst popular approach learning representations stochastic gradient descent supervised learning error function like mean squared cross-entropy error approach proved successful many successful applications ranging difﬁcult problems computer vision patient diagnoses. although method strong track record perfect yet. particularly learning representations backpropagating supervised error signal often learns slowly poorly many problems order address this many modiﬁcations backprop introduced like adding momentum rmsprop adadelta adam etc. quite clear variation backprop work well given task. however variations backprop still tend interfere previously learned representations thereby causing network unlearn relearn representation even task solved leveraging learning previous experiences. another promising approach learning representations generate test process underlying principle behind approaches generate many features random manner test usability features. based certain heuristics features either preserved discarded. furthermore generate test approach combined backprop achieve better rate learning supervised learning tasks. primary motivation behind generate test approaches design distributed computationally inexpensive representation learning method. researchers also looked learning representations fulﬁl certain unsupervised learning objectives like clustering sparsity statistic independence reproducibility data takes third fundamental approach towards learning representations recently learning unsupervised representations allowed designing effective clinical decision making system however exactly clear design learning system continual online learning setting using representations obtained unsupervised learning assume access data prior beginning learning task. consider single-hidden layer neural network single output unit presenting algorithm. parameters rm×n incoming outgoing weights neural network number input units number hidden units. element denoted refers corresponding input unit refers hidden unit. likewise element denoted proposed method summarized pseudo-code algorithm learning system time step receives example element vector denoted xit. mapped onto hidden units incoming weight matrix nonlinearity like tanh sigmoid relu applied summed-product. activations hidden unit given example time step using tanh activation function expressed mathematically diverge conventional learning incoming weights speciﬁcally learning weights consider inﬂuence past values u··· current error would like learn values uijt+ making update using partial derivative term refers past values. interesting current research representation learning usually consider inﬂuence weight current time step uijt ignores effects previous possible values squared error weights squared error current time step. conventional backprop update. however proposed algorithm additional update term φjthijt captures dependencies previous values current estimate current squared error algorithm derived presented eqns. computationally expensive number outgoing weights hidden unit speciﬁcally output units becomes k-dimensional vector dimensions equal output units. leads large computational cost involved computing hijt avoided approximating hijt parameter. approximation involves accumulating error assigned hidden units outgoing weights using compute update term. approximated algorithm referred crossprop-approx. experiments following update equations continual learning tasks refer experiment setting supervised training examples generated presented learning system sequence related tasks. moreover learning system know task switched. generic online feature finding problem ﬁrst introduced sutton generic synthetic feature-ﬁnding test evaluating different representation learning algorithms. primary advantage test inﬁnitely many supervised-learning tasks generated without experimenter bias. test consists single hidden layer neural network called target network real-valued scalar output. input example mdimensional binary input vector element vector take value hidden layer consists linear threshold units threshold parameter parameter controls sparsity hidden layer. weights +}m×n maps input vector hidden units weights linearly combine ltus produce scalar target output weights generated using uniform probability distribution remain ﬁxed throughout task representing stationary function mapping given input vector scalar target output input vector generated randomly using uniform probability distribution. input vector target network used produce scalar target output experiments experiment setup. experiments create instance geoff task. called task generate examples. examples used training learning systems online manner example processed discarded. processing examples task generate task randomly choosing regenerating outgoing target weights training examples generated training modiﬁed task. similarly processing examples task task produced used generating another training examples. learning systems learn online training examples produced sequence related tasks representations learned used solving tasks. important point tasks share feature representation learning system leverage previous learning experiences. experiment setup continual learning perspective learning system experience examples generated sequence related tasks learning task help learning similar tasks. step-size algorithms ﬁxed constant value objective show features learned different algorithms sequence related learning tasks. learning system consisted single hidden layer neural network single output unit. input units hidden units using tanh activation fig. learning curves crossprop colored blue backprop colored red. learning systems know task changed. figure shows learning curves series geoff tasks ﬁgure shows learning curves series mnist tasks. learning curves ﬁgure averaged independent runs used different target networks generating training examples. learning curves ﬁgure single training mnist used. also mnist tasks crossprop-approx. evaluated. results. compare behavior crossprop backprop variations sequence related tasks generated using geoff testbed. figure shows learning curve different algorithms. every examples task switches related task previously described. important note learning system know task changed. learning curves show crossprop reaches similar asymptotic value backprop implying introduced algorithm produces similar solution backprop. terms asymptotic values backprop achieves signiﬁcantly better asymptotic value compared crossprop variations backprop. however interesting note learning algorithms approach solution differently. figure shows euclidean norm weights processing training example initialized value weights. though algorithms reach similar asymptotic values backprop achieves clearly different crossprop. backprop tends frequently modify features even though seen examples generated using previously learned function. speciﬁcally backprop fails leverage previous learning experiences fig. plot shows change incoming weights processing example. speciﬁcally plot shows norm incoming weights processing example initialized value different learning algorithms. plot observed crossprop tends change incoming weights least even task signiﬁcantly changed implying crossprop tends reusable feature representation sufﬁciently solve sequence tasks experiences. hand backprop tends signiﬁcantly relearn feature representation throughout experiment even task solved leveraging previous learning experiences. solving tasks even possible. backprop tends take time ﬁnding feature representation sufﬁciently solve continual problem. clearly case crossprop. proposed algorithm tends feature representation much quicker backprop sufﬁciently solve sequence continual problems reuses solving tasks encounters future. figure shows euclidean norm weights processing example initialized value weights. crossprop tends features much quicker backprop reuses features solving task reduces error moving outgoing weights rather modifying feature representation. furthermore tasks presented learning system solved using single feature representation plots clearly seen crossprop recognizes this. mnist dataset handwritten digits introduced lecun cortes burges though mnist dataset still viewed standard supervised learning benchmark task testing learning algorithms dataset consists grayscale images dimensions. images obtained handwritten digits corresponding labels denote fig. plot shows change outgoing weights processing example. shows norm outgoing weights processing example initialized value different learning algorithms. plot observed crossprop tends change outgoing weights needed feature representation estimate backprop parameter independently minimizes error this parameter race reducing error without coordinating efforts. tends change feature representation accommodating example. experiment setup. adapt mnist dataset continual learning setting task label training images shifted one. example task uses standard mnist training images labels task uses training examples task labels shifted one. similarly task label training examples shifted one. previous experiment step-size different algorithms objective study representations learned algorithms continual learning setting learning system experiences examples sequence related tasks. learning system consisted single hidden layer neural network input units hidden units output units. hidden units used tanh activation function output units used softmax activation function. cross-entropy error function used training network. results. figure shows learning curves methods evaluated mnist tasks. observed geoff tasks learning curves different algorithms converge almost similar points means methods reach similar solutions. however adam rmsprop achieves signiﬁcantly better asymptotic error value compared learning algorithms. much quicker backprop variations. also crossprop tends reuse features solving tasks faces. interesting observe backprop seem settle good feature representation solving sequence continual learning problems. tends na¨ıvely unlearn relearn feature representation even tasks similar solved using feature representation learned ﬁrst task. speciﬁcally backprop seem leverage previous learning experiences encountering task. fig. backprop crossprop seem learn similar feature representations clustering together similar examples. plot shows visualizations features learned backprop crossprop standard mnist task. learning algorithms trained online mnist dataset parameters learned algorithms used generating visualizations. crossprop order draw differences conventional meta-gradient descent approach learning features. plot generated using training examples uniformly sampled mnist dataset. visualize features obtained training learning systems using crossprop backprop. visualization obtained using t-sne approach developed maaten hinton visualizing high-dimensional data giving datapoint location three-dimensional map. here show two-dimensional generated using features learned different learning algorithms. features learned backprop crossprop standard mnist task plotted figures visualizations observed algorithms produce similar feature representations task. algorithms learn feature representation clusters examples according labels. seem much difference looking features. neural networks backprop form powerful hierarchical feature learning paradigm. designing deep neural network architectures allowed many learning systems achieve levels performance comparable humans many domains. many recent research works however fail notice ignore fundamental issues present backprop even though important address them. research works even tend provide ad-hoc solutions overcome fundamental problems posed backprop usually scalable general domains. time many modiﬁcations introduced backprop still fail address fundamental issue backprop backprop tends interfere previously learned representations order accommodate example. prevents directly applying backprop continual learning domains critical achieving artiﬁcial intelligence continual learning setting learning system needs progressively learn hierarchically accumulate knowledge experiences using solve many difﬁcult unseen tasks. setting desirable learning system na¨ıvely unlearns relearns even sees task solved reusing learning past experiences. particularly continual learning setting necessary learning system hierarchically build knowledge previous experiences solving completely unseen task. paper present continual learning tasks adapted standard supervised learning domains geoff testbed mnist dataset. tasks evaluate backprop variations also evaluate proposed meta-gradient descent approach learning features neural network called crossprop. show backprop tends relearn feature representations every task even tasks solved reusing feature representation learned previous experiences. crossprop hand tends reuse previously learned representations tackling unseen tasks. process consistently failing leverage previous learning experiences particularly desirable continual learning setting prevents directly applying backprop settings. addressing particular issue primary motivation work. immediate future work would like study performances metagradient descent approach deep neural networks comprehensively evaluate difﬁcult benchmarks like imagenet arcade learning environment paper introduced meta-gradient descent approach called crossprop learning incoming weights hidden units neural network showed approaches complementary backprop popular algorithm training neural networks. also show using crossprop learning system learn reuse learned features solving unseen tasks. however ﬁrst general work towards comprehensively addressing overcoming fundamental issues posed backprop particularly continual learning domains.", "year": 2016}