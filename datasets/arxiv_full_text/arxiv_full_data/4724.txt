{"title": "Towards Shockingly Easy Structured Classification: A Search-based  Probabilistic Online Learning Framework", "tag": ["cs.LG", "cs.AI"], "abstract": "There are two major approaches for structured classification. One is the probabilistic gradient-based methods such as conditional random fields (CRF), which has high accuracy but with drawbacks: slow training, and no support of search-based optimization (which is important in many cases). The other one is the search-based learning methods such as perceptrons and margin infused relaxed algorithm (MIRA), which have fast training but also with drawbacks: low accuracy, no probabilistic information, and non-convergence in real-world tasks. We propose a novel and \"shockingly easy\" solution, a search-based probabilistic online learning method, to address most of those issues. This method searches the output candidates, derives probabilities, and conduct efficient online learning. We show that this method is with fast training, support search-based optimization, very easy to implement, with top accuracy, with probabilities, and with theoretical guarantees of convergence. Experiments on well-known tasks show that our method has better accuracy than CRF and almost as fast training speed as perceptron and MIRA. Results also show that SAPO can easily beat the state-of-the-art systems on those highly-competitive tasks, achieving record-breaking accuracies. The codes can be found at https://github.com/lancopku", "text": "major approaches structured classiﬁcation. probabilistic gradient-based methods conditional random ﬁelds high accuracy drawbacks slow training support search-based optimization search-based learning methods perceptrons margin infused relaxed algorithm fast training also drawbacks accuracy probabilistic information non-convergence real-world tasks. propose novel shockingly easy solution search-based probabilistic online learning method address issues. method searches output candidates derives probabilities conduct eﬃcient online learning. show method fast training support search-based optimization easy implement accuracy probabilities theoretical guarantees convergence. experiments well-known tasks show method better accuracy almost fast training speed perceptron mira. results also show sapo easily beat state-of-the-art systems highly-competitive tasks achieving record-breaking accuracies. codes found https//github.com/lancopku structured classiﬁcation models popularly used solve structure dependent problems wide variety application domains including natural language processing bioinformatics speech recognition computer vision. solve problems many structured classiﬁcation methods developed major categories. probabilistic gradient-based learning methods conditional random ﬁelds category structured classiﬁcation methods search-based learning methods margin infused relaxed algorithm structured perceptrons related work structured classiﬁcation also includes maximum margin markov networks structured support vector machines probabilistic gradient-based learning methods high accuracy exact calculation gradient probabilistic information. nevertheless methods critical drawbacks first probabilistic gradient-based methods typically support search-based optimization important structured classiﬁcation problems complex structures. tasks complex structures gradient computation usually quite complicated even intractable. mainly dynamic programming calculating gradient hard scale complex structures. hand search technique easier scale complex structures. gradient-based methods like usually applied relatively simple structures like sequential tagging rarely used complex tasks beyond tree structures. take syntactic parsing task tree structures example instead existing systems based perceptrons mira support search-based learning. search-based learning much simpler gradient-based learning search promising output candidates compare oracle labels weight update accordingly. second issue training probabilistic gradient-based methods like computationally expensive quite slow practice. reason training model requires gradient computation gradient-based optimization gradient computation computationally costly especially relatively high dimension. category structured classiﬁcation methods search-based learning methods structured perceptrons mira. major advantage methods support search-based learning gradient needed learning done simply searching comparing promising output candidates oracle labels update model weights accordingly. by-product avoidance gradient computation methods fast training speed compared probabilistic gradient-based learning methods like crf. however also severe drawbacks existing search-based learning methods first existing search-based learning methods like perceptrons mira relatively accuracy compared probabilistic gradient-based learning methods like crf. second real-world tasks search-based learning methods non-convergent i.e. diverges training. large margin classiﬁcation models theoretically search-based learning methods convergent properties based strict separability conditions. however strict separability conditions satisﬁable real-world tasks demonstrated many prior work also shown experiments search-based learning methods diverges dramatically training goes model accuracy goes worse worse training goes existing search-based methods support probabilistic information. magnitude model weights grows dramatically training goes reliable probabilistic information derived. also shown curves model weight magnitude experiments. address issues propose novel shockingly simple solution searchbased probabilistic online learning framework almost drawbacks. proposed method searches top-n output candidates derives probabilities based searched candidates conduct fast online learning updating model weights. show proposed method fast training speed comparable perceptrons mira supports search-based optimization need calculate gradient easy implement accuracy even better reliable probability information theoretical guarantees convergence towards optimum given reasonable conditions. although current stage experiments focused linear-chain tasks method theoretical results apply structured classiﬁcation complex structures example tree graph structures. experiments well-known tasks show method better accuracy almost fast training speed perceptron mira. results also show sapo easily beat state-of-the-art systems highly-competitive tasks achieving record-breaking accuracies. methodology side propose general purpose search-based probabilistic online learning framework sapo structured classiﬁcation. show sapo address variety issues existing methods theoretical justiﬁcations. compared probabilistic gradient-based learning methods like proposed method supports search-based learning avoid complex gradient calculation extra advantages accuracy training speed. compared search-based learning methods like perceptron mira sapo much higher accuracy theoretical empirical justiﬁcations convergence perceptron mira diverge real-world tasks shown experiments. application side several important natural language processing signal processing tasks including part-of-speech tagging biomedical entity recognition phrase chunking activity recognition simple search-based learning method easily beat state-of-the-art systems highly-competitive tasks achieving record-breaking accuracies fast speed. draw sample random training based search top-n outputs {yyy yyyn} every yyyk compute probability every yyyk update weights γpkfff yyy∗ update weights γfff regularize weights proposed search-based probabilistic online learning algorithm sapo schemes follows top-n search scheme calculating probabilities perceptron-style update weights regularizer weights. introduce technical details schemes follows summarize sapo algorithm figure. many methods realize top-n search. method uses search algorithm search algorithm viterbi heuristic function used produce top-n outputs one-by-one eﬃcient manner. backward viterbi algorithm compute admissible heuristic function forward-style search. produce top-n taggings eﬃciently. vector model weights feature vector based simply top-n outputs deﬁned before. deﬁnition top-n search results estimate probability distribution note that although search exact top-n search actually exact top-n search strictly required sapo framework. words replace exact search non-exact beam search scheme sapo algorithm. experiments tested exact search non-exact beam search pruning almost diﬀerence experimental results. accurate towards traditional probability theoretical side show theoretical analysis probability estimation arbitrary-close traditional probability using proper sapo algorithm guaranteed converge towards optimum weights www∗ arbitrary-close distance given reasonable conditions. empirical side show experiments probability estimation good enough real-world tasks even finally sapo uses weight regularizer regularization strength like stochastic regularization adopted stochastic gradient descent following regularization scheme regularization strength turns λ/|s| online learning setting also regularization scaled learning rate thus using regularizer denoted regularization step follows compare sapo structured perceptron stochastic training interesting sapo like uniﬁcation perceptron stochastically trained crf. neglect learning rate regularizer term sapo perceptron algorithm seen extreme case sapo hand stochastically trained seen another extreme case sapo exponentially enumerate possible output taggings words perceptron seen sapo extremely small seen sapo extremely argue sapo natural perceptrons moderate value instead extremely small extremely huge show experiments theoretical analysis extremely small like perceptron lead accuracy non-convergent training extremely huge like also lead loss accuracy high computational cost. practice good enough real-world tasks. mira algorithm also variation nbest mira also uses top-n search interestingly also good enough nbest mira nevertheless sapo substantially diﬀerent compared nbest mira. major diﬀerence sapo probability estimation diﬀerent outputs nbest mira not. nbest mira treat diﬀerent outputs equally without probability diﬀerence cannot seen special case nbest mira. even nbest-mira uses extremely huge top-n search equivalent diﬀerence substantial. also diﬀerences sapo nbest mira. example sapo regularizer term learning rate need minimum change optimization criterion mira weight update. analyze equivalent objective function sapo update term sapo. sapo algorithm search-based optimization algorithm need compute gradient objective function explicit objective function used sapo algorithm. nevertheless interestingly show sapo algorithm convergent converges towards optimum weights www∗ maximizes objective function follows number training samples i.e. weight regularization term controlling overﬁtting. objective function similar objective function crf. equivalently convenience convex-based analysis denote objective function negative form practice sapo decayed learning rate ﬁxed learning rate. following convenience theoretical analysis theoretical analysis focused sapo ﬁxed learning rate. means -norm default work. strongly convex global optimum/minimizer www∗. also assume lipschitz continuous diﬀerentiability constant ∀www www′ proof section theorem shows approximation based learning like sapo also convergent towards optimum objective function. thus approximate true gradient top-n search still keep convergence properties without need calculate exact gradients training crf. speciﬁcally theorem shows sapo able converge towards optimum objective function arbitrary-close distance sapo update term close-enough approximation true gradient since arbitrary-close increasing sapo smallest close-enough approximation achieved. practice setting already empirically close-enough approximation real-world tasks. moreover convergence rate given theorem sapo guaranteed converge updates smallest integer satisfying analysis also explains perceptron algorithm converge practical tasks. discussed before perceptron algorithm essentially treated extreme case sapo extremely small cases satisfy close-enough approximation condition thus cases perceptron algorithm approximation true gradient diverges many structured classiﬁcation methods developed including probabilistic gradient-based learning methods search-based learning methods. probabilistic gradient-based learning methods include conditional random ﬁelds variety extensions dynamic conditional random ﬁelds hidden conditional random ﬁelds latent-dynamic conditional random ﬁelds include margin infused relaxed algorithm structured perceptrons variety related work direction latent structured perceptrons conﬁdence weighted linear classiﬁcation max-violation perceptrons search-based learning methods large-margin online learning methods. related work structured classiﬁcation also includes maximum margin markov networks structured support vector machines training structured classiﬁcation models especially probabilistic gradient-based learning methods like variation models arguably popular training method stochastic gradient descent typically faster convergence rate compared alternative batch training methods limited-memory bfgs quasi-newton optimization methods. training theoretical guarantees converge optimum weights given convex objective function search-based learning methods perceptrons mira dealing overﬁtting probabilistic gradient-based learning methods typically explicit regularization terms widely used regularizer. regularization schemes include regularizer group lasso regularization structure regularization others search-based learning methods like perceptrons mira scheme deal overﬁtting less formal compared regularizer usually using parameter averaging voting conduct experiments natural language processing tasks signal processing tasks quite diversiﬁed characteristics. natural language processing tasks include part-of-speech tagging biomedical named entity recognition phrase chunking. signal processing task sensor-based human activity recognition. tasks boolean features task adopts real-valued features. tasks averaged length samples quite diﬀerent length respectively. dimension tags also diversiﬁed among tasks ranging part-of-speech tagging part-of-speech tagging important highly competitive task natural language processing. standard benchmark dataset prior work derived penntreebank corpus uses sections wall street journal training sections testing following prior work features based unigrams bigrams neighboring words lexical patterns current word features total. following prior work evaluation metric task per-word accuracy. biomedical named entity recognition task bionlp- shared task recognizing kinds biomedical named entities medline biomedical text corpus. training samples test samples. following prior work word pattern features features features total. evaluation metric balanced f-score. phrase chunking phrase chunking task non-recursive cores noun phrases called base identiﬁed. phrase chunking data extracted data conll- shallow-parsing shared task training consists sentences test consists sentences. feature templates based word n-grams part-of-speech n-grams features total. following prior studies evaluation metric task balanced f-score. sensor-based human activity recognition task based real-valued sensor signals data extracted activity recognition dataset task aims recognize human activities using biaxial sensors collect acceleration signals individuals sampling frequency .hz. following prior work activity recognition acceleration features mean features standard deviation energy correlation features features total. training samples test samples. following prior work evaluation metric accuracy. compared proposed sapo algorithm strong baselines existing literature including probabilistic gradient-based learning methods search-based learning methods. probabilistic gradient-based learning methods choose arguably popular model baseline. widely used regularization trained standard training algorithm. search-based learning methods choose structured perceptrons mira arguably popular searchbased learning methods baselines. cases averaged versions perceptrons mira work empirically better naive versions perceptron mira thus also compare sapo averaged versions perceptrons mira. diﬀerentiate naive averaged versions denote perc-naive perc-avg mira-naive mira-avg respectively. moreover mira method nbest versions adopts top-n search update instead viterbi search update. also choose nbest versions mira additional baselines. denote nbest mira naive training mira-nbest-naive denote averaged training mira-nbest-avg. regularization strength tuned among values determined development data provided standard dataset simply -fold cross validation training automatic tuning regularization strength pos-tag bioner word-seg act-recog tasks respectively. give tuning advantage sapo sapo simply uses regularizer learning rate use. tuning based additional tuning sapo. also proposed sapo algorithm top-n search scheme nbest mira use. shown prior work good enough nbest mira. also good enough proposed sapo algorithm. thus nbest mira sapo fast speed. methods features. experiments performed computer intel xeon .ghz cpu. experimental results terms accuracy/f-score shown figure figure figure figure respectively. although tasks diversiﬁed feature types diﬀerent characteristics results quite consistent proposed sapo algorithm best accuracies/f-scores four tasks compared existing baselines. first compare sapo crf. impressing proposed sapo algorithm even better accuracy arguably accurate models structured classiﬁcation. note model baselines already fully optimized conﬁrmed comparing results state-of-the-art reports four tasks superiority reason probability distributed top-n outputs sapo regularized distribution probability distribution possible outputs sense sapo regularizing exponential probability distribution simpler top-n probability distribution. seen probability-based regularizer regularization strength controlled interestingly experimental results suggest type regularization indeed improve accuracy/f-score. observe sapo better four tasks show many cases diﬀerences statistically signiﬁcant. also sapo several times faster terms training time. convergence state sapo achieves similar even better loss function crf. second compare sapo search-based learning methods including naive/average versions perc mira nbest mira. superiorities sapo search-based learning methods even signiﬁcant crf. also conduct signiﬁcance tests based t-test. pos-tag task signiﬁcance test suggests superiorities sapo baselines except statistically signiﬁcant least bio-ner task signiﬁcance test suggests superiorities sapo baselines signiﬁcant least act-recog task superiorities sapo baselines signiﬁcant least method actually outperforms state-of-the-art records competitive natural language processing tasks. datasets standard benchmark datasets directly compared existing work. pos-tagging task highly competitive task many methods proposed best report achieved using bidirectional learning model shen accuracy simple method achieves better accuracy compared state-of-the-art systems. sapo method also achieves exceeds stateof-the-art methods bio-ner chunking tasks also competitive tasks natural language processing communities. second ﬁgures shows www-complexity based number training iterations. www-complexity averaged value weights. sapo convergent small weight complexity training goes perc mira nbest mira linear even super-linear explosion weight complexity training goes weight complexity typically sign controlling generalization risk. left side third ﬁgures shows loss function sapo based number training iterations. sapo converges good even better terms loss function training goes conﬁrms theoretical analysis convergence sapo. right side third ﬁgures show training time iteration terms seconds. sapo computational cost especially compared nbest mira. summarize experiment results demonstrate sapo better accuracy probabilistic gradient-based methods like time fast training speed like perceptrons mira. also sapo convergent towards optimum controllable weight complexity training goes emphasize important advantages sapo shown experiments sapo supports search-based learning gradient information needed gives probability information easy implement. exiting structured classiﬁcation methods problematic. existing probabilistic gradient-based methods slow training support search-based optimization. existing search-based learning methods perceptrons mira relatively accuracy non-convergent real-world tasks. propose novel shockingly easy solution search-based probabilistic online learning framework sapo address issues. sapo fast training support search-based optimization easy implement accuracy probability information theoretical guarantees convergence. although currently focus sequence structures method theories apply structured classiﬁcation complex structures example trees graphs. experiments well-known benchmark tasks demonstrate sapo better accuracy roughly fast training speed perceptrons mira. results also show sapo easily beat state-of-the-art systems highlycompetitive tasks achieving record-breaking accuracies. current implementation top-n search uses simple search algorithm viterbi heuristics. top-n search algorithm fully optimized speed. several top-n search algorithms possibly faster speed. future optimize top-n search algorithm. believe improve training speed sapo. moreover sapo general purpose algorithm structured classiﬁcation arbitrary structures. future apply sapo structured classiﬁcation complex structures e.g. syntactic parsing statistical machine translation.", "year": 2015}