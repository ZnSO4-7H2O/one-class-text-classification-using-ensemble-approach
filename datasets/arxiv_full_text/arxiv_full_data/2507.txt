{"title": "Example Selection For Dictionary Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In unsupervised learning, an unbiased uniform sampling strategy is typically used, in order that the learned features faithfully encode the statistical structure of the training data. In this work, we explore whether active example selection strategies - algorithms that select which examples to use, based on the current estimate of the features - can accelerate learning. Specifically, we investigate effects of heuristic and saliency-inspired selection algorithms on the dictionary learning task with sparse activations. We show that some selection algorithms do improve the speed of learning, and we speculate on why they might work.", "text": "tomoki tsuchida garrison cottrell department computer science engineering university california diego gilman drive mail code jolla {ttsuchidagary}ucsd.edu unsupervised learning unbiased uniform sampling strategy typically used order learned features faithfully encode statistical structure training data. work explore whether active example selection strategies algorithms select examples based current estimate features accelerate learning. speciﬁcally investigate effects heuristic saliency-inspired selection algorithms dictionary learning task sparse activations. show selection algorithms improve speed learning speculate might work. efﬁcient coding hypothesis proposed barlow posits goal perceptual system encode sensory signal efﬁciently represented. based hypothesis past decades seen successful computational modeling low-level perceptual features based dictionary learning sparse codes. idea learn dictionary elements encode naturalistic signals efﬁciently; learned dictionary might model features early sensory processing. starting olshausen field dictionary learning task thus used extensively explain early perceptual features. objective learning task capture statistical structure observed signals faithfully efﬁciently instance unsupervised learning. such dictionary learning usually performed using unbiased sampling data used learning sampled uniformly training dataset. time world contains overabundance sensory information requiring organisms limited processing resources select process information relevant survival tsotsos selection process expressed perceptual action attentional ﬁltering mechanisms. might ﬁrst appear odds goal dictionary learning task since selection process necessarily biases observed data organism. however converse also true better features learned course learning mechanisms selecting relevant change even selection objective stays same. dictionary learning task serve realistic algorithmic model feature learning process organisms capable attentional ﬁltering mutual dependency dictionary learning attentional sample selection bias must taken consideration. work examine effect sampling bias dictionary learning task. particular explore interactions learned dictionary elements example selection algorithms. investigate whether selection algorithm approach even improve upon learning unbiased sampling strategy. heuristics examine also close relationships models attention suggesting plausibly implemented organisms evolving effectively encode stimuli environment. signal column vector restricted exactly positive activations irp≥ dictionary element constrained unit-norm ∀j}. goal dictionary learning recover assuming known. wish calculate maximum posteriori estimate method optimal directions engan alternate optimization scheme guaranteed converge locally optimal solution ˆamap estimation problem scheme also attractive algorithmic model low-level feature learning since optimization process related analysis synthesis phases autoencoder network olshausen field paper henceforth refer problems encoding updating stages corresponding optimizers fenc fupd. l-constrained encoding problem np-hard elad various approximation methods extensively studied sparse coding literature. approach ignore constraint solve remaining nonnegative l-regularized least squares problem larger sparsity penalty λp/k compensate lack constraint. well approximated works well practice since distribution exp). simulations least angle regression algorithm duchi implemented spams package mairal solve this. another approach greedily seek nonzero activations minimize reconstruction errors. matching pursuit family algorithms operate idea effectively approximate encoding model approximation ignores penalty nonzero activations exponentially distributed mostly small approximation also effective. orthogonal matching pursuit algorithm mallat zhang also implemented spams package problem. even simpler variant pursuit-type algorithm thresholding elad k-sparse algorithm makhzani frey algorithm takes largest values sets every component zero algorithm plausibly implemented feedforward phase autoencoder hidden layer competes horizontally picks winners. simplicity algorithm important purposes allow training examples selected encoding stage encoding algorithm must operate much larger number examples updating algorithm. view also motivated nonnegative constraint activations hidden layers likely conveyed nonnegative ﬁring rates. here learning rate decays inversely update epoch update projected back normalizing column. given training examples encoding updating procedure repeated small number times practical issue task small number dictionary elements tend assigned large number activations. produces rich richer effect regularly used elements often used unused elements left initial stages. avoid this activity normalization procedure takes place encoding stage. idea modulate activities mean activity element closer across-element mean mean activities; done cost increasing reconstruction error. equalization modulated corresponding equalization fully egalitarian equalization simulations found empirically provide good balance equalization reconstruction. examine effect example selection process learning extend alternate optimization scheme equations include example selection stage. stage selection algorithm picks examples dictionary update ideally examples chosen make learned dictionary closer ground-truth compared uniform sampling. following describe number heuristic selection algorithms inspired models attention. characterize example selection algorithms parts. first choice goodness measure function maps number reﬂecting goodness instance dictionary element applying {s}n yields goodness values dictionary elements examples. second choice selector function fsel. function dictates subset chosen using values. motivated idea critical examples zhang favors examples large reconstruction errors. paradigm criticality measured correspond ground-truth errors since calculated using current estimate rather ground-truth another related idea select examples would produce large gradients dictionary update equation without regard directions. results observation level noise puts fundamental limit recovery true dictionary better approximation bound obtained observation noise low. follows that somehow collect examples happen noise learning examples might beneﬁcial. motivated consider another idea focuses statistical property activations inspired model visual saliency proposed zhang saliency model called model asserts signals result rare feature activations salient. speciﬁcally model deﬁnes saliency particular visual location proportional self-information feature activation assume nonzero activations exponentially distributed corresponds contrast measure salmap depends consequently salmap impervious changes since signals simulations small monochrome patches saliency single-scale intensity channel orientation channel four directions. done ﬁrst sorting picking examples round-robin fashion examples selected. barring duplicates yields consisting elements element algorithm describes operations take place within learning epoch. simulations consider possible combinations goodness measures selector functions example selection algorithm except salmap. since goodness measures produce different values different dictionary element activations bysum byelement functions select equivalent example sets. order evaluate example selection algorithms present simulations across variety dictionaries encoding algorithms. speciﬁcally compare results using three possible encoding models eight selection algorithms. generate training examples known ground-truth dictionary quantify integrity learned dictionary learning epoch using minimal mean square distance figure ground-truth dictionaries generated examples element generated example patch displayed tiled image ease visualization. white positive black negative. spanning possible permutations. also investigate effect learning. characterize dictionary mutual coherence maxi=j elad measure useful theoretical analysis recovery bounds donoho practical characterization average aj|. regardless exact recovery dictionary coherence challenging coherence high. ﬁrst dictionary comprises gabor patches dictionary inspired fact dictionary learning natural images leads dictionary olshausen field correspond simple receptive ﬁelds mammalian visual cortices jones palmer dictionary relatively incoherent learning problem easier. second dictionary composed alphanumeric letters alternating rotations signs artiﬁcial dictionary within epoch examples generated nonzero activations example examples whose magnitudes sampled exp. selection algorithm picks training learning. experiment initialized random examples training set. both dictionaries violate recovery bound described donoho amiri haykin notes bound prone violated practice; such explicitly chose realistic parameters violate bounds simulations. results figure shows average distance learning epoch. observe byelement selection policies generally work well especially conjunction grad goodness measures. trend especially noticeable alphanumeric dictionary case bysum-selectors perform worse baseline selector chooses examples randomly ranking selector algorithms roughly consistent across learning epochs also robust choice encoding algorithms particular good selector algorithms beneﬁcial even relatively early stages learning contrast simulation amiri haykin surprising early stages learning poor estimates result activation estimates well. nevertheless good selector algorithms soon establish positive feedback loop dictionary activation estimates. interesting exception salmap selector. works relatively well gabor dictionary alphanumeric dictionary. presumably design salmap model model uses oriented gabor ﬁlters feature maps overall effect similar sunbysum algorithm signals generated gabor dictionaries. figure distance true dictionaries. graphs left column show time course learning using lars encoding. legends ordered worst best simulation graphs right column compares performance different encoding models. ordinate distance scale left graphs. order assess robustness example selection algorithms repeated gabor dictionary simulation across range parameter values. speciﬁcally experimented modifying following parameters time starting original parameter values figure shows result simulations. results show good selector algorithms improve learning across wide range parameter values. note number dictionary elements whose results suggest improvement greatest complete dictionary learning cases; advantage selection appears diminish extremely over-complete dictionary learning tasks. work examined effect selection algorithms dictionary learning based stochastic gradient descent. simulations using training examples generated known dictionaries revealed selection algorithms indeed improve learning sense learned dictionaries closer known dictionaries throughout learning epochs. special note success selectors; since selectors simple hold promise general learning applications. studies investigated example selection strategies dictionary learning task although learning algorithms contain procedures implicitly. instance k-svd aharon relies upon identifying group examples particular dictionary element update stage. algorithm arora also makes sophisticated example grouping procedure provably recover dictionaries. cases though focus breaking inter-dependency instead characterizing algorithms notably perceptual systems might improve learning despite inter-dependency. recent paper consider example selection whose cognit algorithm explicitly related perceptual attention. point differentiates work lies generative assumption cognit relies additional information available learner case temporal contiguity generative process. spatially temporally independent generation process generative model considered simpler difﬁcult solve. training thus lead learning incorrect dictionary. however empirically shown case. intuitive reason also underlies design selectors good selection algorithms picks samples high information content. instance samples close zero activation content provide little information dictionary elements compose them even though samples abound generative model exponentially-distributed activations. follows samples provide little beneﬁt inference statistical structure training learner would well-advised discard them. validate this calculated last epoch learning selection algorithm shows selection algorithms picked much higher uniform. however correlation overall performance ranking weak suggesting factor driving good example selection. another factor contributes good learning spread examples within casual observation revealed bysum selector prone picking similar examples whereas byelement selects larger variety examples thus retains distribution faithfully. quantify this measured distance distribution selected examples training examples using histogram intersection distancerubner right columns figure shows distance d||d) tends lower byelement selectors bysum selectors like measure however quantity weakly predictive overall performance suggesting important pick large variety high-snr examples dictionary learning task. several directions plan extend work. theoretical analysis selection algorithms. instance explore conditions learning example selection leads solutions unbiased learning although empirically observed case. curriculum learning paradigm bengio also possible different selection algorithms better suited different stages learning. another apply active example selection processes hierarchical architectures stacked autoencoders restricted boltzmann machines. cases interesting question arises information layer combined make selection decision. intend explore questions future using learning tasks similar work. michal aharon michael elad alfred bruckstein. k-svd algorithm designing overcomplete dictionaries sparse representation. signal processing ieee transactions yoshua bengio j´erˆome louradour ronan collobert jason weston. curriculum learning. proceedings annual international conference machine learning pages david donoho michael elad vladimir temlyakov. stable recovery sparse overcomplete representations presence noise. ieee transactions information theory john duchi shai shalev-shwartz yoram singer tushar chandra. efﬁcient projections onto l-ball learning high dimensions. proceedings international conference machine learning laurent itti christof koch eiebur niebur. model saliency-based visual attention rapid scene analysis. ieee transactions pattern analysis machine intelligence", "year": 2014}