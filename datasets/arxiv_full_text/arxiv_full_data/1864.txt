{"title": "Character-level Recurrent Neural Networks in Practice: Comparing  Training and Sampling Schemes", "tag": ["cs.LG", "cs.CL", "stat.ML"], "abstract": "Recurrent neural networks are nowadays successfully used in an abundance of applications, going from text, speech and image processing to recommender systems. Backpropagation through time is the algorithm that is commonly used to train these networks on specific tasks. Many deep learning frameworks have their own implementation of training and sampling procedures for recurrent neural networks, while there are in fact multiple other possibilities to choose from and other parameters to tune. In existing literature this is very often overlooked or ignored. In this paper we therefore give an overview of possible training and sampling schemes for character-level recurrent neural networks to solve the task of predicting the next token in a given sequence. We test these different schemes on a variety of datasets, neural network architectures and parameter settings, and formulate a number of take-home recommendations. The choice of training and sampling scheme turns out to be subject to a number of trade-offs, such as training stability, sampling time, model performance and implementation effort, but is largely independent of the data. Perhaps the most surprising result is that transferring hidden states for correctly initializing the model on subsequences often leads to unstable training behavior depending on the dataset.", "text": "abstract recurrent neural networks nowadays successfully used abundance applications going text speech image processing recommender systems. backpropagation time algorithm commonly used train networks speciﬁc tasks. many deep learning frameworks implementation training sampling procedures recurrent neural networks fact multiple possibilities choose parameters tune. existing literature often overlooked ignored. paper therefore give overview possible training sampling schemes character-level recurrent neural networks solve task predicting next token given sequence. test different schemes variety datasets neural network architectures parameter settings formulate number take-home recommendations. choice training sampling scheme turns subject number trade-offs training stability sampling time model performance implementation effort largely independent data. perhaps surprising result transferring hidden states correctly initializing model subsequences often leads unstable training behavior depending dataset. keywords recurrent neural networks deep learning backpropagation time optimization performance dynamic sequences discrete tokens abundant encounter daily basis. examples discrete tokens characters words text notes musical composition pixels image actions reinforcement learning agent pages visits tracks listens music streaming service etc. tokens appears sequence often strong correlation consecutive nearby tokens. example similarity neighboring pixels image large since often share similar shades colors. words sentences characters words also correlated underlying semantics language characteristics. paper discrete tokens considered opposed sequences real-valued samples stock prices analog audio signals word embeddings etc. methodology also applicable kinds sequences. cedric boom thomas demeester bart dhoedt ghent university imec idlab technologiepark ghent belgium e-mail {cedric.deboom thomas.demeester bart.dhoedt}ugent.be sequence discrete tokens presented machine learning model designed assess probability next token sequence modeling i’th token sequence. kinds models names autoregressive models recurrent recursive models dynamical systems etc. ﬁeld natural language processing called language models token stands separate word n-gram since models give probability distribution next token sequence sample distribution drawn thus token sequence generated. recursively applying generation step entire sequences generated. example language model capable assessing proper language utterances also generating unseen text. particular type generative models become popular past years recurrent neural network regular neural network ﬁxed-dimensional feature representation transformed another feature representation non-linear function; multiple instances feature transformations applied calculate ﬁnal output neural network. recurrent neural network process feature transformations also repeated time every time step input processed output produced makes suitable modeling time series language utterances etc. dynamic sequences variable length rnns able effectively model semantically rich representations sequences. example graves showed rnns capable generating structured text wikipedia article even continuous handwriting models shown great potential modeling temporal dynamics text speech well audio signals recurrent neural networks also effectively generate images pixel basis shown gregor draw oord pixelrnns next this context recommender systems rnns used successfully model user behavior online services recommend items consume despite fact rnns abundant scientiﬁc literature industry much consensus efﬁciently train kinds models extent knowledge focused contributions literature tackle question. choice training algorithm often depends deep learning framework hand fact multiple factors inﬂuence performance often ignored overlooked. merity pointed training models fundamental trade-offs rarely discussed goal paper study number widely applicable training sampling techniques rnns along respective advantages trade-offs. tested variety datasets neural network architectures parameter settings order gain insights algorithm best suited. next section concept rnns introduced speciﬁcally character-level rnns models trained. section four different training sampling methods rnns detailed. that section present experimental results accuracy efﬁciency performance different methods. also present take-home recommendations range future research tracks. finally conclusions listed section table gives overview symbols used throughout paper order appearance. mentioned introduction paper mainly focuses dynamic sequences discrete tokens. generating modeling sequences core application speciﬁc type recurrent neural networks character-level recurrent neural networks. recurrent neural networks designed maintain summary past sequence memory so-called hidden state updated whenever input token presented. summary used make prediction next token sequence i.e. model hidden state input token input vector hidden state hidden state vector output token output vector parameterized differentiable functions loss function learning rate arbitrary weight arbitrary weight matrix number time steps truncated bptt operation performed number time steps gradients backpropagated truncated bptt nonlinear activation function sigmoid function model output function model hidden state model element-wise vector multiplication operator sequence concatenation operator dataset train test ordered tokens appearing dataset number recurrent layers model dimensionality recurrent layers model given adequate initialization hidden state trained parameterized functions previous scheme used generate inﬁnite sequence tokens. character-level rnns speciﬁcally input tokens discrete stochastic function produces probability mass function possible tokens. produce next token sequence sample mass function simply pick token highest probability this used slice notation means side comment even though name refers character-based language models character-level rnns model wide variety discrete sequences refer reader introduction section. regular feedforward neural networks trained using backpropagation algorithm this certain input ﬁrst propagated network compute output. called forward pass. output compared ground truth label using differentiable loss function. backward pass gradients loss respect parameters network computed application chain rule. finally parameters updated using gradient-based optimization procedure gradient descent neural network terminology parameters network also called weights. loss function network output ground truth label denoted vector weights network standard update rule gradient descent given here so-called learning rate controls size steps taken weight update. practice input samples organized equally-sized batches sampled training dataset loss averaged summed leading less noisy updates. called mini-batch gradient descent. gradient descent ﬂavors rmsprop adam extend equation making optimization procedure even robust recurrent neural networks input applied every time step output certain time step dependent previous inputs shown equation means loss time step needs backpropagated applied inputs time step procedure therefore called backpropagation time sequence long bptt quickly becomes inefﬁcient backpropagating time steps compared backpropagating -layer deep feedforward neural network. unlike feedforward networks however rnns weights shared across time. best seen unroll visualize separate time steps figure scale backpropagation time long sequences gradient update often halted traversed ﬁxed number time steps. procedure called truncated backpropagation time. apart stopping gradient updates backpropagating beginning sequence also limit frequency updates. given training sequence truncated bptt proceeds follows. every time step token processed whenever tokens processed so-called forward pass— hidden state updated times—truncated bptt initiated backpropagating gradients time steps. here analogy sutskever denoted number time steps performing truncated bptt length bptt keep using parameters throughout paper. visual explanatory example truncated bppt found figure shows every time steps gradients backpropagated three time steps. note that order remain data efﬁcient possible preferably less equal since otherwise data points would skipped training. mentioned before rnns keep summary past sequence encoded hidden state representation. whenever input presented hidden state gets updated. update happens depends internal dynamics rnn. simplest version extension feedforward neural network matrix multiplications used perform input transformations vector representation i’th layer neural network matrix containing weights layer vector biases. function nonlinear transformation function also called activation function; often used examples sigmoid logistic function tanh rectiﬁed linear units variants. case rnns hidden state update rewritten style equation follows help model long-term dependencies counter vanishing gradient problem several extensions equation proposed. best known examples long short-term memories recently gated recurrent units comparable characteristics similar performance variety tasks lstms grus incorporate gating mechanism controls extent input stored memory memory forgotten. purpose lstm introduces input forget gate well output gate here symbol stands element-wise vector multiplication. note lstm uses types memories hidden state so-called cell state compared lstms grus extra cell state incorporate gates reset update gate reduces overall number parameters generally speeds learning. choice lstms versus grus dependent application hand experiments paper lstms currently widely used layers. since neural networks multiple layers usually stacked onto another also possible recurrent layers. case output time step input next recurrent layer also time step layer thus processes sequence outputs produced previous layer. will course signiﬁcantly slow bptt. section four schemes presented train character-level rnns sample tokens them. task model independent schemes purpose predict next symbol character sequence given previous tokens. training sampling schemes thus merely practical means solve task later sections effect used scheme performance efﬁciency model studied. already point four schemes presented among basic practical methods train sample rnns course many combinations variants could devised. discussion different schemes general character-level denoted output applying token input. output vector represents probability distribution across characters. notational convenience write i’th hidden state mentioned before isolate four different schemes train rnns sample tokens them. scheme truncated bptt framework fact practical approximation original algorithm. whenever parameters refer deﬁnitions gave section also important keep mind task schemes same namely predict next token given sequence. help understand mechanisms scheme visualized schematically figures training procedures drawn output tokens loss deﬁned. that example main difference scheme compared scheme compute loss ﬁnal output token instead output tokens training. regarding sampling procedures ﬁrst schemes token always sampled starting initial hidden state colored light gray. call principle ‘windowed sampling’. schemes hand sampling token based current hidden state applying previous token input rnn. sampling procedure called ‘progressive sampling’. training procedure scheme observe similar technique hidden state carried across subsequent sequences. next subsections give details training sampling procedures practical details different schemes one. mention schemes described without batching practice mini-batch training usually done motivated section schemes however easily transferred batched setting. isolated three different training procedures character-level rnns. ﬁrst algorithm called multi-loss training rudimentary outline shown algorithm input sequences length subsequently taken train skipping every characters. input token loss calculated output rnn. entire sequence processed average losses update weights calculated. also reset hidden state training sequence. initial hidden state learned backpropagation together weights rnn. practice every input sequence characters initial hidden state same. note lstms hidden state comprises hidden cell vectors equation truncated bptt procedures lines calculates gradients loss respect weights using backpropagation. optimize procedure lines uses gradients update weights using equation single-loss training instead deﬁning loss outputs rnn—which forces make good predictions ﬁrst tokens sequence—we deﬁne loss ﬁnal predicted token sequence. complete training procedure shown algorithm difference algorithm inner loop aggregate loss every output. loss calculated outside loop line ﬁnal output. multi-loss single-loss procedures always start training sequence initial hidden state learned. conditional multi-loss training hand multi-loss training procedure adapted reuse hidden state across different sequences. approach leans much closer original truncated bptt algorithm initial state always reset. outline training method given algorithm since using truncated bptt procedure requires meticulous bookkeeping hidden state every time step observed lines especially true work mini-batch setting also need keep track subsequent batches constructed. next training algorithms explained previous section also discern different sampling procedures. used generate previously unseen sequences. procedures common sampling started seed sequence tokens rnn. done order appropriately bootstrap rnn’s hidden state. seed sequence processed procedures start differ. so-called windowed sampling next token sequence sampled applying seed sequence. newly sampled token concatenated sequence. this hidden state reset learned representation. sampling next token proceeds take last tokens sequence sampled thus feed next token sampled appended sequence hidden state reset. entire windowed sampling procedure sketched algorithm line algorithm used symbol indicate sequence concatenation. progressive sampling next token sequence always sampled given current hidden state previously sampled token. token applied input updates hidden state next token sampled output. initial hidden state therefore never reset. intuitive sampling rnn. entire sampling procedure given algorithm lines bootstrapped using initial hidden state seed sequence. following lines tokens continuously sampled token time inner loop algorithm longer needed. ﬁrst scheme multi-loss training combined windowed sampling procedure main advantage using scheme need hidden state bookkeeping across sequences. especially training cumbersome batched version algorithm. disadvantage sampling slower increases sample token inputs need processed. model contains many layers lead scalability issues. another disadvantage loss deﬁned outputs training. force produce good token candidates seen input tokens. lead short-sighted model mostly looks recent history make prediction next token. scheme potential issue solved using single-loss training. second scheme multi-loss training procedure scheme replaced single-loss equivalent algorithm main advantage allow hidden state certain burn-in period predictions made using knowledge past sequence. burning hidden state also causes able learn long-term dependencies data make prediction seen tokens. potential drawback learning slower since signal backpropagated every sequence compared signals ﬁrst scheme. sampling algorithm hand ﬁrst scheme almost perfectly reﬂects trained i.e. considering ﬁnal token input sequence. scheme number back multi-loss training procedure scheme progressive sampling algorithm used instead windowed sampling. drawback sampling method scheme scalable large values since need feed sequence tokens every token sampled. progressive sampling hand next token sampled immediately every input token. sampling sequences sped factor approximately main advantage scheme. scheme still standard multi-loss training resets hidden state every train sequence. scheme replaces conditional multi-loss training procedure algorithm maintaining progressive sampling algorithm. main disadvantages using particular training algorithm requirement keep track hidden states across train sequences carefully select train sequences dataset hard mini-batch settings. next this whenever weights updated hidden state update reused potentially lead unstable learning behavior. plus side able learn dependencies tokens time steps away since hidden state remembered train sequences. also need learn appropriate initial hidden state eliminated lead small speed-up learning. works literature used rnns language modeling character word level. works listed describe described state-of-the-art results famous benchmarks penn treebank dataset wikitext- hutter prize datasets ﬁrst datasets mainly used benchmark word-level language models hutter prize datasets generally used character-level evaluation. papers however also train character-level models penn treebank dataset. purpose give reader high-level idea schemes used existing literature. intend give complete overview literature rnns language modeling. instead focus highly cited works have point reported state-of-the-art results above-mentioned benchmarks. this attention given recent literature ﬁeld. overview found table distinction made character-level models word-level models models applied levels. bottom three different applications listed used rnns model various sequential problems. immediately notice investigated papers explicitly mention training details regarding loss aggregation hidden state propagation. cases source code manually infer training sampling scheme. source code available contacted authors directly details. whenever could information paper source code authors marked ‘unknown’. scheme number popular recent literature scheme number also widely used. mentioned previously main difference schemes whether transfer hidden state subsequent training sequences takes place not. seems clear consensus topic among researchers. older works graves mikolov transfer hidden state community seems transitioning towards explicitly this. although exists literature describing advantages disadvantages methods think possible explanations this. first going multiple source code repositories noticed source code often reused copying adapting previous work. causes architectural computational designs transfer previous work works. another possible cause lies evolution deep learning frameworks. tensorﬂow keras pytorch made fairly easy train rnns hidden state transfer less straightforward required effort older frameworks theano lasagne. model type reference character-level character-level character-level character-level character-level character-level character-level word& character-level word& character-level word-level word-level word-level word-level word-level word-level word-level word-level word-level word-level music notes phonemes playlist tracks author communication author communication paper paper source code paper source code paper source code source code author communication author communication author communication source code source code author communication author communication author communication paper conclude concise overview shown need clarity transparency literature concerning training sampling details rnns. interest reproducibility also spike awareness research community. paper ﬁrst attempt calling attention different training sampling section training sampling schemes evaluated variety settings. mentioned before task settings same predicting next token character sequence given previous tokens characters. perform evaluation four datasets different characteristics english text finnish text code classical music. next this vary architecture—such number recurrent layers hidden state size—as well truncated bptt parameters. evaluations give recommendations train sample character-level rnns. central part experiments model. this construct standard architecture parameters vary. input one-hot representation current character sequence i.e. vector zeros length ordered characters dataset except single current character’s position next recurrent lstm layers added hidden state dimensionality experiments parameters varied. finally fully connected dense layers ﬁxed dimensionality neurons ﬁnal dense layer dimensionality |v|. ﬁnal layer softmax function applied order arrive probability distribution across possible next characters. complete architecture summarized table including nonlinear activation functions extra details. train model schemes outlined section common practice deep learning gradient-based optimization multiple training sequences grouped batches. sequence batch length tokens ﬁrst used input next token used ground truth signal every input token. paper batch size sequences used across experiments. ensure diverse sequences batch pick sequences equidistant offsets increase every batch. speciﬁcally every i’th batch sequences sampled following offsets train dtrain entire train also circularly shifted training epoch. since scheme hidden states transferred across different batches batching method allows fairly compare four schemes. formula i’th token sequence total number tokens. better model predicting next token lower perplexity measure. context rnns conditional probability equation approximated using hidden state shown equation practice hidden state bootstrapped characters perplexity calculated subsequent characters test set. experiments below every trained batches sequences using batching method described above. schemes experiments standard categorical cross-entropy loss function used calculates inner product output probability vector one-hot vector target token training report perplexity test logarithmically spaced intervals. models trained times always choose every conﬁguration reinitialize network weights random generators initial values. optimization algorithm adam learning rate throughout experiments. experiments performance scheme evaluated four datasets. dataset characteristics datasets. english compiled plays william shakespeare project gutenberg website dataset. finnish language different english. gutenberg website gathered texts finnish playwrights juhani eino leino. results dataset characters unique. music created dataset extracting music notes midi ﬁles. notes played simultaneously midi extract high obtain single sequence subsequent notes. downloaded piano compositions bach beethoven chopin haydn classical archives removed duplicate compositions gathered dataset notes unique ones. datasets available download https//github.com/cedricdeboom/character-level-rnn-datasets www.gutenberg.org github.com/torvalds/linux/tree/master/kernel www.classicalarchives.com several experiments performed evaluate predictive performance rnns trained different conﬁgurations. ﬁrst round experiments architecture models varied. speciﬁcally number recurrent lstm layers also change hidden state size either figure shows plots different architectures trained using scheme four datasets. every architecture plotted lines different settings mentioned above. plots rnns initially perform better rnns learn somewhat faster long term. batches clear difference performance anymore architectures. music dataset architectures observe overﬁtting. dropout ﬁnal dense layers overﬁtting already greatly reduced still observable finnish dataset architectures notice bump around train sequences present conﬁgurations. bump lowered reduce learning rate different momentum-based optimizer rmsprop remains artefact dataset architecture. also perform experiments scheme results shown figure behavior respect architectural differences observed ﬁrst scheme. networks converge somewhat slower seen especially music dataset comparing figures plus side performance curves smoother scheme effects explained fact loss signal training sequence makes learning slower backpropagated gradient higher quality since historical characters taken account. figures conclude architecture indeed inﬂuences efﬁciency training procedure effect observed globally across datasets training schemes. best architecture four datasets parameters i.e. green plots. speciﬁc architecture therefore used next experiments. next schemes compared different datasets. mentioned above architecture used. perplexity plots gathered figure schemes robust across datasets also across different settings since lines close other. scheme also best performing terms perplexity. performance scheme overall worse compared scheme probably fact learning occurs slowly argued before. performance scheme comparable ﬁrst scheme slightly worse robust. since training procedure schemes same hypothesize sampling procedure scheme sometimes difﬁculties recovering errors carried across many time steps. another reason learned make predictions sequences longer tokens. also mention schemes experimented randomly shufﬂing training sequences instead circularly shifting train explained section lead different observations. scheme hidden state transferred across sequences training appears solve problem least conﬁgurations conﬁgurations scheme start performance scheme around train batches—i.e. train sequences graph—some conﬁgurations start diverging cannot isolate consistent motivation explanation. ﬁgures behavior also heavily dependent dataset; difference e.g. finnish music dataset notable. also interesting take look comparison performances different datasets scheme. curves plotted figure schemes notice performance english finnish linux datasets almost equal; music dataset seems harder model architecture. also observe scheme robust changes training parameters since curves close other. variance scheme even scheme highest scheme point would also like discuss data efﬁciency. small values less data particular point training process compared larger values important data resources scarce. figure noticeable that least scheme lines different values close other. experiments general conclusion could small value order data efﬁcient possible. choice seems less impact choice training scheme. additionally using small value improves label reuse multi-loss training algorithms. approximately quantiﬁed i.e. number times label reused training process. comparing performance different models schemes terms number train sequences used certain point time. models also compared terms absolute training sampling time give overview conﬁgurations fastest. next experiment calculate average training time batch sampling time single token english dataset. vary scheme training well architecture. concerning training parameter almost difference training time measurements numbers shown table surprise schemes almost equal training time batch scheme trains signiﬁcantly faster since need compute softmax output training sequence. however noticeable complex architecture smaller relative difference training time decrease architecture architecture. regarding sampling times schemes faster factor compared schemes since need propagate entire sequence sample token. also compare performance different schemes respect changes parameter. scheme perform experiments successively setting parameter rounded nearest integer. every experiment performed music dataset since based previous experiments expect gain insights report model perplexity test function elapsed training time train total batches. results shown figure y-axis clipped maximum achieve informative view. smaller value faster trained batches since leads shorter bptt. ﬁrst scheme robust changes shortest sequence lengths behave noisily ﬁrst seconds training conﬁgurations able reach similar optimal perplexity. second scheme trains much slower scheme experiences instability problems small sequence lengths conﬁgurations stable fully converged batches. scheme almost behavior scheme conﬁgurations reaching optimal perplexity. before robustness changes worse. especially true small values shown blue lines figure finally scheme almost conﬁgurations unstable behave noisily. conﬁgurations even achieve ﬁnal perplexity around lowering small values seems help case. conclude experimentation section recommendations. found global behavior different schemes nearly independent used dataset. good news since tune learning sampling procedure dataset hand. respect arrive following conclusions terms training schemes multi-loss approach recommended. compared singleloss approach multi-loss training efﬁcient. faster individual iterations single-loss approach cannot compensate beneﬁt combining loss multiple positions sequence considering total train time. general recommendation avoid training procedures hidden state transferred input sequences training efﬁcient multi-loss approach without transferred hidden states sequence windowed sampling approach. include ﬁnal section future research tracks area training sampling procedures characterlevel rnns. paper made attempt isolating four common schemes used literature. however multiple hybrid combinations identiﬁed investigated future. straightforward extension intermediate form singlemulti-loss training. example extra parameter could identiﬁed deﬁnes number time steps loss calculated aggregated. edge cases correspond respectively single-loss multi-loss training procedures. possibility decay loss time step combine linear combination calculate ﬁnal loss. single training sequence results i/length− resp. linear exponential decay. consequentially resulting gradient scaled similarly thereby reducing contribution ﬁrst tokens sequence total loss. explained concept character-level rnns models typically trained using truncated backpropagation time. introduced four schemes train character-level rnns sample tokens models. schemes differ approximate truncated backpropagation time paradigm outputs combined ﬁnal loss whether hidden state remembered reset input sequence. that evaluated scheme different datasets conﬁgurations terms predictive performance training time. showed conclusions remain valid across different experimental settings. perhaps surprising result study conditional multi-loss training hidden state carried across training sequences often leads unstable training behavior depending dataset. contrasts sharply observation training procedure used often literature although requires meticulous bookkeeping hidden state carefully designed batching method. single-loss training compared multiloss slower regarding number used train sequences. advantage single-loss training however encourage network make predictions long-term basis since backpropagate loss deﬁned sequence. progressive sampling slightly less robust changes training parameters compared windowed sampling especially datasets difﬁcult model showed music dataset. main advantage progressive sampling orders magnitudes faster windowed sampling. funding hardware used perform experiments paper funded nvidia. conﬂict interest cedric boom funded grant research foundation flanders authors declare conﬂicts interest.", "year": 2018}