{"title": "Look Before You Leap: Bridging Model-Free and Model-Based Reinforcement  Learning for Planned-Ahead Vision-and-Language Navigation", "tag": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "abstract": "Existing research studies on vision and language grounding for robot navigation focus on improving model-free deep reinforcement learning (DRL) models in synthetic environments. However, model-free DRL models do not consider the dynamics in the real-world environments, and they often fail to generalize to new scenes. In this paper, we take a radical approach to bridge the gap between synthetic studies and real-world practices---We propose a novel, planned-ahead hybrid reinforcement learning model that combines model-free and model-based reinforcement learning to solve a real-world vision-language navigation task. Our look-ahead module tightly integrates a look-ahead policy model with an environment model that predicts the next state and the reward. Experimental results suggest that our proposed method significantly outperforms the baselines and achieves the best on the real-world Room-to-Room dataset. Moreover, our scalable method is more generalizable when transferring to unseen environments, and the relative success rate is increased by 15.5% on the unseen test set.", "text": "abstract. existing research studies vision language grounding robot navigation focus improving model-free deep reinforcement learning models synthetic environments. however model-free models consider dynamics real-world environments often fail generalize scenes. paper take radical approach bridge synthetic studies real-world practices—we propose novel planned-ahead hybrid reinforcement learning model combines model-free model-based reinforcement learning solve real-world vision-language navigation task. look-ahead module tightly integrates look-ahead policy model environment model predicts next state reward. experimental results suggest proposed method signiﬁcantly outperforms baselines achieves best real-world roomto-room dataset. moreover scalable method generalizable transferring unseen environments relative success rate increased unseen test set. rather trivial human follow instruction walk beside outside doors behind chairs across room. turn right walk stairs... teaching robots navigate instructions challenging task. complexities arise linguistic variations instructions also noisy visual signals real-world environments rich dynamics. robot navigation visual language grounding also fundamental goal computer vision artiﬁcial intelligence beneﬁcial many practical applications well in-home robots hazard removal personal assistants. fig. example task. embodied agent learns navigate room arrive destination following natural language instructions. blue arrows match orientations depicted pictures corresponding sentence. vision-and-language navigation task training embodied agent ﬁrst-person view humans carry natural language instructions real world figure demonstrates example task agent moves towards destination analyzing visual scene following natural language instructions. diﬀerent vision language tasks visual perception natural language input usually ﬁxed agent interact real-world environment pixels perceives changing moves. thus agent must learn visual input correct action based perception world understanding natural language instruction. although steady progress made natural language command robots still perfect. previous methods mainly employing model-free reinforcement learning train intelligent agent directly mapping observations actions state-action values. modelfree consider environment dynamics usually requires large amount training data. besides evaluated synthetic rather real-world environments signiﬁcantly simpliﬁes noisy visual linguistic perception problem subsequent reasoning process real world. worth noticing humans follow instructions however solely rely current visual perception also imagine environment would look like plan ahead mind actually performing series actions. example baseball catcher outﬁeld players often predict direction rate speed ball travel plan ahead move expected destination ball. inspired fact seek help recent advance model-based task. model-based attempts learn model used simulate environment multi-step lookaheads planning. internal environment model predict future plan ahead agent beneﬁt planning avoiding trial-and-error real environment. therefore paper propose novel approach improves vision-and-language navigation task performance reinforced planning ahead speciﬁcally method ﬁrst time endows intelligent agent environment model simulate world predict future visual perception. thus agent realize directly mapping current real observation planning future observations time perform action based both. furthermore choose real-world room-to-room dataset testbed method. model-free model signiﬁcantly outperforms baseline methods reported dataset. moreover equipped look-ahead module model improves results achieves best dataset. hence contributions three-fold shown figure consider embodied agent learns follow natural language instructions navigate realistic indoor environments. specifically given agent’s initial pose includes spatial position heading elevation angles natural language instruction agent expected choose sequence actions arrive target position vtarget speciﬁed language instruction action consists unique actions i.e. turn left turn right camera camera down move forward stop. order ﬁgure desired action time step agent needs eﬀectively associate language semantics visual observation environment. observation image captured mounted camera. performance agent evaluated success rate psucc ﬁnal navigation error enav consideration sequential-decision making nature task formulate reinforcement learning problem agent sequentially interacts environments learns trial error. action taken agent receives scalar reward environment. agent’s action step determined parametrized policy function training objective optimal parameters maximize discounted cumulative rewards encodes language sequence image frames decodes action sequence at}. basic model consists language encoder encodes instruction word features image encoder extracts high-level visual features recurrent policy network decodes actions recurrently updates internal state supposed encode history previous actions observations. reinforce agent planning ahead improve model’s capability equip agent look-ahead modules employ environment model take account future predictions. mation ﬁnal decision making forms model-free path itself. addition model-based path exploits multiple look-ahead modules realize look-ahead planning imagine possible future trajectories. ﬁnal action chosen action predictor based information model-free model-based paths. therefore method seamlessly integrates model-free model-based reinforcement learning. core component method look-ahead module used imagine consequences planning ahead multiple steps current state order augment agent imagination introduce environment model makes prediction future based state present. since directly predicting image challenging environment model instead attempts predict abstractstate representation represents high-level visual feature. figure showcases internal process look-ahead module consists environment model look-ahead policy trajectory encoder. given abstract-state representation real world step lookahead policy ﬁrst takes input outputs imagined action environment model receives state action predicts corresponding reward look-ahead policy take action environment t+}. look-ahead planning goes model make prediction steps preset trajectory length. lstm encode predicted rewards states along look-ahead trajectory outputs representation shown figure every time step model-based path operates look-ahead processes obtain look-ahead trajectory representation look-ahead trajectories aggregated together passed action predictor information model-based path. environment model given current state action taken reward agent environment model predicts next state shown figure projection function fproj ﬁrst concatenates projects feature space. output transition function ftransition reward function freward obtain fproj ftransition freward learnable neural networks. speciﬁcally fproj linear projection layer ftransition multilayer perceptron sigmoid output freward also multilayer perceptron directly outputs scalar reward. produces action considering context word features {wi} environment state previous action internal hidden state ht−. note directly take encoded word features {wi} input lstm decoder. instead adopt attention mechanism better capture dynamics language instruction dynamically attention words beneﬁcial current action selection. output feature concatenation lstm’s output context vector passed action predictor making decision. recurrent policy model employed individual policy directly outputs action based note model feed context vector lstm output posterior boosts performance solely feeding input. action predictor action predictor multilayer perceptron softmax layer last layer. given information model-free model-based paths input action predictor generates probability environment model learning ideally look-ahead module expected provide agent accurate predictions future observations rewards. environment model noisy itself actually provide misleading information make training even unstable. terms this plug look-ahead module pretrain environment model using randomized teacher policy. policy agent decide whether take human demonstration action random action based bernoulli meta-policy phuman since agent’s policy closer demonstration policy training environment model trained demonstration policy help better predict transitions close optimal trajectories. policy learning pretrained environment model incorporate look-ahead module policy model. ﬁrst discuss general pipeline training agent describe train proposed model. task distinct supervisions used train policy model. first demonstration actions provided simulator pure supervised learning. training objective case simply maximize log-likelihood demonstration action demonstration action. agent quickly learn policy perform relative well seen scenes. however pure supervised learning encourage agent imitate demonstration paths. potentially limits agent’s ability recover erroneous actions unseen environment. also encourage agent explore state-action space outside demonstration path utilize second supervision i.e. reward function. reward function depends environment state agent’s action usually diﬀerentiable terms objective task successfully arrive target position deﬁne reward function based distance metric. denote distance state target indicates whether action reduces agents distance target. obviously reward function reﬂects immediate eﬀect particular action ignores action’s future inﬂuence. account this reformulate reward function discounted cumulative form besides success whole trajectory also used additional binary reward. details reward setting discussed experiment section. reward function objective becomes perform depth-bounded roll-outs using environment model roll-out encoder encoder simulated sample actions current policy parallel save immediate rewards performed actions ended training objective either mixed loss function train whole model supervised learning warm-start model ﬁne-tuning. case mixed loss converges faster achieves better performance. joint train policy model look-ahead module ﬁrst freeze pretrained environment model. step perform simulated depthbounded roll-outs using environment model. since unique actions besides stop action perform corresponding roll-outs. path ﬁrst encoded using lstm. last hidden states paths concatenated feed action predictor. learnable parameters come three components original model-free policy mode roll-out encoder action predictor. pseudo-code algorithm shown algorithm dataset room-to-room dataset ﬁrst dataset visionand-language navigation task real environments. dataset built upon matterportd dataset consists panoramic views constructed rgb-d images building-scale scenes dataset samples paths capturing visual diversity dataset collects navigation instructions average length words reported dataset split training seen validation unseen validation test sets. unseen validation test sets contain environments unseen training seen validation shares environments training set. test released claimed hold seen validation unseen validation test sets separately training validation experiments. implementation details develop algorithms open source code matterportd simulator. resnet- features extracted images without ﬁne-tuning. model-based path perform look-ahead planning possible action environment. j-th lookactions executed shared look-ahead policy. experiments policy model trained model-free path look-ahead policy. hyperparameters tuned validation set. training details found supplementary material. evaluation metrics following conventional wisdom dataset mainly evaluates results three metrics navigation error success rate oracle success rate. navigation error deﬁned shortest path distance navigation graph agent’s ﬁnal position destination vtarget. success rate calculates percentage result trajectories whose navigation errors less oracle success rate also reported distance closest point trajectory destination used calculate error even agent stop there. baselines dataset exists ground-truth shortest-path trajectory instruction sequence starting location target location vtarget. shortest-path trajectory used table results test seen test unseen sets. list best results reported student-forcing performs best. method signiﬁcantly outperforms previous best results also noticeable gain larger improvement test unseen test seen proves method generalized. supervised training. teacher-forcing uses cross-entropy loss train model time step maximize likelihood next gound-truth action given previous ground-truth action. instead feeding ground-truth action back recurrent model sample action based output probabilities action space experiments list results models reported baselines. also include results random agent randomly takes action step. table shows result comparison models baseline models. ﬁrst implement recurrent policy model whose model capacity much stronger baseline models. trained crossentropy loss sampling att-all model signiﬁcantly outperforms previous best model across metrics test sets switching model-free results slightly improved. learning method boosts performance consistently metrics achieves best results dataset validates eﬀectiveness combining model-free model-based task. important fact revealed method brings notable improvement test unseen improvement even larger test seen model-free method gains slight performance boost test unseen set. proves claim easy collect utilize data scalable incorporate look-ahead module decision making. besides method turns generalized better transferred unseen environments. learning curves environment model realize method ﬁrst need train environment model predict future state given present state would plugged look-ahead module. important guarantee eﬀectiveness pretrained environment model. figure plot transition loss reward loss environment model training. evidently losses converge stable point around iterations. also noticeable learning curve reward loss much noisy transition loss. sparsity nature rewards. unlike state transitions usually continuous rewards within trajectory samples sparse high variance thus noisy predict exact reward using mean square error. eﬀect diﬀerent rewards test four diﬀerent reward functions experiments. results shown table global distance reward function deﬁned path assigning reward actions along path. reward measures agent approaches target ﬁnishing path. success reward binary reward path correct actions assigned reward otherwise reward discounted reward deﬁned equation finally discounted success reward used ﬁnal model basically adds success binary reward immediate reward ﬁnal action. discounted cumulative reward calculated using equation experiments ﬁrst rewards much less eﬀective discounted reward functions assign diﬀerent rewards diﬀerent actions. believe discounted reward calculated every time step better reﬂect true value action. ﬁnal evaluation based navigation error also case study intuitive view decision-making process task show test trajectory performed agent figure agent starts position takes sequence actions following natural language instruction reaches destination stops there. observe although actions include forward left right down stop action appear rare result trajectories. cases agent still reach destination even without moving up/down camera indicates dataset limitation action distribution. done language generation conditioned visual inputs. also another line work tries answer questions images. task vision-language grounding relevant task requires ability connect language semantics physical properties environment. task requires ability task-driven. agent task needs sequentially interact environment ﬁnish navigation task speciﬁed language instruction. early approaches robot navigation usually require prior global needs build environment on-the-ﬂy. navigation goal methods usually directly annotated map. contrast work task challenging sense global required goal directly annotated described natural language. setting several methods proposed recently. proposed sequence-to-sequence model language navigation actions. misra formulates navigation sequential-decision process proposes reward shaping eﬀectively train agent. however methods still operate synthetic environments consider either simple discrete observation inputs unrealistic top-down view environment. model-based reinforcement learning using model-based planning long-standing problem reinforcement learning. recently great computational power neural networks makes realistic learn neural model simulate environments complicated environments simulator exposed agent model-based usually suﬀers mismatch learned real environments order combat issue researchers actively working combining model-free model-based recently propose value prediction network whose abstract states trained make predictions future values rather future observations weber introduce imagination-augmented agent construct implicit plans interpret predictions. algorithm shares spirit derived methods. instead testing games ﬁrst time adapt combination model-based model-free real-world vision-and-language task. experiments demonstrate superior performance proposed approach also tackles common generalization issue modelfree applying unseen scenes. besides equipped look-ahead module method simulate environment incorporate imagined trajectories making model scalable model-free agents. future plan explore potential model-based transfer", "year": 2018}