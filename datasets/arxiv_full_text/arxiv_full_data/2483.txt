{"title": "Provable Bounds for Learning Some Deep Representations", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We give algorithms with provable guarantees that learn a class of deep nets in the generative model view popularized by Hinton and others. Our generative model is an $n$ node multilayer neural net that has degree at most $n^{\\gamma}$ for some $\\gamma <1$ and each edge has a random edge weight in $[-1,1]$. Our algorithm learns {\\em almost all} networks in this class with polynomial running time. The sample complexity is quadratic or cubic depending upon the details of the model.  The algorithm uses layerwise learning. It is based upon a novel idea of observing correlations among features and using these to infer the underlying edge structure via a global graph recovery procedure. The analysis of the algorithm reveals interesting structure of neural networks with random edge weights.", "text": "give algorithms provable guarantees learn class deep nets generative model view popularized hinton others. generative model node multilayer neural degree edge random edge weight algorithm learns almost networks class polynomial running time. sample complexity quadratic cubic depending upon details model. based upon novel idea observing correlations among features using infer underlying edge structure global graph recovery procedure. analysis algorithm reveals interesting structure neural networks random edge weights. ∗princeton university computer science department center computational intractability. email aroracs.princeton.edu. work supported grants ccf- ccf- ccf- dms- simons investigator grant. ‡microsoft research england. email ronggemicrosoft.com. part work done author graduate student princeton university supported part grants ccf- ccf- ccf- dms- simons investigator grant. §princeton university computer science department center computational intractability. email tengyucs.princeton.edu. work supported grants ccf- ccf- ccf- dms- simons investigator grant. np-hardbecause many layers hidden variables connected nonlinear operations. usually imagines np-hardness barrier provable algorithms inputs learner drawn simple distribution worst-case. hope recently borne case generative models hmms gaussian mixtures etc. learning algorithms provable guarantees given however supervised learning neural nets even random inputs still seems hard cracking cryptographic schemes holds depth- neural nets even ands thresholds however modern deep nets justneural nets underlying assumption reverse generative model distribution close empirical input distribution. hinton promoted viewpoint suggested modeling level restricted boltzmann machine reversiblein sense. vincent suggested using many layers denoising autoencoder generalization consists pair encoder-decoder functions viewpoints allow diﬀerent learning methodology classical backpropagation layerwise learning fact unsupervised learning. bottom layer learnt unsupervised fashion using provided data. gives values next layer hidden variables used data learn next higher layer ﬁnal thus learnt also good generative model distribution bottom layer. practice unsupervised phase followed supervised training. viewpoint reversible deep nets promising theoretical work involves generative model also seems around cryptographic hardness. many barriers still remain. known mathematical condition describes neural nets denoising autoencoders. furthermore learning even single layer sparse denoising autoencoder seems least hard learning sparse-used overcomplete dictionaries provable bounds recent manuscript recent work suggests classical backpropagation-based learning neural nets together modern ideas like convolution dropout training also performs well though authors suggest unsupervised pretraining help further. layers obvious observed vectorappears bottommost layer. makes assumptions problem learning network given samples bottom layer still harder breaking cryptographic schemes. make following additional assumptions unknown ground truth deep feature/node activates/inhibits features layer below activated/inhibited features layer above small constant; words ground truth complete graph. graph edges chosen random algorithm learns almost networks class eﬃciently sample complexity; theorem algorithm outputs network whose generative behavior statistically indistinguishable ground truth net. pair adjacent layers constitutes denoising autoencoder sense vincent al.; lemma since model deﬁnition already includes decoder involves showing existence encoder completes autoencoder. encoder actually neural network reverse appropriately changing thresholds computation nodes. reverse computation stable dropouts noise. distribution generated two-layer cannot represented single layer neural turn suggests random t-layer network cannot represented t/-level neural net. note properties assumed modern deep work example heuristic trick called weight tying. fact provably hold random generative model seen theoretical validation assumptions. context. recent papers given theoretical analyses models multiple levels hidden features including svms however none solves task recovering ground-truth neural network given output distribution. though real-life neural nets random consideration random deep networks makes sense theory. sparse denoising autoencoders reminiscent objects error-correcting codes compressed sensing etc. ﬁrst analysed random case. mentioned provable reconstruction hidden layer known autoencoder already seems nonlinear generalization compressed sensing whereas even usual version formally proving diﬃcult however since showing limitations even -layer neural nets major open problem computational complexity theory. deep learning papers mistakenly cite paper result result actually exists weaker. compressed sensing seems possible adjacency matrix random-like properties fact result single layer generative model sparse denoising autoencoder seen analog fact random matrices good compressed sensing/sparse reconstruction general matrices berinde sparse matrices). course compressed sensing matrix edge weights known whereas learnt main contribution work. furthermore show algorithm learning single layer weights extended layerwise learning entire network. generative neural model hidden layers vectors binary variables layer) observed layer bottom. number vertices layer denoted edges layers abstract assume appendix allow diﬀer. weighted graph layers degree edge weights generative model works like neural threshold every node layer initialized assignment nodes picked uniformly among sets size node layer computes weighted neighbors layer possible stay generative deep model last layer also values. calculations require fraction lowermost layer could model assumes handcoded used real data produce sparse encoding bottom layer generative model. however desires generative model observed layer sparse allowing real-valued assignments observed layer remove threshold gates there. model described here. random deep assumption assume ground truth edges layers chosen randomly subject expected degree being edge carries weight chosen randomly model ρl{gi}). also consider —because leads simpler eﬃcient learner—a model edge weights random instead called ρ{gi}). recall vector input layer random subset nodes. seen since network random degree applying ρn-sparse vector layer likely produce following density successive layers etc.. assume density last layer density last-but-one layer last layer real-valued dense. degree density large constant network model ρ{gi}) learnt using samples time. network model ρ{gi}) learnt polynomial time using n/η) samples statistical distance true distribution generated learnt model. algorithmic ideas. unable analyse existing algorithms. instead give learning algorithms exploit structure makes random networks interesting ﬁrst place i.e. layer denoising autoencoder. crux algorithm twist hebbian rule things together wire together. setting layerwise learning adapted follows nodes layer together likely connected node higher layer. algorithm consists looking pairwise correlations putting together information globally. global procedure boils graph-theoretic problem reconstructing bipartite graph given pairs nodes distance variant graph square root problem np-complete worst-case instances solvable sparse random graphs. note current algorithms also seen leveraging correlations. putting together information done language nonlinear optimization ground truth network indeed particular local optimum reasonable formulation. would interesting show existing algorithms provably ground truth polynomial time currently seems diﬃcult. ideas useful practice? think using global reconstruction procedure leverage local correlations seems promising especially avoids usual nonlinear optimization. proof currently needs hidden layers sparse edge structure ground truth network random like. finally note random neural nets seem useful so-called reservoir computing perhaps provide useful representational power real data. empirical study left future work. throughout need well-known properties random graphs expected degree fact expanders; properties appear appendix. important unique neighbors property appears next section. mentioned earlier modern deep nets research often assumes approximately preserve information even allows easy going back/forth representations adjacent layers below denotes lower layer higher layer. popular choices include logistic function soft etc.; simple threshold function model. definition autoencoder consists decoding function encoding function sy+b) linear transformations ﬁxed vectors nonlinear function acts identically coordinate. autoencoder denoising high probability drawn distribution hidden layer noise vector. deﬁnition slightly diﬀerent actually stronger since exactly according generative model. deﬁnition implies existence encoder makes penalty term exactly zero. show lemma single-layer network denoising autoencoder high probability noise distribution allowed every output independently probability uses weight tying. algorithm outlined learns network layer layer starting bottom. thus step learning single layer network focus step noted amounts learning nonlinear dictionaries random dictionary elements. algorithm illustrates leverage sparsity randomness support graph pairwise -wise correlations combined graph recovery procedure section ﬁrst give simple algorithm outline works better parameters. construct correlation graph using samples call recovergraph learn positive edges partialencoder encode learngraph/learndecoder learn graph/decoder layer claim random sample output layer related pairs probability least unrelated pairs probability first consider related pair vertex edges neighbors excluding size cannot much larger choice parameters know event conditioned probability least hence probability least conversely unrelated must diﬀerent causes namely nodes additionally connected respectively edges. chance existing random sparse assignment union bound. leveraging -wise correlation sketch used pairwise correlations recover weights roughly. turns using -wise correlations allow correlations weaker requirement /d/. call three observed nodes related connected common node hidden layer edges. prove claim analogous above says related triple probability least probability unrelated triples roughly thus long ./d/ possible related triples correctly. graph recover algorithm modiﬁed -uniform hypergraph consisting related triples recover edges. suppose generative neural model weights single layer assignment hidden layer random ρn-sparse vector /d/. algorithm runs time uses samples recover ground truth high probability randomness graph samples. weights real numbers. sketch leave details appendix. surprisingly steps still work. proofs used sign edge weights magnitude edge weights arbitrary. proofs steps relies unique neighbor property node unique positive neighbors next level always matter small positive weights might also notice partialencoder using support weights. step turned problem unsupervised learning hidden graph supervised outputs linear classiﬁers inputs thus weights edges learnt desired accuracy. consider multi-layer networks show learnt layerwise using slight modiﬁcation one-layer algorithm layer. technical level diﬃculty analysis following single-layer learning assumed higher layer’s assignment random ρn-sparse binary vector. multilayer network assignments intermediate layers satisfy this show correlations among enough carry forth argument. simplicity describe algorithm model ρl{gi}) edge weights also keep notation simple describe bound correlations bottom-most layer holds almost verbatim higher layers. deﬁne expected number layer unique neighbor property expect roughly fraction also subsequent layers obtain conditions ensure further turn occur together probability least bound ﬁrst step along fact combined neighbors good probability picking neighbors edges. related turns probability interest plus term depends whether common parent layer graph restricted edges. intuitively picking common parents could result choice parameters also additional term implies desired conclusion. before graph recovery edges graph bottom layer. used single layer algorithm encode obtain values before many pairs thus using precisely reasoning step earlier obtain full graph bottom layer. graph reconstruction consists recovering graph given information subgraphs prototypical problem graph square root problem calls recovering graph given pairs nodes whose distance np-hard. know contains property says implies condition statement false. show unique common cause equal property know must vertex outside property connected vertices therefore hence following arguments must equal algorithm successnote model lowest layer real-valued threshold gates. thus earlier learning algorithm cannot applied however paradigm identifying correlations using graph recover used. next step show bound correlations before. simplicity state assuming layer random assignment sparsity full version state keeping mind higher layers previous sections. section show two-layer network weights expressive layer network arbitrary weights. two-layer network consists random graphs random weights edges. viewed generative model input output sgn)). show single-layer network even arbitrary weights arbitrary threshold functions must generate fairly diﬀerent distribution. idea cancellations possible two-layer network simply cannot accomodated single-layer network even using arbitrary weights. precisely even single output node cannot well-represented simple threshold function. first observe output determined values nodes layer ancestors. hard show layer edge node ancestor. consider structure figure assuming parents focus values values values impossible layer network ﬁrst rigorous analysis interesting subcases problem beneﬁcial triggering improvements e.g. role played bayes nets rigorous analysis message-passing algorithms trees graphs tree-width. spirit view consideration random neural model suggests graph random-like properties. would interested empirical study randomness properties actual deep nets learnt real life. layers convolution decidedly nonrandom. layers backpropagation starting complete graph random-like.) network randomness crucial single-layer learning. provable layerwise learning rely support random crucial controlling correlations among features appearing hidden layer provable layerwise learning weaker assumptions would interesting. would like thank yann lecun ankur moitra sushant sachdeva linpeng tang numerous helpful discussions throughout various stages work. work done ﬁrst third fourth authors visiting epfl.", "year": 2013}