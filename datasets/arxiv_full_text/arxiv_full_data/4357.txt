{"title": "Dimensionality Reduction via Regression in Hyperspectral Imagery", "tag": ["stat.ML", "cs.CV"], "abstract": "This paper introduces a new unsupervised method for dimensionality reduction via regression (DRR). The algorithm belongs to the family of invertible transforms that generalize Principal Component Analysis (PCA) by using curvilinear instead of linear features. DRR identifies the nonlinear features through multivariate regression to ensure the reduction in redundancy between he PCA coefficients, the reduction of the variance of the scores, and the reduction in the reconstruction error. More importantly, unlike other nonlinear dimensionality reduction methods, the invertibility, volume-preservation, and straightforward out-of-sample extension, makes DRR interpretable and easy to apply. The properties of DRR enable learning a more broader class of data manifolds than the recently proposed Non-linear Principal Components Analysis (NLPCA) and Principal Polynomial Analysis (PPA). We illustrate the performance of the representation in reducing the dimensionality of remote sensing data. In particular, we tackle two common problems: processing very high dimensional spectral information such as in hyperspectral image sounding data, and dealing with spatial-spectral image patches of multispectral images. Both settings pose collinearity and ill-determination problems. Evaluation of the expressive power of the features is assessed in terms of truncation error, estimating atmospheric variables, and surface land cover classification error. Results show that DRR outperforms linear PCA and recently proposed invertible extensions based on neural networks (NLPCA) and univariate regressions (PPA).", "text": "accuracy. instance hyperspectral instruments nasa’s airborne visible infra-red imaging spectrometer covers wavelength region using spectral channels nominal spectral resolution metop/iasi infrared sounder poses even complex image processing problems acquires channels ifov. actually improvements spectral resolution called advances signal processing exploitation algorithms capable summarizing information content components possible addition eventual high dimensionality complex interaction radiation atmosphere objects surface leads irradiance manifolds consist non-aligned clusters change nonlinearly diﬀerent acquisition conditions fortunately shown that given spatial-spectral smoothness signal intrinsic dimensionality data small used eﬃcient signal coding knowledge extraction reduced features high dimensionality problem aﬀecting hyperspectral data often multispectral data processing applications involve using spatial multi-temporal multi-angular features combined spectral features cases representation space becomes redundant pose challenging problems collinearity algorithms. cases coding classiﬁcation bio-geo-physical parameter retrieval applications reduces ﬁnding appropriate features necessarily ﬂexible nonlinear. order features recent years number feature extraction dimensionality reduction methods presented. based nonlinear functions allow describing data manifolds exhibit nonlinear relations comprehensive review). approaches range local methods kernel-based spectral decompositions neural networks projection pursuit formulations despite theoretical advantages nonlinear methods fact classical principal component analysis still widely used dimensionality reduction technique real remote sensing applications mainly because diﬀerent properties make useful real examples easy apply since involves solving linear convex problem straightforward out-of-sample extension. moreover transformation invertible result features extracted easily interpreted. dimensionality reduction algorithms involve nonlinearities rarely fulﬁll properties. nonlinear models usually complex formulations introduce number non-intuitive free parameters. tuning parameters implies strong assumptions manifold paper introduces unsupervised method dimensionality reduction regression algorithm belongs family invertible transforms generalize principal component analysis using curvilinear instead linear features. identiﬁes nonlinear features multivariate regression ensure reduction redundancy coeﬃcients reduction variance scores reduction reconstruction error. importantly unlike nonlinear dimensionality reduction methods invertibility volume-preservation straightforward out-of-sample extension makes interpretable easy apply. properties enable learning broader class data manifolds recently proposed non-linear principal components analysis principal polynomial analysis illustrate performance representation reducing dimensionality remote sensing data. particular tackle common problems processing high dimensional spectral information hyperspectral image sounding data dealing spatial-spectral image patches multispectral images. settings pose collinearity ill-determination problems. evaluation expressive power features assessed terms truncation error estimating atmospheric variables surface land cover classiﬁcation error. results show outperforms linear recently proposed invertible extensions based neural networks univariate regressions last decades technological evolution optical sensors provided remote sensing analysts rich spatial spectral temporal information. particular increase spectral resolution hyperspectral sensors general infrared sounders particular opens doors application domains poses methodological challenges data analysis. distinct highlyresolved spectra oﬀered hyperspectral images allow characterize land-cover classes unprecedented mitted. permission ieee must obtained users including reprinting/ republishing material advertising promotional purposes creating collective works resale redistribution servers lists reuse copyrighted components work works. ./jstsp... characteristics high computational cost training. complexity reduces applicability nonlinear feature extraction speciﬁc data i.e. performance methods signiﬁcantly improve many remote sensing problems moreover methods problems obtain out-of-sample predictions mandatory real applications. another critical point transform involved nonlinear models hard interpret. problem could alleviated methods invertible since could data back input domain understand results therein. invertibility allows characterize transformed domain evaluate quality. however invertibility scarcely achieved manifold learning literature. instance spectral kernel methods involve implicit mappings original curvilinear coordinates implicit features easily invertible interpretable desirable properties straightforward methods projections onto explicit features input domain. explicit features either straight lines curves. family projection methods understood generalization linear transforms extending linear components curvilinear components. family ranges extreme cases rigid approaches features straight lines input space ﬂexible non-parametric techniques closely follow data self-organizing maps related sequential principal curves analysis family discussed section below. extreme cases undesirable different reasons limited performance complex tuning free parameters and/or unaﬀordable computational cost projection-onto-explicit-features context autoencoders nonlinear-pca approaches based ﬁtting functional curves principal polynomial analysis represent convenient intermediate points extreme cases family. note methods shown better performance variety real data actually case theoretically ensured obtain better results pca. method proposed here dimensionality reduction regression represents qualitative step towards ﬂexible family multivariate nature regression keeping convenient properties make suitable practical high dimensional problems therefore extends applicability general manifolds encountered remote sensing data analysis. following taxonomy three methods could included principal curves analysis framework framework includes parametric non-parametric methods. nlpca exploit idea behind framework deﬁne generalizations controlled ﬂexibility. extend theoretical analysis method experimental conﬁrmation performance hyperspectral images. remainder paper organized follows. section reviews properties shortcomings projection-onto-explicit-features family pointing qualitative advantages proposed drr. section introduces mathematical details drr. describe transform diﬀerences ppa. derive explicit expression inverse prove volume preservation property drr. theoretical properties demonstrated illustrated controlled examples diﬀerent complexity. section address important high dimensional problems remote sensing estimation atmospheric state vectors infrared atmospheric sounding interferometer hyperspectral sounding data dimensionality reduction classiﬁcation spatio-spectral landsat image patches. experiments compared conventional recent fast nonlinear generalizations belong class invertible transforms nlpca comparisons made terms reconstruction error expressive power extracted features. paper concluding remarks section illustrate represents step forward regard nlpca family projections onto explicit curvilinear features ranging rigid ﬂexible extremes. first review basic details previous projection methods illustrate advantages proposed method easy visualize example. principal components independent components linear features input space column vectors rigid techniques single global features regardless input. contrary ﬂexible techniques adapt features local properties input. examples include ﬂexible grid adapted data samples represented projections onto local axes deﬁned edges parallelepiped corresponding closest node. similarly local-pca local-ica project data onto local axes corresponding closest code vector. generally local-to-global methods integrate local-linear representations single global curvilinear representation particular using fact local eigenvectors tangent ﬁrst secondary principal curves sequential principal curves analysis integrates local pcas along local metric sets line element along curves. spca inverted taking lengths along sequence principal curves drawn origin similarly spca assumes grid curves adapted data. however opposed spca learn whole grid segments principal curves sample. methods identify explicit curves/features follow data hard train require many samples reliable make hard high-dimensional scenarios. methods proposed generalize rigid representations considering curvilinear features instead straight lines instance nlpca invertible internal representation computed stage neural network matrices represent sets linear receptive ﬁelds ﬁxed point-wise nonlinearities. inverse autoencoder used make curvilinear coordinates explicit. fitting general parametric curves done diﬃcult unconstrained nature problem alternatively follows deﬂationary sequence single polynomial depending straight line computed time. specifically i-th stage accounts i-th curvilinear dimension using elements one-dimensional projection onto leading vector polynomial prediction average orthogonal subspace residual cannot predicted projection. prediction using univariate polynomial remove possible nonlinear dependencies linear subspace orthogonal complement. despite convenience univariate nature restricts kind dependencies taken account since information orthogonal subspace could obtained dimensions used prediction. moreover using single parameter build i-th polynomial implies i-th curvilinear feature shape along curve. addresses limitations using multivariate instead univariate regressions nonlinear predictions. result improves energy compaction extends applicability general manifolds keeping simplicity make suitable high dimensional problems advantages illustrated fig. compare representative invertible representations family curved noisy manifolds class introduced delicado class manifolds originally presented illustrate concept secondary principal curves convenient since easily control complexity problem introducing tilt secondary principal curves along ﬁrst principal curve controlled complexity useful point limitations previous techniques limitations alleviated using model. performance compared input domain dimensionality reduction error accuracy identiﬁed curvilinear features. manifolds come two-dimensional space latent variables result dimensionality reduction error depends unfolding ability forward transform closer transformed data rectangle smaller error truncating representation. hand identiﬁed features depend inverse transform bends cartesian grid latent space better model represents curvature data bigger ﬁdelity identiﬁed features. start considering performance easy case manifold tilt along second principal curve. previously reported techniques perform expected hand progressively ﬂexible techniques reduce distortion dimensionality reduction better unfold test data. result removing third dimension rigid-to-ﬂexible family progressively introduces less error. hand identiﬁed features input domain progressively similar actual curvilinear latent variables going rigid ﬂexible extremes. speciﬁc easy example proposed outperforms even ﬂexible spca terms. moreover since particular manifold require increased ﬂexibility slightly outperforms terms. results complex manifold provide insight techniques since pushes limits. firstly note increase complexity illustrated increase errors methods. instance linear identiﬁes features cases doubles normalized mses. reduction performance relevant spca twisted manifold certainly challenges fast generalizations mses dramatically increase nlpca ppa. even though nlpca identiﬁes certain tilt secondary feature along ﬁrst curve seems rigid follow data structure. displays diﬀerent problem stated above construction i-th curvilinear feature figure performance family invertible representations illustrative manifolds diﬀerent complexity. complexity considered manifolds depends tilt secondary principal curves along ﬁrst principal curve sample data shown together ﬁrst secondary principal curves generated latent variables input domain. results diﬀerent techniques considered manifolds depicted color input data previously reported representations range rigid schemes ﬂexible schemes spca including practical nonlinear generalizations nlpca examples intermediate ﬂexibility extreme cases. performance compared terms reconstruction error removing third dimension terms mean squared distance identiﬁed actual curvilinear features related unfolding ability model related ability appropriate curvilinear features assumed latent cartesian grid used training samples optimal settings methods. dimensionality reduction tested highlighted curvilinear grid sampled true latent variables. features input space identiﬁed inverting cartesian grid transform domain. latent grid scaled every representation minimize standard deviations errors come models trained diﬀerent data realizations. repeated secondary curves along ﬁrst curve both blue cases). good enough data manifolds required symmetry leads dramatic errors method deal relations three variables manifold blue performance nlpca. latter eﬀect frequently appears hyperspectral images diﬀerent nonlinear relations spectral channels occur different objects finally note clearly improves challenging example blue mainly uses multiple dimensions predict lower variance dimension data. result handle non-stationarity along principal curves leading better unfolding tilted secondary features removes symmetry requirement broadens class manifolds suited drr. removes second order dependencies signal components i.e. scores decorrelated equivalently casted linear transform minimizes reconstruction error ﬁxed number features neglected. however general non-gaussian sources particular hyperspectral images scores still display signiﬁcant statistical relations scheme presented tries nonlinearly remove information still shared diﬀerent components. well known that even though leads domain decorrelated dimensions complete independence guaranteed signal gaussian probability density function here propose scheme remove redundancy idea simple predict redundant information coeﬃcient extracted others. non-predictable information retained data representation. speciﬁcally start linear representation outlined above then propose predict coeﬃcient multivariate regression function generalizes particular case uslinear-drr prediction would equal zero. note result using classical solution since uncorrelated αi−. therefore i.e. linear-drr reduces pca. eralize linear functions obtain least good results pca. ﬂexibility functions regard linear case reduce variance residuals hence reconstruction error dimensionality reduction. quentially undo forward transformation ﬁrst predict coeﬃcient previous coeﬃcients using known regression function then known residual correct prediction volume preserving transform nonlinear transform preserves volume input space determinant jacobian prove nature ensures jacobian fulﬁlls property. thought sequential algorithm dimension addressed time elementary transform consisting prediction substraction mathematically convenient formulate jacobian sequential view prevent parallelization discussed later. hence jacobian rn×n kernel matrix entries parameters must tuned regression regularization parameter kernel parameters. experiments used standard squared exponential kernel function quite convenient scheme implements ﬂexible nonlinear regression functions reduces solving matrix inversion problem unique solution. oﬀers moderate training testing computational cost includes regularization natural also oﬀers possibility generate multi-output nonlinear regression. latter important feature extend scheme multiple outputs approximation. finally successfully used many real applications including remote sensing data analysis involving hyperspectral data however noted that even cases previous feature extraction mandatory attain signiﬁcant results section give experimental evidence performance proposed algorithm illustrative settings. first show results truncation error multispectral image classiﬁcation problem including spatial context. evaluate performance terms reconstruction error expressive power features perform multi-output regression challenging problem involving hyperspectral infrared sounding data. focusing experiments arbitrary. applications imply challenging high dimensional data multispectral image classiﬁcation contextual information stacked spectral information highly increases dimensionality hyperspectral infrared sounding data used estimate atmospheric state vectors densely sampled. cases input space become redundant collinearity introduced either spatial features spectral continuity natural sources. experiments compare members invertible projection family described section suited high dimensional scenarios. implies focusing nlpca excluding spca prohibitive cost. whatever derivatives determinant simple matrix always every point therefore determinant global jacobian guaranteed including transform orthonormal. parallelization multivariate regression qualitative advantage methods however computationally expensive. fortunately proposed allows trivial parallel implementation forward transform. note prediction component actually done subset original scores. therefore prediction functions step sequential implementation necessary. obvious computational advantage necessarily requires sequential implementation represents qualitative advantage since feature depends previous nonlinear features nonlinear regressions depend linear features previous curvilinear coeﬃcients. opposed forward transform inverse parallelizable since order predict i-th coeﬃcient need previous linear implies operating sequentially kernel function reproducing product hilbert space rn×d matrix containing training samples rows i-th column estimated denotes submatrix containing columns used input data prediction model aj\\i represents feature vector a\\i. prediction function figure reconstruction error results contextual multispectral image classiﬁcation. comparison nlpca diﬀerent number extracted features mean absolute reconstruction error relative respect error going means better results figure classiﬁcation results contextual multispectral image classiﬁcation. comparison nlpca diﬀerent number extracted features classiﬁcation error relative classiﬁcation error respect accuracy going means better results image consisting pixels spatial resolution classes identiﬁed image namely soil cotton crop grey soil damp grey soil soil vegetation stubble damp grey soil. total labeled samples available. contextual information included stacking neighboring pixﬁrst problem compare dimensionality reduction performance terms mean absolute error original domain. note kind evaluation used invertible methods. method data transformed inverted using less dimensions. equivalent truncate dimensions pca. order illustrate advantage using given method instead results shown percentage regard performance %maemethod maemethod/maepca maemethod maepca refer obtained considered method respectively. figure shows results experiment. divided available labeled data sets equal number samples. samples randomly selected original image dataset. reconstruction test averaged independent realizations shown. several conclusions obtained speciﬁcally nlpca obtains good results number extracted features obtained rapidly degrades performance extracted features revealing clear inability handle high-dimensional problems. note available implementation nlpca restricted extract features. given number extracted features reconstruction error increases substantially regard shows better results nlpca better suited number extracted features. nevertheless noticeable cases better methods revealing maximum gain features. second problem dataset shows classiﬁcation results using inverted data original input space diﬀerent methods. used standard linear discriminant analysis inverted data. cases used randomly selected examples training amount testing. test results averaged realizations shown fig. performance results indicate similar trends observed reconstruction error fig. essentially outperforms methods especially noticeable number components used reconstruction classiﬁcation. number components increase show similar results. results suggest better compacts information lower number components useful reconstruction data classiﬁcation. table shows computation cost considered methods training testing. experiments used training test samples main conclusions extracted nlpca computationally costly algorithm training testing. http//www.nlpca.org/ while sophisticated nonlinear classiﬁers could used here actually interested setting allows study expressive power extracted features. homologous setting also used regression experiments next subsection. analyze beneﬁts using estimation atmospheric parameters hyperspectral infrared sounding data reduced dimensionality. ﬁrst motivate problem describe considered dataset. again interested analyzing impact reduced dimensionality reconstruction error diﬀerent task case retrieval geophysical parameters. temperature water vapor atmospheric parameters high importance weather forecast atmospheric chemistry studies observations spaceborne high spectral resolution infrared sounding instruments used calculate proﬁles atmospheric parameters unprecedented accuracy vertical resolution work focus data coming infrared atmospheric sounding interferometer microwave humidity sensor advanced microwave sensor unit onboard metop-a satellite. iasi instrument poses major dimensionality challenge dense spectrum sampling amsu spectra consist twenty values together iasi spectra consist spectral channels spectral resolution apodization spatial resolution nadir instantaneous field view size altitude huge data dimensionality typically requires simple computationally eﬃcient processing techniques. retrieval techniques available metopiasi level product processing facility computationally inexpensive method based linear regression principal components measured brightness spectra atmospheric state parameters. introduce scheme alternative pca. application important dimensionality reduction minimizes reconstruction error identiﬁed features useful retrieval stage. used collection datasets input data diﬀerent sensors iasi amsu. considered output atmospheric variables diverse e.g. temperature moisture surface pressure. dataset provided eumetsat preprocessed input data -dimensional. input vector consisted following scalar indicating secant satellite zenith angle radiance values amsu sensors values iasi sensor. data iasi actually three separate sets scores each three diﬀerent iasi bands. note that despite intra-band decorrelation vector elements still exhibit statistical dependency signiﬁcant even second order level among diﬀerent bands instruments. data predicted dimensional. output vector consists following data corresponding surface temperature moisture skin temperature surface pressure; data corresponding altitude proﬁles temperature moisture ozone model levels each. example surface temperature shown fig. data provided oﬃcial european center medium-range weather forecasting model http//www.ecmwf.int/ march experiment study representation power small number features extracted drr. input features processed nlpca presented method. here quality transformation evaluated solely mean absolute error input space original signal reconstructed relevant coeﬃcients retained. figure illustrates eﬀect reconstructing input data using nlpca diﬀerent numbers components. hand reported performance similar better nlpca reconstruction error. hand important note results absolute relative terms show clearly obtains less reconstruction error arbitrary number features. figure illustrates eﬀect using features either retrieval physical parameters described before. used linear regression features-to-parameters estimation. plotted mean absolute error diﬀerent number features. plots show eﬀect using diﬀerent dimensionality reduction methods retrieval. figure shows results ﬁrst dataset illustration purposes note using features estimate features clear beneﬁts. instance using features obtains accuracy using components. times training testing shown table experiment took training test samples previous experiment nlpca expensive training test respectively. experiment however times notably higher increase dimensionality mostly bigger training set. introduced novel unsupervised method dimensionality reduction application multivariate nonlinear regression approximate projection higher variance scores. method shown generalize achieve data compression better features prediction competitive nonlinear methods like nlpca ppa. besides unlike nonlinear dimensionality reduction methods easy apply out-of-sample extension invertible learned transformation volume-preserving. focused challenging problems spatial-spectral multispectral land cover classiﬁcation atmospheric parameter retrieval hyperspectral infrared sounding data. extension cope multiset/output regression well impact data dimensionality noise sources explored future. authors wish thank hultberg european organisation exploitation meteorological satellites darmstadt germany kindly providing iasi datasets used paper.", "year": 2016}