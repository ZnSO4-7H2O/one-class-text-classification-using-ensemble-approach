{"title": "Lateral Connections in Denoising Autoencoders Support Supervised  Learning", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "We show how a deep denoising autoencoder with lateral connections can be used as an auxiliary unsupervised learning task to support supervised learning. The proposed model is trained to minimize simultaneously the sum of supervised and unsupervised cost functions by back-propagation, avoiding the need for layer-wise pretraining. It improves the state of the art significantly in the permutation-invariant MNIST classification task.", "text": "show deep denoising autoencoder lateral connections used auxiliary unsupervised learning task support supervised learning. proposed model trained minimize simultaneously supervised unsupervised cost functions back-propagation avoiding need layerwise pretraining. improves state signiﬁcantly permutationinvariant mnist classiﬁcation task. combining auxiliary task help train neural network proposed suddarth kergosien sharing hidden representations among task network generalizes better. hinton salakhutdinov proposed auxiliary task could unsupervised modelling inputs. ranzato szummer used autoencoder reconstruction auxiliary task classiﬁcation performed training layer-wise. sietsma proposed corrupt network inputs noise regularization method. denoising autoencoders principle create unsupervised models data. rasmus showed modulated lateral connections denoising autoencoder change properties fundamental making suitable auxiliary task supervised training lateral connections allow detailed information directly decoder relieving pressure higher layers represent information allowing concentrate abstract features. contrast deep denoising autoencoder encoder discard information similarly typical supervised learning tasks discard irrelevant information. lateral connections optimal model shape pyramid like i.e. dimensionality layers lower bottom layers also true typical supervised learning tasks opposed traditional denoising autoencoders prefer layers equal size. paper builds previous work shows using denoising autoencoder lateral connections auxiliary task supervised learning improves network’s generalization capability hypothesized valpola proposed method achieves state-of-the-art results permutation invariant mnist classiﬁcation task. figure conceptual illustration model encoder path multilayer perceptron network bold arrows indicating fully connected weights upwards downwards thin arrows neuron-wise connections. normalized preactivations denoised versions denoised reconstruction input. projections dimensions activations class prediction. follow ioffe szegedy apply batch normalization preactivation including topmost layer l-layer network ensure fast convergence reduced covariate shift. formally input component-wise batch normalization estimates calculated minibatch rectiﬁcation nonlinearity replaced softmax output batch normalization reported reduce need dropout-style regularization isotropic gaussian noise inputs supervised cost average negative probability targets given inputs layer sizes decoder symmetric encoder corresponding decoder layer calculated lateral connection vertical connection lateral connections restricted unit encoder layer connected unit corresponding superscripts dropped avoid clutter sigmoid nonlinearity trainable parameters. type parametrization allows network information higher layer highest layer lowest layer valpola discusses denoising functions represent corresponding distributions. proposed parametrization suits many different distributions e.g. supersub-gaussian multimodal. parameter deﬁnes distance peaks multimodal distributions moreover kind decoder function able emulate additive modulated connections analyzed rasmus cost function unsupervised path mean squared error dimensionality data parameters model include encoder decoder. encoder decoder roughly number parameters matrices equal size. difference comes per-neuron parameters encoder decoder order evaluate impact unsupervised auxiliary cost generalization performance tested model mnist classiﬁcation task. randomly split data examples training examples validation. validation used evaluating model structure hyperparameters ﬁnally train model test error evaluation. improve statistical reliability considered average runs different random seeds. supervised unsupervised cost functions training data. model training took epochs minibatch size equalling weight updates. used adam optimization algorithm weight updates adjusting learning rate according schedule learning rate linearly reduced zero last epochs starting tested models layer sizes ----- latter worked better reported paper. best input noise level chosen plenty hyperparameters various model structures left tune satisﬁed reported results. figure illustrates auxiliary cost impacts validation error showing error function multiplier auxiliary task clearly beneﬁcial case best tested value figure average validation error function unsupervised auxiliary cost multiplier average test error cases runs. corresponds pure supervised training. error bars show sample standard deviation. training included samples validation test error labeled samples used. i.e. misclassiﬁed examples average signiﬁcantly lower previously reported comparison computed average test error case i.e. supervised learning batch normalization multi-prediction deep boltzmann machine train back-propagation variational inference. targets inference include supervised targets unsupervised targets used training simultaneously. connections inference network somewhat analogous lateral connections. speciﬁcally inference paths observed inputs reconstructed inputs highest layers. compared approach mp-dbm requires iterative inference initialization hidden activations whereas case inference simple single-pass feedforward procedure. showed denoising autoencoder lateral connections compatible supervised learning using unsupervised denoising task auxiliary training objective achieved good results mnist classiﬁcation task signiﬁcant margin previous state art. conjecture good results supervised unsupervised learning happening concurrently means unsupervised learning focus features supervised learning ﬁnds relevant. proposed model simple easy implement many existing feedforward architectures training based back-propagation simple cost function. quick train convergence fast especially batch normalization. proposed architecture implements complex functions modulated connections without signiﬁcant increase number parameters. work improved extended many ways. currently studying impact adding noise also including auxiliary layer-wise reconstruction costs ||ˆz−z|| working extending preliminary experiments larger datasets semi-supervised learning problems convolutional networks. kingma adam method stochastic optimization. arxiv.. ranzato szummer semi-supervised learning compact document representations deep networks. proceedings international conference machine learning icml pages acm. vincent larochelle lajoie bengio manzagol p.-a. stacked denoising autoencoders learning useful representations deep network local denoising criterion. journal machine learning research", "year": 2015}