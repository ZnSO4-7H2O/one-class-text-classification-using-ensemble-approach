{"title": "Neuroevolution-Based Inverse Reinforcement Learning", "tag": ["cs.NE", "cs.AI", "cs.LG"], "abstract": "The problem of Learning from Demonstration is targeted at learning to perform tasks based on observed examples. One approach to Learning from Demonstration is Inverse Reinforcement Learning, in which actions are observed to infer rewards. This work combines a feature based state evaluation approach to Inverse Reinforcement Learning with neuroevolution, a paradigm for modifying neural networks based on their performance on a given task. Neural networks are used to learn from a demonstrated expert policy and are evolved to generate a policy similar to the demonstration. The algorithm is discussed and evaluated against competitive feature-based Inverse Reinforcement Learning approaches. At the cost of execution time, neural networks allow for non-linear combinations of features in state evaluations. These valuations may correspond to state value or state reward. This results in better correspondence to observed examples as opposed to using linear combinations. This work also extends existing work on Bayesian Non-Parametric Feature Construction for Inverse Reinforcement Learning by using non-linear combinations of intermediate data to improve performance. The algorithm is observed to be specifically suitable for a linearly solvable non-deterministic Markov Decision Processes in which multiple rewards are sparsely scattered in state space. A conclusive performance hierarchy between evaluated algorithms is presented.", "text": "abstract—the problem learning demonstration targeted learning perform tasks based observed examples. approach learning demonstration inverse reinforcement learning actions observed infer rewards. work combines feature based state evaluation approach inverse reinforcement learning neuroevolution paradigm modifying neural networks based performance given task. neural networks used learn demonstrated expert policy evolved generate policy similar demonstration. algorithm discussed evaluated competitive feature-based inverse reinforcement learning approaches. cost execution time neural networks allow non-linear combinations features state evaluations. valuations correspond state value state reward. results better correspondence observed examples opposed using linear combinations. work also extends existing work bayesian non-parametric feature construction inverse reinforcement learning using non-linear combinations intermediate data improve performance. algorithm observed speciﬁcally suitable linearly solvable non-deterministic markov decision processes multiple rewards sparsely scattered state space. conclusive performance hierarchy evaluated algorithms presented. concept reinforcement learning motivated modeling learning experience. environment segmented states contain information describe environment segment. learner also called agent beneﬁt differently depending state creates notion rewards corresponding state. seeks optimal actions states based rewards observed taking actions states time. example would child learning build pyramid blocks. inverse reinforcement learning motivated learning examples someone showing child build block pyramid child trying replicate process. opposed agent observe rewards; attempts recover based observed examples. observations form traces stateaction pairs. allows agent understand environment terms evaluation state. therefore intuitive means implement learning demonstration describes problem agent learns perform task observing done. approach exploit state information reconstructing rewards state features combinations. obtain non-linear function work employs neural network ﬁrst contribution. speciﬁcation network structure limit efﬁciency generated network. reason neural network generated cumulative modiﬁcation simple feed-forward network. approach summarized figure overview section v-a. opposed mapping states rewards work proposes generation state values state rewards derived. improves robustness noise since state values typically exhibit steep transitions. state values therefore favours real-world applications robotics. neural networks also allows inherent advantages regression trees unlike regression trees neural networks capable learning non-linear data boundaries. able generate abstract features hidden neurons. finally fact neural networks approximate function sufﬁcient data makes intuitively preferable. further ﬁtting highly non-uniform function neural networks better kernel functions regression generalizing non-locally scaling larger datasets. kernel functions typically generalize locally. work involves composite state features priors estimate rewards several algorithm iterations. second contribution work utilizing non-linear nature neural networks improve performance case complex reward structure. done using neural network reward estimation. section describes relevant work neuroevolution feature based irl. section discusses feature based approach problem. followed details proposed neuroevolution based algorithm section section experimental evaluation algorithm followed concluding remarks section fig. neat-irl summary. population neural networks used generate mappings state features state rewards. networks evolved structure connections using genetic algorithms fitness network determined closeness optimal policy generated rewards based network demonstrated policy. feature construction inverse reinforcement learning uses regression trees quadratic programming optimization involves selection sub-tree without signiﬁcant loss regression ﬁtness. firl acknowledged limited capabilites representing reward function uses non-linear combination features. another technique called gpirl based gaussian process regression reward function kernel non-linear combination state features. recent techniques also incorporated non-parametric bayesian framework improve learning. means number parameters used models increases based training data. composite features deﬁned logical conjunctions state features. model extends deﬁning prior composite features. reward function additionally assumed generated composition sub-tasks performed space. algorithm targets detection sub-goals estimate ﬁnal policy states state space bayesian non-parametric firl uses indian buffet process deﬁne priors composite state features. deﬁnes distribution inﬁnitely large binary matrices. used determine number composite features composite features themselves. features corresponding weights recorded several iterations algorithm. values either aggregated used estimating maximum a-posteriori state reward values. results provide competitive performance calculating reward functions using mean-based result performs signiﬁcantly better map-based result focusing action matching rather reward matching. reason map-based results excluded comparison. importantly emphasizes importance state reward values computed iteration ﬁnally used. non-linear combination data intuitively provides better performance linear combination experimental results indicate superiority bnp-firl gpirl non-deterministic ones used evaluate work. expectation-maximization based approach reward function modeled weighted functions parameters optimal policy model deﬁned probability distribution actions state state space based optimal policy. algorithm attempts simultaneously estimate weights parameters. algorithm compared algorithms individually compared standard maximum entropy irl. visual observation performance algorithms indicates kernel based approach competitive better than expectationmaximization based approach. deep learning problems explored inputs ﬁrst layer deep neural network state features. performance algorithm shown surpass existing algorithms algorithm focuses achieving correct expected state value whereas algorithm focuses learning optimal policy. intuitively expected work perform competitively deep neural networks. reason premise algorithms similar state features input neural network evaluate state reward state value output neural network. addition neuroevolution allows compact network dynamic construction network. comparison technique therefore regarded outside scope work. finally work maximum likelihood covers three problem spaces linear non-linear multiple intentions irl. linearity non-linearity context reward function modeled function state features. multiple intentions refers setting comprises multiple reward functions. algorithm emphasizes methods suitable uniﬁed approach mentioned problem spaces. however noted specialized algorithms suitable number experts reward function shape known. performance algorithms competitive. performance mlirl problem setting therefore evaluated competitive mlirl therefore excluded comparison work. lines firl neural network provide mapping state features state value. since neural network compactly represent complex combinations inputs internal neurons represent informative features obtained firl. alternative implementation extending bnp-firl neural network used provide state value based feature weight data gathered several iterations state reward estimation bnp-firl algorithm. since function generated unknown complexity. implies uncertainty optimal number layers nodes layer used. nodes later layers neural network able deﬁne function take several nodes earlier layers neural network deﬁne. therefore trade-off number hidden layers number nodes layer. this using ﬁxed structure neural network scenario sub-optimal. neuroevolution solves problem generating optimal neural network using techniques genetic programming evolutionary programming simulated annealing genetic algorithms evolution strategies evolutionary algorithms memetic algorithms direct encoding scheme neurons connections neural network explicitly speciﬁed genotype. case indirect encoding scheme values expressed implicitly smaller parts genotype indirect encoding suitable solving tasks high degree regularity approximating function problem lacks regularity. well established direct encoding-based neuroevolution technique neuroevolution augmenting topolgies evolves structure parameters neural network therefore preferred work. demonstration bias introduced neat-based agent learning. done providing advice form rule-based grammar. however provide embedded undertanding preference state state space. explored mapping states actions different methods demonstration studied video game environment neat-based also implemented multiple agent setting groups genomes share ﬁtness information. state values used generate corresponding policy. incorporate learning demonstration policy matched demonstration neural network evolved thereof. directed neuroevolution considered extension better insight evaluation state. purpose document proposed work referred neat-irl. formal deﬁnition repeated reference. states actions transition probabilities states action taken state. additionally state-action pair corresponds reward discount factor used aggregating rewards corresponding trajectory stateaction pairs. policy describes actions taken state space. optimal policy then maximizes expected discounted rewards given states episodic task alternatively case continual task optimal policy maximizes rewards lifetime learning agent. state value expected return arbitrary followed starting state. concept extended deﬁne linear linearly solvable maximum entropy control) lmdp deﬁned state costs correspondence alternative passive dynamics describes transition probabilities absence control. following policy opposed occurs cost divergence cost makes optimization problem convex removing questions local optima additionally exponential transformation optimal function transforms associated bellaman equation linear function. optimal function corresponds function evaluated goal learn reward function state based parts given policy broader sense goal able generate policy state space correlated demonstrated. purpose work demonstrations received algorithm assumed performed expert meaning assumed optimal. demonstration consists numerous examples trace optimal policy state space. represented form sequences state-action pairs method generate policy generate state values state based state features. assumed weighted combination state features provide quantitative evaluation state ﬁrst problem then learn mapping additionally values weights features several iterations bnp-firl used different ways derive state reward value. second problem then learn non-linear mapping values state reward produces policy consistent given examples domain used work grid world markov decision process concept extended state spaces. grid world aligned experiments case grid world agent possible actions move move down move left move right nothing. case deterministic action taken always action selected. however non-deterministic action taken sometimes random irrespective action selected. examples provided traces subsequent states state space demonstration policy. multiple traces overlap means single state covered examples. examples overlap states involved examples upper bound length example example policy perspective given figure examples serve demonstration bnp-firl summarized figure bnp-firl decomposes reward product composite features weights total composite features size column binary vector size indicating presence absence feature state. size therefore |s|. product thus vector size |s|. denotes matrix atomic features column contains binary data items making size |s|. matrix indicates atomic features comprise composite feature. therefore size. bernoulli distributed binary matrix used negate atomic features generating composite features. m-dimensional binary vector indicating atomic features form composite features. constant parameter poisson distribution calculated bernoulli distribution used generate priors finally beta distributed using closed interval deﬁned fig. policy sampling grid world demonstrations length part ﬁgure shows optimal policy mdp. demonstration state selected random. optimal policy followed steps equal demonstration length fig. bnp-firl summary reward function computed product composite features associated weights values selected optimize posterior probability given parameter representing conﬁdence actions optimal algorithm iteratively converges value maximizes probability. values computed iteration stored vector. compute mean-based results rewards iteration used ﬁnal reward. mean ﬁnal stage bnp-firl algorithm denoted bnp-firl. artiﬁcial neural network neural network function approximation model inspired biological neural networks. model composed interconnected nodes neurons exchange information produce desired output. work neural networks containing single output node. neat evolves neural networks using guided itness unction. member population corresponds genotype genome phenotype genome consists node genes connection genes genotype phenotype mapping summarized figure neat begins relatively less complex neural networks increases complexity based itness requirement. speciﬁcally initial network perceptron-like comprises input output neurons. gene evolved either addition neuron connection path creating connection existing neurons. neural networks represented genome population neat considered state features input produce state value output. corresponding policy evaluated state space. therefore suitable ﬁtness function coherence generated demonstrated policies. implementation done using coherence generated action directions demonstration ﬁtness value. values intuitive cosine angle generated example actions. fitness computed accumulated states included demonstration. demonstrated actions replicated correctly algorithm terminated. summarized figure dimensions thus vary across iterations variation composite features considered. nonlinear combination time therefore possible. however size constant iteration. values iterations algorithm serves input neural network. output neural network used resultant state reward vector. firl gpirl neat-irl consider individual states opposed traces state sequences making memory efﬁcient terms storage examples. however firl gpirl generate function produces state rewards opposed neat-irl produces state values. state rewards evaluate immediate desirability whereas state values evaluate long term desirability state. agent therefore seeks actions lead states higher value highest reward maximize long term reward state values also observe smoother transition amongst states close proximity though necessary state rewards. further converting state values policy requires single computational step case state rewards however policy accumulated several epochs computation thereby resulting greater time complexity. additionally policy generated using state values robust noise generated based state rewards. reason case state values policy evaluation requires comparison adjacent state values. ﬁnal policy therefore affected state values close enough noise change action selection. case policy generation using state rewards noise would accumulated epoch computation would therefore signiﬁcant effect generated policy. disadvantage generating state values transfer poorly mdps similar feature sets opposed state rewards however problem space concern transfer learning. shows behaviour replicated multi layer perceptron neural network sufﬁciently large number hidden neurons. stands requirement weight decay practice implies limitation neural networks approximation behaviour instead exactness. further models optimized data exactly speciﬁc hyper-parameter values implies trade-off exactness over-ﬁtting data. neat-irl does however introduce parameters problem increases degrees freedom. performance ﬁxed parameters vary different environments. algorithm performance therefore need evaluated across parameters optimal value assignments. functions deﬁned linear combination variables subset functions deﬁned nonlinear combination variables. non-linear combinations therefore powerful expressing relationships among fig. genotype phenotype mapping example neat genotype corresponds encoding structure connections neural network. represented genes. phenotype actual neural network generated based phenotype. variables direct towards better function approximations cost function complexity. case mean-based result bnp-firl particular linear combination simply variables. allows signiﬁcant scope improvement approximation ﬁnal value state rewards. originally conceptualized python implementation based multineat neat-irl currently implemented matlab using evaluated using existing tools toolkit containing firl gpirl implementation bnp-firl exists extended version toolkit used experiments work. inherent toolkit exist binary state features grid size patterns contain patterns consistent column grid thereby forming coordinate system. exempliﬁed figure additionally state rewards assigned randomly macro block. algorithms evaluated many randomly generated grid worlds compared well estimate randomly generated reward structure. firl gpirl consistently produce better results algorithms compared case similar bnp-firl additionally gpirl performs consistently better firl. therefore sufﬁcient evaluate gpirl neat-irl bnp-firl bnp-firl examining performance improvements. neat-irl compared work done sight dependency rule based learning. algorithms evaluated number ways. neatirl evaluated individually also comparison gpirl bnp-firl bnp-firl. toolkit deﬁnes misprediction score probability agent take non-optimal action state. measured based matching expert policy policy generated algorithm. scores evaluated linear standard mdps. possible actions state grid world default misprediction score macroblock size speciﬁes number adjacent cells grid assigned reward value experiments. done state features correspond unique rewards. average values computed executions. furthermore neat-irl execution early generated policy completely matched demonstrated examples. however lead underﬁtting also possible contributor hindering performance neat-irl terms misprediction score. note default values used neat additionally gpirl evaluated using default grid size i.e. grid training samples length each. reduced values used evaluation neatirl bnp-firl computational tractability. primary results discussed work conﬁguration certain supplementary evaluations conﬁguration number samples case scaled value avoid inference based little data. demonstrations include total states justiﬁed considering proportions used original setting primary grid allows standardized evaluation algorithms whereas smaller grid allows tractable analysis mdps algorithms evaluated. remaining neat conﬁguration including parameters used gpirl parameters used default note smaller data samples interpolation used gpirl exceeded possible number points thereby causing mathematical errors. ﬁxed limiting interpolation maximum possible case number interpolations possible. fig. number samples evaluation misprediction performance bnp-firl competitive gpirl bnp-firl. time complexity least gpirl bnp-firl competitive bnp-firl. context misprediction score performance algorithms using neural networks competitive compared algorithms non-deterministic setting deterministic setting. favours algorithm real world setting non-determinism exists various sources noise. also case better performance neural networks observed linear standard mdp. attributed neural network able perform better easily solvable given parameters. additionally composite features thus reward function iterations provide better performance neural networks bnp-firl compared state features neat-irl. bnpfirl observed consistently better gpirl. performance neat-irl improves slower pace algorithms. attributed values unoptimized mdps used. demonstrated section values tunable improve performance algorithm. execution time least gpirl four experimental settings. trade-off better performance increased execution time observed bnp-firl. increase signiﬁcant case implies linear harder problem npb-firl solve. additional layer neat ﬁnal stage denote performance data standard dotted line used denote performance data linear mdp. additionally term computational complexity interchangeably used computational time complexity execution time calculated seconds. additionally misprediction score observed lower testing linear standard irrespective types used training algorithm. expected larger population genomes provides better performance. note beyond threshold value increase population contribute signiﬁcant optimization. capacity search performed extra population genomes limited size current state space. execution time linear implementation neatirl linear population size. coherent behaviour increase number maximum generations results lower misprediction score. subject stagnation manner similar discussed population size. neat-irl execution time linear maximum number generations expected since computation constant generation algorithm. bnpfirl compared across three aspects amount determinism different values tests ability algorithms reconstruct reward function across different amounts data. evaluations neat parameters arbitrarily fig. number samples evaluation gpirl outperforms bnp-firl bnp-firl neat-irl misprediction score. time complexity neat-irl signiﬁcantly greater methods. fig. number samples evaluation collective misprediction performances bnp-firl bnp-firl better better gpirl neat-irl. time complexity least gpirl bnp-firl competitive bnp-firl. execution time neat-irl remains largest across experimental settings. presence less demonstration data algorithm often match examples discontinue evolving network further. number examples increases ﬁtting becomes difﬁcult causes increase number generations. explains gradual increase time complexity. since limit maximum number generations executed time complexity later stagnates. bnp-firl internally uses matrix multiplication based technique solve given state calculated rewards. however neat-irl processes state based state values neighbouring state values. therefore possible additional conditional sequences result signiﬁcantly increased time complexity neatirl opposed bnp-firl. perhaps then neural networks generate state reward instead state value could integrate solution method used bnp-firl reduce time complexity. however mapping state features rewards argued decrease performance compared mapping state features state values performance improvement perspective figure examines setting neural networks provides competitive performance algorithms examination set. however smoother graph required better comparison. reason average considered executions. results shown figure execution time data similar average figure therefore omitted. ﬁgure establishes performance bnpfirl competitive bnp-firl better gpirl neat-irl. ﬁgure also establishes setting neural networks alone outperforms gpirl. experiment figure also extended observe performance presence samples shown figure trends gpirl bnp-firl bnp-firl continue limitation performance neat-irl attributed neat parameter settings observed. compare bnp-firl bnp-firl algorithms neural networks tested better adaptability linear boundaries compared bnp-firl. algorithms evaluated various non-deterministic linear mdps observe algorithm performance different optimal policies different mdps. overall performance algorithms indistinguishable. however closer look speciﬁc seed values shows bnp-firl performs noticeably better npbfirl certain seed values. policies mdps situations shown figure figure fig. number samples evaluation performance bnp-firl remains competitive bnp-firl. algorithms continue perform better gpirl neat-irl even increased number training samples. fig. number samples evaluation bnpfirl outperforms bnp-firl gpirl neat-irl misprediction score. time complexity neat-irl signiﬁcantly greater methods. table represent misprediction scores corresponding bnp-firl bnp-firl respectively. overcome ﬂuctuation numbers averages smaller number runs results averaged runs. presence single goal state bnp-firl signiﬁcantly outperforms bnp-firl. however number goals increases bnp-firl eventually competes later signiﬁcantly outperforms bnp-firl. perspective increasing number goals results complex policy. state reward hence state value surface optima. visually exempliﬁed figure figure hypothesis tested scale goals reward value placed corner grid. average executions bnp-firl results signiﬁcantly lower misprediction score compared bnp-firl hypothesis neural network allows learn complicated reward structures therefore conﬁrmed. fig. number samples evaluation bnp-firl bnp-firl collectively perform better gpirl neat-irl. average iterations asserts competitive performances firl bnp-firl. mdps corresponding seed values unclear whether number goal states discriminates performance algorithms. hypothesized bnp-firl performs better bnp-firl presence multiple goal states. evaluate this mdps manually constructed goals randomly placed mdp. done associating arbitrary reward value states. differences performances algorithms also compared signiﬁcance using tailed t-test. results summarized fig. solutions level complexity optimal policies different seed values. unclear whether number goal states depicts situation bnp-firl would outperform bnp-firl. experiments conclude algorithms using neat perform better non-deterministic linear bnpfirl gpirl useful considering real world mdps contain uncertainty action caused various sources noise. given competitive performance bnp-firl bnp-firl bnpfirl evaluated experiments highlight possibility mdps multiple goals favourable conﬁrmed cases mdps containing multiple goal states. bnp-firl better estimate complex reward structure corresponding hierarchy evaluated algorithms established bnp-firl ranked highest. additionally neat parameters tuned improve performance time complexity given examples. current implementation neat-irl also capable greater time efﬁciency. computations speciﬁc genome population parallelized. further neat-irl policy prediction currently done states. limited demonstrated states since determines ﬁtness. future work bnp-firl integrated multiple agent settings also extended incorporate cost sharing information caruana niculescu-mizil empirical comparison supervised learning algorithms proceedings international conference machine learning. trends machine learning vol. choi k.-e. bayesian nonparametric feature construction inverse reinforcement learning proceedings twenty-third international joint conference artiﬁcial intelligence. aaai press fig. example solutions presence less goals optimal policy shows smooth direction towards goal. encoded using simple reward function opposed non-linear function required presence complex policy fig. example solutions increase number goals optimal policy becomes complex less goals alger deep inverse reinforcement learning. vroman maximum likelihood inverse reinforcement learning ph.d. dissertation rutgers university-graduate school-new brunswick whiteson stone stanley miikkulainen kohl automatic feature selection neuroevolution proceedings annual conference genetic evolutionary computation. karpov valsalam miikkulainen human-assisted neuroevolution shaping advice examples proceedings annual conference genetic evolutionary computation. miikkulainen feasley johnson karpov rajagopalan rawal tansey multiagent learning neuroevolution advances computational intelligence. springer puterman markov decision processes discrete stochastic dynamic barto reinforcement learning introduction. press lilley frean neural networks replacement gaussian processes? intelligent data engineering automated learningideal springer rasmussen gaussian processes machine learning chervenski multineat http//multineat.com/ mayr matlab neat http//nn.cs.utexas.edu/?neatmatlab levine popovic koltun stanford.edu/projects/gpirl/irl toolkit.zip", "year": 2016}