{"title": "Accelerated Gradient Temporal Difference Learning", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "The family of temporal difference (TD) methods span a spectrum from computationally frugal linear methods like TD({\\lambda}) to data efficient least squares methods. Least square methods make the best use of available data directly computing the TD solution and thus do not require tuning a typically highly sensitive learning rate parameter, but require quadratic computation and storage. Recent algorithmic developments have yielded several sub-quadratic methods that use an approximation to the least squares TD solution, but incur bias. In this paper, we propose a new family of accelerated gradient TD (ATD) methods that (1) provide similar data efficiency benefits to least-squares methods, at a fraction of the computation and storage (2) significantly reduce parameter sensitivity compared to linear TD methods, and (3) are asymptotically unbiased. We illustrate these claims with a proof of convergence in expectation and experiments on several benchmark domains and a large-scale industrial energy allocation domain.", "text": "high rate amount data limited difﬁcult acquire feature vectors small data efﬁciency primary concern quadratic least squares methods preferred. methods directly compute value function minimizes mspbe thus lstd computes value function linear methods converge. course many domains neither light weight linear methods data efﬁcient least squares methods good match. signiﬁcant effort focused reducing computation storage costs least squares methods order span lstd. ilstd method achieves sub-quadratic computation time step still requires memory quadratic size features. tlstd method uses incremental singular value decomposition achieve sub-quadratic computation storage. basic idea many domains update matrix lstd replaced rank approximation. practice tlstd achieves runtimes much closer compared ilstd achieving better data efﬁciency. related idea random projections reduce computation storage lstd approaches scalar parameter controls balance computation cost quality solution. paper explore approach called accelerated gradient performs quasi-second-order gradient descent mspbe. develop family algorithms interpolate linear methods lstd without incurring bias. combined low-rank approximation converges expectation ﬁxed-point convergence rate dependent choice rank. unlike previous subquadratic methods consistency guaranteed even rank chosen one. demonstrate performance versus many linear subquadratic methods three domains indicating match data efﬁciency lstd signiﬁcantly less computation storage unbiased unlike many alternative subquadratic methods signiﬁcantly reduces parameter sensitivity step-size versus linear methods signiﬁcantly less sensitive choice rank parameter tlstd enabling smaller rank chosen providing efﬁcient increfamily temporal difference methods span spectrum computationally frugal linear methods like data efﬁcient least squares methods. least square methods make best available data directly computing solution thus require tuning typically highly sensitive learning rate parameter require quadratic computation storage. recent algorithmic developments yielded several sub-quadratic methods approximation least squares solution incur bias. paper propose family accelerated gradient methods provide similar data efﬁciency beneﬁts least-squares methods fraction computation storage signiﬁcantly reduce parameter sensitivity compared linear methods asymptotically unbiased. illustrate claims proof convergence expectation experiments several benchmark domains large-scale industrial energy allocation domain. reinforcement learning common strategy learn optimal policy iteratively estimate value function current decision making policy—called policy evaluation— update policy using estimated values. overall efﬁciency policy iteration scheme directly inﬂuenced efﬁciency policy evaluation step. temporal difference learning methods perform policy evaluation estimate value function directly sequence states actions rewards produced agent interacting unknown environment. family temporal difference methods span spectrum computationally-frugal linear stochastic approximation methods data efﬁcient quadratic least squares methods. stochastic approximation methods temporal difference learning gradient methods perform approximate gradient descent mean squared projected bellman error methods require linear computation time step linear memory. linear td-based algorithms well suited problems high dimensional feature vectors —compared available resources— domains agent interaction occurs copyright association advancement artiﬁcial intelligence rights reserved. s.t. m/dµ called trace-decay parameter stationary distribution induced following importance sampling ratio reweights samples generated give expectation well-studied weighting occurs dµ). on-policy setting minimizes mspbe found on-policy temporal difference learning algorithm called recently emphatic weighting introduced emphatic algorithm denote metd. weighting includes long-term information importantly ametd matrix induced emphatic weighting positive semi-deﬁnite later ensure convergence algorithm onoff-policy sampling. used necessarily positive semi-deﬁnite diverge common strategies obtain minimum objective stochastic temporal difference techniques directly approximating linear system solving weights lstd ﬁrst class constitute linear complexity methods computation storage including family gradient methods true online methods several others complete summary). extreme quadratic computation storage approximate incrementally solve system given batch samples compute solution amtw bmt. least-squares methods typically implemented incrementally using sherman-morrison formula requiring storage computation step. paper focus problem policy evaluation learning value function given ﬁxed policy. model interaction agent environment markov decision process denotes states denotes actions s×a× encodes one-step state transition dynamics. discrete time step agent selects action according behavior policy environment responds transitioning state according emits scalar reward objective policy evaluation estimate value function expected return state target policy denotes expectation deﬁned future states encountered selecting actions according return denoted discounted future rewards given actions selected according scalar depends discounts contribution future rewards exponentially time. generalization transition-based discounting enables uniﬁcation episodic continuing tasks adopt here. standard continuing case constant standard episodic setting episode point ending inﬁnite return. common on-policy evaluation setting otherwise policy evaluation problem said off-policy. domains number states large state continuous feasible learn value state separately must generalize values states using function approximation. case linear function approximation state represented ﬁxed length feature vectors approximation value function formed linear combination learned weight vector wxt. goal policy evaluation learn samples generated following given general form next question approximate natural choices diagonal approximation low-rank approximation. storing using diagonal approximation would require linear time space. low-rank approximation rank represented truncated singular value decomposition ukσkv storage requirement required matrix-vector multiplications because vector ukσk sequence matrix-vector multiplications. exploratory experiments revealed low-rank approximation approach signiﬁcantly outperformed diagonal approximation. general however many approximations could used important direction atd. incremental previously proved effective incremental estimation reinforcement learning total computational complexity algorithm fully incremental update mini-batch updates samples. notice algorithm reduces exactly step-size. extreme equivalent iterative form lstd. appendix discussion implementation details. previous convergence results temporal difference learning algorithms ﬁrst step prove expected update converges ﬁxed point. unlike previous proofs convergence expectation require true full rank. generalization important shown previously often low-rank even features linearly independent further effective low-rank requiring full-rank would limit typical use-cases atd. across main idea ﬁrst prove convergence weightings give positive semi-deﬁnite general proof weightings appendix. assumption diagonalizable exists invertible rd×d normalized columns diagonal rd×d diag qλq−. assume ordering assumption max. finally introduce assumption used characterize convergence rate. condition previously used enforce level smoothness system. assumption linear system deﬁned qλq− satisfy discrete picard condition theorem assumptions rank-k approximation qλkq− rd×d zero otherwise. metd expected updating rule converges ﬁxed-point fact quadratic loss optimal descent direction a−eµemt] sense argmin∆w loss a−eµemt]. computing hessian updating requires quadratic computation practice quasi-newton approaches used approximate hessian. additionally recent insights using approximate hessians stochastic gradient descent fact speed convergence methods maintain approximation hessian sample gradient. hessian approximation provides curvature information signiﬁcantly speed convergence well reduce parameter sensitivity step-size. objective improve sample efﬁciency linear methods avoiding quadratic computation asymptotic bias. first need approximation provides useful curvature information also sub-quadratic storage computation. second need ensure approximation lead biased solution propose achieve approximating sampling eµemt] using δtet unbiased sample. proposed accelerated temporal difference learning update—which call atd—is regularization poor approximation discards information—as rank approximation— updating using result biased solution case tlstd instead sampling eµemt] show theorem yields unbiased solution even poor approximation regularization ensure consistency providing full rank preconditioner denominator cancels giving desired result. theorem gives insight utility speeding convergence well effect consider positive deﬁnite on-policy learning theorem guarantees convergences ﬁxed-point expected update exactly expected update. compare convergence rate using convergence rate. take instance setting common second-order methods rate convergence reduces maximum maxj∈{...k} ηtλt+ maxj∈{k+...rank} |−ηλj|tλj. early learning convergence rate dominated ηλ|tλ because largest relative ηλj|t small hand larger pick smaller |−ηλj|tλj much smaller value i.e. ηtλt+ small small gets smaller ηλk+|tλk+ becomes larger slowing convergence. low-rank domains however could quite small preconditioner could still improve convergence rate early learning—potentially signiﬁcantly outperforming proof general result stationary iterative methods applicable case full rank. theorem states given singular consistent linear system range stationary iteration converges solution following three conditions satisﬁed. ηλj| true proof condition change number positive eigenvalues rank unchanged. proof condition iii. show nullspaces equal sufﬁcient prove invertible nullspace nullspace. nullspace nullspace. figure parameter sensitivity boyan’s chain constant step-size decayed step-sizes plots above point summarizes mean performance algorithm setting linear methods lstd regularizer using percentage error compared true value function. decayed step-size case n+episode values values tested—corresponding sides graph. lstd algorithm parameters decay. algorithm achieves lowest error domain exhibits little sensitivity it’s regularization parameter boyan’s chain agent’s objective estimate value function based low-dimensional dense representation underlying state ambition experiment investigate performance domain pre-conditioner matrix full rank; rank truncation applied. compared linear-complexity methods true online true online etd) lstd reporting percentage error relative true value function ﬁrst steps averaged independent runs. swept large range step-size parameters trace decay rates regularization parameters tested ﬁxed decaying step-size schedules. figure summarizes results. lstd achieve lower error compared linear baselines—even thought linear method tuned using combinations step-sizes terms sensitivity choice step-size exhibit large effect performance whereas true-online least sensitive learning rate. lstd using sherman-morrison update sensitive regularization parameter; parameter free nature lstd slightly overstated literature. second batch experiments investigated characteristics classic benchmark domain sparse high-dimensional feature representation perfect approximation value function possible—mountain tile coding. policy evaluated stochastically takes action direction sign velocity performance measured computing truncated monte quasi-second order method meaning sensitivity parameters reduced thus simpler parameters. convergence rate provides intuition that reasonably chosen regularizer small—smaller typical stepsize additionally stochastic update expected update make typical conventions stochastic gradient descent parameters. previous stochastic second-order methods choose small ﬁxed value. choice represents small ﬁnal step-size well matching convergence rate intuition. bias subquadratic methods. update derived ensure convergence minimum mspbe either on-policy off-policy setting. algorithm summarizes past information improve convergence rate without requiring quadratic computation storage. prior work aspired goal however resultant algorithms biased. ilstd algorithm shown converge speciﬁc class feature selection mechanisms class however include greedy mechanism used ilstd algorithm select descent direction. random projections variant lstd signiﬁcantly reduce computational complexity compared conventional lstd projections size reduction comes cost increase approximation error fast lstd randomized updates batch data; algorithm could incrementally using mini-batches size though nice theoretical characterization algorithm restricted finally related algorithm tlstd also uses low-rank approximation used differently used tlstd. tlstd algorithm uses similar approximation tlstd uses compute closed form solution thus biased fact bias grows decreasing proportionally magnitude largest singular value choice decoupled ﬁxed point balance learning speed computation fear asymptotic bias. following experiments investigate on-policy setting thus make standard version simplicity. future work explore off-policy domains emphatic update. results presented section generated thousand individual experiments three different domains. space constraints detailed descriptions domain error calculation parameter settings discussed detail appendix. included wide variety baselines experiments additional related baselines excluded study also discussed appendix. figure learning curves percentage error versus time steps averaged runs rank lstd several baselines described text. sensitivity plot respect learning rate linear methods regularization parameter matrix methods. tlstd algorithm parameter besides rank little sensitivity it’s regularization parameter. carlo estimate return states sampled stationary distribution used grain tile coding state resulting dimensional feature representation exactly units active every time step. tested true online true online sub-quadratic methods including ilstd tlstd random projection lstd fast lstd wide range parameters swept large set. performance averaged independent runs. ﬁxed step-size schedule used linear baselines achieved best performance. results summarized ﬁgure lstd exhibit faster initial learning compared methods. particularly impressive since less size fast lstd projected lstd perform considerably worse linear td-methods ilstd exhibits high parameter sensitivity. tlstd tunable parameter besides performs poorly high stochasticity policy—additional experiments randomness action selection yielded better performance tlstd never equal atd. true online linear methods perform well compared required sweeping hundreds combinations whereas exhibited little sensitivity it’s regularization parameter achieved excellent performance parameter setting used boyan’s chain. additional experiment mountain clearly exhibit beneﬁt existing methods. used setting above except additional features added feature vector randomly rest zero. noisy feature vector meant emulate situation robot sensor becomes unreliable generating noisy data remaining sensors still useful task hand. results summarized figure naturally methods adversely effected change however atd’s remaining experiments paper excluded methods without true online traces perform worse true online counterparts experiments. result matches results literature rank approximation enables agent ignore unreliable feature information learn efﬁciently. tlstd suggested previous experiments seem cope well increase stochasticity. ﬁnal experiment compares performance several sub-quadratic complexity policy evaluation methods industrial energy allocation simulator much larger feature dimension report percentage error computed monte carlo rollouts averaging performance independent runs selecting testing parameters extensive policy optimized ahead time ﬁxed feature vectors produced tile coding resulting dimensional feature vector units active step. although feature dimension still relatively small quadratic method like lstd nonetheless would require million operations time step thus methods exploit rank approximations particular interest. results indicate tlstd achieve fastest learning expected. instrinsic rank domain appears small compared feature dimension—which exploited tlstd —while performance tlstd indicates domain exhibits little stochasticity. appendix contains additional results domain—in small rank setting signiﬁcantly outperforms tlstd. paper introduced family learning algorithms take fundamentally different approach previous incremental algorithms. idea preconditioner temporal difference update similar quasi-newton stochastic gradient descent update. prove expected update consistent empirically demonstrated improved learning speed parameter insensitivity even signiﬁcant approximations preconditioner. paper begins scratch surface potential preconditioners atd. remains many avenues explore utility preconditioners diagonal approximations eigenvalues estimates matrix factorizations approximations amenable inversion. family algorithms provides promising avenue effectively using results stochastic gradient descent improve sample complexity feasible computational complexity.", "year": 2016}