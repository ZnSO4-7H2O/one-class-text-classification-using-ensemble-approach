{"title": "Learning Hierarchical Information Flow with Recurrent Neural Modules", "tag": ["cs.LG", "cs.AI"], "abstract": "We propose ThalNet, a deep learning model inspired by neocortical communication via the thalamus. Our model consists of recurrent neural modules that send features through a routing center, endowing the modules with the flexibility to share features over multiple time steps. We show that our model learns to route information hierarchically, processing input data by a chain of modules. We observe common architectures, such as feed forward neural networks and skip connections, emerging as special cases of our architecture, while novel connectivity patterns are learned for the text8 compression task. Our model outperforms standard recurrent neural networks on several sequential benchmarks.", "text": "propose thalnet deep learning model inspired neocortical communication thalamus. model consists recurrent neural modules send features routing center endowing modules ﬂexibility share features multiple time steps. show model learns route information hierarchically processing input data chain modules. observe common architectures feed forward neural networks skip connections emerging special cases architecture novel connectivity patterns learned text compression task. model outperforms standard recurrent neural networks several sequential benchmarks. deep learning models make modular building blocks fully connected layers convolutional layers recurrent layers. researchers often combine strictly layered task-speciﬁc ways. instead prescribing connectivity priori method learns route information part learning solve task. achieve using recurrent modules communicate routing center inspired thalamus. warren mcculloch walter pitts invented perceptron ﬁrst mathematical model neural information processing laying groundwork modern research artiﬁcial neural networks. since then researchers continued looking inspiration neuroscience identify deep learning architectures efforts directed learning biologically plausible mechanisms attempt explain brain behavior interest achieve ﬂexible learning model. neocortex communication areas broadly classiﬁed pathways direct communication communication thalamus model borrow latter notion centralized routing system connect specializing neural modules. experiments presented model learns form connection patterns process input hierarchically including skip connections known resnet highway networks densenet feedback connections known play important role neocortex improve deep learning learned connectivity structure adapted task allowing model trade-off computational width depth. paper study properties goal building understanding interactions recurrent neural modules. computation modules unrolled time. possible path hierarchical information highlighted green. show model learns hierarchical information skip connections feedback connections section figure several modules share learned features routing center. dashed lines used dynamic reading only. deﬁne static dynamic reading mechanisms section section deﬁnes computational model. point critical design axes explore experimentally supplementary material. section compare performance model three sequential tasks show consistently outperforms multi-layer recurrent networks. section apply best performing design language modeling task observe model automatically learns hierarchical connectivity patterns. inspiration work neurological structure neocortex. areas neocortex communicate principal pathways cortico-cortico-pathway comprises direct connections nuclei cortico-thalamo-cortico comprises connections relayed thalamus. inspired second pathway develop sequential deep learning model modules communicate routing center. name proposed model thalnet. system comprises tuple computation modules route respective features shared center vector example instance thalnet model shown figure every time step module reads center vector context input optional task input module produces directed center output modules additionally produce task output feature vector function modules send features routing center merged single feature vector experiments simply implement concatenation reading mechanism allows modules mechanism obtain context input read individual features allowing complex selective reuse information modules. initial center vector zero vector. figure thalnet model perspective single module. example module receives input produces features center output context input determined linear mapping center features previous time step. practice apply weight normalization encourage interpretable weight matrices choice input output modules depends task hand. simple scenario exactly input module receiving task input number side modules exactly output module producing predictions. output modules trained using appropriate loss functions gradients ﬂowing backwards fully differentiable routing center modules. modules operate parallel reads target center vector previous time step. unrolling multi-step process seen figure ﬁgure illustrates ability arbitrarily route modules time steps suggest sequential nature model even though application static input possible allowing observing input multiple time steps. hypothesize modules center route information chain modules producing ﬁnal output tasks require producing output every time step repeat input frames allow model process multiple modules ﬁrst producing output. communication modules always spans time step. discuss implementations reading mechanism modules deﬁned section draw distinction static dynamic reading mechanisms thalnet. static reading conditioned independent parameters. dynamic reading conditioned current corresponding module state allowing model adapt connectivity within single sequence. investigate following reading mechanisms simplest form static reading consists fully connected layer weights r|c|×|φ| illustrated figure approach performs reasonably well exhibit unstable learning dynamics learns noisy weight matrices hard interpret. regularizing weights using penalties help since cause side modules read anymore. weight normalization. found linear mappings weight normalization parameterization effective. this context input computed w|w| scaling factor weights r|c|×|φ| euclidean matrix norm |w|. fast softmax. achieve dynamic routing condition reading weight matrix current module features seen form fast weights providing biologically plausible method attention apply softmax normalization computed weights element context computed weighted average center elements rather weighted sum. speciﬁcally fast gaussian. compact parameterization dynamic routing consider choosing context element gaussian weighted average mean variance vectors learned conditioned context input computed r|c| gaussian density function density evaluated index based distance mean. reading mechanism requires |ci| parameters module thus makes dynamic reading practical. reading mechanisms could also select modules high level instead individual feature elements. explore direction since seems less biologically plausible. moreover demonstrate knowledge feature boundaries necessary hierarchical information emerges using ﬁne-grained routing theoretically also allows model perform wider class computations. investigate properties performance model several benchmark tasks. first compare reading mechanisms module designs simple sequential task obtain good conﬁguration later experiments. please refer supplementary material precise experiment description results. weight normalized reading mechanism provides best performance stability training. thalnet models four modules conﬁguration experiments section. explore performance thalnet conduct experiments three sequential tasks increasing difﬁculty sequential permuted mnist. images mnist data pixels every image ﬁxed random permutation show model sequence rows. model outputs prediction handwritten digit last time step must integrate remember observed information previous rows. delayed prediction combined permutation pixels makes task harder static image classiﬁcation task multi-layer recurrent neural network achieving test error. standard split training images testing images. similar spirit cifar- data feed images model row. ﬂatten color channels every model observes vector elements every time step. classiﬁcation given observing last image. task difﬁcult mnist task image show complex often ambiguous objects. data contains training images testing images. text language modeling. text corpus consisting ﬁrst bytes english wikipedia commonly used language modeling benchmark sequential models. every time step model observes byte usually corresponding character encoded one-hot vector length task predict distribution next character sequence. performance measured bits character computed following cooijmans train ﬁrst evaluate performance following corpus. figure performance permuted sequential mnist sequential cifar text language modeling tasks. stacked baseline reaches higher training accuracy cifar fails generalize well. tasks thalnet clearly outperforms baseline testing accuracy. cifar recurrency within modules speeds training. pattern shows text experiment thalnet using parameters matches performance baseline parameters. step number refers repeated inputs discussed section smooth graphs using running average since models evaluated testing batches rolling basis. choices feed-forward layers layers implementing modules test fully connected layers layer fully connected followed followed fully connected sandwiched fully connected layers models pick largest layer sizes number parameters exceed training performed epochs batches size using rmsprop learning rate language modeling simulate thalnet steps token described section allow output module read information current input making prediction. note task model uses half capacity directly since side modules integrate longer-term dependencies previous time steps. baseline without extra steps steps token allowing apply full capacity twice token respectively. makes comparison difﬁcult favouring baseline. suggests architectural modiﬁcations explicit skip-connections modules could improve performance. text task requires larger models. train thalnet modules size feed forward layer size layer each totaling million model parameters. compare standard baseline language modeling single units totaling million parameters. train batches sequences containing bytes using adam optimizer default learning rate scale gradients exceeding norm results epochs training shown figure training took days thalnet steps token days baseline steps token days baseline without extra steps. figure shows training testing training curves three tasks described section. thalnet outperforms standard networks three tasks. interestingly thalnet experiences much smaller training testing performance baseline trend observed across experimental results. text task thalnet scores using parameters baseline scores using parameters model thus slightly improves baseline using fewer parameters. result places thalnet baseline regularization methods designed language modeling also applied model. baseline performance consistent published results lstms similar number parameters hypothesize information bottleneck reading mechanism acting implicit regularizer encourages generalization. compared using large freedom modeling input-output mapping thalnet imposes local structure input-output mapping implemented. particular encourages model decompose several modules stronger intra-connectivity extra-connectivity. thus extend every module needs learn self-contained computation. using routing center model able learn structure part learning solve task. section explore emergent connectivity patterns. show model learns route features hierarchical ways hypothesized including skip connections feedback connections. purpose choose text corpus medium-scale language modeling benchmark consisting ﬁrst bytes wikipedia preprocessed hutter prize model observes one-hot encoded byte time step trained predict future input next time step. comparably small models able experiments quickly comparing thalnet models ff-gru-ff modules layer sizes experiments weight normalized reading. focus exploring learned connectivity patterns. show competitive results task using larger models section simulate time steps allow output module receive information current input frame discussed section models trained epochs batches size containing sequences length using rmsprop learning rate general observe different random seeds converging similar connectivity patterns recurring elements. figure shows trained reading weights various reading mechanisms along connectivity graphs manually deduced. image represents reading weight matrix modules pixel shows weight factors multiplied produce single element context vector module. weight matrices thus dimensions |ci|. white pixels represent large magnitudes suggesting focus features positions. weight matrices weight normalized reading clearly resemble boundaries four concatenated module features φ··· center vector even though model notion origin ordering elements center vector. similar structure emerges fast softmax reading. weight matrices sparser weights weight normalization. course sequence observe weights staying constant others change magnitudes time step. suggests optimal connectivity might include static dynamic elements. however reading mechanism leads less stable training. problem could potentially alleviated normalizing fast weight matrix. fast gaussian reading distributions occasionally tighten speciﬁc features ﬁrst last modules modules receive input emit output. modules learn large variance parameters effectively spanning center features. could potentially addressed reading using mixtures gaussians context element instead. generally weight normalized fast softmax reading select features targeted way. figure reading weights learned different reading mechanisms modules text language modeling task alongside manually deducted connectivity graphs. plot weight matrices produce context inputs four modules bottom. images show focus input modules followed side modules output modules bottom. pixel gets multiplied center vector produce scalar element context input visualize magnitude weights percentile. include connectivity graph fast gaussian reading reading weights clearly structured. figure shows manually deducted connectivity graphs modules. arrows represent main direction information model. example incoming arrows module figure indicate module mainly attends features produced modules infer connections larger weight magnitudes ﬁrst third quarters reading weights module typical pattern emerges experiments seen connectivity graphs weight normalized fast softmax reading namely output module reads features directly input module. direction connection established early training likely direct gradient path output input. later side modules develop useful features support input output modules. another pattern module reads modules combines information. figure module takes role reading modules distributing features input module. additional experiments four modules observed pattern emerge predominantly. connection pattern provides efﬁcient information sharing cross-connecting modules. connectivity graphs figure include hierarchical computation paths modules. include learn skip connections known improve gradient popular models resnet highway networks densenet furthermore connectivity graphs contain backward connections creating feedback loops modules. feedback connections known play critical role neocortex inspired work describe recurrent mixture experts model learns dynamically pass information modules. related approaches found various recurrent multi-task methods outlined section. modular neural networks. thalnet consists several recurrent modules interact exploit other. modularity common property existing neural models. learn matrix tasks robot bodies improve multitask transfer learning. learn modules modules speciﬁc objects present scene selected object classiﬁer. approaches specify modules corresponding speciﬁc task variable manually. contrast model automatically discovers exploits inherent modularity task require one-to-one correspondence modules task variables. column bundle model consists central column several mini-columns around applied temporal data observe structural similarity modules mini-columns case weights shared among layers mini-columns authors mention possibility. learned computation paths. learn connectivity modules alongside task. various methods multi-task context also connectivity modules. fernando learn paths multiple layers experts using evolutionary approach. rusu learn adapter connections connect ﬁxed previously trained experts exploit information. approaches focus feed-forward architectures. recurrency approach allows complex ﬂexible computational paths. moreover learn interpretable weight matrices examined directly without performing costly sensitivity analysis. neural programmer interpreted presented reed freitas related dynamic gating mechanisms. work network recursively calls parameterized perform tree-shaped computations. comparison model allows parallel computation modules unrestricted connectivity patterns modules. memory augmented rnns. center vector model interpreted external memory multiple recurrent controllers operating preceding work proposes recurrent neural networks operating external memory structures. neural turing machine proposed graves follow-up work investigate differentiable ways address memory reading writing. thalnet model multiple recurrent controllers accessing center vector. moreover center vector recomputed time step thus confused persistent memory typical model external memory. presented thalnet recurrent modular framework learns pass information neural modules hierarchical way. experiments sequential permuted variants mnist cifar- promising sign viability approach. experiments thalnet learns novel connectivity patterns include hierarchical paths skip connections feedback connections. current implementation assume center features vector. introducing matrix shape center features would open ways integrate convolutional modules similaritybased attention mechanisms reading center. matrix shaped features easily interpretable visual input less clear structure leveraged modalities. direction future work apply paradigm tasks multiple modalities inputs outputs. seems natural either separate input module modality multiple output modules share information center. believe could used hint specialization speciﬁc patterns create controllable connectivity patterns modules. similarly interesting direction explore proposed model leveraged learn remember sequence tasks. believe modular computation neural networks become important researchers approach complex tasks employ deep learning rich multi-modal domains. work provides step direction automatically organizing neural modules leverage order solve wide range tasks complex world. merriënboer bahdanau bengio. properties neural machine translation encoder–decoder approaches. syntax semantics structure statistical translation page fernando banarse blundell zwols rusu pritzel wierstra. pathnet evolution channels gradient descent super neural networks. arxiv preprint arxiv. graves wayne reynolds harley danihelka grabska-barwi´nska colmenarejo grefenstette ramalho agapiou hybrid computing using neural network dynamic external memory. nature kirkpatrick pascanu rabinowitz veness desjardins rusu milan quan ramalho grabska-barwinska overcoming catastrophic forgetting neural networks. proceedings national academy sciences page krueger maharaj kramár pezeshki ballas goyal bengio larochelle courville zoneout regularizing rnns randomly preserving hidden activations. arxiv preprint arxiv. salimans kingma. weight normalization simple reparameterization accelerate training deep neural networks. advances neural information processing systems pages shazeer mirhoseini maziarz davis hinton dean. outrageously large neural networks sparsely-gated mixture-of-experts layer. arxiv preprint arxiv. figure test performance sequential mnist task grouped module design reading mechanism plots show median bottom accuracy design choices. recurrent modules train faster pure fully connected modules weight normalized reading stable performs best. ff-gru-ff modules perform similarly ff-gru limiting size center. sequential variant mnist compare reading mechanisms described section along implementations module function. sequential mnist model observes handwritten digits pixels bottom time step. prediction given last time step model integrate remember observed information sequence. makes task challenging static setting multi-layer recurrent network achieving error task. implement modules test various combinations fully connected recurrent layers gated recurrent units modules require amount local structure allow specialize. test fully connected layers layer fully connected followed followed fully connected sandwiched fully connected layers addition compare performance stacked baseline layers. models pick largest layer sizes number parameters exceed train epochs batches size using rmsprop learning rate figure shows test accuracy module designs reading mechanisms. thalnet outperforms stacked baseline conﬁgurations. assume structure imposed model acts regularizer. perform performance comparison section results module designs shown figure appendix. observe beneﬁt recurrent modules exhibit faster stable training fully connected modules. could explained fact pure fully connected modules learn routing center store information time long feedback loop. fully connected layer recurrent layer also signiﬁcantly improves performance. fully connected layer produce compact feature vectors scale better large modules although ff-gru beneﬁcial later experiments results reading mechanisms area shown figure reading mechanism small impact model performance. weight normalized reading yield stable performance linear fast softmax reading. experiments weight normalized reading stability predictive performance. include results fast gaussian reading here performed performance range methods. thalnet route information input output multiple time steps. enables trade shallow deep computation paths. understand this view thalnet smooth mixture experts model modules recurrent experts. module outputs features center vector linear combination read next time step effectively performs mixing expert outputs. compared recurrent mixture experts model presented shazeer model recurrently route information mixture multiple times increasing number mixture compounds. highlight extreme cases modules could read identical locations center. case model wide shallow computation time step analogous graves extreme module reads different module recovering hierarchy recurrent layers. gives deep narrow computation stretched multiple time steps. between exist spectrum complex patterns information differing dynamic computation depths. comparable densenet also blends information paths different computational depth although purely feed-forward model. using state-less modules model could still leverage recurrence modules center store information time. however bounds number distinct computation steps thalnet could apply input. using recurrent modules computation steps change time increasing ﬂexibility model. recurrent modules give stronger prior using feedback shows improved performance experiments. viewing equations model deﬁnition might think model compares long short-term memory however exists limited similarity models. empirically observed lstms performed similarly baselines given parameter budget. lstm’s context vector processed element-wise thalnet’s routing center cross-connects modules. lstm’s hidden output better candidate comparison thalnet’s center features allows relate recurrent weight matrix lstm layer linear version reading mechanism. could relate thalnet module multiple lstm units. however lstm units perform separate scalar computations modules learn complex interactions multiple features time step. alternatively could lstm units small thalnet modules reading exactly four context elements each namely input three gates. however computational capacity local structure individual lstm units comparable thalnet modules used work.", "year": 2017}