{"title": "Information fusion in multi-task Gaussian processes", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "This paper evaluates heterogeneous information fusion using multi-task Gaussian processes in the context of geological resource modeling. Specifically, it empirically demonstrates that information integration across heterogeneous information sources leads to superior estimates of all the quantities being modeled, compared to modeling them individually. Multi-task Gaussian processes provide a powerful approach for simultaneous modeling of multiple quantities of interest while taking correlations between these quantities into consideration. Experiments are performed on large scale real sensor data.", "text": "paper evaluates heterogeneous information fusion using multi-task gaussian processes context geological resource modeling. speciﬁcally empirically demonstrates information integration across heterogeneous information sources leads superior estimates quantities modeled compared modeling individually. multi-task gaussian processes provide powerful approach simultaneous modeling multiple quantities interest taking correlations quantities consideration. experiments performed large scale real sensor data. applications space-exploration mining agriculture automation modeling underlying resource fundamental problem. applications eﬃcient ﬂexible high-ﬁdelity representation geology critical. challenges realizing dealing problems uncertainty incompleteness. uncertainty incompleteness virtually ubiquitous sensor based application sensor capabilities limited. problem magniﬁed ﬁeld automation scenario sheer scale application. incompleteness major problem large scale resource modeling endeavor sensors limited range applicability. signiﬁcant contributor issue cost sampling collecting data expensive. geological data typically collected various sensors/processes widely diﬀering characteristics consequently lead diﬀerent kinds information. often resource characterized numerous quantities quantities often correlated. given issues large scale geological resource modeling needs representation handle spatially correlated incomplete uncertain data. must correlation homogeneous quantities modeled also heterogeneous quantities. paper uses gaussian process representation resource data similar described ideally suited handling spatially correlated data. paper uses extension basic gaussian process model multi-task gaussian process simultaneously model multiple quantities interest. proposed model captures spatial correlations individual quantities also totally diﬀerent quantities together quantify resource. quantities modeled paper exhibit strong correlation known geological sciences. paper presents empirical evaluation understand simultaneous modeling multiple quantities interest better modeling quantities independently nonstationary kernels eﬀective stationary kernels modeling geological data. experiments performed large scale real sensor data. gaussian processes powerful non-parametric bayesian learning techniques handle correlated uncertain incomplete data. used range ﬁelds gaussian process web-site lists several examples. produce scalable multi-resolution model entity consideration. yield continuous domain representation data hence sampled desired resolution. incorporate handle uncertainty statistically sound manner represent spatially correlated data appropriately. model spatial correlation given data estimate values unknown points interest. basically perform standard interpolation technique known kriging work modeled large scale terrain modeling using gps. proposed non-stationary kernels model large scale discontinuous spatial data. performance comparison based stationary non-stationary kernels well several standard interpolation methods applicable alternative representations terrain data reported. non-stationary neural network kernel found superior stationary squared exponential kernel least good standard interpolation techniques range terrain work presented paper builds representation. however addresses problem simultaneous modeling multiple heterogeneous quantities interest context geological resource modeling. requires modeling usage correlations quantities towards improving predictions instance data fusion using gaussian processes. data fusion context gaussian processes necessitated presence multiple multisensor multi-attribute incomplete and/or uncertain data sets entity modeled. preliminary attempts towards addressing problem include former bears hierarchical learning ﬂavor demonstrates used model expensive process modeling approximate cheap process using many input-output data approximate process samples available expensive process together order learn latter. work attempts generalize arbitrary transformations priors linear transformations. hints framework could used introduce heteroscedasticity information diﬀerent sources could fused. however speciﬁcs fusion actually performed beyond scope work. girolami integrated heterogeneous feature types within gaussian process classiﬁcation setting protein fold recognition application domain. feature representation represented separate fusion uses idea individual feature representations considered independent hence composite covariance function would deﬁned terms linear gaussian process priors. recent work reece integrated hard data obtained sensors soft information obtained human sources within gaussian process classiﬁcation framework. problem/approach diﬀerent work presented here. uses heterogeneous information domains mutually independent sources information transformed kernel representation combined using product rule focus thus encoding representing diﬀerent kinds information common mathematical framework using kernels. paper concerned higher level data fusion problem heterogeneous-source information integration represented using kernel methods. experiments paper demonstrate case information source homogeneous domain e.g. heterogeneous input data real numbers. approach presented paper improves estimate several diﬀerent quantities simultaneously modeled explicitly modeling correlation multiple heterogeneous information sources. case heterogeneous information types represented separate kernels combined using product done simpler data fusion approaches based heteroscedastic variants applied. however application approach presented paper based multi-output multi-task require non-trivial derivation auto cross covariances kernels applied heterogeneous information types. examples related works multiple sources kind information within single representation framework include whereas former uses single output incorporate in-situ surface spectra information remotely sensed spectra information kilometer scale environment latter uses implicit surface representation object grasped manipulated. representation incorporates visual haptic laser data single representation object. data sensor modalities conditions prior based implied surface point recent approaches demonstrating data fusion gaussian processes context large scale terrain modeling based heteroscedastic dependent address problem fusing multiple multi-sensor data sets single quantity interest. paper describes framework extending concept multiple heterogeneous quantities interest. work treated data-fusion problem combining diﬀerent noisy samples common entity modeled. machine learning community idea referred heteroscedastic works treated data fusion problem improving regression modeling spatial correlations several dependent representing respective data sets. idea inspired recent machine learning contributions multi-task multi-output dependent modeling including latter based kriging terminology idea akin co-kriging work performed model complexity analysis multiple approaches data fusion using applied context large scale terrain modeling. work presented paper focuses generic approaches context geological resource modeling. signiﬁcantly stronger evaluation discussion big-picture issues relating application approach practical problems fusion heterogeneous data kernels tying together diﬀerent prior works studied approach enhancements presented work. work provided preliminary ﬁndings geological resource modeling using various combinations stationary kernel including squared exponential matern sparse covariance function geological resource modeling data taken mine found matern matern sqexp kernel combination provided best performance terms prediction error. paper reports detailed multi-metric benchmarking experiment using cross validation methods performed multi-task equivalent independently optimized provide exact independent comparison them. objective quantify beneﬁt simultaneous modeling multiple quantities modeling using correlations modeling quantities separately. paper also compares data fusion using multiple stationary nonstationary kernels context modeling geological data. extensive review kernel methods applied modeling vector valued functions presented recent survey paper paper discusses diﬀerent approaches develop kernels multi-task applications draws parallels regularization perspective problem bayesian one. latter perspective discussed gaussian processes. work presented paper focuses approaches reviewed speciﬁcally addresses modeling information fusion multi-task geological data using gaussian processes developed using process convolution approach. paper presents detailed empirical study approach applied large scale real world problem order evaluate eﬃcacy information fusion understand modeling capabilities diﬀerent kernels data understand broader approach-related questions application perspective. paper also ties together past works authors within process convolution theme. gaussian processes stochastic processes wherein ﬁnite subset random variables jointly gaussian distributed. thought gaussian probability distribution function space. characterized mean function covariance function together specify distribution functions. context geological resource modeling covariance function kernel models relationship random variables corresponding given data. take numerous forms stationary squared exponential kernel given ldepth diagonal covariance function kernel; diag measure quickly modeled function changes east north depth directions; signal variance. parameters least lnorth ldepth referred kernel hyperparameters. augmented input vectors diagonal length-scale matrix given diag− bias factor dimensionality input data. variables least lnorth ldepth constitute kernel hyperparameters. kernel represents covariance function neural network single hidden layer input output inﬁnitely many hidden nodes using sigmoidal transfer function hidden nodes. hornik showed neural networks universal approximators neal observed functions produced network would tend gaussian process. prior work found kernel eﬀective sqexp kernel modeling discontinuous data. matern kernel another stationary kernel diﬀering sqexp kernel latter inﬁnitely diﬀerentiable consequently tends strong smoothing nature argued detrimental modeling physical processes takes form speciﬁed equation dimension input data length-scale matrix measure quickly modeled function changes east north depth directions; signal variance. parameters least lnorth ldepth referred kernel hyperparameters. regression using uses fact ﬁnite training data test data jointly gaussian distributed. assuming noise free data idea shown expression leads standard regression equations yielding estimate uncertainty training points i=...n test points denotes matrix covariances evaluated pairs training test points. terms deﬁned likewise. event data modeled noisy noise hyperparameter also learnt hyperparameters covariance matrix training data replaced equations hyperparameters learnt using various techniques cross validation based approaches maximum-a-posteriori approaches using markov chain monte carlo techniques maximizing marginal likelihood observed training data paper adopts latter approach based intuition suited large data sets. marginal likelihood maximized described equation problem addressed paper described follows. objective model multiple heterogeneous quantities entity consideration data fusion aspect problem improved estimation quantities integration quantities interest. quantity modeled using separate objective improve prediction estimates given models. multi-task gaussian processes extend gaussian processes handle multiple correlated outputs simultaneously. main advantage technique model exploits spatial correlation data corresponding output also outputs. improves regression/prediction output given others thus performing data fusion. figure shows simulated example concept. number outputs/tasks need simultaneously modeled denoted equations represent respectively mtgp data fusion model regression estimates uncertainties subject following modiﬁcations basic notation. denotes input location values selected training data individual data sets. kernel used even diﬀerent kernel could used diﬀerent data sets using technique demonstrated convolution process technique demonstrated paper covariance matrix training data given figure simple demonstration mtgp/dgp concept demonstrating data fusion. sine waves modeled. inverted function other. noisy samples available whereas noisy samples part merely using green samples would result poor prediction sine wave areas devoid samples. using spatial correlation sampled sine wave enables mtgp approach improve prediction green sampled sine wave. ﬁgure shows predictions given second taken alone ﬁgure shows uncertainty predictions second taken alone taken together ﬁrst clear reduction uncertainty observed. here represents cross covariance data sets. terms model covariance noisy observed data points thus also take noise components individual data sets consideration. corresponding noise free terms respectively given derived using process convolution approach formulating gaussian processes; details follow next subsection. covariance matrix test points training points given assuming needs evaluated particular test point. mean variance concentration estimate thus obtained applying equations incorporating multiple outputs/tasks multiple gp/noise hyperparameters deriving appropriate auto cross covariances functions model spatial correlation individual data sets. data fusion thus achieved mtgp approach correlating individual heterogeneous outputs/tasks using correlation information improve prediction estimates them. main challenge multi-task derivation closed form cross covariance functions. process convolution approach modeling proposed address problem. cited paper modeled convolution smoothing kernel gaussian white noise process expressed relationship smoothing kernel corresponding covariance function fourier transform noted stationary isotropic kernels existed one-to-one relationship covariance function smoothing kernel non-isotropic and/or non-stationary kernels unique solution smoothing kernel hinted approach used develop models complex properties consequence approach modeling amounted modeling hyperparameters smoothing kernel. second point above paper suggested smoothing kernel covariance function could obtained inverse fourier transform square root spectrum covariance function. process convolution approach mtgps used stationary sqexp kernel nonstationary kernel smoothing kernel identiﬁed covariance function cross-covariance covariance functions derived kernel correlation respective smoothing kernels following mathematical formalism based mathematically represents observed data equation expressed combination noise-free gaussian white noise process modeled convolution smoothing kernel gaussian white noise process shown equation stationary and/or isotropic smoothing kernel would take form would function distance input points. covariance functions smoothing kernels respectively cross covariance derived shown equation auto covariance deduced cross covariance expression take form shown equation smoothing kernel work suggested covariance function could written convolution basis functions cross-covariance covariance functions could derived kernel correlation respective basis functions paper proved resulting cross-covariance would positive deﬁnite. order basis function particular covariance function paper derived expression terms fourier transform. relationship identical suggested valid stationary kernels only. paper also derives closed form cross-covariance functions diﬀerent combinations stationary kernels including squared exponential matern sparse covariance function developed authors paper argues methods using smoothing kernel basis functions actually equivalent former providing sound basis explain latter well powerful framework develop complex models space-time models nonstationary gps. insight obtained methodology identifying smoothing kernel process convolution approach. covariance function stationary kernel exact one-to-one relationship covariance function smoothing kernel pointed whose expression derived covariance function nonstationary several possible smoothing kernels lead covariance function pointed however attempting express kernel separable form thereby identifying smoothing kernel would possible approach form kernel form allowed separation. needless idea would applicable restricted class covariance functions ﬁnding universal approach identifying smoothing kernel nonstationary kernel remains open question. given smoothing kernel covariance functions consideration cross-covariance terms derived kernel correlation demonstrated assume length scale matrices based cross auto covariances stationary sqexp kernel given equations respectively. corresponding expressions nonstationary kernel derived given equations respectively. matern kernel expressions cross covariance auto covariance derived given equations respectively. also based cross covariance function sqexp matern kernel given equations term kernel data length scale matrix σij. given equation excluding signal variance term likewise equation refers matern kernel data length scale matrix given equation terms equations inspired term models task similarity individual tasks. incorporating auto cross covariances provides additional ﬂexibility multi-task modeling process. symmetric matrix size learnt along hyperparameters. thus hyperparameters system need learnt include task similarity values length scale values respectively individual sqexp/matern kernels noise values corresponding noise observed data sets. learning hyperparameters adapting learning procedure described multiple outputs/tasks experiments conducted large scale geological resource data made real sensor data. data consists measurements region australia undergone drilling chemical assays determine composition. holes generally apart tens hundreds meters deep. within hole data collected interval measurements include position data along concentrations three elements element- element- element- hereafter denoted respectively. three quantities known correlated hence objective models improve others’ prediction estimates capturing correlation quantities. data shown figure methodology testing described section multiple metrics used evaluate methods described section results obtained presented discussed section outputs data fusion process provided best performing model suggested evaluation also presented. objective experiment compare multi-task approach conventional approach quantify data fusion mtgp actually improves estimation. second objective experiments compare nonstationary kernel stationary sqexp kernel matern kernel combination proved eﬀective prior testing towards aims fold cross validation experiment performed data kernels. motivated work suggests fold stratiﬁed cross validation best testing estimation accuracy machine learning methods real world data sets. mtgp simple approaches require optimization step model learning. optimization step method result diﬀerent local minima trial thus one-on-one comparison approaches quantify relative performances exact comparison required. benchmarking experiment presented paper provides exact comparison mtgp approaches. this addition this three independent optimized estimates test points also compared. thus eﬀect information integration context geological resource modeling seen terms exact comparison independent comparison cross validation block sampling technique used version patch sampling method used idea rather selecting test points uniformly blocks data test robustness approach better support points query point situated farther away uniform point selection. data gridded blocks diﬀerent sizes. collections blocks represent individual folds. cross validation test fold designated test fold points used exclusively testing. folds together constituted evaluation data small subset labeled training data. note technique testing naturally lead larger errors. test fold concentrations respectively show concentrations three elements region interest. central region points surrounded sparse sets points pre-ﬁltered applying proposed algorithm. metrics deﬁned following section) estimated ﬁrst using mtgp approach approach using parameters optimized mtgp parameters ﬁnally independently optimized three quantities. result fold cross validation test point evaluation tougher test conditions would attainable uniform sampling test points. block sizes chosen empirically proportion dimensions whole data view performing stratiﬁed cross validation test. block sizes chosen resulting implications cross validation testing shown table smaller block size results fold similar number points thus results stratiﬁed cross validation test. increasing block size prediction error increases stratiﬁcation reduced hence variance prediction error also increases. uniform sampling test points considered limiting case block sampling smallest block size possible. figure example block sampling geological resource data set. blocks sampled diﬀerent sizes. yellow blocks represent blocks folds used cross validation testing. test points within blocks support data away them outside blocks. sampling method therefore stronger test robustness approach estimating quantity interest compared uniformly sampling test points. estimation errors however higher obtained uniformly sampled points. multiple metrics used understand various methods tested. brieﬂy described below. evaluated test point fold cross validation test. result would represented mean standard deviations values across folds. squared error represents squared diﬀerence predicted concentration known concentrations test points. mean test points popular metric context paper. referring equations test point variance represents variance predicted concentrations test points. lower good outcome also low. model high would poor model result would suggest model conﬁdent inaccurate estimates. better outcome would model high correspondingly high i.e. model inaccurate predictions also uncertain predictions. figures show predicted concentrations entire region interest well section views output uncertainty predictions constitute produced using multi-task using neural network kernel. tables show results cross validation testing geological resource data neural network matern squared exponential matern matern squared exponential kernels. three tables visualized numerous graphs summarize main trends observed; located appendix. figures depict main results table figures depict main results table figures depict main results table following observations made results obtained. test point situated farther away. increasing test block size also results reduced stratiﬁcation fold cross validation e.g. test points whereas another points. results increased standard deviation prediction error. fold stratiﬁed cross validation generally considered representative performance measure however testing multiple larger block sizes provides better understanding model’s behavior robustness. table concentration estimation; fold cross validation results using block sampling various block sizes; multi-task derived mtgp independently optimized using neural network matern squared exponential matern matern squared exponential kernel combination identical test data. error metrics expressed squared units table concentration estimation; fold cross validation results using block sampling various block sizes; multi-task derived mtgp independently optimized using neural network matern squared exponential matern matern squared exponential kernel combination identical test data. error metrics expressed squared units table concentration estimation; fold cross validation results using block sampling various block sizes; multi-task derived mtgp independently optimized using neural network matern squared exponential matern matern squared exponential kernel combination identical test data. error metrics expressed squared units model. typically multiple attempts performed best results obtained pursued/used. iteration consisted stochastic optimization step and/or gradient based optimization step training data chosen uniformly data. work uses blocklearning approximation approximates total marginal likelihood sequence marginal likelihoods computed blocks points comprising training data. size block deﬁned computational resources available. stochastic optimization step time consuming part; attempt started completely random parameters. code unoptimized matlab code running typically -core processor based machine. times cores used process; multiple processes also shared system. note experiments paper analytical gradients optimization hyperparameters; design choice made interest stability comparability optimization results across kernels. analytical gradients signiﬁcantly reduce total training time. training time also reduced signiﬁcantly various ways including approximations intelligently setting initial parameters scaling data etc. number training attempts iterations total training time successful attempt attempts iterations total training time hours attempts iterations total training time hours sqexp attempts iteration total training time hours attempts iterations iterations took hours attempts iterations total training time hours attempts iteration total training time hours sqexp attempts iteration total training time hours rather individual training times relative amount training required produce reasonable parameters interest. experience suggests kernel based mtgp/gp models converged faster better compared kernels. marginally outperforms kernel smallest block sizes tested. note however considering test sizes three elements observation kernel produces lower higher meaning conﬁdent values worse/higher kernel. makes higher model poorer mtgp based kernel. note also test block size increases advantage performance mtgp based kernel based kernel becomes distinctive. values smaller kernel values remain range whereas kernel rise signiﬁcantly. proves mtgp-nn better performing robust mtgp-mm. latter property suggests mtgp-nn able cope better incomplete data sets. independent metrics lower lower clearly demonstrates beneﬁts information fusion across heterogeneous information sources improve individual predictions using mtgp model. however mtgp using kernel combination proves better derived independently optimized respect metric. perspective mtgp-ms model competitive models small block sizes. larger block sizes using independently optimized proves trust worthy modeling option increase error corresponding increase uncertainty independent models. exception behavior seen results mtgp model poor case. attributed inferior parameters relevant element obtained optimization process. based kernel; better corresponding derived/independent models inferior ﬂuctuating trend. element mtgp-sqexp worse equivalent model based kernel well corresponding models. prediction variance. smallest block size mtgp-sqexp produces relatively high prediction variance. basically suggests model conﬁdent poor estimates outcome. results high poor model. block size increases prediction variance increases relative prediction error resulting decreasing trend. elements largest block size results stronger increase prediction error variance prediction resulting increase nlp. overall mtgp-sqexp model poor. behavior model using sqexp kernel competitive results respect gpi-mm kernel possible poor performance mtgp-sqexp poor optimization output investigation result ongoing ﬁndings expected change conclusions paper. general stationary kernels tested seemed inadequate increase prediction uncertainty increasing test block size worsening predictions. leads higher metric poor model overly conﬁdent worsening predictions. behavior attributed correlation proﬁle stationary kernels tested share correlation decreases increasing distance support data point interest trend. results stationary kernels able cope large test block sizes support data farther away contrast nonstationary kernel sigmoidal proﬁle handle issue across range test block sizes. metric taken alone misleading. experiments reinforced need multimetric analysis. metric provides information prediction error describe prediction uncertainty important understanding model reliable otherwise. metrics provided insights diﬀerence performance diﬀerent models kernels. model conﬁdent poor predictions unreliable worsening predictions outcome provided equivalent increase prediction uncertainty. figure figures respectively show predicted concentrations superimposed input data section-view output data uncertainty predicted concentrations view. expectedly uncertainty around regions input/given data exist rapidly rises predictions away areas typically fringe areas. section view shows regions corresponding regions high concentration. corresponding regions figures show concentrations respectively. figure figures respectively show predicted concentrations superimposed input data section-view output data uncertainty predicted concentrations view. expectedly uncertainty around regions input/given data exist rapidly rises predictions away areas typically fringe areas. section view shows violet regions corresponding regions concentration. corresponding regions figure show high concentration figure show concentration. figure figures respectively show predicted concentrations superimposed input data section-view output data uncertainty predicted concentrations view. expectedly uncertainty around regions input/given data exist rapidly rises predictions away areas typically fringe areas. section view shows violet regions corresponding regions concentration. corresponding regions figure show high concentration figure show concentration. basis study attempt made answering fundamental questions know good mtgp model model kernel means intended ready-made prescription universal formula short-cut used substitute context speciﬁc statistically decisions developing gaussian process models. rather reﬂection authors’ experiences based scope past work domains terrain modeling. note numerous sophisticated techniques beyond scope work change inferences. multiple kernel family would provide good method validating general behavior/trends model question. instance developing mtgp model based sqexp kernel developing matern kernel based mtgp model could provide means validate behavior mtgp-sqexp model. model hyperparameter optimization performed paper based maximizing marginal likelihood. typically error metrics suﬃciently suggestive model good. cross validation test could also performed ensure indeed case. however also important check model question under/over conﬁdent given level error. done standalone test comparison alternative models test cases. developing mtgp model good idea compare equivalent derived model independently optimized model. availability information eﬀective information mtgp model ideally result signiﬁcantly lower error metrics signiﬁcant improvement conﬁdence reduction negative loss metric. obviously depends data hand constraints modeling problem. following purely indicative based experiences multiple problem domains change considering alternative kernels novel ways treating modeling problem approximation methods. time complexity computational resources premium. need method works independently optimized models using neural network kernel matern kernel would competitive solution. note outcome good data modeled information sources cannot leveraged. need best possible model range test sizes know data changes multi-task models kernel representative variation data e.g. uniform variation eﬀectively modeled using matern squared exponential kernels. need model cope sparse data and/or incomplete data sets neural network kernel based mtgp models depending computational complexity constraints model accuracy requirements. good multi-attribute data. need model well fast independent models attributes using either neural network kernel kernel suited data would provide competitive solution. independent models result ability parallelize modeling process signiﬁcantly reduce possibility poor models consequence reduction number model parameters. note good application dependent would certainly require well sampled noisy reasonably complete paper studied problem geological resource modeling using multi-task gaussian processes concentrations three elements modeled predicted region interest using mtgp well individual gaussian processes quantities. paper demonstrates mtgps perform signiﬁcantly better individual modeling problem eﬀectively integrate heterogeneous sources information improve individual predictions them. beneﬁts information integration using mtgp independent task geological resource modeling quantiﬁed multi-metric multi-test-size cross validation study performed exact independent comparison mtgps gps. multi-task gaussian process models based neural network kernel shown competitive robust option across range test block sizes.", "year": 2012}