{"title": "Controllable Invariance through Adversarial Feature Learning", "tag": ["cs.LG", "cs.AI", "cs.CL"], "abstract": "Learning meaningful representations that maintain the content necessary for a particular task while filtering away detrimental variations is a problem of great interest in machine learning. In this paper, we tackle the problem of learning representations invariant to a specific factor or trait of data. The representation learning process is formulated as an adversarial minimax game. We analyze the optimal equilibrium of such a game and find that it amounts to maximizing the uncertainty of inferring the detrimental factor given the representation while maximizing the certainty of making task-specific predictions. On three benchmark tasks, namely fair and bias-free classification, language-independent generation, and lighting-independent image classification, we show that the proposed framework induces an invariant representation, and leads to better generalization evidenced by the improved performance.", "text": "learning meaningful representations maintain content necessary particular task ﬁltering away detrimental variations problem great interest machine learning. paper tackle problem learning representations invariant speciﬁc factor trait data. representation learning process formulated adversarial minimax game. analyze optimal equilibrium game amounts maximizing uncertainty inferring detrimental factor given representation maximizing certainty making task-speciﬁc predictions. three benchmark tasks namely fair bias-free classiﬁcation language-independent generation lighting-independent image classiﬁcation show proposed framework induces invariant representation leads better generalization evidenced improved performance. produce data representation maintains meaningful variations data eliminating noisy signals consistent theme machine learning research. last years dominant paradigm ﬁnding representation shifted manual feature engineering based speciﬁc domain knowledge representation learning fully data-driven often powered deep neural networks universal function approximators deep neural networks easily uncover complicated variations data leading powerful representations. however systematically incorporate desired invariance learned representation controllable remains open problem. possible avenue towards solution devise dedicated neural architecture construction desired invariance property. typical example parameter sharing scheme pooling mechanism modern deep convolutional neural networks take advantage spatial structure image processing problems allowing induce generic feature representations fully connected networks. since invariance care vary greatly across tasks approach requires design architecture time invariance desideratum shows time-consuming inﬂexible. belief invariance speciﬁc attribute input data alternative approach build probabilistic model random variable corresponding attribute explicitly reason invariance. instance variational fair auto-encoder employs maximum mean discrepancy eliminate negative inﬂuence speciﬁc nuisance variables removing lighting conditions images predict person’s identity. similarly setting domain adaptation standard binary adversarial cost central moment discrepancy utilized learn features domain invariant. however invariance inducing criteria suffer similar drawback deﬁned measure divergence pair distributions. consequently express invariance belief w.r.t. pair values random variable time. attribute multinomial variable takes values combinatorial number pairs added express belief representation invariant attribute. problem even dramatic attribute represents structure exponentially many possible values attribute simply continuous variable. motivated aforementioned drawbacks difﬁculties work consider problem learning feature representation desired invariance. creating uniﬁed framework generic enough easily plugged different models ﬂexible express invariance belief quantities beyond discrete variables limited value choices. speciﬁcally inspired recent advancement adversarial learning formulate representation learning minimax game among three players encoder maps observed data deterministically feature space discriminator looks representation tries identify speciﬁc type variation hope eliminate feature predictor makes invariant representation make predictions typical discriminative models. provide theoretical analysis equilibrium condition minimax game give intuitive interpretation. three benchmark tasks different domains show proposed approach improves upon vanilla discriminative approaches encourage invariance also outperforms existing approaches enforce invariant features. given observation/input interested task predicting target based value using discriminative approach. addition access intrinsic attribute well prior belief prediction result invariant possible dependency scenarios here marginally independent. example image classiﬁcations lighting conditions identities persons independent. data generation process cases marginally independent. example fairness classiﬁcations sensitive factors gender. saving credit health condition person. related inherent bias within data. using latent variable model dependency data generation process show corresponding dependency graphs figure unlike vanilla discriminative models outputs conditional distribution model make predictions invariant intuition that explaining away effect independent conditioned although marginally independent. consequently accurate estimation intuitively inform guide model remove information undesired variations. example want learn representation image invariant lighting condition model learn brighten input knows original picture dark vice versa. also multi-lingual machine translation word surface form different meanings different languages. instance gift means present english means poison german. hence knowing language source sentence helps inferring meaning sentence conducting translation. input highly complicated structure employ dedicated model algorithm extract expressive representation thus extract representation want representation preserve variations necessary predict eliminating information achieve aforementioned goal employ deterministic encoder obtain representation encoding namely noted using additional input. given obtained representation target predicted predictor effectively models distribution construction instead modeling directly discriminative model formulate captures conditional distribution additional information coming surely feeding encoder means guarantees induced feature invariant thus order enforce desired invariance eliminate variations factor adversarial game introducing discriminator inspects representation ensure invariant concretely discriminator trained predict based encoded representation effectively maximizes likelihood simultaneously encoder ﬁghts minimize likelihood inferring correct discriminator. intuitively discriminator encoder form adversarial game discriminator tries detect attribute data encoder learns conceal note framework theory type data long represents attribute example real value scalar/vector take many possible values complex sub-structure parse tree natural language sentence. paper focus mainly instances discrete label multiple choices. plan extend framework deal continuous structured future. formally jointly play following minimax game hyper-parameter adjust strength invariant constraint true underlying distribution empirical observations drawn from. note problem domain adaption seen special case problem bernoulli variable representing domain model access target source domain training. section theoretically analyze given enough capacity training time whether minimax game converge equilibrium variations preserved variations removed. theoretical analysis done non-parametric limit i.e. assume model inﬁnite capacity. addition discuss equilibriums minimax game independent/dependent since discriminator predictor transformed deterministically substitute deﬁne joint distribution follows here used fact encoder deterministic transformation thus distribution merely delta function denoted intuitively absorbs randomness implicit distribution own. also note joint distribution depends transformation deﬁned encoder. thus equivalently rewrite objective analyze equilibrium condition objective ﬁrst deduce optimal discriminator optimal predictor given encoder prove global optimality minimax game. claim given ﬁxed encoder optimal discriminator outputs optimal predictor corresponds proof. proof uses fact objective functionally convex w.r.t. distribution taking variations obtain stationary point function detailed proof included supplementary material equilibrium analysis objective consists conditional entropies different signs. optimizing ﬁrst term amounts maximizing uncertainty inferring based essentially ﬁltering information representation. contrary optimizing second term leads increasing certainty predicting based implicitly objective deﬁnes equilibrium minimax game. win-win equilibrium firstly cases attribute entirely irrelevant prediction task terms reach optimum time leading win-win equilibrium. example lighting condition image removed still/better classify identity people image. enough model capacity optimal equilibrium solution would regardless value competing equilibrium however cases optimization objectives competing. example fair classiﬁcations sensitive factors gender help overall prediction accuracies inherent biases within data. words knowing help predicting since marginally independent learning fair/invariant representation harmful predictions. case optimality entropies cannot achieved simultaneously deﬁnes relative strengths objectives ﬁnal equilibrium. show general applicability framework experiment three different tasks including sentence generation image classiﬁcation fair classiﬁcations. different natures data present speciﬁc model instantiations use. sentence generation multi-lingual machine translation testbed sentence generation. concretely translation pairs several source languages target language. source sentence translated scalar denoting source language belongs translated sentence target language. recall used input obtain language-invariant representation. make full employ separate encoders encs sentences language words encs encs different encoder. representation sentence captured hidden states lstm encoder time step. employ single lstm predictor different encoders. often used language generation probability output predictor parametrized autoregressive process i.e. lstm attention model compute discriminator also parameterized lstm gives enough capacity deal input multiple timesteps. instantiated multinomial distribution computed softmax layer last hidden state discriminator lstm. classiﬁcation classiﬁcation experiments input either picture feature vector. three players minimax game constructed feedforward neural networks. feed encoder embedding vector. possible approaches optimize framework adversarial setting. ﬁrst similar alternating approach used generative adversarial nets alternately train adversarial components freezing third one. approach control balancing encoder discriminator effectively avoids saturation. another method train three components together gradient reversal layer particular encoder admits gradients discriminator predictor gradient discriminator negated push encoder opposite direction desired discriminator. chen found second approach easier optimize since discriminator encoder fully sync optimized altogether. hence adopt latter approach. experiments adam learning rate section perform empirical experiments evaluate effectiveness proposed framework. ﬁrst introduce tasks corresponding datasets consider. then present quantitative results showing superior performance proposed framework discuss qualitative analysis veriﬁes learned representations desired invariance property. experiments include three tasks different domains fair classiﬁcation predictions unaffected nuisance factors; language-independent generation conducted multi-lingual machine translation problem; lighting-independent image classiﬁcation. fair classiﬁcation fair classiﬁcation three datasets predict savings credit ratings health conditions individuals variables gender speciﬁed nuisance variable would like consider decisions german dataset small dataset samples describing whether person good credit rating. sensitive nuisance variable factored gender. adult income dataset data points objective predict whether person savings dollars sensitive factor age. task health dataset predict whether person spend days hospital following year. sensitive variable also dataset contains entries. follow -fold train/validation/test splits feature preprocessing used encoder predictor parameterized single-layer neural networks. three-layer neural network batch normalization employed discriminator. batch size number hidden units experiments. multi-lingual machine translation multi-lingual machine translation task french english german english pairs iwslt dataset pairs fr-en sentences pairs de-en sentences training set. test pairs fr-en sentences pairs de-en sentences. evaluate bleu scores using standard moses multi-bleu.perl script. here indicates language source sentence. opennmt multi-lingual experiments. encoder two-layer bidirectional lstm units direction. discriminator one-layer single-directional lstm units. predictor two-layer lstm units attention mechanism follow johnson byte pair encoding subword units cross-lingual input. every model epochs. batch size image classiﬁcation extended yale dataset image classiﬁcation task. comprises face images people different lighting conditions upper right lower right lower left upper left front. variable purged lighting condition. label identity person. follow louizos train/test split validation used samples used training data points used testing. one-layer neural network encoder one-layer neural network prediction. discriminator two-layer neural network batch normalization. batch size hidden size fair classiﬁcation results three fairness tasks shown figure compare model prior works learning fair representations learning fair representations variational fair autoencoder results directly using representation also shown. ﬁrst study much information retained learned representation using logistic regression predict factor cannot recognized representations learned three models targeting fair representations. accuracy classifying similar trivial baseline predicting majority label shown black line. performance predicting label shown second row. vfae suffer adult german datasets removing information comparison model’s performance suffer even making fair predictions. speciﬁcally german model’s accuracy compared achieved vfae lfr. adult model’s accuracy vfae accuracies respectively. health dataset models’ performances barely better majority baseline. unsatisfactory performances models extreme imbalance dataset data label. also investigate fair representations would alleviate biases machine learning models. measure unbiasedness evaluating models’ performances identifying minority groups. instance suppose task predict savings nuisance factor savings threshold adequate otherwise insufﬁcient. people advanced generally fewer savings biased model would tend predict insufﬁcient savings advanced age. contrast unbiased model better factor information recognize people stereotypes. concretely groups pooled possible value seek minority groups deﬁne minority biased category group. ﬁrst calculate accuracy biased category report average performance categories. compute instance-level average performance since category hold dominant amount data among categories. figure fair classiﬁcation results different representations. denotes directly using observation representation. black lines ﬁrst second show performance predicting majority label. biased categories third explained fourth paragraph section shown third figure german adult achieve higher accuracy biased categories even though overall accuracy similar lower baseline employ fairness constraints. speciﬁcally adult performance biased categories baseline’s accuracy german accuracy biased categories baseline achieves results show model able learn unbiased representation. multi-lingual machine translation results systems multi-lingual machine translation shown table compare model attention based encoder-decoder trained bilingual data multi-lingual data encoderdecoder trained multi-lingual data employs single encoder source languages. firstly multi-lingual systems outperform bilingual encoder-decoder even though multi-lingual systems similar number parameters translate languages shows learning figure t-sne visualizations images extended yale original pictures clustered lighting conditions representation learned model clustered identities individuals invariant representation leads better generalization case. better generalization transferring statistical strength data languages. comparing multi-lingual systems model outperforms baseline multi-lingual system languages improvement french-to-english bleu score. also verify design decisions framework ablation studies. firstly without discriminator model’s performance worse standard multi-lingual system rules possibility gain model comes parameters separating encoders. secondly employ separate encoders model’s performance deteriorates difﬁcult learn cross-lingual representation veriﬁes theoretical advantage modeling instead mentioned section intuitively german french different grammars vocabulary hard obtain uniﬁed semantic representation performing operations. means encoder needs enough capacity reach equilibrium minimax game. also observe discriminator needs enough capacity provide faithful gradients towards equilibrium. speciﬁcally instantiating discriminator feedforward neural network w./w.o. attention mechanism work experiments. image classiﬁcation report results table baselines regularizations remove lighting conditions. advantage factoring lighting conditions shown improved accuracy classifying identities best baseline achieves accuracy terms removing framework ﬁlter lighting conditions since accuracy classifying drops shown table also visualize learned representation t-sne comparison visualization original pictures figure that without removing lighting conditions images clustered based lighting conditions. removing information lighting conditions images clustered according identity person. speciﬁc case problem takes values domain adaption attracted large amount research interest. domain adaptation aims learn domain-invariant representations transferable domains. example image classiﬁcation adversarial training shown able learn invariant representation across domains enables classiﬁers trained source domain applicable target domain. moment discrepancy regularizations also effectively remove domain speciﬁc information purpose. learning language-invariant representations classiﬁers trained source language applied target language works targeting development fair bias-free classiﬁers also learn representations invariant nuisance variables could induce bias hence makes predictions fair data-driven models trained using historical data easily inherit bias exhibited data. zemel proposes regularize distance representation distributions data different nuisance variables enforce fairness. variational fair autoencoder targets problem variational autoencoder approach maximum mean discrepancy regularization. work also related learning disentangled representations separate different inﬂuencing factors input data different parts representation. ideally part learned representation marginally independent other. early work tenenbaum freeman propose bilinear model learn representation style content disentangled. information theory perspective chen augments standard generative adversarial networks inference network whose objective infer part latent code leads generated sample. information carried chosen part latent code retained generative sample leading disentangled representation. discussed section methods bear drawback cost used regularize representation pairwise scale well number values attribute take could large. louppe propose adversarial training framework learn representations independent categorical continuous variable. basic assumption theoretical analysis attribute irrelevant prediction limits capabilities analyzing fairness classiﬁcations. propose generic framework learn representations invariant speciﬁed factor trait. cast representation learning problem adversarial game among encoder discriminator predictor. theoretically analyze optimal equilibrium minimax game evaluate performance framework three tasks different domains empirically. show invariant representation learned resulting better generalization improvements three tasks.", "year": 2017}