{"title": "Dependency Parsing with LSTMs: An Empirical Evaluation", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "We propose a transition-based dependency parser using Recurrent Neural Networks with Long Short-Term Memory (LSTM) units. This extends the feedforward neural network parser of Chen and Manning (2014) and enables modelling of entire sequences of shift/reduce transition decisions. On the Google Web Treebank, our LSTM parser is competitive with the best feedforward parser on overall accuracy and notably achieves more than 3% improvement for long-range dependencies, which has proved difficult for previous transition-based parsers due to error propagation and limited context information. Our findings additionally suggest that dropout regularisation on the embedding layer is crucial to improve the LSTM's generalisation.", "text": "propose transition-based dependency parser using recurrent neural networks long short-term memory units. extends feedforward neural network parser chen manning enables modelling entire sequences shift/reduce transition decisions. google treebank lstm parser competitive best feedforward parser overall accuracy notably achieves improvement long-range dependencies proved difﬁcult previous transition-based parsers error propagation limited context information. ﬁndings additionally suggest dropout regularisation embedding layer crucial improve lstm’s generalisation. complementary approaches transitionbased dependency parsers emerged recently. feature engineering approach relies handcrafted feature templates model interactions between sparse lexical features. manually crafting feature templates requires substantial expertise extensive trial-and-error approach state-of-the-art parsers many languages contrast neural network approach enables automatic learning feature combinations non-linear hidden layers mitigates sparsity issues sharing similar low-dimensional distributed representations related words architecture chen manning parser performs training oracle conﬁguration independently another disregarding fact oracles training sentence represent whole sequence intertwined decisions. proposed extension uses recurrent neural network long short-term memory units time step transition system lstm theoretical access entire history past decisions lstms naturally suited modelling sequences shown promising results e.g. machine translation text-vision modelling particularly focus lstm’s performance identifying long-range dependencies. dependencies proved difﬁcult greedy transition-based parsers including feedforward baselines train oracle independently. difﬁculty attributed main reasons long-range dependencies ambiguous classiﬁers access limited context window longer arcs constructed shorter arcs transition-based parsing thus increasing chance error propagation. contrast lstm abilities modelling whole sequences training oracles memorise past context information likely beneﬁcial longer dependencies. despite lstm’s theoretical advantages practice prone overﬁtting feedforward architecture even number parameters. additional contribution work empirical investigation suggests dropout particularly applied embedding layer substantially improves lstm’s generalisation ability regardless hidden layer size. model extension chen manning uses feedforward neural network predict next transition arc-standard system. arc-standard conﬁguration consists buffer stack dependency arcs parse tree built successively making transitions features concatenation embeddings words ﬁrst second left-/right-most children words leftmost leftmost rightrightmost children words conﬁguration time neural network ﬁrst computes hidden layer input calculates probability transition output vector importantly adds inputs based past information addition previous state leads recurrence enables modelling training entire sequence transitions. recurrence cause vanishing gradient problem lstm architecture solves introducing memory cells could store information long time intervals keep gradients diminishing. input gates control stored memory cell output gates control whether stored information used computations. allows information beginning sentence inﬂuence transition actions sentence. forget gates used erase information current memory cell. crucially lstm uses input predictions also exploits values previous memory cell hidden layer gates values gates bounded sigmoid multiplication components modulates information passed through. furthermore investigate models’ performance long-range dependencies reporting result terms labelled precision recall breakdown dependency lengths test table result also plotted figures despite models’ similar overall accuracy lstm model outperforms cubic baseline precision recall dependency lengths greater lstm’s performance degrades slowly dependency length increases. discover regularisation important lstm parser feedforward architectures. table compares relative improvement dropout feedforward lstm constraining models number parameters corresponding hidden units lstm. observe lstm becomes competitive dropout. conﬁgurations oracle transition actions time sentence maximise log-likelihood oracle transition actions given equation parameters including word label embeddings probability parser takes transition action time optimise gradient backpropagation time sentence feeding parser gold sequence conﬁgurations {cit}|si| parser reaches ﬁnal conﬁguration gradients backpropagated prediction time time conducted experiments google treebank consisting portion ontonotes corpus additional domains dependency types. models trained training corpus parameters optimised using baselines re-implemented chen manning parser setting including results feedforward model tanh activation function better-performing cubic counterpart. training done maximum epochs stopped early better found consecutive epochs. result google treebank summarised table represent feedforward baselines tanh cubic activations respectively. lstm model outperforms feedforward baseline tanh activation function achieving competitive accuracy cubic baseline. lstm trained adadelta optimiser using decay rate embeddings similarly initialised feedforward baselines weight connections initialised using mechanism glorot bengio used automatic tags stanford bi-directional tagger tagging accuracies domains. training set). results dropout regularisation table along epoch best found. indicate dropout embedding-hidden hidden-output connections respectively. dropout generally results slower convergence technique outperforms signiﬁcantly improves model’s accuracy importantly found input dropout crucial hiddenoutput dropout achieves accuracy dropout input hidden layers suggesting model achieve good accuracy input dropout alone. found dropout rates effective. further found dropout generally improves lstms recently various neural network models achieved state results many parsing tasks languages including google treebank dataset used paper. vinyals used lstms sequence-to-sequence constituency parsing makes prior assumption parsing problem. dependency parsing stenetorp presented compositional model similar constituency parser socher recently works dyer kiperwasser goldberg proposed transition-based lstm models automatically extract real-valued feature vectors parser conﬁguration. transition-based parser dyer used stack lstm architecture composition functions obtain continuous low-dimensional representation stack represent partial trees along buffer history actions. work stack lstm model similarly used greedy decoding although primary difference used lstm form temporal recurrence hidden states. used feature extraction template chen manning replaced feedforward connections lstm network dyer instead used stack lstm means extract dense features parser conﬁguration without explicit temporal recurrence. neural network models also used structured training transition-based parsing achieving state results various dataset. weiss used structured perceptron model feedforward transition-based dependency parser. augmented tritraining method unlabelled data model achieved impressive domain data google treebank similarly used work. zhou used beam search contrastive learning maximise probability entire gold sequence respect sequences beam. andor similarly proposed globally normalised model using beam search conditional random fields loss achieved state results benchmark english dataset. parsing model similar used temporal recurrence hidden states parsing although lstms instead elman rnns. work additionally investigates effect dropout model performance demonstrate efﬁcacy temporal recurrence better capture longrange dependencies. present transition-based dependency parser using recurrent lstm units. motivation exploit entire history shift/reduce transitions making predictions. lstm parser competitive feedforward neural network parser chen manning overall notably improves accuracy longrange dependencies. also show importance dropout particularly embedding layer improving model’s accuracy.", "year": 2016}