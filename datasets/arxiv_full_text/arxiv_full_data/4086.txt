{"title": "Local Similarities, Global Coding: An Algorithm for Feature Coding and  its Applications", "tag": ["cs.CV", "cs.AI"], "abstract": "Data coding as a building block of several image processing algorithms has been received great attention recently. Indeed, the importance of the locality assumption in coding approaches is studied in numerous works and several methods are proposed based on this concept. We probe this assumption and claim that taking the similarity between a data point and a more global set of anchor points does not necessarily weaken the coding method as long as the underlying structure of the anchor points are taken into account. Based on this fact, we propose to capture this underlying structure by assuming a random walker over the anchor points. We show that our method is a fast approximate learning algorithm based on the diffusion map kernel. The experiments on various datasets show that making different state-of-the-art coding algorithms aware of this structure boosts them in different learning tasks.", "text": "sparse coding refers class algorithms sparse representations level features. unsupervised learning algorithm given unlabeled input data learns complete bases coding vectors capture high-level features images. sparse coding uses linear combination small number bases best represent input image reconstruction based optimization problem. authors state sparse coding method similar done visual cortex bases resemble receptive ﬁelds. sparse coding problem formulated follows controls sparsity coding schema. enforce sparsity constraint problem must solved called optimization. however proven ﬁnding optimal solution optimization np-hard. solve problem approximately many optimization approaches proposed solve problem greedy approaches others convexify problem replacing norm making dictionary speciﬁc dataset several methods proposed learn instead using predeﬁned dictionaries. recently supervised dictionary learning methods also proposed increase discriminant power sparse coding algorithm. sparse coding variations successfully applied many computer vision tasks like image denoising image classiﬁcation face recognition regularized sparse coding extension sparse coding proposed cope data manifold structure enforces smooth variation sparse codes respect manifold structure data points adding graph xy∈x− c)wxy sparse coding optimization function reﬂects local similarity data points ambient space. abstract—data coding building block several image processing algorithms received great attention recently. indeed importance locality assumption coding approaches studied numerous works several methods proposed based concept. probe assumption claim taking similarity data point global anchor points necessarily weaken coding method long underlying structure anchor points taken account. based fact propose capture underlying structure assuming random walker anchor points. show method fast approximate learning algorithm based diffusion kernel. experiments various datasets show making different state-of-the-art coding algorithms aware structure boosts different learning tasks. building block many different image processing algorithms extract high level representation images order transform nonlinear high dimensional learning problem low-level feature space simpler coding space coding used wide variety applications computer vision like background modeling super resolution tracking detailed review) object recognition image classiﬁcation pose estimation image annotation assuming denotes level representation image dictionary matrix composed learned visual bases columns. goal coding algorithm dimensional vector i’th element indicates afﬁnity data point i’th basis many coding algorithms recently proposed different approaches compute afﬁnities based similarity measurement others reconstruction criteria primitive methods called vector quantization feature vector assigned nearest basis learned k-means clustering. approach information loss representing image feature high hardassignment nature. soft assignment coding visual word ambiguity modeled assigning feature number labeled points limited many data points bases local linear patch intersection linear patches around support vectors. points fails predict label result cross validation phase parameters coding method chosen number bases contribute code datum increases bases selected local linear patch around datum. smoothness labeling function many learning algorithms based assumption labeling function changes smoothly respect underlying distribution data given smoothness assumption holds dataset likely data points share linear local neighborhoods still close enough considered context. case kernel function capable capturing similarities among data points must reasonable non-zero values within global non-linear patch. consequently encoder tries improve performance learning algorithm using bases linear local patch around data point coded. distinctive neighborhood structure since distribution bases varies different regions feature space neighborhood structure data point distinctive region. however methods globally ﬁxed number neighborhood bases achieve good performance average. considering ﬁxed number bases around data point variations distribution density bases regions selected bases regions large number bases outside linear local patch feature selected. insufﬁcient number bases computational complexity memory limitations number bases usually limited thousands. therefore regions feature space selected bases linear local patch around data point sparse leading unstable inaccurate coding. alleviating problem reasonable coding method bases local patch sparse regions. growing window size increases performance algorithm offering global kernel quality coding method degrades since selected bases close linear patch primal assumption behind local encoders tenable anymore. paper solve aforementioned problem propose method dubbed local similarities global coding coding coefﬁcients calculated accurately respect small windows containing linear patch underlying bases structure. random walker connects coding information window together computes ﬁnal code data point. conclude locality essential sparsity. based idea local coordinate coding proposed uses assumption despite features nonlinear structure high dimensional space manifold composed locally linear patches. achieved state-of-the-art performance linear classiﬁer digit classiﬁcation nevertheless it’s high computational cost suitable large-scale learning problems. large-scale version named locality-constrained linear coding proposed. guarantees locality incorporating k-nearest bases coding process minimizes reconstruction term local linear patches bases cknn contains non-zero coefﬁcients columns matrix k-nearest bases data point dictionary learned k-means clustering algorithm. recent years many locality-constrained coding methods proposed applied successfully image classiﬁcation large scale object categorization general object tracking assumption behind locality constraint methods datum bases incorporated coding almost linear local patch. furthermore recent researches coding problem show sparse coding many well-known datasets leads locality preserving codes concomitantly. researches reveal fact sparse coding usually works well bases incorporated coding datum close linear patch. however situations assumption completely true. example figure datum coded respect bases nonlinear path. window around datum shows local bases used code. clear window size grows error approximating local bases structure linear increases coding methods based simplifying assumption fail. explained situation rather simple example experiments show coding methods tendency bases outside local linear patches code data real world datasets. phenomena result following reasons fig. comparing local coding method lsgc datum coded respect bases nonlinear path. window around datum shows local bases contributed coding local coding method large window effect violates assumption local bases lied dimensional linear subspace leads inaccurate code. lsgc coding coefﬁcients calculated accurately respect small windows containing linear patch underlying structure. compare performance lsgc usps coil datasets respectively. data points sorted around query image based value linear kernel function corresponding coding space. linear kernel coding space high values images classes class query image experiment difﬁcult query images selected clarify effect choosing small window sizes. note parameter lsgc tuned best accuracy image classiﬁcation task. precise ﬁrst calculate coding vector basis respect bases somehow captures underlying local structure bases. then weighted graph constructed whose nodes weight deduced coding instance corresponding basis similar deduced coding basis corresponding basis finally coding obtained naturally random walk constructed graph. though encoding appears similar markov random ﬁeld classiﬁcation task lsgc differs basically lsgc runs graph whose nodes learned visual bases. process depicted figure using small windows calculate coding vectors coding algorithm free bases outside local linear patch around data point estimated codes remain accurate. theoretically show linear kernel function lsgc coding space approximates diffusion kernel used repeatedly manifold learning applications. shown approximation error converges zero rate number bases. compared diffusion kernel lsgc coding method inductive fast makes appropriate large scale settings. furthermore mapping input data coding space applying fast linear method training testing preliminary conference version paper appears extend work following three aspects compared present theoretical justiﬁcation proposed encoding algorithm reveals components success coding scheme. lsgc encoder based sparse coding besides encoder appeared show proposed method used framework coding applications introduce study different versions lsgc encoder based sparse coding besides encoder appeared validate effectiveness lsgc encoder expand experiments several datasets ways. first behaviors proposed methods studied vast variety settings. second clustering regression problems learning tasks support main contribution paper thoroughly. rest paper organized follows. section ii-a assuming relations deﬁned coding method propose lsgc encoding algorithm. section ii-b discusses ways used deﬁne matrix encoding method. present experimental evaluations encoding algorithm three learning task section compare proposed method several cost complexity coding data points number images number visual bases number non-zero elements small locality sparsity constraint. many learning tasks number images order thousands. however number bases constant relatively small make algorithm practical interest number images relatively large. computational cost reasonable compared algorithms reviewed previous sections cost computational efﬁciency practice linear learning algorithm used explicitly mapping points coding space studying kernel behind coding algorithm help assess properties coding scheme. given linear learning algorithm used coding space problem viewed nonlinear based kernel corresponding coding algorithm. lsgc kernel function residual term value paths t-steps visited least times visited least times. lemma easily validated algebraically expanding ct.ct using chapman-kolmogorov equation considering fact paths contribute ˜pt− equation visit since included graph seen lemma residual term small enough respect kernel behind proposed method approximation well-known diffusion kernel calculates similarities graph nodes considering paths connecting speciﬁc step length. since close points graph connected several paths other kernel assigns higher values close points underlying structure. worth noting diffusion kernel successfully applied several applications manifold learning problems assume relation matrix given. since encoding algorithm used framework nature properties speciﬁc matrix needs computed coding algorithms. subsection methods computing proposed sparse coding details constructing adjacency matrices encoding algorithms follows relations bases coding vector positive elements computed corresponding coding algorithm. computation depends coding algorithm used framework discussed shortly section. diagonal matrix elements denotes degrees edges connecting i’th basis others. easily shown normalization element matrix shows dependency basis calculated conventional coding algorithms viewed one-step random walk framework proposed method utilizes t-step walks capture nonlinear dependencies among bases follows sparse coding pairwise relations bases coding method based contribution bases sparse codes other. purpose exclude basis coded dictionary compute sparse code original formulation algorithm dictionary without basis matrix learned negative values. duplicating number bases letting sparse codes length however result optimization problem determined solution equation sparse codes ﬁrst bases sparse codes remaining bases therefore non-negativity constraint trivially satisﬁed. positive relation matrix bases written columns k-nn bases elements rknn non-zero relations k-nn bases. locality constraint method situation little different sparse coding non-negativity relation matrix truly satisﬁed duplicating bases. adding −dbi local dictionary bases locality basis coded basic assumption method satisﬁed. however amplitude coefﬁcients code naturally captures bases relations feature space. consequence suggested previously absolute value coefﬁcients weights among bases |r|. satisfy positivity constraint. world dataset shown table datasets belong three main categories objects hand written digits hand written letters. size datasets vary instances number attributes vary sparse coding algorithm features normalized unit norm deriving semantically reasonable coding vectors. worth mention that processing time method close original encoding algorithm. fact cost computing matrix matrix vector multiplication equation compared computational cost corresponding coding method takes less time average. consequently processing times reported tables. detailed settings algorithms described experiment accordingly. part primary goal learn nonlinear function deﬁned spiral dataset. experiment points sampled well-known spiral shaped manifold shown figure experiment bases learned data points used training. performance different algorithms compared regression task using ridge regression algorithm coding space. results shown figure image shows results regression different coding methods corresponding function. average root mean squared error independent runs also reported method. methods parameters tuned achieve best result. sparse coding method intentionally excluded experiment since reported sparse coding fails capture nonlinear structure spiral poor performance dataset. since method employ smoothness assumption able predict values function points distant labeled points. however lsgc much better performance predicts labels unlabeled points labeled points accurately. hand although result smooth regions shifted true values. smoothness predicted function smooth nature gaussian kernel considering uses euclidean distance value function parts highly affected others small euclidean distance large manifold distance. fig. contribution bases sample data point. filled circles show bases position radius bases reﬂects amplitude corresponding coding coefﬁcients coding query data point depicted square ﬁgure. blue colors show positive negative coefﬁcient values respectively. section compare classiﬁcation performance lsgc original representation three types coding methods similarity based soft assignment coding locality based local coordinate coding locality-constrained linear coding sparsity based sparse coding regularized sparse coding method train linear kernel coding space class method evaluate performance different algorithms. methods parameters fold cross validation. wide range test values selected ensure proper value method tested cross validation. test values sparse regularization parameter test values k-nn. test best bandwidth parameter sac. scaling factor mean standard deviation data points used since proper value depends variance data points. test parameters used reported explore later lsgc results sensitive parameter base method base method coding vectors local enough. globalization ﬁnal coding vector controlled step size parameter thus test limited values base methods step size parameter selected algorithm best performance dictionary size reported. tables ii-vi average performance algorithms independent shown. generally lsgc improves accuracy base methods propagating coefﬁcients. improvement tangible especially fig. nonlinear regression results spiral dataset. goal learn nonlinear function ﬁgure ﬁgure colors indicate values function data point. root mean square error ridge regression method reported method. take closer look method assigns coding coefﬁcients data point depicted contribution bases sample data point figure eliminating dimensions better visibility. seen although assigns codes smooth manner respect euclidean distance bases data point cannot said manifold distance considered. fact bases large manifold distance sample point contributions mislead regressor cause shift true values discussed above. similarly assigns meaningful codes bases near data point bases outside linear local patch performance algorithm degrades. however lsgc leads global coding vector contribution bases coding vector reduces respect manifold distance data point. large window size results degraded classiﬁcation accuracy since coding coefﬁcients assigned respect euclidean distance bases. phenomena restricts bases linear local patch data point. evaluation methods evaluate clustering performance normalized mutual information accuracy standard criteria used. criteria true label points considered true cluster labels computed cluster labels compared them. assume true clusters computed clusters labeled indices. e.g. contains points i’th class points i’th computed cluster. mutual information sets computed follows ci∈cc probabilities selected point clusters probability selected point belongs clusters simultaneously. computed mutual information interval normalization applied standard entropy corresponding set. easily veriﬁed match completely regardless labels equal one. criterion ﬁrst computed cluster labeled classes dataset kuhn-munkres algorithm denote functions return true computed cluster label given point respectively map) kuhn-munkres mapping function maps value value range function accuracy clustering algorithm deﬁned follows experiments parameters method selected empirically best performance. however methods ﬁxed k-nn parameter core parameter core sparse encoder datasets. experiment performed times average performance reported. also k-means algorithm performed times different random initialization best result reported method. tables viii show measure methods different number clusters coil digit datasets respectively. tables mentioned number classes selected randomly dataset run. seen tables modiﬁed methods outperform others vast margin. number labeled data point limited. however letter dataset lsgc fails improve performance base methods. reason that roughly speaking letter dimensional dataset thus bases could reside much lower dimensional structure. figure show inﬂuence different choices classiﬁcation performance lsgc. shown parameter ensures coding vectors base method local enough algorithm performance sensitive exact value. prior knowledge help decrease number test values cross validation increase speed algorithm. another application coding algorithms investigate effectiveness different methods image clustering. comparisons made coil digit datasets well-known application. different coding methods studied sparse coding regularized sparse coding lsgc. experiments ﬁrst applied reduce dimensionality preserving percent data energy k-means performed coding space cluster data. also k-means data baseline n-cut famous spectral clustering algorithm reported following experiments. theoretical point view linear kernel coding space approximation diffusion kernel wellknown kernel manifold learning literature. experimental results different learning tasks show effectiveness method. assumption implicitly method bases imitate structure data points. thus value graph containing bases intermediate nodes approximates transition probability graph data points. therefore remains open issue theoretically bound approximation error graph contains bases nodes inner bases. idea lower bound ˜pj/˜pj arbitrary points graph substitute start following trivial inequality diameter sphere containing sampled data points. limited support data distribution guarantees existence have huang yang kulikowsk robust tracking using local sparse appearance model k-selection computer vision pattern recognition ieee conference ieee gemert veenman smeulders geusebroek visual word ambiguity ieee transactions pattern analysis machine intelligence vol. yang gong huang linear spatial pyramid matching using sparse coding image classiﬁcation computer vision pattern recognition cvpr ieee conference pati rezaiifar krishnaprasad orthogonal matching pursuit recursive function approximation applications wavelet decomposition signals systems computers conference record twenty-seventh asilomar conference ieee aharon elad bruckstein k-svd algorithm designing overcomplete dictionaries sparse representation signal processing ieee transactions vol. lewicki sejnowski learning overcomplete representa zheng chen wang zhang graph regularized sparse coding image representation image processing ieee transactions vol. w.-h. tsang l.-t. chia laplacian sparse coding hypergraph laplacian sparse coding applications ieee transactions pattern analysis machine intelligence zhang improved local coordinate coding using local tangents proc. intl conf. machine learning zhang ladicky torr saffari learning anchor planes classiﬁcation advances neural information processing systems semi-supervised learning literature survey belkin niyogi sindhwani manifold regularization geometric framework learning labeled unlabeled examples journal machine learning research vol. jaakkola partially labeled classiﬁcation markov random walks advances neural information processing systems proceedings conference vol. press coifman lafon maggioni nadler warner zucker geometric diffusions tool harmonic analysis structure deﬁnition data diffusion maps proceedings national academy sciences united states america vol. shaban rabiee farajtabar ghazvninejad from local similarity global coding; application image classiﬁcation computer vision pattern recognition ieee conference lerman shakhnovich deﬁning functional distance using manifold embeddings gene ontology annotations proceedings national academy sciences vol. singer erban kevrekidis coifman detecting intrinsic slow variables stochastic dynamical systems anisotropic diffusion maps proceedings national academy sciences vol. amirreza shaban received b.sc. degree school electrical computer engineering university tehran iran m.sc. degree computer engineering sharif university technology iran currently pursuing ph.d. degree ohio state university. since fall department computer science engineering ohio state university. currently working prof. belkin ph.d. degree. research interests include machine learning image processing computer vision. hamid rabiee received b.s. m.s. degrees electrical engineering csulb electrical computer engineering ph.d. electrical computer engineering purdue university west lafayette member technical staff at&t bell laboratories. worked senior software engineer intel corporation. also universities adjunct professor electrical computer engineering since september joined sharif university technology tehran iran. founder sharif universitys advanced information communication technology research center sharif university advanced technologies incubator sharif digital media laboratory mobile value added services laboratories. currently professor computer engineering sharif university technology. initiator director national international level projects context undp international open source network iran’s national development plan. received numerous awards honors industrial scientiﬁc academic contributions acted chairman number national international conferences holds three patents. also senior member ieee. mahyar najibi received b.sc. degree school computer engineering iran university science technology studied industrial engineering second b.sc. major. received m.sc. degree artiﬁcial intelligence school computer engineering sharif university technology iran currently member sparse signal processing laboratory sharif university technology. research interests mainly focused machine learning computer vision sparse signal processing.", "year": 2013}