{"title": "Learning to Play with Intrinsically-Motivated Self-Aware Agents", "tag": ["cs.LG", "cs.AI", "cs.CV", "stat.ML", "68"], "abstract": "Infants are experts at playing, with an amazing ability to generate novel structured behaviors in unstructured environments that lack clear extrinsic reward signals. We seek to mathematically formalize these abilities using a neural network that implements curiosity-driven intrinsic motivation. Using a simple but ecologically naturalistic simulated environment in which an agent can move and interact with objects it sees, we propose a \"world-model\" network that learns to predict the dynamic consequences of the agent's actions. Simultaneously, we train a separate explicit \"self-model\" that allows the agent to track the error map of its own world-model, and then uses the self-model to adversarially challenge the developing world-model. We demonstrate that this policy causes the agent to explore novel and informative interactions with its environment, leading to the generation of a spectrum of complex behaviors, including ego-motion prediction, object attention, and object gathering. Moreover, the world-model that the agent learns supports improved performance on object dynamics prediction, detection, localization and recognition tasks. Taken together, our results are initial steps toward creating flexible autonomous agents that self-supervise in complex novel physical environments.", "text": "figure agent embedded three-dimensional environment move around apply forces visible objects close proximity receive visual input. policies allow agent learn general-purpose world-model? contrast human infants exhibit wide range interesting apparently spontaneous visuo-motor behaviors including navigating environment seeking attending novel objects engaging physically objects novel surprising ways short young children excellent playing scientists crib create intentionally events informative exciting aside play behaviors active learning process driving self-supervised learning representations underlying sensory judgments motor planning capacities observations infant play improve artiﬁcial intelligence? theorists long realized playful behavior absence rewards mathematically formalized loss functions encoding intrinsic reward signals agent chooses actions result novel predictable states maximize learning ideas rely virtuous cycle agent actively self-curricularizes pushes boundaries world-model-prediction systems achieve. world-modeling capacity improves used novel becomes cycle starts again. here build ideas using tools modern deep reinforcement learning create artiﬁcial agent learns play. construct simulated interactive physical environment agent move around infants experts playing amazing ability generate novel structured behaviors unstructured environments lack clear extrinsic reward signals. seek mathematically formalize abilities using neural network implements curiosity-driven intrinsic motivation. using simple ecologically naturalistic simulated environment agent move interact objects sees propose world-model network learns predict dynamic consequences agent’s actions. simultaneously train separate explicit selfmodel allows agent track error world-model uses self-model adversarially challenge developing world-model. demonstrate policy causes agent explore novel informative interactions environment leading generation spectrum complex behaviors including ego-motion prediction object attention object gathering. moreover world-model agent learns supports improved performance object dynamics prediction detection localization recognition tasks. taken together results initial steps toward creating ﬂexible autonomous agents self-supervise complex novel physical environments. truly autonomous artiﬁcial agents must able discover useful behaviors complex environments without humans present constantly pre-specify tasks rewards. ability beyond today’s advanced autonomous robots. example nasa’s curiosity rover explore mars pre-conﬁgured task programs. severely limits curiosity’s long-term utility cannot tasks help learn take better advantage martian environment time. ∗equal contribution department psychology computer science stanford university stanford usa. correspondence nick haber <nhaberstanford.edu> damian mrowca <mrowcastanford.edu> figure intrinsically-motivated self-aware agent architecture. world-model solves dynamics prediction problem. simultaneously self-model learned seeks predict world-model’s loss. actions chosen antagonize world-model leading novel surprising events environment physically objects sees world interesting interactions possible sparse unless actively sought. describe neural network architecture agent learns world-model predicts consequences agent’s actions either forward inverse dynamics prediction. agent optimizes accuracy world-model separate explicit selfmodel neural network simultaneously learns predict errors agent’s world-model. based selfmodel agent uses action policy seeks take actions adversarially challenge current state world-model. demonstrate intrinsicallymotived self-aware architecture stably engages virtuous reinforcement learning cycle described above spontaneously learning understand self-generated ego-motion selectively attention localize recognize interact objects without concepts built learning occurs emergent active self-supervised process capacities arise distinct developmental milestones like human infants. results steps toward creating mathematically well-motived ﬂexible autonomous agents intrinsic motivation learn spontaneously generate useful behaviors adapting unknown environments. work connects variety existing ideas selfsupervision active learning deep reinforcement learning. basic level auto-encoders develop representations reconstructing input images explicit self-supervised auxiliary tasks include semantic segmentation pose estimation solving jigsaw puzzles colorization rotation self-supervision videos form future frame prediction potential surpass performance aforementioned methods challenge facing frame prediction sequences recorded videos boring little interesting dynamics occurring frame next. order encourage interesting events happen useful agent capacity interact environment least select data sees training. traditional active learning agent seeks learn supervised task using little labeled data possible ability request labeled data necessary recent optimization methods trade-off uncertainty diversity obtain diversiﬁed sets hard examples heuristics assign labels data examples high conﬁdence querying labels examples conﬁdence going beyond selection examples pre-determined recent work robotics study learning tasks interactive visuo-motor environments. particular finn levine ebert tried learn self-supervised visuo-motor tasks robot arms. results promising suffer challenges predict forward dynamics pixel space orchestrate random pushing motions generate training data. works intrinsically driven mechanism would bias robot explore environment structured way. demonstrated reasonable exploration-exploitation tradeoffs achieved intrinsic reward terms formulated information gain. frank information gain maximization implement artiﬁcial curiosity humanoid robot. kulkarni combine intrinsic motivation hierarchical action-value functions operating different temporal scales goal-driven deep reinforcement learning. achiam sastry formulate surprise intrinsic motivation kl-divergence true transition probabilities learned model probabilities. held generator network optimized using adversarial training produce tasks always appropriate level difﬁculty agent automatically produce curriculum navigation tasks learn. jaderberg show target tasks improved using auxiliary intrinsic rewards. closest work formulation intrinsic reward signal pathak work uses curiosity antagonize future prediction signal latent space inverse dynamics prediction task improve learning video games showing intrinsic motivation leads faster ﬂoor-plan exploration game environment. work differs using physically realistic threedimensional environment shows context intrinsic motivation lead substantially sophisticated agent-object behavior generation also show learned representation transfers improved performance analogs real-world visual tasks object localization recognition. underlying difference technical approach introduction explicit self-model representing agent’s awareness internal state. difference viewed terms explicit model-based architecture place model-free setup. knowledge self-supervised setup explicitly self-modeling agent uses intrinsic motivation learn restructure environment explored prior work. agent situated physically realistic simulated environment built unity along several objects. objects interact according newtonian physics simulated physx engine. agent’s avatar sphere swivels place moves around receives images forward-facing camera agent apply forces torques three dimensions object view within ﬁxed distance agent’s position. although ﬂoor walls environment static agent objects collide them. action space agent subset ﬁrst dimensions specify ego-motion restricting agent movement forward/backward motion horizontal planar rotation remaining dimensions specify forces torques τyτz applied objects sorted lower-leftmost upper-rightmost object relative agent’s ﬁeld view. coordinates bounded constants normalized agent consists world-model self-model world-model tasked learn dynamics prediction problem based inputs environment. self-model tries estimate world-model’s losses several time steps future function potential agent actions. action choice policy based self-model chooses actions antagonize world-model’s learning. section formalize ideas mathematically. figuring tractable dynamics prediction problem make target intrinsic motivation important ﬁrst challenge. begin abstract mathematical treatment expose issues. deﬁne partially observable markov decision process state space transition dynamics observations action space speciﬁed external reward. physics environment described above states encode positions velocities physical properties agent objects physical space. dynamics updates given newtonian physics. observations images rendered agent-mounted camera. actions encode self-motions agent well forces/torques agent apply objects. within context agents make decisions action take time accumulating histories state-action pairs windows histories. informally dynamics prediction problem pairing complementary subsets data inputs true values generated goal agent learn inputs true-values. mathematically deﬁne ﬁvetuple input space true-value space histories inputs histories true-values loss function agent’s goal antagonize world-model could predict incurred future time steps function current action simple antagonistic policy could seek maximize number time steps future. given proposed next action selfmodel predicts probability distributions discrete classes world-model loss number future time steps. penalized softmax cross-entropy loss. note future losses aside ﬁrst depend state world-model also future actions taken self-model hence needs predict expectation future policy. context physical environment loss predictions interpreted self prediction maps action space given current state interpretation useful intuitively visualizing strategy agent taking given situation given hyperparameter follows expectation values time step although sophisticated functions combine rewards time possible practice execute policy evaluating lstat− uniform random samples sample k-way discrete distribution probabilities proportional agent’s world-model tries learn reconstruct missing true-value input datum. done regardless whether actually induce well-deﬁned mapping makes diagram commute. obstructions existence commuting arise degeneracy system dynamics natural dynamics problem forward dynamics prediction. notational convenience historik denote secal observable quence values steps past steps future deﬁne temporal observation window available agent. forward dynamics prediction deﬁned letting observation corresponding state words agent trying predict next observation given past observations past actions current action. real physical domains true-values correspond bitmap image arrays future frames loss function either loss pixels discretization thereof. despite recent progress frame prediction problem remains quite challenging part dimensionality true-value space large. practice substantially easier solve inverse dynamics prediction. deﬁned words however suffer substantial degeneracy consider case agent pressing object downward ground. matter force downward applied object move input information insufﬁcient determine true-value was. sophisticated concept tries solve highdimensionality degeneracy simultaneously latent space future prediction case begin system solving inverse dynamics prediction problem assume parametrization world= models factor composition non-overlapping sets parameters. case call encoding decoding range latent space problem. deﬁne -time-step future prediction problem trajectories given timevarying encoding i.e. at−b) convolutional neural networks base architecture learn world-models self-models speciﬁc experiments described below networks encoding structure common architecture involving twelve convolutional layers two-stride pools every layer fully-connected layer encode states lower-dimensional latent space shared weights across time. inverse dynamics task encoding layer network combined actions two-layer fully-connected network softmax classiﬁer used predict action latent space future prediction task encoding layer used latent space latent model estimated another copy encoding network. self-model encoding layer combined action twolayer fully connected network predict world-model loss action. parameters trained end-to-end stochastic gradient descent randomly initialization. compare variety agents deﬁned different combinations world-model task policy mechanism. several baseline models include inverse dynamics problem random action policy problem random-weight encoding random policy problem random-weight encoding self-model based policy baselines compared powerful agents fully learnable encoding self-model policy problem latent space future prediction comparison models evaluate three types metrics dynamics prediction tasks measure inverse dynamics prediction performance held-out validation subsets data easy dataset drawing uncontrolled background distribution events dominated ego-motion; hard dataset enriched events observed challenging e.g. frames object present. metric measures active learning gains assessing extent agent self-constructs training data hard subset retaining performance easy dataset. also look emergent behavior quantifying appearance interesting behaviors attention acting objects navigation planning ability cause multiple objects interact. track much time agent spends playing object also relationship behavior appears observables sharp changes overall world-model loss. finally measure task transfer including ability agent model predict object presence location category identity. randomly place agent objects square units room. total train agent blue objects different shapes i.e. cones cylinders cuboids pyramids spheroids varied aspect ratios. gather data using simulation instances asynchronously different seeds objects. reinitialize scene every steps. simulation maintains buffer time steps. model updates examples randomly sampled simulation buffer form batch size train models using adam algorithm learning rate ﬁrst place agent room object evaluate ability predict inverse dynamics attend localize recognize navigate towards objects. ego-motion learning. fig. shows total training loss curves id-sp lf-sp models baselines. random-encoding idrw-rp model learns ineffectively background random data distribution. models learn ego-motion prediction effectively. id-rp model quickly converges loss value remains effectively learned ego-motion prediction without antagonistic policy since ego-motion interactions common background random data distribution. id-sp lf-sp models also learn egomotion effectively seen initial decrease loss seen fig. ﬁnal loss fig. depicts easy ego-motion validation dataset. ego-motion accuracy reported table close total validation accuracy reached point. emergence object attention. learned-weight agents implementing loss increases initial decrease ego-motion learning seen fig. shown fig. loss increase corresponds emergence object attention. id-sp lf-sp agent exhibit increased frequency frames object present coinciding increase world-model loss though id-sp agent substantially pronounced. convergence id-sp agent interacting objects time. baselines models almost never interact object. world-model loss increases self-model driven agents since object interactions much harder predict simple ego-motions. navigation planning. agents also exhibit navigation planning abilities. fig. give visualizations self prediction maps projected onto agent’s position respective time. maps generated uniformly sampling actions action space evaluating applying post-processing smoothing algorithm. show exemplary sequence time steps. self prediction maps show agent predicting higher loss figure object experiments. world-model training loss. percentage frames object present. world-model test-set loss easy ego-motion-only data objects present. world-model test-set loss hard validation data object present agent must solve object physics prediction. actions moving towards object reach play state. result self-model driven agents take actions navigate closer object. improved object dynamics prediction. object attention navigation lead agents substantially different data distributions baselines. evaluate inverse dynamics prediction performance held hard object interaction validation set. here id-sp lf-sp agents outperform baselines predicting harder object interaction subset signiﬁcant margin showing increased object attention translates improved inverse dynamics prediction table crucially even though id-sp lf-sp substantially decreased fraction time spent ego-motion interactions previously observed fig. still retain high performance easier task. improved task transfers. next measure agent’s abilities solve tasks directly trained including object presence localization recognition. build linear estimators learned features agent world-model trained off-line validation datasets consisting image pairs labeled respectively object’s presence pixel-wise centroid position -way object category. results reported test sets comprising image pairs each. note test sets contain substantial variation position pose size rendering tasks nontrivial. self-model driven agents substantially outperform alternatives three transfer tasks shown table agent outperforms baselines inverse dynamics object presence metrics id-sp outperforms lf-sp localization recognition. emergence multi-object interactions. second experiment increase number objects two. beginning training object experiment observe similar stages object experiment id-sp lf-sp. loss temporarily decreases agent learns predict ego-motion rises attention shifts towards objects interacts with. id-sp agents sufﬁciently long time horizon robustly observe emergence additional stage loss increases corresponding agent gathering playing objects simultaneously. reﬂected increase object play time object play time moreover average distance agent objects decreases time seen fig. observe additional stage either id-sp shorter time horizon lf-sp model even longer horizons unsurprisingly id-rp baseline random policy experiences quick loss drop ﬂattening loss. id-sp agent discovered take advantage increased difﬁculty therefore interestingness object conﬁgurations interestingly training objects present improves recognition transfer performance compared object scenarios potentially greater complexity two-object conﬁgurations especially notable id-sp agent constructs substantially increased percentage two-object events. figure navigation planning behavior. exemplary model roll-out consecutive time steps. force vectors objects depict predicted actions would maximize world-models loss. ego-motion self prediction maps drawn center agents position. colors correspond high blue colors loss predictions. agent starts without seeing object predicts higher loss turns around explore object. object view self-model predicts higher loss agent approaches object away turns towards object keep view close. table performance comparison. ego-motion interaction accuracy compared play non-play states. object frequency presence recognition measured localization mean pixel error. models trained object room unless otherwise stated. task accuracy easy accuracy easy accuracy hard accuracy hard accuracy hard accuracy hard object frequency object presence error localization error recognition accuracy recognition accuracy object training constructed simple general intrinsic motivation mechanism physically-embedded agent makes world-model explicitly creates self-aware meta-model world-model uses selfmodel adversarially antagonize world-model. shown architecture allows agent spontaneously generate spectrum emergent naturalistic behaviors. self-curricularization active learning process agent achieves several developmental milestones suitably increasing complexity learns play. starting random actions quickly learns dynamics ego-motion. then without given explicit supervision signal presence location object discards ego-motion prediction boring begins focus attention objects interesting. lastly multiple objects available gathers objects bring interaction range other. throughout agent ﬁnds towards challenging data distribution moment hard enough expose agent situations still understandable exploitable agent. intrinsically motived policy leads performance gains understanding object dynamics useful tasks system receive explicit training signal. occurs without pre-trained visual backbone world-model intentionally initialized ﬁlter weights pre-trained imagenet classiﬁcation. result constitutes partial progress replacing training visual backbone task large-scale image classiﬁcation interactive self-supervised task proof-of-concept complex milestones pofigure object experiments. world-model training loss. percentage frames object present. percentage frames objects present. average distance agent objects unity units. average objects must close agent simultaneously. tentially reached developing understanding object categories physical relations. combination spontaneous behavior leading improved world-model well suited designing agents must self-supervise real-world reinforcement learning scenarios rewards sparse potentially unknown. variety limitations current work need overcome future work. first make results better transfer real world environment agent need realistic. hand better graphics physics varied interesting visual objects important allow better transfer learned behavior real-world visuo-motor interactions. also important create properly embodied agent visible arms tactile feedback allowing realistic interactions. addition including animate agents lead complex interactions potentially also better learning imitation scenario self-model component architecture need aware agent itself also make predictions actions agents connecting known cognitive science theory mind second reinforcement learning techniques used improved better handle complex interactions beyond demonstrated here. interactions part larger experiment e.g. placing object table ramp watching fall sophisticated policies likely necessary better ability handle temporally extended reward schedules. also likely necessary recurrent networks meet working memory demands scenarios. third world-model needs better representations improve predicting complex interactions. current approach suffers degenerate cases inverse dynamics prediction problem problem correspond well-deﬁned map. though latent space approach meant part ameliorate issue found entirely effective solution context. resolve issue likely innovate dynamics prediction tasks world-model better integrate interaction antagonistic action policies self-model. begus katarina gliga teodora southgate victoria. infants learn want learn responding infant pointing leads superior learning. plos ./journal.pone.. hong seunghoon donghun kwak suha honglak bohyung. weakly supervised semantic segmentation using web-crawled videos. cvpr ieee computer society isbn ----. elhamifar ehsan sapiro guillermo yang allen sastry shankar. convex optimization framework active learning. iccv ieee computer society isbn ----. frank mikhail leitner jrgen stollenga marijn frster alexander schmidhuber jrgen. curiosity driven reinforcement learning motion planning humanoids. front. neurorobot. jaderberg mnih volodymyr czarnecki wojciech marian schaul leibo joel silver david kavukcuoglu koray. reinforcement learning unsupervised auxiliary tasks. corr abs/. kalchbrenner oord aron simonyan karen danihelka vinyals oriol graves alex kavukcuoglu koray. video pixel networks. icml volume jmlr workshop conference proceedings jmlr.org kidd celeste piantadosi steven aslin richard goldilocks effect human infants allocate attention visual sequences neither simple complex. plos journal.pone.. kulkarni tejas narasimhan karthik saeedi ardavan tenenbaum josh. hierarchical deep reinforcement learning integrating temporal abstraction intrinsic motivation. nips mitash chaitanya bekris kostas boularias abdeslam. self-supervised learning system object detection using physics simulation multi-view pose estimation. iros ieee isbn ----. zhang richard isola phillip efros alexei colorful image colorization. leibe bastian matas jiri sebe nicu welling eccv volume lecture notes computer science springer isbn ----. noroozi mehdi favaro paolo. unsupervised learning visual representations solving jigsaw puzzles. eccv volume lecture notes computer science springer isbn ---. pathak deepak agrawal pulkit efros alexei darrell trevor. curiosity-driven exploration selficml volume jmlr supervised prediction. workshop conference proceedings jmlr.org popov ivaylo heess nicolas lillicrap timothy hafner roland barth-maron gabriel vecerik matej lampe thomas tassa yuval erez riedmiller martin. data-efﬁcient deep reinforcement learning dexterous manipulation. arxiv preprint arxiv. singh satinder lewis richard barto andrew sorg jonathan. intrinsically motivated reinforcement learning evolutionary perspective. ieee trans. autonomous mental development", "year": 2018}