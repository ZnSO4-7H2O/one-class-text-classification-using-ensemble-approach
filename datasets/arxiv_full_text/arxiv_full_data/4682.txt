{"title": "Scaling Life-long Off-policy Learning", "tag": ["cs.AI", "cs.LG"], "abstract": "We pursue a life-long learning approach to artificial intelligence that makes extensive use of reinforcement learning algorithms. We build on our prior work with general value functions (GVFs) and the Horde architecture. GVFs have been shown able to represent a wide variety of facts about the world's dynamics that may be useful to a long-lived agent (Sutton et al. 2011). We have also previously shown scaling - that thousands of on-policy GVFs can be learned accurately in real-time on a mobile robot (Modayil, White & Sutton 2011). That work was limited in that it learned about only one policy at a time, whereas the greatest potential benefits of life-long learning come from learning about many policies in parallel, as we explore in this paper. Many new challenges arise in this off-policy learning setting. To deal with convergence and efficiency challenges, we utilize the recently introduced GTD({\\lambda}) algorithm. We show that GTD({\\lambda}) with tile coding can simultaneously learn hundreds of predictions for five simple target policies while following a single random behavior policy, assessing accuracy with interspersed on-policy tests. To escape the need for the tests, which preclude further scaling, we introduce and empirically vali- date two online estimators of the off-policy objective (MSPBE). Finally, we use the more efficient of the two estimators to demonstrate off-policy learning at scale - the learning of value functions for one thousand policies in real time on a physical robot. This ability constitutes a significant step towards scaling life-long off-policy learning.", "text": "pursue life-long learning approach artiﬁcial intelligence makes extensive reinforcement learning algorithms. build prior work general value functions horde architecture. gvfs shown able represent wide variety facts world’s dynamics useful long-lived agent also previously shown scaling—that thousands on-policy gvfs learned accurately real-time mobile robot work limited learned policy time whereas greatest potential beneﬁts life-long learning come learning many policies parallel explore paper. many challenges arise oﬀ-policy learning setting. deal convergence eﬃciency challenges utilize recently introduced algorithm. show tile coding simultaneously learn hundreds predictions simple target policies following single random behavior policy assessing accuracy interspersed on-policy tests. escape need tests preclude scaling introduce empirically validate online estimators oﬀ-policy objective finally eﬃcient estimators demonstrate oﬀ-policy learning scale—the learning value functions thousand policies real time physical robot. ability constitutes signiﬁcant step towards scaling life-long oﬀ-policy learning. life-long learning approach artiﬁcial intelligence based learning long-stream sensorimotor interaction generated agent interacting environment life-long learning emphasizes continual learning autonomous agent long periods time perhaps months years. data problem requires algorithms scale eﬃciently learn multitude diverse facts large stream sensorimotor data. purse novel general value functions provide expressive language representing sensorimotor knowledge long-lived agent’s interaction world knowledge represented approximate value functions reward function outcome function pseudo-termination function conditioned policy. gvfs provide semantics experiential knowledge grounded sensorimotor data veriﬁable agent without human intervention—essential scalability. recent computational studies shown predictions represented gvfs learned massive scale high degree accuracy. on-policy sampling mobile robot learned thousands predictions future sensor readings state variables several time scales operating time step. predictions shown accurate compared optimal oﬀ-line solution. previous work focused limited form prediction learning consequences single policy. greatest potential beneﬁt life-long learning comes learning many policies parallel using oﬀ-policy learning. parallel learning introduces dimension scaling considered typical sequential life-long learning systems demonstrated scale real time robot. many challenges arise oﬀ-policy learning setting. scaling life-long learning requires eﬃcient learning methods robust oﬀ-policy sampling. recently developed gradient temporal-diﬀerence learning method linear function approximation learn gvfs robot. gradient methods learning methods scale linearly size feature require constant computation time step require memory forgetting process therefore methods available oﬀ-policy life-long learning scale mobile robot. arguably computational constraints big-data problem make batch least-squares approaches inappropriate life-long learning recent work machine learning highlighted value simple online learning methods data problems evaluating oﬀ-policy learning scale poses additional challenge determining prediction accuracy policies never executed robot. ﬁrst show robot learn hundreds predictions several policies interspersed on-policy tests. however tests require interrupting learning placing upper bound number policies robot learn about. propose eﬃciently computable online measures oﬀpolicy learning progress based oﬀ-policy objection function using online measures demonstrate learning thousand gvfs thousand unique target policies real time robot. results represent signiﬁcant step towards scaling life-long learning. begin consider problem prediction conventionally formulated reinforcement learning. interaction agent environment modelled discrete-time dynamical system function approximation. total future discounted reward reward special signal received conditional agent following particular policy. time scale prediction controlled discount factor terms deﬁned precise quantity predicted called return common on-policy setting policy conditions value function also policy used select actions generate training data. general however policies diﬀerent. policy conditions value function called target policy target learning process paper uniformly denote policy generates actions behaviour called behaviour policy conventional algorithms q-learning applied function approximation on-policy setting become unstable oﬀ-policy setting. fewer algorithms work reliably oﬀ-policy setting. algorithm gradient-td algorithm designed learn oﬀ-policy sampling function approximation incremental prediction algorithm similar except additional secondary learned weights additional step size parameter algorithm retains computational advantages computational complexity step operate online matrix possible feature vectors projection matrix projects value function onto space representable λ-weighted bellman operator target policy discount factor diagonal matrix whose diagonal entries correspond state visitation frequency behaviour policy addition learning multiple policies approach learn multiple things policy. cases captured notion general value functions. envision architecture many predictive questions posed answered generalized form value function. figure horde architecture large-scale oﬀ-policy learning. horde consists large independent instances algorithm updating making predictions parallel shared features. features typically sparse encoding sensorimotor information predictions previous time step. whole system operated parallel real time. example ‘what would eﬀect rotational velocity future actions consisted clockwise rotation’. policy-contingent questions substantially broaden knowledge acquired system dramatically increases scale learning—millions distinct predictive questions easily constructed space policies sensors time scales. figure provides graphical depiction parallel learning architecture. architecture called horde sutton several desirable characteristics. horde real time linear computational complexity gtd. architecture potentially scalable distributed nature oﬀ-policy learning experiments performed evaluate ability learn scale practice. system modular question speciﬁcation behaviour policy function approximation architecture completely independent. depicted figure predictions used input function approximator. enables predictive state information learning compositional predictions similar network ﬁrst question consider whether horde architecture supports largescale oﬀ-policy prediction real time physical robot. evaluations performed custom-built holonomic mobile robot robot diverse sensors detecting external entities also internal status robot dock autonomously charging station continually twelve features. tile coder comprised many overlapping tilings single sensors pairs sensors. tile coding scheme produced sparse feature vector components features ones including bias feature whose value always details feature representation given previous work conducting fair evaluation presents challenge oﬀ-policy predictions robot; direct evaluate prediction policy follow policy period time measure return. direct on-policy test excursion interspersed baseline oﬀ-policy behaviour used learning. followed procedure evaluate predictions learned oﬀ-policy robot learning updates suspended test excursions. every baseline learning behaviour time step random action selected probability otherwise last executed action repeated. normal execution interrupted probabilistically test excursion; average interruption occurred every seconds. test excursion consisted selecting constant action policies following seconds. test excursion completed robot spent seconds moving centre continued random behaviour policy. robot hours visiting portions many times. produced samples half time spent test excursions. used horde learn answers predictive questions experience generated behaviour described above. question formed combining constant action policy prediction target sensors. question form current time expected discounted future values robot follows constant pseudo-termination probability ease comparison predictions across sensors diﬀerent output ranges values sensor scaled maximum minimum values speciﬁcations observed sensor values bounded time-step resulted updates exactly learners parallel question used identical learning parameters total computation time cycle conditions well within duty cycle robot. entire architecture .ghz dual-core laptop connected robot dedicated wireless link. figure graph presents ﬁrst major result ﬁrst demonstration learning hundreds policy-contingent predictions consumer laptop. x-axis number relevant test excursions observed question. black heavy stroke line shows average error entire questions return error typical exponential learning proﬁle. return errors normalized variance return question yielding percentage variance unexplained. several individual curves exhibit non-monotonic shape discrepancies samples observed test excursions learning—diﬃcult avoid evaluating performance online individual test excursions. architecture place update many oﬀ-policy predictions real time robot evaluated on-policy test performance. precisely test execution questions pertaining selected test policy compared prediction beginning test truncated sample return gathered test excursion nmsre represents percentage variance returns remains unexplained predictor. questions whose sample returns constant thus zero sample variance deﬁne nmsre one. figure illustrates main result accurate oﬀ-policy predictions learned real time physical robot scale. predictions learned randomized behaviour policy shared feature representation using identical parallel instances gtd. note question-speciﬁc tuning learning parameters features needed. another signiﬁcant result divergence observed question. note average nmsre questions ﬁnished substantial portion variance returns explained predictions. learning parameters important—divergence observed earlier runs aggressive settings thus demonstrating convergence oﬀ-policy setting trivial. accuracy predictions learned previous experiment evaluated return error observed on-policy test excursions. tests consume considerable wall-clock time sample system must follow target policy long enough capture probability mass inﬁnite sample return multiple samples required estimate nmsre. interspersing on-policy tests evaluate learning progress places limit number target policies time-scale given subtle deﬁciencies on-policy tests. experimenter must choose testing regime frequency. depending often tests executed trade-oﬀ often nmsre updated. changes environment novel robot experiences cause inaccurate nmsre estimates majority time-steps used training. testing greater frequency ensures estimated nmsre closely matches current prediction quality slows learning. sures prediction accuracy relative sample returns ignoring function approximation error. arbitrary question nmsre never zero though provide indication quality feature representation. algorithm instead minimizes mspbe. common technical assumptions mspbe converge zero error. mspbe estimated real time learning provides up-to-date measure performance without sacriﬁcing valuable robot data evaluation. using derivation given sutton rewrite error algorithm uses second modiﬁable weights form quasi-stationary estimate last terms namely product inverse feature covariance matrix expected td-update. leads following linear-complexity approximation mspbe expected td-update term approximated samples δtet eligibility trace vector. additionally prediction error non-zero samples target policy agree behaviour policy used account eﬀects. leads natural incremental algorithms sampling current mspbe here exponential traces mspbetvector mspbetscalar updated time step proportionally ﬁrst measure accurate approximation equation second requires storing single real-valued scalar. ﬁrst step evaluating online estimates mspbe compare exact values mspbe simulation domain. used simple state markov chain absorbing state deterministic figure comparison online estimates mspbe true sample mspbe simple markov chain. left ﬁgure ﬁgure shows measures provide good estimate mspbe matching proﬁle magnitude simple domain. right ﬁgure compares online estimates react major change learning. episodes primary weight vector random values again online estimates track true sample mspbe closely. transitions episodes beginning middle chain. transitioning right-side terminal state produced reward transitions incurred reward. used inverted feature representation sutton determine validity measures compared vector scalar mspbe estimates true mspbe expensive sample estimate mspbe computation true mspbe requires complete knowledge chain mdp. sample mspbe requires incremental estimate expected feature covariance matrix inverse operation. sample mspbe represents best possible samplebased estimate mspbe; online measures expected track true mspbe better sample mspbe. experiment used single instance results averaged independent runs. target policy selected move-right action probability behaviour policy selected move-right probability figure illustrates results chain experiment comparison online measures provide accurate estimate mspbe simple chain domain. figure compares eﬀect change world learning. episodes weights random values secondary weights traces used computing online estimates reset. results depicted figure illustrate online estimates sample mspbe true mspbe react similarly change. signiﬁcant change similarly chain experiment. robot exactly before subset predictions learned hours. time learned weight vector question zero time steps. change eﬀectively reinitializes question eﬀects accuracy predictions. experiment recorded nmsre mspbetvector mspbetscalar every time step questions except test excursions. note nmsre updated test completes mspbe measures updated every non-test timestep. figure compares convergence proﬁle reaction change three error measures terms training time. chain experiments three measures react quickly change. note mspbe estimates initially zero vector takes time adapt useful value. finally note mspbetvector mspbetscalar exhibit similar trends indicating bellman error estimated minimal storage requirements. figure ﬁgure compares nmsre estimates mspbe averaging performance predictive questions. ﬁgure illustrates several important points validate mspbe useful error measure. mspbe measures computed oﬀ-policy shape nmsre requires on-policy excursion behaviour. mspbe estimates converge slowly nmsre indicating although test performance improving learning still occurring. mspbe estimates react slowly change secondary weights also reset; incorrect values aﬀect estimates mspbe long change. mspbe measures react quickly change environment. experiment shows scalar estimate mspbe performs almost identically vector version. graph illustrates signiﬁcant result oﬀ-policy learning progress eﬃciently estimated online without time consuming on-policy tests. additionally nmsre requires twice twice much wall-clock time mspbe estimates. free limitations physically performing test excursions evaluate predictions learn much larger questions. section consider scaling number target policies prediction time scales rn|a| copy subvector otherwise zero vector; action copy oﬀset random policies generated selecting components random assigning value independently drawn uniform distribution ﬁnal experiment tested well architecture scales number target policies. robot’s behaviour before learning enabled every step hours experience. questions sensors randomly generated policies. value corresponds second prediction would require seconds accurately evaluate using nmsre. questions evaluated according mspbetscalar learned cycle time -core desktop computer ram; satisfying real-time requirement temporally-extended consequences many diﬀerent behaviours possible real time. learning progress measured mspbe results previous section strongly coupled on-policy prediction errors. note ability monitor learning progress across many diﬀerent behaviours possible availability mspbe. acquiring many predictions many diﬀerent courses behaviour robot acquire detailed partial models dynamics environmental interaction. many ideas paper precursors literature. idea policy-contingent predictions developed along options framework temporal abstraction learning oﬀ-policy function approximation developed importance sampling approach runs online exhibit exponentially slow learning progress. learning many diﬀerent policies also support active figure scaling oﬀ-policy learning robot policies. estimated mean squared projected bellman error oﬀ-policy predictions distinct randomly generated policies. heavy stroke black line denotes average estimated error full questions. curves provide clear indication learning progress prediction. result provides clear demonstration signiﬁcance estimating mspbe incrementally learning. massively scaling number policies condition predictions possible online performance measure. idea building models data explored oﬀ-policy real-time setting. thrun mitchell showed learning sensor models oﬄine. many bayesian approaches online state estimation robotics process vast volume observations real time learn system dynamics online. kalman ﬁlter viewed learning adaptive model dynamics online appropriate small models well-understood dynamics. atkeson schaal showed learning small one-time-step models. kober peters showed on-policy episodic learning robot. recent online spectral approach ﬁnds small predictive state models robotics. sophisticated incremental method making multiple action-conditional predictions unclear approach operate real-time constraints. previous work introduced horde architecture also demonstrated parallel oﬀ-policy learning robot. work suggestive demonstrate approach scaled practice. experiment required unique parameter function approximation scheme behaviour policy; overhead practical learning thousands predictions. paper shows large diverse policy-contingent predictions learned using shared feature common learning parameters data generated single random behaviour policy. provided ﬁrst demonstrations large scale oﬀ-policy learning robot. shown gradient methods used learn hundreds temporally-extended policy-contingent predictions oﬀ-policy sampling. achieve goal required resolution several challenges unique oﬀpolicy setting. signiﬁcantly developed online estimate oﬀpolicy learning progress based bellman error increase computational complexity horde architecture sampled without interrupting learning good correspondence traditional mean squared prediction error. addition policy contingent what-if questions dramatically increases scope scale questions learned horde providing evidence signiﬁcance horde life-long learning. experiments robot limited several immediate directions future work. questions learned predictive questions policy general value functions also support learning control policies using greedy-gq control variant linear complexity. predictive accuracy improvements achieved employing adaptive behaviour policies powerful function approximators signiﬁcantly increased scaling obtained computational resources. boots siddiqi gordon online spectral learning algorithm partially observable nonlinear dynamical systems. proc. conf. association advancement artiﬁcial intelligence. modayil white sutton multi-timescale nexting reinforcement learning robot. proc. int. conf. adaptive behaviour. precup sutton paduraru koop singh oﬀ-policy advances neural information processing quadrianto smola caetano vishwanathan petterson multitask learning without label correspondences. advances neural information processing systems sutton maei precup bhatnagar silver szepesv´ari wiewiora fast gradient-descent methods temporal-diﬀerence learning linear function approximation. proc. int. conf. machine learning. sutton modayil delp degris pilarski white precup horde scalable real-time architecture learning knowledge unsupervised sensorimotor interaction. proc. int. conf. autonomous agents multiagent systems.", "year": 2012}