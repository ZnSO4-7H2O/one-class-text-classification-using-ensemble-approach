{"title": "Learning Sparse Representations in Reinforcement Learning with Sparse  Coding", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "A variety of representation learning approaches have been investigated for reinforcement learning; much less attention, however, has been given to investigating the utility of sparse coding. Outside of reinforcement learning, sparse coding representations have been widely used, with non-convex objectives that result in discriminative representations. In this work, we develop a supervised sparse coding objective for policy evaluation. Despite the non-convexity of this objective, we prove that all local minima are global minima, making the approach amenable to simple optimization strategies. We empirically show that it is key to use a supervised objective, rather than the more straightforward unsupervised sparse coding approach. We compare the learned representations to a canonical fixed sparse representation, called tile-coding, demonstrating that the sparse coding representation outperforms a wide variety of tilecoding representations.", "text": "instance-based approaches locally weighted regression sparse distributed memories proto-value functions manifold learning techniques neural network approaches including standard feedforward neural networks well random representations linear threshold unit search evolutionary algorithms like neat surprisingly however little investigation using sparse coding reinforcement learning. sparse coding approaches developed learn models transfer learning outside work however little explored. nonetheless sparse coding representations several advantages including naturally enable local models computationally efﬁcient much simpler train complicated models neural networks biologically motivated observed representation mammalian cortex work develop principled sparse coding objective policy evaluation. particular formulate joint optimization basis value function parameters provide supervised sparse coding objective basis informed utility prediction. highlight importance using bellman error mean-squared return error objective discuss projected bellman error suitable. show that despite nonconvex objective local minima global minima minimal conditions. avoid need careful initialization strategies needed previous optimality results sparse coding using recent results general dictionary learning settings particularly extending beyond smooth regularizers using γ-convergence. using insight provide simple alternating proximal gradient algorithm demonstrate utility learning supervised sparse coding representations versus unsupervised sparse coding variety tile-coding representations. variety representation learning approaches investigated reinforcement learning; much less attention however given investigating utility sparse coding. outside reinforcement learning sparse coding representations widely used non-convex objectives result discriminative representations. work develop supervised sparse coding objective policy evaluation. despite non-convexity objective prove local minima global minima making approach amenable simple optimization strategies. empirically show supervised objective rather straightforward unsupervised sparse coding approach. compare learned representations canonical ﬁxed sparse representation called tile-coding demonstrating sparse coding representation outperforms wide variety tilecoding representations. introduction tasks large state action spaces tabular representations feasible reinforcement learning algorithms typically rely function approximation. whether learning value function policy models success function approximation techniques hinges quality representation. typically representations hand-crafted common representations including tile-coding radial basis functions polynomial basis functions fourier basis functions automating feature discovery however alleviates burden potential signiﬁcantly improve learning. representation learning techniques reinforcement learning typically drawn large literature unsupervised supervised learning. common approaches include feature selection including regularization value function parameters matching pursuit basis-function adaptation approaches begin formalizing representation learning component. many unsupervised representation learning approaches consist factorizing input observations x∈r|s|×d basis dictionary b∈rk×d representation r|s|×k. rows form bases columns weighting amongst bases observation though simple approach encompasses broad range models including isomap locally linear embeddings sparse coding sparse coding objective βφφd squared frobenius norm; rk×d learned basis dictionary; determine magnitudes regularizers; |s|×|s| diagonal matrix giving distribution states corresponding stationary distribution policy ||z|| weighted norm. reconstruction error promotes sparsity entries preferring entries entirely pushed zero rather spreading magnitude across frobenius norm regularizer ensures become large. without regularizer magnitude shifted producing pushing zero nullifying utility regularizer. optimizing sparse coding objective would select sparse representation observation approximately reconstructs further however would like learn representation also optimized towards approximating value function. towards need jointly learn provides approximate value function. optimization must balance accurately recreating approximating value function this must choose objective learning consider types objectives ﬁxed-point objectives squared-error objectives. common ﬁxed-point objectives mean-squared bellman error also called bellman residual mize scalar reward signal provided environment. interaction usually modeled markov decision process consists states; ﬁnite actions; transition function describes probability reaching state given state action ﬁnally reward function returns scalar value transitioning state-action state state environment said markov important goal reinforcement learning policy evaluation learning value function policy. value function approximates expected return. return state total discounted future reward discounted following policy given reward function transition probabilities solution analytically obtained −rπ. practice however likely prohibitively large state space. typical strategy setting function approximation learn trajectory samples sequence states actions rewards drawn start-state distribution commonly linear function assumed parameter vector feature function describing states. approximation however typically longer satisfy bellman equation exist equals γpπφw r|s|×k. instead focus minimizing error true value function. reinforcement learning algorithms temporal difference learning residual gradient therefore focus ﬁnding approximate solution bellman equation despite representation issue. quality representation critical accurately approximating also balancing compactness representation speed learning. sparse coding sparse representations proven successful machine learning reinforcement learning particularly ﬁxed bases tile coding radial basis functions kernel representations. natural goal therefore explore work investigate learning sparse representations automatically. algorithm sparse coding derive algorithm sparse coding policy evaluation scope. generically consider either msre. trajectory samples {}t− objective msre γj−irj+ consider possible powers norm theory relies using practice perform equivalently provides slightly simpler optimization. loss averaged obtain sample average limit converges expected value averaged loss also scale-invariant—in terms numbers samples—to choice regularization parameters. scope consists alternating amongst three variables proximal gradient update nondifferentiable norm. loss terms differentiable; solve variables ﬁxed simply used gradient descent. solve ﬁxed however cannot standard gradient descent update regularizer nondifferentiable. proximal update consists stepping direction gradient smooth component objective—which differentiable—and projecting back sparse solution using proximal operator soft thresholding operator. convergence alternating minimization follows results block coordinate descent non-smooth regularizers minimum error solution immediately basis parameters. basis parameters considered change slow timescale weights fast timescale reﬂection type separate minimization. menache avoided problem explicitly using two-stage approach using mspbe approaches learning parameters using score function squared bellman error update bases. basis learning approach however unsupervised. representation learning strategies mspbe developed using local projections strategies however incorporate sparse coding. |s|×|s| diagonal matrix giving distribution states corresponding stationary distribution policy; ||z|| weighted norm; projection matrix linear value functions φdφ)−φd. family algorithms converge minimum mspbe whereas residual gradient algorithms typically msbe overview). useful properties though arguably mspbe widely used. differ ﬁxed-point objectives placement expectation. consider msbe expected value expected squared error prediction state reward plus value possible next state. msbe hand squared error prediction state expected reward plus expected value next state. though mspbe msbe constitute common objectives chosen reinforcement learning squared-error objectives also shown useful particularly learning online sparse coding however mspbe suitable choice—compared msbe msre—for reasons. first msbe msre convex whereas mspbe not. second projection onto space spanned features mspbe solved zero error features therefore inform choice mspbe produces stage approach features learned completely unsupervised prediction performance inﬂuence this problem seems overlooked approaches basis adaptation based mspbe adaptive bases algorithm projected bellman error mirror descent basis adaptation example abpbe immediately obvious would problem stochastic approximation approach taken. however written minimization basis parameters weights would obtain function afﬁne function part regularizer must weighted frobenius norm weightings column; here weighting using regularization parameters ﬁrst columns regularization parameter last column part inner dimension true assumption common setting sparse coding. part pseudo-huber loss columns convex centered twice-differentiable. part sequence converges uniformly recall deﬁnition uniform convergence. sequence functions {fn} uniformly convergent limit every exists recall complete metric space uniformly cauchy uniformly convergent. sequence uniformly cauchy take fundamental theorem γ-convergence {fn} equi-coercive family functions minimizers converge minimizers sequence functions {fn} equi-coercive exists lower semi-continuous coercive function {−∞∞} every function coercive clear coercive well lower semicontinuous further regularizer non-negative. therefore family {fn} equi-coercive minimizers converge minimizers direction local minimum isolated local minimum exists sequence local minimizer sufﬁciently small frobenius norm regularizers strongly convex objective strictly convex respect further because full rank strictly convex function local minima global minima section show despite nonconvexity objective scope nice property local minima fact global minima. consequently though many different local minima fact equivalent terms objective. result justiﬁes simple alternating minimization scheme convergence local minima ensures optimal solution obtained. need following technical assumption. guaranteed true sufﬁciently large white assumption given following function convex rt+×d+ theorem objective equation assumption full-rank local minima global minima; local minimum global minimum. proof. ﬁrst statement construct limit twicedifferentiable functions γ-converge scope objective this show minimizers sequence converge minimizers vice-versa local minimizers twicedifferentiable functions global minimizers conclude corresponding minimizers global minimizers. figure learning curves scope versus variety tile coding representations three domains. graphs depict early learning; numbers brackets correspond ﬁnal error samples. errors sampled every samples msre used optimization lines irregular pattern. differences nonetheless statistically signiﬁcant average runs small standard error bars omitted. scope outperforms best representations mountain acrobot using compact sparse representation; puddle world performs poorly discuss text. larger representations likely perform poorly hashing. respect therefore locally objective strictly convex respect therefore know local minima isolated exists local minimizers since local minimizers global minimizers converge means global minimum second statement already showed loss cast factorization clear loss regularizers positively homogenous order minimum guaranteed exist objective loss function continuous bounded goes inﬁnity parameters experimental results address question learn useful representations using scope? therefore tackle setting representation ﬁrst learned used avoid conﬂating incremental estimation utility representation. particularly evaluate estimation accuracy well qualitatively understanding types sparse representations learned scope. domains. conducted experiments three benchmark domains mountain puddle world acrobot domains episodic discount termination. data mountain generated using standard energy-pumping policy policy randomness. data puddle world generated policy chooses north probability east probability step starting position lower-left corner grid goal top-right corner. data acrobot generated near-optimal policy. evaluation. measure value function estimation accuracy using mean absolute percentage value error rollouts compute true value estimates. mapve xtest test states ttest number samples test estimated value state true value state computed using extensive rollouts. errors averaged runs. algorithms. compare using several ﬁxed tile-coding representations. uses overlapping grids observation space. sparse representation well known perform well mountain puddle world acrobot. varied granularity grid-size number tilings number active features observation. grid either mountain puddle world acrobot. explore grid size performed poorly omitted. mountain puddle world number features respectively hashed dimensions; acrobot number features hashed hashed sizes much larger chosen consistency scope representation learned batch gradient descent update msre algorithms line search select step-sizes. regularization weights chosen based lowest cumulative error. convenience ﬁxed learning scope representations regularization parameters chosen using -fold cross-validation training samples ﬁxed give reasonable level sparsity. data used learn representation; learning curves weights learned scratch learned dimension smaller tile coding investigate scope learn compact sparse representation. tested unsupervised sparse coding error poor discuss differences representations learned supervised unsupervised sparse coding below. learning curves. ﬁrst demonstrate learning increasing number samples figure weights recomfigure learned representations samples respectively scope unsupervised sparse coding non-negative unsupervised sparse coding puddle world. representations learned mountain acrobot similar structure. non-negative sparse coding additional constraint entry non-negative. goal addition determine constraints could improve prediction accuracy unsupervised sparse coding; though representation qualitatively looks reasonable prediction performance remained poor. puted using entire batch given number samples. across domains scope results faster learning mountain acrobot obtains lowest ﬁnal error. matching performance meaningful wellunderstood optimized domains. acrobot it’s clear larger needed resulting relatively poor performance whereas scope still perform well compact learned sparse representation. learning curves provide insight learn effective sparse representations scope also raise questions. issue scope effective puddle world representations namely reason appears optimize msre obtain representation surrogate mapve. measuring msre instead mapve test data scope consistently outperforms optimizing representation weights according msre overﬁtting issues; extensions msbe improvements selecting regularization parameters alleviate issue. learned representations. also examine learned representations unsupervised sparse coding scope shown figure draw conclusions results structure observations sufﬁcient unsupervised sparse coding combination supervised unsupervised losses sufﬁciently constrain space obtain discriminative representations. two-dimensional four-dimensional observations relatively easy reconstruct observations using small subset dictionary atoms unsupervised representations even additional non-negativity constraints narrow search space less distributed darker thicker blocks frequently pick less features. supervised sparse coding representation however sparsity pattern smoother distributed features selected least sample level sparsity similar. veriﬁed utility supervised sparse coding optimizing supervised loss without including unsupervised loss; resulting representations looked similar purely unsupervised representations. combination losses therefore much effectively constrains regularizes space learning demonstrated scope ideal conditions. intentionally chosen focus question learn effective sparse representations using scope objective? promising results here future work needs investigate utility jointly estimating representation learning value function well providing incremental algorithms learning representations setting regularization parameters. conclusion work investigated sparse coding policy evaluation reinforcement learning. proposed supervised sparse coding objective joint estimation dictionary sparse representation value function weights. provided simple algorithm uses alternating minimization variables proved simple easy-touse approach principled. ﬁnally demonstrate results three benchmark domains mountain puddle world acrobot variety conﬁgurations tile coding. paper provides view using dictionary learning techniques machine learning reinforcement learning. lays theoretical empirical foundation investigating sparse coding dictionary learning approaches policy evaluation suggests show promise. formalizing representation learning dictionary learning problem facilitates extending recent upcoming advances unsupervised learning reinforcement learning setting. example though considered batch gradient descent approach ﬁrst investigation sparse coding objective amenable incremental estimation several works investigating effective stochastic gradient descent algorithms generality approach easy understand optimization make promising direction representation learning reinforcement learning. references alekh agarwal animashree anandkumar prateek jain praneeth netrapalli. learning sparsely used overcomplete dictionaries alternating minimization. ann. conf. learning theory michal aharon michael elad alfred bruckstein. k-svd algorithm designing overcomplete dictionaries sparse representation. ieee transactions signal processing haitham ammar karl tuyls matthew taylor kurt driessens gerhard weiss. reinforcement learning transfer sparse coding. inter. conf. autonomous agents multiagent systems christopher atkeson morimoto. nonparametric representation policies value functions trajectory-based approach. advances neural information processing systems leemon baird. residual algorithms reinforcement learning function approximation. inter. conf. mach. learning shalabh bhatnagar doina precup david silver richard sutton hamid maei csaba szepesvári. convergent temporaldifference learning arbitrary smooth function approximation. advances neural information processing systems george konidaris sarah osentoski philip thomas. value function approximation reinforcement learning using fourier basis. inter. conf. mach. learning manuel loth manuel davy philippe preux. sparse temporal difference learning using lasso. symposium approximate dynamic programming reinforcement learning sridhar mahadevan mauro maggioni. proto-value functions laplacian framework learning representation control markov decision processes. machine learning research sridhar mahadevan stephen giguere nicholas jacek. basis adaptation sparse nonlinear reinforcement learning. aaai conference artiﬁcial intelligence julien mairal francis bach jean ponce guillermo sapiro andrew zisserman. supervised dictionary learning. advances neural information processing systems volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski stig petersen charles beattie amir sadik ioannis antonoglou helen king dharshan kumaran daan wierstra shane legg demis hassabis. human-level control deep reinforcement learning. nature ronald parr lihong gavin taylor christopher painter-wakeﬁeld michael littman. analysis linear models linear value function approximation feature selection reinforcement learning. inter. conf. mach. learning richard sutton steven whitehead. online learning random representations. inter. conf. mach. learning richard sutton hamid maei doina precup shalabh bhatnagar david silver csaba szepesvári eric wiewiora. fast gradientdescent methods temporal-difference learning linear function approximation. inter. conf. mach. learning richard sutton. generalization reinforcement learning successful examples using sparse coarse coding. advances neural information processing systems shimon whiteson matthew taylor peter stone. adaptive tile coding value function approximation. technical report yangyang wotao yin. block coordinate descent method regularized multiconvex optimization applications nonnegative tensor factorization completion. siam imaging sciences huizhen dimitri bertsekas. basis function adaptation methods cost approximation mdp. symposium adaptive dynamic programming reinforcement learning", "year": 2017}