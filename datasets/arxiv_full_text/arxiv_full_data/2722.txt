{"title": "Neural Block Sampling", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Efficient Monte Carlo inference often requires manual construction of model-specific proposals. We propose an approach to automated proposal construction by training neural networks to provide fast approximations to block Gibbs conditionals. The learned proposals generalize to occurrences of common structural motifs both within a given model and across models, allowing for the construction of a library of learned inference primitives that can accelerate inference on unseen models with no model-specific training required. We explore several applications including open-universe Gaussian mixture models, in which our learned proposals outperform a hand-tuned sampler, and a real-world named entity recognition task, in which our sampler's ability to escape local modes yields higher final F1 scores than single-site Gibbs.", "text": "efﬁcient monte carlo inference often requires manual construction model-speciﬁc proposals. propose approach automated proposal construction training neural networks provide fast approximations block gibbs conditionals. learned proposals generalize occurrences common structural motifs within given model across different models allowing construction library learned inference primitives accelerate inference unseen models model-speciﬁc training required. explore several applications including open-universe gaussian mixture models learned proposals outperform hand-tuned sampler real-world named entity recognition task sampler’s ability escape local modes yields higher ﬁnal scores single-site gibbs. model-based probabilistic inference highly successful paradigm machine learning applications tasks diverse movie recommendation visual scene perception music transcription monitoring nuclear tests among many others. people learn plan using mental models indeed entire enterprise modern science viewed constructing sophisticated hierarchy models physical mental social phenomena. probabilistic programming provides formal representation models sample-generating programs promising ability explore rich range models. unfortunately generic inference methods singlesite gibbs sampling often perform poorly suffering slow mixing stuck local optima. effective real-world inference often requires block proposals update multiple variables together overcome near-deterministic long-range dependence structures. however computing exact gibbs proposals large blocks quickly becomes intractable practice common invest signiﬁcant effort handengineering proposals speciﬁc particular model. work propose learn tractable block samplers form approximate gibbs proposals reused within given model across models containing similar structural motifs. recent work recognized wide range models represented compositions simple components domain-speciﬁc models still reuse general structural motifs chains grids rings trees learning ﬂexible samplers improve inference within speciﬁc model even previously unseen models containing similar structures additional training required. contrast techniques compile inference procedures speciﬁc given model learning inference artifacts generalize novel models valuable allowing model builders quickly explore wide range possible models. explore application approach wide range models. grid-structured models inference competition learned proposal signiﬁcantly outperforms gibbs sampling even given model-speciﬁc training. open-universe gaussian mixture models show simple learned block proposal yields performance comparable modelspeciﬁc hand-tuned sampler generalizes models trained additionally apply method named entity recognition task showing learned block proposals effectively ability escape local modes yields higher-quality solutions standard gibbs sampling approach. great interests using learned feedforward inference networks generate approximate posteriors. variational autoencoders train inference network jointly parameters forward model maximize variational lower bound within framework burda utilize another neural network adaptive proposal distribution improve convergence variational inference. however parametric variational distribution means typically limited capacity represent complex potentially multimodal posteriors incorporating discrete variables structural uncertainty. related line work developed data-driven proposals importance samplers training inference network prior samples used proposal given observed evidence. particular generalize framework probabilistic programming able automatically generate train neural proposal network given arbitrary model described probabilistic program. approach differs focus mcmc inference allowing modular proposals subsets model variables depend latent quantities exploit recurring substructure generalize models containing analogous structures additional training. several approaches proposed adaptive block sampling sets variables exhibiting strong correlations identiﬁed dynamically inference costly joint sampling used blocks likely beneﬁcial largely complementary current approach assumes blocks given attempts learn fast approximate proposals. exciting recent works train model-speciﬁc mcmc proposals machine learning techniques. using adversarial training song directly optimize similarity posterior values proposed values symmetric mcmc proposal parametrized special neural network architecture. considering stochastic inverses graphical models inverse mcmc stuhlm¨uller learns gibbslike proposals training density estimators using either prior posterior data. however works limitations applicable models require bootstrapping well model-speciﬁc training using global information e.g. samples containing variables. work differ approach simpler scalable requiring local information generating local proposals reused within across different models. section explores comparison inverse mcmc empirically. taking much broader view approach work learning approximate local update scheme seen related approximate message passing recent advances learning gradient-based optimizers continuous objectives work propose framework using neural network approximate gibbs proposal block variables. learned proposal speciﬁc particular block structure conditioning corresponding approximate markov blanket. together refer components structural motif. crucially proposals model parameters instead provided network input trained network reused perform inference novel models parameterizations previously observed. inference networks parameterized mixture density networks trained minimize divergence true posterior conditional approximate proposal given prior samples generated model. approximate proposals accepted rejected following metropolishastings rule maintain correct stationary distribution even though proposals approximate. following sections describe approach detail. although approach generalizes arbitrary probabilistic programs simplicity focus models represented factor graphs. model consists variables represented nodes graph along factors specifying joint probability distribution described parameters particular paper focuses primarily directed models factors speiciﬁes conditional probability distributions variable given parents. undirected models crfs sec. factors arbitrary functions associated cliques given observations evidence variables inference attempts compute conditional distribution remaining variables. standard approach gibbs sampling variable successively resampled conditional distribution given variables cases conditional fact depends graph. subset known markov blanket v¬i. gibbs update viewed proposal accepted construction thus inheriting guarantee limiting distribution sampling process desired posterior conditional distribution models tight coupling adjacent variables proposals resample single variable time tend slowly. many cases necessary resample multiple variables simultaneously i.e. block proposal. block proposals yield much faster mixing step step much slower; cost computing storing block conditional distribution generally exponential size block becoming intractable large blocks. motivates approach paper train fast feedforward neural networks approximate block proposals much lower computational cost. associate learned proposal structural motif determines shape network inputs outputs. general structural motifs arbitrary subgraphs interested motifs represent interesting conditional structure sets variables block proposed variables conditioning variables given motif multiple instantiations model even across models. concrete example fig. shows instantiations structural motif consecutive variables chain model. instantiation want approximate conditional distribution middle variables given neighboring four. formalize notion structural motif. deﬁnition. structural motif graph nodes partitioned components parameterized joint distribution whose factorization consistent graph structure. speciﬁes functional form conditional speciﬁc parameters. motif many instantiations within particular graphical model well across different models. figure instantiations structural motif directed chain length motif consists consecutive variables markov blanket four neighboring variables. instantiation separated block proposed variables conditioning variables deﬁnition. graphical model instantiation motif includes subset model variables induced subgraph isomorphic motif partition preserved isomorphism subset model parameters required specify joint distribution motif variables. would typically deﬁne structural motif ﬁrst picking block variables jointly sample selecting conditioning intuitively natural choice conditioning markov blanket however ﬁxed requirement could either subset superset might deliberately choose alternate conditioning e.g. subset markov blanket gain faster proposal superset idea learning longer-range structure. fundamentally however markov blankets depend larger graph structure might consistent across instantiations given motif allowing represent generic conditioning leaves greater ﬂexibility instantiating motifs. provides another view approximation problem. choose motif complex structures instantiation conditionals often quite different different thus difﬁcult approximate. therefore choosing structural motif represents trade-off generality proposal easiness approximate. approach works structural motif complying deﬁnition suggest using common structures motifs chain certain length fig. since loss average expected values priors train neural block proposals using minibatch sgd. training sample batch size randomly select motif instantiation generate sample values prior deﬁned instantiation. speciﬁcally batch index neural block mcmc procedure outlined algorithm worth pointing framework allows great amount ﬂexibility. interested good proposal speciﬁc part particular model. neural block proposal trained underlying motif instantiated part. cases order learn general proposal grid models work grid-shaped motif instantiated every possible grid model extend training procedure described sec. modifying match arbitrary distribution instantiations. experiment sec. approach train proposal binary-valued grid models. therefore potentially possible store library neural block proposals trained common motifs speed inference previously unseen models. choose mixture density networks proposal parametrization. form neural network whose outputs parametrize mixture distribution mixture component variables uncorrelated. given sufﬁciently large network number mixture components mdns represent arbitrary joint distributions. function case neural block proposal parametrized weights function represents proposals structural motif taking current values local parameters outputting distribution then goal becomes optimize close true conditional. network output mixture weights represented explicitly. within mixture component distributions bounded discrete variables directly represented probability tables distributions continuous variables represented isotropic gaussians mean variance. avoid degenerate proposals threshold variance gaussian component least optimize kullback-leibler divergence measure closeness proposal true conditional order minimize divergence across instantiations possible values supp total loss linear combination expected divergences instantiations. figure motif general grid models. conditioning variables form markov blanket proposed variables dashed gray arrows show possible irrelevant dependencies. figure sample runs single-site gibbs neural block mcmc block gibbs true conditionals. model compute random initializations three algorithms one. plots show average error algorithm. epochs plots epochs better show comparison true block gibbs ﬁnishes less epochs within given time. section evaluate method learning neural block proposals single-site gibbs sampler well several model-speciﬁc mcmc methods. parameterizing mdns activation hidden layers size max{input size output size} output proposal distribution mixture components. details architecture experiment available supplementary material. start common structural motif graphical models grids. section focus binary-valued grid models sorts relative easiness directly compute posteriors. speciﬁcally test performance mcmc algorithms compare estimated posterior marginals true posterior marginals computed using ijgp then inference task variables calculated error absolute deviation marginal probabilities. neural block proposals potential achieve good performance models shared structural motifs without extra training. ﬁrst experiment consider motif fig. instantiated arbitrary binary-valued grid bayesian networks neural block proposal takes cpts variables well current assignments conditioning variables markov blanket. order proposal generalize need consider motif instantiations possible binary-valued grid models argued sec. thus train proposal random grid generated sampling entry i.i.d. following mixed distribution evaluate performance trained neural block proposal grid nodes inference competition. epoch latent variable identify propose block fig. variable located center. possible e.g. variable boundaries close evidence single-site gibbs resampling used instead. figure shows performance neural block mcmc singles-site gibbs terms error integrated time models. models divided three classes grid- grid- grid- according percentage deterministic relations. neural block mcmc signiﬁcantly outperforms gibbs sampler nearly every model. notice improvement less signiﬁcant percentage deterfigure motif triangle model fig. conditioning variables form markov blanket proposed variables dashed gray arrows possible irrelevant dependencies. figure error w.r.t. epochs triangle model fig. semitransparent line show single mcmc runs. opaque lines show averages mcmc runs algorithm. numbers parentheses amounts training data. ministic relations increases. largely fact proposal structure fig. easily handle dependency among proposed nodes. expect increased block size yield stronger performance models many deterministic relations. investigate behavior neural block mcmc detail compare single-site gibbs exact block gibbs proposal block several grid models different percentages deterministic relations. figure shows performance w.r.t. time epochs models. single-site gibbs performs worst models since gets stuck quickly local modes. block proposal mcmc methods neural block mcmc performing error w.r.t. time shorter computational time. however neural block proposal approximate true block gibbs proposal worse terms error w.r.t. epochs expected. detailed comparisons models available supplementary material. summary experiment results show neural block proposals achieve signiﬁcantly faster better mixing using much less computation overhead calculating exact gibbs block proposal. neural block proposals also used modelspeciﬁcally training particular model. subsection demonstrate method achieve comparable performance complex taskspeciﬁc mcmc method inverse mcmc figure illustrates triangle grid network experiment identical stuhlm¨uller used evaluate inverse mcmc. method chose motif shown fig. underlying takes assignments conditioning variables relevant cpts outputs block proposal variables. proposal trained ininverse mcmc algorithm builds auxiliary data structures ofﬂine speed inference. given inference task computes trains inverse graph latent variable latent variable bottom evidence variables top. graphs used mcmc procedures. experiment inverse mcmc frequency density estimator trained posterior samples proposal block size gibbs proposals precomputed following original approach stuhlm¨uller difﬁcult compare methods w.r.t. time. methods require ofﬂine training inverse mcmc needs train inverse graphs scratch evidence nodes changes neural block mcmc needs one-time training different inference tasks model. experiment setting inference epoch methods propose values average latent variable. figure shows meaningful comparison error w.r.t. epochs among singlesite gibbs neural block mcmc inverse mcmc different amount training data. learned neural block proposal trained using samples able achieve comparable performance inverse mcmc trained using samples builds modelspeciﬁc data structure next consider open-universe gaussian mixture models number mixture components unknown subject prior. similarly dirichlet process gmms typically treated handdesigned model-speciﬁc split-merge mcmc algorithms. figure except bottom right average likelihoods mcmc algorithms inference tasks total various gmms. bottom right trace plots runs initiations different values neural block mcmc explores sample space signiﬁcantly faster gibbs sdds. mixtures unif{ point comes uniformly randomly active mixtures. task infer posterior mixture means {µj}j=...m indicators showing whether active {vj}j=...m labels {zi}i=...n mixture index comes from. formally model written unif{ conditioned variables inference task indeed equivalent ﬁrst sampling figure uncollapsed model known number components component means sampling modify observed points cluster labels reprealgorithm sent open-universe model proposal actruncated form determines whether cepted rejected cluster active rule collapsed deterministically. model. afterwards resampled approach less sensitive different values leads good performance variously sized gmms shown below. details supplementary material. training notice mixture models symmetries must broken used input neural network particular mixtures permuted ways points ways. following similar procedure sort values according ﬁrst principal component also feed ﬁrst principal component vector mdn. using prior samples able train neural block proposal mentioned structural motif. inference randomly choose clusters propose proposal. model parameters. notice completely dependent experiment instead sampling gmm’s many nearly-deterministic relations e.g. cause vanilla single-site gibbs stuck local optima unable efﬁciently explore state space jumping across different values. solve issues split-merge mcmc algorithms restricted gibbs split-merge smart-dumb/dumb-smart hand-designed model-speciﬁc mcmc moves split merge mixture components. neural block mcmc framework possible deal nearly-deterministic relations proposal block including however would require long training time large network cause resulting proposal able generalize gmms various sizes. although proposal trained speciﬁc number mixtures number points also apply gmms larger randomly selecting mixtures points proposal. figure shows neural block mcmc performs various sizes compared split-merge gibbs sdds. particular notice model gets larger gibbs sdds mixes slowly neural block mcmc still mixes fairly fast outperforms gibbs sdds. bottom right fig. shows trace plot algorithms multiple runs observation. gibbs sdds takes long time high likelihood explanation fails explore possible ones efﬁciently. neural block mcmc hand mixes quickly among possible explanations. named entity recognition task inferring named entity tags words natural language sentences. tackle train conditional random ﬁeld model representing joint distribution tags word features particular model contains weights word features tags well high order factors consecutive tags. test sentence build chain markov random ﬁeld containing tags variables using extracted word features learned model apply mcmc methods like single-site gibbs sample tags. experiment dataset sentences taken conll- shared task. model trained adagrad sweeps training dataset. goal train good neural block proposals chain mrfs built test sentences. order experiment different block sizes train three proposals motif three four consecutive proposed variables markov blanket. proposal takes local parameters assignments markov blanket variables outputs proposal mixture components. difﬁculty generating natural language sentences reuse training dataset model train neural block proposals. evaluate learned neural block proposals previously unseen test dataset sentences. figure plots performance neural block mcmc single-site gibbs w.r.t. time epochs entire test dataset. block size grows larger learned proposal takes time mix. evenfigure average scores average likelihoods entire test dataset. epoch variables every test proposed roughly algorithms. scores measured using states highest likelihood seen markov chain traces. better show comparison epoch plots epochs time plots likelihoods shown don’t include normalization constant. tually block proposals generally achieve better performance single-site gibbs terms scores likelihoods. therefore shown ﬁgure mixed proposal single-site gibbs neural block proposals achieve better mixing without slowing much. interesting observation neural block mcmc sometimes achieves higher scores even passing single-site gibbs likelihood implying likelihood best imperfect proxy performance task. paper proposes explores novel idea training neural nets approximate block gibbs proposals. proposals trained ofﬂine applied directly novel models given common structural motifs. experiments show neural block sampling approach help overcome local modes comparing single-site gibbs sampling achieve comparable performance model-specialized methods. current stage framework requires user manually detect common structural motifs choose apply pretrained block sampler. interesting direction investigate given library trained block proposals inference system automatically detect common structural motifs apply appropriate samplers help convergence real-world applications. daniel ritchie anna thomas hanrahan noah goodman. neurally-guided procedural models amortized inference procedural graphics programs using neural networks. sugiyama luxburg guyon garnett editors advances neural information processing systems pages curran associates inc. stephane ross daniel munoz martial hebert andrew bagnell. learning message-passing inference macomputer vision chines structured prediction. pattern recognition ieee conference pages ieee david spiegelhalter andrew thomas nicky best wally gilks lunn. bugs bayesian inference using gibbs sampling. version http//www. mrc-bsu. cam. uk/bugs david stern ralf herbrich thore graepel. matchbox large scale online bayesian recommendations. proceedings international conference world wide pages daniel turek perry valpine christopher paciorek clifford anderson-bergman automated parameter blocking efﬁcient markov chain monte carlo sampling. bayesian analysis sergio g´omez matthew hoffman david pfau schaul nando freitas. learning learn gradient descent gradient descent. sugiyama luxburg guyon garnett editors advances neural information processing systems pages curran associates inc. roger grosse ruslan salakhutdinov william freeman joshua tenenbaum. exploiting compositionality explore large space model structures. conference uncertainly artiﬁcial intelligence pages auai press sonia jain radford neal. split-merge markov chain monte carlo procedure dirichlet process mixture model. journal computational graphical statistics tejas kulkarni pushmeet kohli joshua tenenbaum picture probabilistic programvikash mansinghka. ming language scene perception. proceedings ieee conference computer vision pattern recognition pages tuan atlm baydin frank wood. inference compilation universal probabilistic programming. international conference artiﬁcial intelligence statistics fort lauderdale mentioned main paper sec. parameterizing mdns experiments activation size max{input size output size} depending task output proposal distribution mixture components. directed binary-valued grid models higher amount mixtures experiments because variables proposed general discrete highly multi-modal conditionals. architecture underlying network structure mapping cpts motif variables conditioning variable values proposal distribution proposed variables mixture components. architecture underlying network structure mapping cpts motif variables conditioning variable values proposal distribution proposed variables mixture components. architecture neural block proposal trained using mixtures data points. proposes mixture components underlying network structure. mdn’s inputs include observed points {xi}i component means {µj}j component active indicators {vj}j values proposed mixtures replaced zeros. orders figure additional sample runs single-site gibbs neural block mcmc block gibbs true conditionals grid models. results obtained setting fig. main paper. model compute random initializations three algorithms one. plots show average error algorithm. epochs plots epochs better show comparison. grid-k represents model deterministic relations. sorted along ﬁrst principle component break symmetry. addition inputs also contain principle component indicators components proposed. outputs proposal distribution mixture components. proposal inference collapsed order avoid nearly-deterministic relations e.g. still train general proposal unconstrained choose consider collapsed model without ﬁrst experiment intuitive approach adds resampling step proposal. proposal step trained proposal ﬁrst used propose mixtures proposed rule applied lastly either accept reject proposed values. method gives good performance small models suffers greatly acceptance ratio number observed points grows large. therefore eventually choose approach described main paper sec. i.e. applying rule collapsed model resampled afterwards. since acceptance ratio longer depends approach behaves much scalable ﬁrst experiments. outperforms sdds split-merge mcmc gmms various sizes shown fig. main paper. architecture underlying hidden layers size ×max{input size output size} output size varying according number proposed variables. maps local parameters motif variables conditioning variable values proposal consecutive proposed variables mixture components.", "year": 2017}