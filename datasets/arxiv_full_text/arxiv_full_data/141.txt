{"title": "Stochastic Deep Learning in Memristive Networks", "tag": ["stat.ML", "cs.AI", "cs.LG", "cs.NE"], "abstract": "We study the performance of stochastically trained deep neural networks (DNNs) whose synaptic weights are implemented using emerging memristive devices that exhibit limited dynamic range, resolution, and variability in their programming characteristics. We show that a key device parameter to optimize the learning efficiency of DNNs is the variability in its programming characteristics. DNNs with such memristive synapses, even with dynamic range as low as $15$ and only $32$ discrete levels, when trained based on stochastic updates suffer less than $3\\%$ loss in accuracy compared to floating point software baseline. We also study the performance of stochastic memristive DNNs when used as inference engines with noise corrupted data and find that if the device variability can be minimized, the relative degradation in performance for the Stochastic DNN is better than that of the software baseline. Hence, our study presents a new optimization corner for memristive devices for building large noise-immune deep learning systems.", "text": "several memristive devices explored realizing cross arrays neuromorphic systems recently phase change memory arrays used store synaptic weights -layer neural network handwritten digit classiﬁcation achieving accuracy mnist database numerical simulation studies based experimentally observed programming characteristics pr−xcaxmno synapses suggest mnist recognition accuracies exceeding achievable linear symmetric conductance response observed improve accuracy; hence several strategies proposed compensate linear asymmetric conductance response typical memristive devices projected based dnns provide speed× improvement power compared based implementations therefore highly efﬁcient neuromorphic systems developed memristive device characteristics improved algorithms co-optimized account non-ideal limitations. here ﬁrst stochastic weight updates train -layer deep network double precision ﬂoating-point weights. study performance equivalent network uses cross architecture typical memristive devices limited dynamic range resolution conductance variability best knowledge paper presents ﬁrst study noise resilience inference using stochastic learning dnns non-ideal memristive devices. main insight study device parameter optimize learning efﬁciency dnns variability programming characteristics. dnns memristive synapses even dynamic range resolution levels trained based stochastic updates achieve close ﬂoating point base-line accuracies benchmark hand-written digit recognition task. furthermore degradation performance stochastic memristive dnns used inference engines noise corrupted data better software baseline. paper organized follows ﬁrst discuss fundamental basics training methods accelerate training using stochastic weight updates. describe network used stochastic learning handwritten digit classiﬁcation memristive model crossbar compatible implementation. finally compare performance stochastic learning network software baseline demonstrate superior noise-tolerance characteristics stochastic based inference engines. abstract—we study performance stochastically trained deep neural networks whose synaptic weights implemented using emerging memristive devices exhibit limited dynamic range resolution variability programming characteristics. show device parameter optimize learning efﬁciency dnns variability programming characteristics. dnns memristive synapses even dynamic range discrete levels trained based stochastic updates suffer less loss accuracy compared ﬂoating point software baseline. also study performance stochastic memristive dnns used inference engines noise corrupted data device variability minimized relative degradation performance stochastic better software baseline. hence study presents optimization corner memristive devices building large noise-immune deep learning systems. inspired computational efﬁciency human brain processing unstructured data neural networks explored since wide variety data analytics applications. latest generation deep neural networks achieved impressive successes rivaling typical human performance thanks ability capture hidden features unstructured data using multiple layers neurons however number layers networks increase training becomes computationally intense time consuming physically separated execution memory units conventional neumann machines. motivated exploration non-von neumann architectures closely integrated processing units local memory elements dense cross arrays memristive devices recently proposed dnns implemented cross arrays resistive processing units store multiple analog states adjust conductivity simple voltage pulses devices implemented cross array accelerate training weights array updated parallel. scheme vector cross product operations complexity required back-propagation algorithm network training implemented simple operation stochastic streams representing neuronal signals complexity however order maintain high accuracies stringent speciﬁcations satisﬁed memristive devices resolution conductance levels within dynamic range ieee. personal material permitted. permission ieee must obtained uses current future media including reprinting/republishing material advertising promotional purposes creating collective works resale redistribution servers lists reuse copyrighted component work works. alter state rpu. determine product equation equivalent stochastic streams column cross bar; conductance change depending coincidence stochastic pulse streams. stochastic weight update represented here refers length bernoulli sequence minimum conductance resolution result pulse pair overlap. high-level design studies suggest order maintain network accuracies close ideal software performance devices resolution least programmable levels within dynamic range stringent requirement realize nanoscale devices. study study performance characteristics dnns implemented using models memristive devices representative experimental devices today non-idealities include limited dynamic range resolution variability conductance levels attained programming. study performance characteristics dnns trained using stochastic methods compare performance used inference engines noisecorrupted data. studies reveal stochastic learning method helps mitigate non-ideal effects variability inherent nanoscale devices. −layer network fully connected neurons used study hand written digit classiﬁcation images mnist database used training testing networks. input images pre-processed mean normalization network trained minimizing multi-class cross-entropy objective function sigmoid activation function hidden layers training involves steps forward pass calculate activation functions backward pass calculating weight update required synapses network. forward pass input neuron layer determined based outputs neurons previous layer strength synapses layers according relation non-linear transformation function sinh tanh relu denoting heaviside step function. back propagation error layer back determine error previous layer determine weight update steps complexity network training becomes highly time consuming implemented neumann machines physically separated memory computational units. training accelerated using cross array memristive devices cross point representing synaptic weights neuronal computational circuits periphery operations equations also parallelized leveraging fact programming pulses applied periphery used read program cells array parallel currents involved sufﬁciently small. forward backward pass implemented parallel read operation weight update operation implemented using idea coincidence detection stochastic streams representing real numbers equivalent multiplication operation. stochastic computing framework number represented bernoulli sequence binary random variable probability length bernoulli sequence real numbers range product obtained ﬁnding average binary sequence represent bitwise logical operation bernoulli sequences variables length bernoulli sequence increases error estimated average decreases. thus multiplication operation implemented efﬁciently using simple logic gates coincidence detection operation. resistive processing units based implementation dnns parallel weight update achieved applying programming pulses amplitude ±vs/ cross-bar wires minimum amplitude necessary softmax function output layer. weights updated every image variable learning rate scheme employed learning. refer network uses stochastic pulses forward pass back propagation weight update ‘stochastic dnn’. base line network response deep learning network ﬂoating-point synaptic weight resolution shown fig. epoch training consists presentation images mnist training set. stochastic training error function number bits stochastic code. epochs maximum test accuracy baseline-ﬂoating point stochastic bits stochastic test accuracy drops used. following sections bits used simulations involving stochastic comparable accuracy deterministic improvement throughput learning acceleration. fig. left unidirectional weight update scheme memristive devices synapse cross bar. synaptic weight devices selectively programmed increase decrease right illustration memristive programming starting initial conductance ﬁnal state determined pulse overlap zero mean gaussian noise standard deviation representing programming noise. ﬁnal state under-estimates required conductance change illustrates over-estimate. ratio determines impact programming noise; denotes resolution conductance levels. order represent positive negative synaptic weights devices used synapse effective synaptic weight difference device conductances shown memristive devices exhibit incremental programming direction; hence assume unidirectional device programming scheme conductance device always increments positive direction. increase device selectively programmed limited conductance resolution on-off ratio device saturate maximum conductance state preventing weight updates learning. order avoid facilitate continuous learning devices periodically reset every image presented network. implementation memristive conductance response assumed linear on-off ratio resolution conductance states. first study performance stochastic handwritten digit classiﬁcation function programming variability memristive device. show close base-line accuracies maintained even standard deviation programmed distribution one-third separation levels device. analyze noise resilience characteristics stochastic inference engines devices close ideal conductance variability. fig. comparison training error stochastic bits deterministic ﬂoating point accuracy synaptic weights. accuracy test training also shown. network trained bits used stochastic dnns rest paper. performed numerical simulations incorporating characteristics memristor devices stochastic updates. using memristive synapses minimum programming variability stochastic test accuracy lesser baseline. maximum test accuracy stochastic dnns function programming variability memristive devices shown fig. negligible drop test accuracy even though performance falls quickly programming variability increases further. study performance stochastic dnns close ideal memristive synaptic weights used inference engines noise corrupted test data. network trained using study inference accuracy zero mean gaussian noise variances added normalized mnist test test accuracy network noise-corrupted data shown fig. compared demonstrate highly noise-resilient deep neural networks memristive devices synapse trained using stochastic updates. even limited on-off ratio dynamic range device performance memristive network within base-line ﬂoating point simulation. efﬁcient implementation on-chip machine learning algorithms could circumvent non-ideal characteristics memristive devices designed. detrimental impact non-linearities nanoscale devices minimized used dnns stochastic codes data encoding signal transmission inference engines especially noisy inputs. indiveri linares-barranco legenstein deligeorgis prodromakis integration nanoscale memristor synapses neuromorphic computing architectures nanotechnology vol. burr experimental demonstration tolerancing largescale neural network using phase-change memory synaptic weight element electron devices meeting ieee international ..–.. jang park burr hwang jeong optimization conductance change pr−xcaxmno-based synaptic devices neuromorphic systems ieee electron device letters vol. fumarola accelerating machine learning non-volatile memory exploring device circuit tradeoffs ieee international conference rebooting computing sidler large-scale neural networks implemented nonvolatile memory synaptic weight element impact conductance response european solid-state device research conference sept burr large-scale neural networks implemented non-volatile memory synaptic weight element comparative performance analysis ieee international electron devices meeting panwar rajendran ganguly arbitrary spike time dependent plasticity memristor analog waveform engineering ieee electron device letters vol. june fig. maximum generalization accuracy stochastic trained memristive devices increasing programming variability. ratio standard deviation conductance variability bin-width conductance states. respective non-noise test accuracy stochastic network higher noise-resilience accuracy drops baseline ﬂoating point network drops stochastic network’s degradation minimized bits used stochastic encoding inference even though learning performed stochastic network accuracy drops hence stochastic weight updates compensate noise input well variability introduced nanoscale devices. fig. test accuracy stochastic dnns baseline ﬂoating point network used inference engine noise-corrupted test set. average inference response stochastic experiments shown here. stochastic out-performs baseline. inset shows noise corrupted input image corresponding", "year": 2017}