{"title": "Accelerating Neural Architecture Search using Performance Prediction", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Methods for neural network hyperparameter optimization and meta-modeling are computationally expensive due to the need to train a large number of model configurations. In this paper, we show that standard frequentist regression models can predict the final performance of partially trained model configurations using features based on network architectures, hyperparameters, and time-series validation performance data. We empirically show that our performance prediction models are much more effective than prominent Bayesian counterparts, are simpler to implement, and are faster to train. Our models can predict final performance in both visual classification and language modeling domains, are effective for predicting performance of drastically varying model architectures, and can even generalize between model classes. Using these prediction models, we also propose an early stopping method for hyperparameter optimization and meta-modeling, which obtains a speedup of a factor up to 6x in both hyperparameter optimization and meta-modeling. Finally, we empirically show that our early stopping method can be seamlessly incorporated into both reinforcement learning-based architecture selection algorithms and bandit based search methods. Through extensive experimentation, we empirically show our performance prediction models and early stopping algorithm are state-of-the-art in terms of prediction accuracy and speedup achieved while still identifying the optimal model configurations.", "text": "bowen baker∗ otkrist gupta∗ ramesh raskar nikhil naik media laboratory cambridge harvard university cambridge {bowen otkrist raskar naik}mit.edu methods neural network hyperparameter optimization meta-modeling computationally expensive need train large number model conﬁgurations. paper show standard frequentist regression models predict ﬁnal performance partially trained model conﬁgurations using features based network architectures hyperparameters time-series validation performance data. empirically show performance prediction models much effective prominent bayesian counterparts simpler implement faster train. models predict ﬁnal performance visual classiﬁcation language modeling domains effective predicting performance drastically varying model architectures even generalize model classes. using prediction models also propose early stopping method hyperparameter optimization meta-modeling obtains speedup factor hyperparameter optimization meta-modeling. finally empirically show early stopping method seamlessly incorporated reinforcement learning-based architecture selection algorithms bandit based search methods. extensive experimentation empirically show performance prediction models early stopping algorithm state-of-the-art terms prediction accuracy speedup achieved still identifying optimal model conﬁgurations. present signiﬁcant human expertise labor required designing high-performing neural network architectures successfully training different applications. ongoing research areas—meta-modeling hyperparameter optimization—attempts reduce amount human intervention required tasks. hyperparameter optimization methods snoek focus primarily obtaining good optimization hyperparameter conﬁgurations training human-designed networks whereas meta-modeling algorithms design neural network architectures scratch. sets algorithms require training large number neural network conﬁgurations identifying right hyperparameters right network architecture—and hence computationally expensive. sampling many different model conﬁgurations likely many subpar conﬁgurations explored. human experts quite adept recognizing terminating suboptimal model conﬁgurations inspecting partial learning curves. paper seek emulate behavior automatically identify terminate subpar model conﬁgurations order speedup meta-modeling hyperparameter optimization methods. method parameterizes learning curve trajectories simple features derived model architectures training hyperparameters early time-series measurements learning curve. features train frequentist regression models predicts ﬁnal validation accuracy partially trained neural network conﬁgurations using small training fully trained curves image classiﬁcation language modeling domains. predictions uncertainty estimates figure example learning curves example learning curves experiments considered paper. note diversity convergence times overall learning curve shapes. prior work neural network performance prediction using bayesian methods proposed method signiﬁcantly accurate accessible efﬁcient. hope work leads inclusion neural network performance prediction early stopping practical neural network training pipeline. neural network performance prediction limited work predicting neural network performance training process. domhan introduce weighted probabilistic model learning curves utilize model speeding hyperparameter search small convolutional neural networks fully-connected networks building domhan klein train bayesian neural networks predicting unobserved learning curves using training fully partially observed learning curves. methods rely expensive markov chain monte carlo sampling procedures handcrafted learning curve basis functions. also note swersky develop gaussian process kernel predicting individual learning curves automatically stop restart conﬁgurations. meta-modeling deﬁne meta-modeling algorithmic approach designing neural network architectures scratch. earliest meta-modeling approaches based genetic algorithms bayesian optimization recently reinforcement learning methods become popular. baker q-learning design competitive cnns image classiﬁcation. zoph policy gradients design state-of-the-art cnns recurrent cell architectures. several methods architecture search proposed year since publication baker zoph hyperparameter optimization deﬁne hyperparameter optimization algorithmic approach ﬁnding optimal values design-independent hyperparameters learning rate batch size along limited search network design space. bayesian hyperparameter optimization methods include based sequential model-based optimization gaussian processes neural networks snoek however random search grid search commonly used practical settings recently introduced hyperband multiarmed bandit-based efﬁcient random search technique outperforms state-of-the-art bayesian optimization methods. ﬁrst describe model neural network performance prediction followed description datasets used evaluate model ﬁnally present experimental results. modeling learning curves goal model validation accuracy neural network conﬁguration epoch using previous performance observations conﬁguration trained epochs record time-series validation accuracies. train population conﬁgurations obtaining note problem formulation similar klein propose features derived neural network conﬁguration along subset time-series accuracies t=...τ train regression model estimating model predicts neural network conﬁguration using feature y–τ}. clarity train regression models successive model uses point time-series validation data. shall subsequent sections sequential regression models computationally precise methods train single bayesian model. features features based time-series validation accuracies architecture parameters hyperparameters include validation accuracies t=...τ ﬁrst-order differences validation accuracies include total number weights number layers. include hyperparameters used training neural networks e.g. initial learning rate learning rate decay experiment small deep cnns trained image classiﬁcation datasets lstms trained penn treebank language modeling dataset. figure shows example learning curves three datasets considered experiments. provide brief summary datasets below. please appendix section details search space preprocessing hyperparameters training settings datasets. datasets varying architectures deep resnets sample resnet architectures train tinyimagenet* dataset epochs. vary depths ﬁlter sizes number convolutional ﬁlter block outputs. network depths vary deep resnets sample -layer resnet architectures search space similar zoph varying kernel width kernel height number kernels. train models epochs cifar-. metaqnn cnns sample model architectures search space detailed baker allows varying numbers orderings convolution pooling fully connected layers. models layers svhn experiment layers cifar- experiment. architecture trained svhn cifar- datasets epochs. lstm sample lstm models train penn treebank dataset epochs evaluating perplexity validation set. vary number lstm cells hidden layer inputs datasets varying hyperparameters cuda-convnet train cuda-convnet architecture varying values initial learning rate learning rate reduction step size weight decay convolutional fully connected layers scale power local response normalization layers. train models cifar- epochs svhn epochs. table frequentist model comparison report coefﬁcient determination four standard methods. model trained samples learning curve. ν-svr works best average though large margin. figure predicted true values final performance show shape predictive distribution three experiments metaqnn models deep resnets lstms. ν-svr model trained conﬁgurations data learning curve. predict validation classiﬁcation accuracy metaqnn deep resnets perplexity lstms. choice regression method describe results performing ﬁnal neural network performance. experiments train srms randomly sampled neural network conﬁgurations. obtain best performing method using random hyperparameter search -fold cross-validation. compute regression performance remainder dataset using coefﬁcient determination repeat experiment times report results standard errors. experiment different frequentist regression models including ordinary least squares random forests ν-support vector machine regressions seen table ν-svr linear kernels perform best datasets though large margin. rest paper ν-svr unless otherwise speciﬁed. ablation study feature sets table compare predictive ability different feature sets training time-series features obtained learning curve along features architecture parameters hyperparameters features explain largest fraction variance cases. datasets varying architectures important hyperparameter search datasets important expected. features almost match resnet dataset indicating choice architecture large inﬂuence accuracy resnets. figure shows true predicted performance test points three datasets trained features. generalization depths also test whether srms accurately predict performance out-of-distribution neural networks. particular train along features resnets dataset using models number layers less threshold test models number layers greater averaging runs. value varies table ablation study feature sets time-series features refers partially observed learning curves architecture parameters refer number layers number weights deep model hyperparameters refer optimization parameters learning rate. results learning curve used compare neural network performance prediction ability srms three existing learning curve prediction methods bayesian neural network learning curve extrapolation method last seen value heuristic training present subset fully observed learning curves also partially observed learning curves training set. present partially observed curves ν-svr training felt fair comparison ν-svr uses entire partially observed learning curve inference. methods incorporate prior learning curves training. figure shows obtained method predicting ﬁnal performance versus percent learning curve used training model. neural network conﬁguration spaces across datasets either srms outperform competing methods. lastseenvalue heuristic becomes viable conﬁgurations near convergence performance worse deep models. also srms outperform method experiments even remove extreme prediction outliers produced lce. finally outperforms lastseenvalue methods iterations observed worse proposed method. summary show simple frequentist srms outperforms existing bayesian approaches predicting neural network performance modern deep models computer vision language modeling tasks. since experiments perform stepwise learning rate decay; conceivable performance srms results lack sharp jump basis functions. experimented exponential learning rate decay basis functions designed for. trained random nets elrd metaqnn-cifar nets. predicting learning curve ν-svr bnn. comparison illuminates another beneﬁt method require handcrafted basis functions model learning curve types. training inference speed comparison another advantage regression approach speed. srms much faster train inference proposed bayesian methods core intel ν-svr training points trains seconds inference takes seconds. comparison code takes seconds code takes seconds hardware inference. speed hyperparameter optimization meta-modeling methods develop algorithm determine whether continue training partially trained model conﬁguration using sequential regression models. would like sample total neural network conﬁgurations begin sampling training conﬁgurations create training train model predict given current best performance observed ybest would like terminate ybest training conﬁguration given partial learning curve sample different training sets plot mean shade corresponding standard error. compare method last seen value heuristic absent results model indicate achieve positive results cuda-convnet svhn dataset shown appendix figure however case poor out-of-sample generalization mistakenly terminate optimal conﬁguration. assume estimate modeled gaussian perturbation true value probability note general uncertainty depend conﬁguration number points observed learning curve. frequentist models admit natural estimate uncertainty assume independent still dependent estimate leave cross validation. estimate model uncertainty given conﬁguration observed learning curve termination criteria balances trade-off increased speedups risk prematurely terminating good conﬁgurations. many cases want several conﬁgurations close optimal purpose ensembling. offer modiﬁcations case. first relax termination criterion allow conﬁgurations within optimal performance complete training. alternatively criterion based best conﬁguration observed guaranteeing high probability conﬁgurations fully trained. early stopping meta-modeling baker train q-learning agent design convolutional neural networks. method agent samples architectures large ﬁnite space traversing path input layer termination layer. however metaqnn method uses gpu-days train neural architectures similar experiment zoph utilized gpu-days train models cifar-. amount computing resources required approaches makes prohibitively expensive large datasets larger search spaces. main computational expense reinforcement learning-based meta-modeling methods training neural network conﬁguration epochs detail performance ν-svr speeding architecture search using sequential conﬁguration selection. first take random models metaqnn search space. simulate metaqnn algorithm taking random orderings running early stopping algorithm. compare early stopping figure simulated speedup metaqnn search space compare three variants early stopping algorithm presented section ν-svr trained using ﬁrst learning curves algorithm tested independent orderings model conﬁgurations. triangles indicate algorithm successfully recovered optimal model half orderings indicate not. algorithm baseline similar probability threshold termination criterion. trains ﬁrst fully observed curves model trains individual partial curve begin early termination immediately. despite burn time needed still able signiﬁcantly outperform model addition ﬁtting model learning curve takes minutes modern expensive mcmc sampling necessary model time point learning curve observed. therefore full meta-modeling experiment involving thousands neural network conﬁgurations method could faster several orders magnitude compared based current implementations. furthermore simulate early stopping resnets trained cifar-. found probability threshold resulted recovering model consistently. however even conservative threshold search sped factor baseline. computational resources full experiment zoph method could provide similar gains large scale architecture searches. enough however simply simulate speedup meta-modeling algorithms typically observed performance order update acquisition function inform future sampling. reinforcement learning setting performance given agent reward also empirically verify substituting cause metaqnn agent converge subpar policy. replicating metaqnn experiment cifar- integrating early stopping q-learning procedure disrupt learning resulted speedup speedup relatively conservative value training models epochs also resulting performance original results baker figure metaqnn cifar- early stopping full metaqnn algorithm cifar dataset early stopping. ν-svr probability threshold light blue bars indicate average model accuracy decrease represents shift greedy policy. also plot cumulative best show agent continues better architectures. figure simulated accuracy iterations hyperband show trajectories maximum performance versus total computational resources used consecutive hyperband runs f-hyperband remains hyperband curve iterations less aggressive settings converge better ﬁnal accuracy. triangle marks completion full hyperband algorithm. recently introduced hyperband random search technique based multi-armed bandits obtains state-of-the-art performance hyperparameter optimization variety settings. hyperband algorithm trains population models different hyperparameter conﬁgurations iteratively discards models certain percentile performance among population computational budget exhausted satisfactory results obtained. fast hyperband present fast hyperband algorithm based early stopping scheme. iteration successive halving hyperband trains conﬁgurations epochs. f-hyperband train predict early stopping within iteration successive halving. initialize f-hyperband exactly vanilla hyperband except trained models iterations begin early stopping future successive halving iterations train iterations. this exhibit initial slowdown hyperband burn-in phase. also introduce parameter denotes proportion models iteration must trained full iterations. similar setting criterion based best model previous section. appendix section algorithmic representation f-hyperband. empirically evaluate f-hyperband using cuda-convnet trained cifar- svhn datasets. figure shows f-hyperband evaluates number unique conﬁgurations hyperband within half compute time achieving ﬁnal accuracy within standard error. reinitializing hyperparameter searches previously-trained srms achieve even larger speedups. figure appendix shows achieve speedup cases. paper introduce simple fast accurate model predicting future neural network performance using features derived network architectures hyperparameters time-series performance data. show performance drastically different network architectures jointly learned predicted image classiﬁcation language models. using simple algorithm speedup hyperparameter search techniques complex acquisition functions q-learning agent factor hyperband—a state-of-the-art hyperparameter search method—by factor without disturbing search procedure. outperform competing methods performance prediction terms accuracy train test time speedups obtained hyperparameter search methods. hope simplicity success method allow easily incorporated current hyperparameter optimization pipelines deep neural networks. advent large scale automated architecture search methods vital exploring even larger complex search spaces. references bowen baker otkrist gupta nikhil naik ramesh raskar. designing neural network architectures using reinforcement learning. international conference learning representations corinna cortes xavier gonzalvo vitaly kuznetsov mehryar mohri scott yang. adanet adaptive structural learning artiﬁcial neural networks. international conference machine learning frank hutter holger hoos kevin leyton-brown. sequential model-based optimization general algorithm conﬁguration. international conference learning intelligent optimization springer aaron klein stefan falkner jost tobias springenberg frank hutter. learning curve prediction bayesian neural networks. international conference learning representations lisha kevin jamieson giulia desalvo afshin rostamizadeh ameet talwalkar. hyperband novel bandit-based approach hyperparameter optimization. international conference learning representations david schaffer darrell whitley larry eshelman. combinations genetic algorithms neural networks survey state art. international workshop combinations genetic algorithms neural networks jasper snoek oren rippel kevin swersky ryan kiros nadathur satish narayanan sundaram mostofa patwary prabhat ryan adams. scalable bayesian optimization using deep neural networks. international conference machine learning masanori suganuma shinichi shirakawa tomoharu nagao. genetic programming approach designing convolutional neural network architectures. arxiv preprint arxiv. deep resnets sample resnet architectures train tinyimagenet† dataset epochs. vary depths ﬁlter sizes number convolutional ﬁlter block outputs. filter sizes sampled number ﬁlters sampled resnet block composed three convolutional layers followed batch normalization summation layers. vary number blocks giving networks depths varying network trained epochs using nesterov optimizer. learning rate learning rate reduction momentum respectively. deep resnets sample -layer resnet architectures search space similar zoph varying kernel width kernel height number kernels. train models epochs cifar-. architecture consists layers conv pool conv pool conv softmax. conv layer followed batch normalization relu nonlinearity. block conv layers densely connected residual connections also share kernel width kernel height number learnable kernels. kernel height width independently sampled number kernels sampled finally randomly sample residual connections block conv layers. network trained epochs using rmsprop optimizer weight decay initial learning rate learning rate reduction epoch cifar- dataset. metaqnn cnns sample model architectures search space detailed baker allows varying numbers orderings convolution pooling fully connected layers. models layers svhn experiment layers cifar- experiment. architecture trained svhn cifar- datasets epochs. table displays state space metaqnn algorithm. layer parameters layer depth receptive ﬁeld size stride receptive ﬁelds representation size layer depth representation size layer depth consecutive layers neurons previous state type table experimental state space metaqnn. layer type list relevant parameters values parameter allowed take. networks sampled beginning starting layer. convolutional layers allowed transition layer. pooling layers allowed transition layer pooling layers. fully connected layers allowed transition fully connected softmax layers. convolutional pooling layer fully connected layer current image representation size space randomly sample simulate behavior metaqnn well directly metaqnn early stopping. lstm sample lstm models train penn treebank dataset epochs. number hidden layer inputs lstm cells varied steps network trained epochs batch size trained models using stochastic gradient descent. dropout ratio used prevent overﬁtting. dictionary size words used generate embeddings vectorizing data. cuda-convnet train cuda-convnet architecture varying values initial learning rate learning rate reduction step size weight decay convolutional fully connected layers scale power local response normalization layers. train models cifar- epochs svhn epochs. table show hyperparameter ranges cuda convnet experiments. training random forest divided data training validation used cross validation techniques select optimal hyperparameters. model trained full training data using best hyperparameters. random forests varied number trees varied ratio number features ν-svr perform random search hyperparameter conﬁgurations space loguniform uniform loguniform figure cuda convnet svhn performance prediction results plot performance method versus percent learning curve observed cuda convnet svhn experiment. ν-svr sample different training sets plot mean shade corresponding standard error. compare method last seen value heuristic absent results model indicate achieve positive figure simulated speedup hyperband hyperband iteration show speedup using f-hyperband algorithm hyperband consecutive runs major jump speedup comes iteration trained models full iterations. algorithm text replicates algorithm except initialize dictionaries store training data store performance prediction models. correspond dictionary containing datasets prediction target epoch correspond dataset predicting based observed hold corresponding performance prediction model. assume performance prediction model train function predict function return prediction standard deviation prediction. addition standard hyperband hyperparameters include described section iteration successive halving train conﬁgurations epochs; denotes fraction models full iterations. similar setting criterion based best model previous section. also detail run_then_return_validation_loss function algorithm algorithm runs conﬁgurations adds training data observed learning curves trains performance prediction models enough training data present uses models terminate poor conﬁgurations. assumes function max_k returns value list less values. hyperparameter conﬁgurations resources training conﬁgurations next iteration successive halving dictionary storing training data dictionary storing performance prediction models", "year": 2017}