{"title": "The Kernel Pitman-Yor Process", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In this work, we propose the kernel Pitman-Yor process (KPYP) for nonparametric clustering of data with general spatial or temporal interdependencies. The KPYP is constructed by first introducing an infinite sequence of random locations. Then, based on the stick-breaking construction of the Pitman-Yor process, we define a predictor-dependent random probability measure by considering that the discount hyperparameters of the Beta-distributed random weights (stick variables) of the process are not uniform among the weights, but controlled by a kernel function expressing the proximity between the location assigned to each weight and the given predictors.", "text": "abstract—in work propose kernel pitman-yor process nonparametric clustering data general spatial temporal interdependencies. kpyp constructed ﬁrst introducing inﬁnite sequence random locations. then based stick-breaking construction pitman-yor process deﬁne predictor-dependent random probability measure considering discount hyperparameters beta-distributed random weights process uniform among weights controlled kernel function expressing proximity location assigned weight given predictors. authors proposed kernel stick-breaking process ksbp imposes assumption clustering probable feature vectors close prescribed space associated explicitly spatial temporal position modeled data. ksbp capable exploiting available prior information regarding spatial temporal relations dependencies modeled data. nonparametric bayesian modeling techniques especially dirichlet process mixture models become popular statistics last years performing nonparametric density estimation theory based observation inﬁnite number component distributions ordinary ﬁnite mixture model tends limit dirichlet process prior eventually nonparametric bayesian inference scheme induced model yields posterior distribution proper number model component densities rather selecting ﬁxed number mixture components. hence obtained nonparametric bayesian formulation eliminates need inference number mixture components necessary represent modeled data. interesting alternative dirichlet process prior nonparametric bayesian modeling pitman-yor process prior pitman-yor processes produce power-law distributions allow better modeling populations comprising high number clusters popularity number clusters high popularity indeed pitman-yor process prior viewed generalization dirichlet process prior reduces speciﬁc selection parameter values. gaussian process-based coupled method joint segmentation multiple images proposed. inspired advances motivated interesting properties paper come different approach towards predictordependent nonparametric bayesian clustering. ﬁrst introduce inﬁnite sequence random spatial temporal locations. then based stick-breaking construction pitman-yor process deﬁne predictor-dependent random probability measure considering discount hyperparameters beta-distributed random weights process uniform among weights controlled kernel function expressing proximity location assigned weight given predictors. obtained random probability measure dubbed kernel pitman-yor process non-parametric clustering data general spatial temporal interdependencies. empirically study performance kpyp prior unsupervised image segmentation text-dependent speaker identiﬁcation compare kernel stick-breaking process dirichlet process prior. remainder paper organized follows section provide brief presentation pitman-yor process well kernel stick-breaking process desirable properties clustering data spatial temporal dependencies. section proposed nonparametric prior clustering data temporal spatial dependencies introduced relations existing methods discussed efﬁcient variational bayesian algorithm model inference derived. number unique values scales total number draws. note also that pitman-yor process reduces dirichlet process case number unique values grows slowly characterization distribution random variable drawn provided stick-breaking construction sethuraman consider inﬁnite collections independent random variables drawn beta distribution independently drawn base distribution stick-breaking representation given alternative approaches allowing taking account additional prior information regarding spatial temporal dependencies modeled datasets kernel stick-breaking process introduced basic notion formulation ksbp consists introduction predictor-dependent prior promotes clustering adjacent data points prescribed space. consider observed data points associated positions measurement taken arranged d-dimensional lattice. example cases sequential data modeling observed data points naturally associated one-dimensional lattice depicts temporal succession i.e. time point measurements taken. cases computer vision applications might dealing observations measured different locations two-dimensional three-dimensional space take prior information account ksbp postulates random process comprises function predictors related observable data points expressing location prescribed space speciﬁcally assumed innovation parameter denoted essentially distribution placed distribution. suppose randomly draw sample distribution subsequently independently draw random variables integrating joint distribution variables shown exhibit clustering effect. speciﬁcally given ﬁrst samples shown sample either drawn base distribution probability selected existing draws according multinomial allocation probabilities proportional number previous draws allocation {θc}c distinct values taken variables denoting number values equal distribution given shown form pitman-yor process functions similar dirichlet process. suppose randomly draw sample distribution subsequently independently draw random variables observe yields expression quite similar also possessing rich-gets-richer clustering property i.e. samples assigned draw likely subsequent samples assigned draw. further draw likely sample assigned draw effects together produce power-law distribution many unique values observed rarely particular innovation parameter process conditioned satisfy dist distance metric used employed kernel function. random probability measure kernel pitman-yor process denote stick-breaking construction kpyp follows directly deﬁnition relevant discussions section considering kpyp cluster locations {ˆxc}∞ kernel function satisfying constraints innovation parameter selecting appropriate form kernel function ksbp allows obtaining prior probabilities derived clusters depend values predictors indeed closer location observation location assigned cluster higher prior probability becomes. thus ksbp prior promotes construction clustering adjacent data points. example typical selection kernel radial basis function kernel obtain clustering algorithm takes account prior information regarding adjacencies observed data locations space promoting clustering data adjacent space discouraging clustering data points relatively near feature space locations space purpose seek provide location-dependent nonparametric prior clustering observed data motivated deﬁnition properties pitman-yor process discussed previous section effect goals work introduce random probability measure which given ﬁrst samples drawn sample associated measurement location distributed according inference nonparametric models conducted bayesian setting typically means variational bayes monte carlo techniques here prefer variational bayesian approach better computational costs. purpose additionally impose gamma prior innovation parameter variational bayesian inference formalism consists derivation family variational posterior distributions approximate true posterior distribution {zn}n innovation parameter apparently inﬁnite dimensional setting bayesian inference tractable. reason value variational posterior property i.e. equal zero {zn}n parameters truncated model prior distribution imposed hyperparameters model comprising {ψc}c hyperparameters priors imposed innovation parameter likelihood parameters model. variational bayesian inference consists derivation approximate posterior maximization variational free energy deﬁnition observe difference kpyp ksbp ksbp multiplies stick variables sharing beta prior bounded kernel function centered location unique stick obtain predictor dependent random probability measure. instead kpyp considers stick variables different beta priors prior stick variable employing different discount hyperparameter deﬁned bounded kernel centered location unique stick. kpyp controls assignment observations clusters discounting clusters centers clustered data points locations space observe given observation location cluster center increase value kernel function induces much greater increase expected value stick variable employed kpyp compared increase expectation stick variable employed ksbp. hence predictor dependent prior probabilities cluster assignment kpyp appear vary steeply employed kernel function values compared ksbp. finally regarding model hyperparameters obtain hyperparameters employed kernel functions maximization lower bound heuristically select values rest. regarding determination locations assigned obtained clusters obtained either random selection maximization variational free energy them. latter procedure conducted means appropriate iterative maximization algorithm; here employ popular l-bfgs algorithm purpose. random selection estimation means variational free energy optimization using l-bfgs algorithm shall evaluated experimental section paper. blei jordan variational methods dirichlet process int. conf. machine learning york july pitman two-parameter poisson-dirichlet distribution derived stable subordinator annals probability vol. goldwater grifﬁths johnson interpolating types tokens estimating power-law generators advances neural information processing systems vol. schwaighofer tresp w.-y. zhang collaborative ensemble learning combining collaborative content-based information ﬁltering hierarchical bayes proceedings conference uncertainty artiﬁcial intelligence martin fowlkes malik database human segmented natural images application evaluating segmentation algorithms measuring ecological statistics proc. int’l conf. computer vision vancouver canada july wang shterev wang carin dunson hierarchical kernel stick-breaking process multi-task image analysis proceedings international conference machine learning icml unnikrishnan pantofaru hebert measure objective evaluation image segmentation algorithms proc. ieee conf. computer vision pattern recognition diego june", "year": 2012}