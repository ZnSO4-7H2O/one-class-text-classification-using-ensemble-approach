{"title": "A Baseline for Detecting Misclassified and Out-of-Distribution Examples  in Neural Networks", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.", "text": "consider related problems detecting example misclassiﬁed out-of-distribution. present simple baseline utilizes probabilities softmax distributions. correctly classiﬁed examples tend greater maximum softmax probabilities erroneously classiﬁed out-of-distribution examples allowing detection. assess performance deﬁning several tasks computer vision natural language processing automatic speech recognition showing effectiveness baseline across all. show baseline sometimes surpassed demonstrating room future research underexplored detection tasks. machine learning classiﬁers employed real-world tasks tend fail training test distributions differ. worse classiﬁers often fail silently providing highconﬁdence predictions woefully incorrect classiﬁers failing indicate likely mistaken limit adoption cause serious accidents. example medical diagnosis model consistently classify high conﬁdence even difﬁcult examples human intervention. resulting unﬂagged erroneous diagnoses could blockade future machine learning technologies medicine. generally importantly estimating model error great concern safety high-conﬁdence predictions frequently produced softmaxes softmax probabilities computed fast-growing exponential function. thus minor additions softmax inputs i.e. logits lead substantial changes output distribution. since softmax function smooth approximation indicator function uncommon uniform distribution outputted out-of-distribution examples. indeed random gaussian noise mnist image classiﬁer gives prediction conﬁdence predicted class probability show later. throughout experiments establish prediction probability softmax distribution poor direct correspondence conﬁdence. consistent great deal anecdotal evidence researchers however work also show prediction probability incorrect out-of-distribution examples tends lower prediction probability correct examples. therefore capturing prediction probability statistics correct in-sample examples often sufﬁcient detecting whether example error abnormal even though prediction probability viewed isolation misleading. prediction probabilities form detection baseline demonstrate efﬁcacy various computer vision natural language processing automatic speech recognition tasks. prediction probabilities create consistently useful baseline times less effective revealing room improvement. give ideas future detection research contribute addition baseline methods another contribution work designation standard tasks evaluation metrics assessing automatic detection errors out-of-distribution examples. large number well-studied tasks across three research areas using standard neural network architectures perform well them. out-of-distribution detection provide ways supply out-of-distribution examples test time like using images different datasets realistically distorting inputs. hope researchers pursue tasks future work surpass performance baselines. summary softmax classiﬁer probabilities directly useful conﬁdence estimates estimating model conﬁdence bleak previously believed. simple statistics derived softmax distributions provide surprisingly effective determine whether example misclassiﬁed different distribution training data demonstrated experimental results spanning computer vision natural language processing speech recognition tasks. creates strong baseline detecting errors out-of-distribution examples hope future research surpasses. paper interested related problems. ﬁrst error success prediction predict whether trained classiﬁer make error particular held-out test example; predict correctly classify said example? second inout-of-distribution detection predict whether test example different distribution training data; predict within distribution? present simple baseline solving problems. evaluate solution evaluation metrics. mentioning evaluation metrics ﬁrst note comparing detectors straightforward using accuracy. detection classes detector outputs score positive negative class. negative class likely positive class model always guess negative class obtain high accuracy misleading must specify score threshold positive examples classiﬁed correctly depends upon trade-off false negatives false positives faced issue employ area receiver operating characteristic curve metric threshold-independent performance evaluation curve graph showing true positive rate false positive rate other. moreover auroc interpreted probability positive example greater detector score/value negative example consequently random positive example detector corresponds auroc perfect classiﬁer corresponds auroc sidesteps issue threshold selection area precision-recall curve sometimes deemed informative auroc ideal positive class negative class greatly differing base rates aupr adjusts different positive negative base rates. reason aupr second evaluation metric. curve plots precision recall other. baseline detector aupr approximately equal precision perfect classiﬁer aupr consequently base rate positive class greatly inﬂuences aupr detection must specify class positive. view this show auprs treat success/normal classes positive show areas treat error/abnormal classes positive. treat error/abnormal classes positive multiplying scores labeling positive. note treating error/abnormal classes positive classes change auroc since score successfully classiﬁed value score erroneously classiﬁed value auroc begin experiments section describe simple baseline uses maximum probability softmax label distribution neural network classiﬁers. section describe method uses additional auxiliary model component trained reconstruct input. follows retrieve maximum/predicted class probability softmax distribution thereby detect whether example erroneously classiﬁed out-of-distribution. speciﬁcally separate correctly incorrectly classiﬁed test examples example compute softmax probability predicted class i.e. maximum softmax probability. groups obtain area curves. areas summarize performance binary classiﬁer discriminating values/scores across different thresholds. description treats correctly classiﬁed examples positive class denoted success succ tables. error treat incorrectly classiﬁed examples positive class; label incorrectly classiﬁed examples positive take negatives softmax probabilities predicted classes scores. treat in-distribution correctly classiﬁed test examples positive softmax probability predicted class score treat out-of-distribution examples positive negative aforementioned probability. since auprs success error classiﬁers depend rate positive examples list area random detector would achieve base values. also upcoming results list mean predicted class probability wrongly classiﬁed examples demonstrate softmax prediction probability misleading conﬁdence proxy viewed isolation. pred. prob columns show shortcoming out-of-distribution examples. table labels aside begin experimentation datasets vision consider tasks natural language processing automatic speech recognition. following experiments aurocs differ random baselines high statistical signiﬁcance according wilcoxon rank-sum test. following computer vision tasks three datasets mnist cifar- cifar mnist dataset handwritten digits consisting training testing examples. meanwhile cifar- colored images belonging different classes training testing examples. cifar- difﬁcult different classes training testing examples. table correctly classiﬁed incorrectly classiﬁed examples sufﬁciently distinct thus allow reliable discrimination. note area curves degrade image recognizer test error. next consider using softmax distributions determine whether example inoutof-distribution. test examples in-distribution examples. out-ofdistribution examples realistic images noise. cifar- cifar- realistic images scene understanding dataset consists different scenes mnist grayscale realistic images three sources. omniglot images handwritten characters rather handwritten digits mnist. next notmnist consists typeface characters. last realistic images cifar-bw black white rescaled cifar- images. synthetic gaussian data also tried using divergence softmax distribution uniform distribution detection. divergence values detector aurocs auprs highly correlated aurocs auprs detector using maximum softmax probability. divergence similar entropy table softmax predicted class probability allows discrimination correctly incorrectly classiﬁed test examples. pred. prob wrong mean softmax probability wrongly classiﬁed examples showcasing shortcoming direct measure conﬁdence. succ/err base values aurocs auprs achieved random classiﬁers. entries percentages. in-distribution out-of-distribution cifar-/sun cifar-/gaussian cifar-/all cifar-/sun cifar-/gaussian cifar-/all mnist/omniglot mnist/notmnist mnist/cifar-bw mnist/gaussian mnist/uniform mnist/all results shown table notice mean predicted/maximum class probabilities prediction probability alone translated conﬁdence softmax distribution uniform cifar-. shows softmax probabilities viewed direct representation conﬁdence. fortunately out-of-distribution examples sufﬁciently differ prediction probabilities in-distribution examples allowing successful detection generally high area curves. reproducibility specify model architectures. mnist classiﬁer three-layer neuron-wide fully-connected network trained epochs adam uses gelu nonlinearity standard normal distribution. initialize weights according suited arbitrary nonlinearities. cifar- cifar- train wide residual network epochs stochastic gradient descent using restarts gelu nonlinearity standard mirroring cropping data augmentation. ﬁrst task binary sentiment classiﬁcation using imdb dataset dataset polarized movie reviews training test reviews. task allows determine classiﬁers trained relatively small dataset still produce informative softmax distributions. task linear classiﬁer taking input average trainable randomly initialized word vectors dimension train epochs adam early stopping based upon held-out training reviews. again table shows softmax distributions differ correctly incorrectly classiﬁed examples prediction probabilities allow detect reliably examples right wrong. customer review movie review datasets out-of-distribution examples. customer review dataset reviews products rather movies movie review dataset snippets professional movie reviewers rather full-length amateur reviews. leave test examples imdb in-distribution examples out-of-distribution examples test reviews customer review movie review datasets respectively. table displays detection results showing similar story table turn text categorization tasks determine whether softmax distributions useful detecting similar out-of-distribution examples. following text categorization tasks train classiﬁers predict subject text processing. newsgroups dataset different newsgroup subjects total documents whole dataset. reuters dataset eight different news subjects nearly stories total. reuters dataset news subjects slightly news stories; dataset three stories single subject. newsgroups dataset train linear classiﬁer -dimensional word vectors epochs. meanwhile reuters retuers one-layer neural networks bag-of-words input gelu nonlinearity optimized adam epochs. train subset subjects leaving newsgroup subjects newsgroups news subjects reuters news subjects reuters leaving rest out-of-distribution examples. table shows datasets architectures detect errors dependably table informs softmax prediction probabilities allow detecting out-of-distribution subjects. part-of-speech tagging newswire social media text next challenge. wall street journal portion penn treebank contains distinct tags. social media pos-annotated tweets contain tags. tagger train bidirectional long short-term memory recurrent neural network three layers neurons layer randomly initialized word vectors trained corpus epochs stochastic gradient descent batch size tweet tagger simpler twolayer neural network gelu nonlinearity weight initialization according pretrained word vectors trained corpus million tweets hidden layer size training tweets epochs adam early stopping validation tweets. error detection results table out-ofdistribution detection tagger tweets well weblog data english treebank results shown table since weblog data closer style newswire tweets harder detect whether weblog sentence outof-distribution tweet. indeed since tagging done word-level detecting whether word out-of-distribution given word contextual features. mind easier detect words out-of-distribution tweets blogs. table detecting out-of-distribution tweets blog articles part-of-speech tagging. values percentages. *these examples atypically close training distribution. consider task uses softmax values construct entire sequences rather determine input’s class. sequence prediction system uses bidirectional lstm two-layers clipped gelu nonlinearity optimized epochs rmsprop trained timit corpus lstm trained connectionist temporal classiﬁcation predicting sequences phones given mfccs energy ﬁrst second deltas frame. trained lstm learns phone label probabilities spike momentarily mostly predicting blank symbols otherwise. softmax used differently typical classiﬁcation problems providing unique test detection methods. in-distribution out-of-distribution timit/timit+airport timit/timit+babble timit/timit+car timit/timit+exhibition timit/timit+restaurant timit/timit+street timit/timit+subway timit/timit+train timit/chinese timit/all distribution detection. mixing timit audio realistic noises aurora- dataset keep timit audio volume noise volume giving mean approximately speakers still clearly audible human confuse phone recognizer prediction edit distance doubles. outof-distribution examples test examples thchs- dataset chinese speech corpus. table shows results. crucially performing detection compute softmax probabilities ignoring blank symbol’s logit. blank symbol’s presence softmax distributions time steps predict blank symbol high conﬁdence without blank symbol better differentiate normal abnormal distributions. modiﬁcation softmax prediction probabilities allow detect whether example out-of-distribution. seen softmax prediction probabilities enable abnormality detection show information sometimes useful detection. demonstrate this exploit learned internal representations neural networks. start training normal classiﬁer append auxiliary decoder reconstructs input shown figure auxiliary decoders sometimes known increase classiﬁcation performance decoder scorer trained jointly in-distribution examples. thereafter blue layers figure frozen. train layers clean noised training examples sigmoid output layers scores normal input consequently noised examples abnormal class clean examples normal class sigmoid trained output class input belongs. training consequently normal classiﬁer auxiliary decoder call abnormality module. gains abnormality module demonstrate possible research avenues outperforming baseline. test abnormality module revisiting timit task different architecture show auxiliary components greatly improve detection. system three-layer neuron wide classiﬁer auxiliary decoder abnormality module. network takes input frames must predict phone center frame features frame. weights initialized according network trains epochs abnormality module trains two. abnormality module sees clean examples negative examples timit examples distorted either white noise brown noise pink noise various volumes. note abnormality module trained type noise added test examples. nonetheless table shows simple noised examples translate effective detection realistically distorted audio. detect abnormal examples comparing typical abnormality table abnormality modules generalize novel distortions detect out-of-distribution examples even severely degrade accuracy. values percentages. module outputs clean examples outputs distorted examples. noises aurora- added timit examples volume. also thchs- dataset chinese speech. unlike before thchs- training examples rather test examples fully connected networks evaluate whole training sufﬁciently quickly. worth mentioning fully connected deep neural networks noise robust abnormality module still detect whether example out-of-distribution. remarkable note network’s frame classiﬁcation error entire test dataset average classiﬁcation error distorted examples .%—this unlike bidirectional lstm pronounced performance decline. classiﬁcation degradation slight softmax statistics alone provide useful outof-distribution detection. contrast abnormality module provided scores allowed detection different-but-similar examples. practice important determine whether example out-of-distribution even greatly confuse network abnormality module facilitates this. finally much like previous experiment train mnist classiﬁer three layers width time also auxiliary decoder abnormality module rather relying softmax statistics. abnormal examples blur rotate gaussian noise training images. gains abnormality module shown table consistent out-of-sample detection improvement compared softmax prediction probabilities. even highly dissimilar examples abnormality module improve detection. abnormality module demonstrates cases baseline beaten exploiting representations network suggesting myriad research directions. promising future avenues utilize intra-class variance distance example another predicted class abnormally high out-of-distribution another path feed vector summarizing layer’s activations vector layer. determine activation patterns abnormal out-of-distribution examples. others could make detections ﬁne-grained out-of-distribution example known-unknown unknown-unknown? different avenue detect correct classiﬁcations output probability correct detection. appendix show baseline evaluation metrics future research utilize estimating probability correct classiﬁcation. ideas improving error out-of-distribution detection. hope detection methods tested variety tasks architectures researcher’s choice. basic demonstration could include following datasets mnist cifar imdb tweets vision-only demonstrations transfer well architectures datasets. reporting aupr auroc values important underlying classiﬁer’s accuracy since always-wrong classiﬁer gets maximum aupr error detection error positive class. also future research need exact values paper comparisons. machine learning systems evolve tethering evaluations exact architectures datasets paper needless. instead could simply choose variety datasets architectures possibly like compare detection method detector based softmax prediction probabilities classiﬁers. basic recommendations others surpass baseline underexplored challenge. demonstrated softmax prediction probability baseline error out-of-distribution detection across several architectures numerous datasets. presented abnormality module provided superior scores discriminating normal abnormal examples tested cases. abnormality module demonstrates baseline beaten cases implies room future research. hope researchers investigate architectures make predictions view abnormality estimates others pursue reliable methods detecting errors out-of-distribution inputs knowing machine learning system fails strikes highly important. would like thank john wieting tang karen livescu greg shakhnarovich reviewers suggestions. would also like thank nvidia corporation donating several titan gpus used research. john garofolo lori lamel william fisher jonathan fiscus david pallett nancy dahlgren victor zue. timit acoustic-phonetic continuous speech corpus. linguistic data consortium kevin gimpel nathan schneider brendan oconnor dipanjan daniel mills jacob eisenstein michael heilman dani yogatama jeffrey flanigan noah smith. part-of-speech tagging twitter annotation features experiments. association computational linguistics mohit iyyer varun manjunatha jordan boyd-graber daum´e iii. deep unordered composition rivals syntactic methods text classiﬁcation. association computational linguistics andrew maas raymond daly peter pham huang andrew christopher potts. learning word vectors sentiment analysis. association computational linguistics khanh nguyen brendan o’connor. posterior calibration exploratory analysis natural language processing models. empirical methods natural language processing pang lillian shivakumar vaithyanathan. thumbs sentiment classiﬁcation using machine learning techniques. empirical methods natural language processing joan pastor-pellicer francisco zamora-mart´ınez salvador espa˜na-boquera mar´ıa jos´e castrobleda. case accuracy estimation comparing induction algorithms. international conference machine learning michael seltzer dong yongqiang wang. investigation deep neural networks noise robust speech recognition. ieee international conference acoustics speech signal processing jianxiong xiao james hays krista ehinger aude oliva antonio torralba. database large-scale scene recognition abbey zoo. ieee conference computer vision pattern recognition yuting zhang kibok honglak lee. augmenting supervised neural networks unsupervised objectives large-scale image classiﬁcation. international conference machine learning figure neural network classifying diamond image auxiliary decoder abnormality module. circles neurons either gelu sigmoid activation. blurred diamond reconstruction precedes subtraction elementwise squaring. probability vector softmax probability vector. blue layers train in-distribution data layers train inout-of-distribution examples. throughout work show softmax probability predicted class consistently high thus poor proxy estimating conﬁdence classiﬁcation. addition frequently high softmax prediction probability bounded value greater zero. this note binary sentiment classiﬁer’s minimum conﬁdence out-of-distribution example since highest softmax probability must least out-ofdistribution example want less conﬁdence conﬁdence reﬂect probability correct classiﬁcation. evaluate well conﬁdence model reﬂects probability correct classiﬁcation present metrics. first establish notation. denote conﬁdence model inputs. output near reﬂects high conﬁdence high probability underlying classiﬁcation model correctly classiﬁed input. also true false. possible conﬁdence model evaluation metric accuracy. compute this must threshold conﬁdence however model output probabilities slightly obtain perfect accuracy. probabilities near insufﬁciently informative lack numerical resolution reason avoid thresholding metrics. instead conﬁdence metric call probability alignment score note highly accurate model conﬁdence model obtain high probability alignment predicting constantly. meet challenge also calculating soft score test deﬁne soft score tp/. soft score second ﬁnal metric evaluating conﬁdence model. using metrics establish baseline using logits softmax. represent logits vector example baseline conﬁdence model logistic sigmoid. evaluate baseline using mnist twitter imdb setups before. take median result runs. case cifar- train using wideresnet. results table indicate probabilities positively aligned nonzero soft score. however results also indicate wide room improvement. appendix presented simple baseline method estimating classiﬁcation conﬁdence metrics evaluating conﬁdence model. baseline draws idea using softmax classiﬁcation information create conﬁdence model. hope appendix gives ideas future research directions serves baseline provides metrics conﬁdence model evaluation.", "year": 2016}