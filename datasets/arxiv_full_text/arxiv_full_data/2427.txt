{"title": "Focus of Attention for Linear Predictors", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We present a method to stop the evaluation of a prediction process when the result of the full evaluation is obvious. This trait is highly desirable in prediction tasks where a predictor evaluates all its features for every example in large datasets. We observe that some examples are easier to classify than others, a phenomenon which is characterized by the event when most of the features agree on the class of an example. By stopping the feature evaluation when encountering an easy- to-classify example, the predictor can achieve substantial gains in computation. Our method provides a natural attention mechanism for linear predictors where the predictor concentrates most of its computation on hard-to-classify examples and quickly discards easy-to-classify ones. By modifying a linear prediction algorithm such as an SVM or AdaBoost to include our attentive method we prove that the average number of features computed is O(sqrt(n log 1/sqrt(delta))) where n is the original number of features, and delta is the error rate incurred due to early stopping. We demonstrate the effectiveness of Attentive Prediction on MNIST, Real-sim, Gisette, and synthetic datasets.", "text": "present method stop evaluation prediction process result full evaluation obvious. trait highly desirable prediction tasks predictor evaluates features every example large datasets. observe examples easier classify others phenomenon characterized event features agree class example. stopping feature evaluation encountering easyto-classify example predictor achieve substantial gains computation. method provides natural attention mechanism linear predictors predictor concentrates computation hard-to-classify examples quickly discards easy-to-classify ones. modifying linear prediction algorithm adaboost include attentive method prove original number features error rate incurred early stopping. demonstrate effectiveness attentive prediction mnist real-sim gisette synthetic datasets. wish avoid evaluating weak hypotheses example evaluate less features easy-to-classify examples. however ﬁlter example need know whether informative not. majority vote used measure important example learning proportional magnitude majority vote. strong agreement features majority vote larger magnitude features disagree. goal compute least number weak hypotheses possible decide whether majority vote importance threshold. filtering un-informative examples trying compute hypotheses possible closely related problems intuition behind work prevalent many natural decision making domains. example ﬁnance suppose interested buying certain stock ﬁnancial advisors disposal. sequentially whether stock ﬁrst four consecutive advisors agree stop decide buy. probability might incur error since remaining advisors might vote opposite direction. hand advisors vote opposing no... gain enough conﬁdence either direction asking them. case also medicine doctor tries diagnose whether certain condition sequence tests. come negative stop diagnose healthy positive stop diagnose particular condition. however test oppose other keep testing reﬁned tests. finally face detection type attention mechanism also holds. seminal work proposed attentional cascade face detector could stop evaluation classiﬁer important features. thinking eyes figure examples classiﬁed. ﬁrst hard classify second easy. budgeted learning approach would evaluate number features examples whereas stochastic would evaluate features according hard example classify maintaining average budget brownian motion analysis margin based learning algorithms. terms margin full margin describe summation feature evaluations partial margin summation part feature evaluations. calculation margin broken example dataset. break-up allows algorithm make decision evaluation feature whether next feature also evaluated feature evaluation stopped label predicted. making decision evaluation able early stop evaluation features examples large partial margin evaluated features. examples large partial margin unlikely full margin required threshold. therefore rejecting examples early large savings computation achieved. quite different budgeted approach constant smaller number features evaluated examples case examples treated equally. instead looking classiﬁcation error look stop-error. stop-errors occur algorithm stops partial feature evaluation predicts label opposite label would predicted evaluated features. demonstrate simple rule comparing partial constant stopping-threshold speed-up linear predictor maintaining generalization accuracy. paper proposes simple novel test based sequential analysis stopping methods brownian motion drastically improve computational efﬁciency margin based learning algorithms. method accurately stops evaluation margin result entire summation evident. furthermore novel algorithm easily parallelized. margin based learning spurred countless algorithms many different disciplines domains. directly applicable machine learning algorithms margin based online learning algorithms. many margin based online algorithms base model update margin example stream. online algorithms exponentiated gradient online boosting update respective models using margin based potential function. passive online algorithms perceptron online passive-aggressive algorithms deﬁne margin figure example differences budgeted prediction attentive prediction. attentive prediction thresholds partial scores whereas budgeted prediction thresholds number features evaluated. based ﬁltering criterion update updates algorithm’s model value margin falls deﬁned threshold. algorithms fully evaluate margin example means evaluate features every example. recently shalev-shwartz proposed pegasos online solver. solver stochastic gradient descent solver produces maximum margin classiﬁer training process. however cost assigned feature evaluation would like design efﬁcient learner actively choose features would like evaluate. similar work idea learning feature budget ﬁrst introduced machine learning community ben-david dichterman authors introduced formal framework analysis learning algorithm restrictions amount information extract. speciﬁcally allowing learner access ﬁxed amount attributes smaller entire attributes. recently cesa-bianchi reyzin studied efﬁciently learn linear predictor feature budget also clarkson extended perceptron algorithm efﬁciently learn classiﬁer sub-linear time. similar active learning algorithms developed context label active learning algorithms presented unlabeled examples decide examples labels query cost. algorithm’s task labels little possible achieving speciﬁed accuracy reliability rates typically selective sampling active learning algorithms algorithm would ignore examples easy classify labels harder classify examples close decision boundary. work stems connecting underlying ideas active learning domains attribute querying label querying. main idea typically algorithm query many attributes examples easy classify. labels examples label query active learning setting typically queried. examples attributes would agree classiﬁcation example therefore algorithm need evaluate many deciding importance. novel constant sequential thresholded test test designed control rate stop-errors margin based learning algorithm makes. section describes adaptation. task ﬁltering framework would speed-up margin-based prediction algorithms quickly classifying obvious examples. quick classiﬁcation done creating test stops score evaluation process given partial computation score. measure difﬁculty simulation constant-stst boundary required decision error rate x-axis actual y-axis. since applying constant-stst results lower error rates required observe boundary conservative. classifying example magnitude score. deﬁne prediction threshold examples predicted negative score smaller rest predicted positive. statistically problem generalized ﬁnding test early stopping computation partial weighted independent random variables result full summation guaranteed high probability. given required decision error rate derive constant sequential thresholded test provide constant early stopping threshold maintains required conﬁdence. weighted independent random variables deﬁned wixi weight assigned random variable require deﬁne full partial sum. computed partial random variable know value stopping threshold coordinate deﬁned pelossof previously proposed curved-stst looking following conditional probability event stop event occurs partial crosses stopping point along stopping curve yielding prediction boundary stop simplicity deriving curtailed method stems fact joint probability stopping time probability needed explicitly calculated upper bound conditional. resulting curved stopping boundary gives constant conditional error probability throughout curve means rather conservative boundary. however interested controlling stop errors given examples interested slightly different conditional probability case many classiﬁcation tasks signiﬁcantly negatives positives dataset. formulation results aggressive boundary allows higher stop error rates beginning evaluation lower stop error rates end. boundary stops evaluations early less later later approach natural interpretation want shorten feature evaluation obvious negative samples want prolong evaluations positive samples. constant boundary achieves exact error spending characteristic. note equation ﬂipped order stop evaluation positive examples well. stated equation conditional probability function conditioned examples interest. therefore case interested limiting stop error rate examples important. upper bound conditional make approximation allow apply boundary-crossing probability estimation brownian bridge. apply brownian bridge conditional probability note constant stop threshold. last approximation holds event rare i.e. large event concentrated close approximate stop error rate calculating corresponding boundary crossing probability. lemma inf{i ﬁrst crossing time random walk constant probability following decision error approximately equal using boundary prediction directly implication error rate classiﬁer since error rate attentive predictor equal error rate full predictor plus stop error rate. take. without loss generality theorem proved case early stop computation positive predictions applied also mirrored case early stopping negative predictions. theorem suppose random variables bounded i.e. |xi| constant apply stopping rule linear prediction. different classiﬁcation tasks ’gisette’ ’mnist ’real-sim’ treat data way. split data training test sets train classiﬁer training predict held test set. calculate mean kernel value support vector positive test elkil store constants. removes trend positive set. experiments induce independence support vectors randomly permute order support vectors calculate partial scores. experiments sorted support vectors |αi| observed unstable results. conducted three experiments test loss attentive approach versus budgeted approach full classiﬁer. tested algorithm gisette dataset includes training examples test examples features. trained dataset linear kernel resulting model support vectors. second dataset mnist digits comprised images number number training included images test trained kernel obtained model support vectors. finally trained real-sim dataset comprised examples dimensions. data split randomly chosen test examples rest used training. training used linear kernel results full training seen ﬁgure red. obtained highly accurate classiﬁers. compare attentive prediction budgeted prediction thresholded partial scores certain threshold obtained confusion matrix threshold well average number support vectors evaluated. budgeted prediction exact budget average number support vectors evaluated attentive predictor obtained comparable confusion matrix. since bounds seen simulation tight tested possible thresholds setting {min entire dataset. also prediction threshold tested example test attentivep redict−µ centered kernel evaluation vector weights obtained svm. similarly tested budgeted prediction budget equal average number features calculated attentivepredict entire test budgetedp redict results early stopping algorithms seen ﬁgure shows precision recall different algorithms three sets. attentivepredict outperforms budgetedpredict tasks. reason probably since attentive threshold applied sided test therefore likely reject mostly negative examples classify negatives thereby lowering false positive rate classiﬁer. negative examples predicted full feature evaluation positive chance rejected figure comparison attentive prediction budgeted prediction. precision-recall actual performance classiﬁer. bottom computation/stop-error tradeoff. attentive prediction outperforms budgeted prediction classiﬁcation tasks. since attentive predictor thresholds negatives constant predictions void magnitude steep fall precision recall plot point. terms stop-error/computation tradeoff attentive prediction requires less computation budgeted prediction required error rate less interim point. furthermore since distribution partial scores positives typically negatives attentive threshold positive examples would never reach negative examples might thereby improving rate expense. bottom part ﬁgure indeed attentive predictor produces lower stop error rates budgeted predictor range stop error rate. beyond that attentive threshold gets close positive distribution beginning feature evaluation aggressive. conservative attentive predictor used range. observe attentive predictor signiﬁcantly lower number evaluated without much loss predictive power. future direction wish study performance algorithms independence assumptions broken sorting data feature weight sampling according measures importance. thresholding process creates natural attention mechanism linear predictors. examples easy classify ﬁltered quickly without evaluating many features. hand examples hard classify majority features evaluated. spending little computation easy examples computation hard interesting examples attentive algorithms exhibit stochastic focus-of-attention mechanism. reyzin. boosting feature budget. icml/colt budgeted learning workshop rosenblatt. perceptron probabilistic model information storage organization shai shalev-shwartz yoram singer nathan srebro andrew cotter yoram singer nathan srebro andrew cotter. pegasos primal estimated sub-gradient. international conference machine learning", "year": 2012}