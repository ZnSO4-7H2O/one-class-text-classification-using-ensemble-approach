{"title": "Ways of Conditioning Generative Adversarial Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "The GANs are generative models whose random samples realistically reflect natural images. It also can generate samples with specific attributes by concatenating a condition vector into the input, yet research on this field is not well studied. We propose novel methods of conditioning generative adversarial networks (GANs) that achieve state-of-the-art results on MNIST and CIFAR-10. We mainly introduce two models: an information retrieving model that extracts conditional information from the samples, and a spatial bilinear pooling model that forms bilinear features derived from the spatial cross product of an image and a condition vector. These methods significantly enhance log-likelihood of test data under the conditional distributions compared to the methods of concatenation.", "text": "gans generative models whose random samples realistically reﬂect natural images. also generate samples speciﬁc attributes concatenating condition vector input research ﬁeld well studied. propose novel methods conditioning generative adversarial networks achieve state-of-the-art results mnist cifar-. mainly introduce models information retrieving model extracts conditional information samples spatial bilinear pooling model forms bilinear features derived spatial cross product image condition vector. methods signiﬁcantly enhance log-likelihood test data conditional distributions compared methods concatenation. goal generative model learn underlying probabilistic distribution unlabeled data disentangling explanatory factors data. applicable tasks classiﬁcation regression visualization policy learning reinforcement learning. prominent generative model found useful realistic sample generation semisupervised learning super resolution text-to-image image inpainting. potentiality supported theory model enough capacity learned distribution converge distribution real data. representative power highly enhanced deep learning techniques various methods introduced stabilize learning process. conditional ﬁrst introduced generate samples speciﬁc labels single generator research made thoroughly despite practical usage. example used generate images descriptive sentences attribute vectors. conditioned gans learn conditional probability distribution condition kind auxiliary information describing data. mostly modeled concatenating condition vectors layers generator discriminator gan. even though forms joint representation hard fully capture complex associations different modalities. introduce small variant bilinear pooling provides multiplicative interaction elements vectors. provide computationally efﬁcient spatially sensible bilinear operation image vector calculated cross product last dimension image resulting image increased channels. oracle extract conditional information perfectly sample train generate samples oracle ﬁgure given conditions them. information retrieval model pre-trained model plays role oracle. addition objective maximizes lower bound mutual information given condition extracted condition. idea originated infogan information-theoretic extension able learn disentangled representations completely unsupervised manner. conditional concatenates condition vector input generator discriminator. variants method successfully applied obtained visuallydiscriminative vector representation text descriptions concatenated vector every layer discriminator noise vector generator. used similar method generate face images binary attribute vectors hair styles face shapes etc. structure-gan generates surface normal maps concatenated noise vector style-gan styles maps. spatial bilinear pooling mainly inspired studies multimodal learning. question multimodal learning model uncover correlated interaction vectors different domains. order achieve this various methods element-wise operations factorized restricted boltzmann machine bilinear pooling etc) proposed numerous challenging tasks. based models require expensive mcmc sampling makes difﬁcult scale large datasets. bilinear pooling expressive vector concatenation element-wise operations inefﬁcient squared complexity solve problem addressed space time complexity bilinear features using tensor sketch. information retrieving model uses core algorithm infogan recover disentangled representations maximizing mutual information inducing latent codes. infogan input noise vector decomposed source incompressible noise latent code auxiliary output discriminator retrieve latent codes. infogan utilizes variational information maximization deal intractability mutual information. unlike infogan randomly generates latent codes explicitly condition information latent codes. networks generator tries generate real data given noise discriminator classiﬁes real data pdata fake data represents probability real data. objective true data distribution deceiving playing following minimax game objective conditioned gans conditional probability distribution condition describes objective learn correctly labeled dataset generator conditioned additional input generators used experiment take input means vector concatenation. input well regularization terms guide generator. objective conditioned optimize equation compare proposed models commonly used conditioned gans. ﬁrst conditional concatenates second fully conditional concatenates every layer including dimension layer size depth replicate vector spatially match size feature perform depth concatenation. models including ours structure concatenated propose spatial bilinear pooling provides multiplicative interaction elements vectors. dimension image performs cross product pixel image gathers resulting vectors spatially make image. information retrieving approximator measures generator irgan maximizes ﬁtting conditional distribution experiments approximators pre-trained classiﬁers mnist cifar. irgan optimizes equation deﬁned lower bound mutual information paper assume entropy constant omit simplicity. difference sampled data distribution rather pre-deﬁned distribution since explicitly given datasets. trained conditioned gans mnist cifar- utilized techniques proposed dcgan. generators identical structure variations exist discriminators auxiliary networks. measured log-likelihood test data ﬁtting results shown table irgan showed best result mnist didn’t performed well cifar- inaccurate classiﬁer whose classiﬁcation accuracy found showed stable results datasets labels surpassing cgan fcgan. paper proposed effective models conditional distribution accurately. however need experiments complex conditions multi-labels text descriptions style embedding verify models. since cross product long vectors inefﬁcient compressing bilinear pooling promising alternative spatial bilinear pooling conditions high dimensional vectors.", "year": 2016}