{"title": "Bayesian Inference with Posterior Regularization and applications to  Infinite Latent SVMs", "tag": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "abstract": "Existing Bayesian models, especially nonparametric Bayesian methods, rely on specially conceived priors to incorporate domain knowledge for discovering improved latent representations. While priors can affect posterior distributions through Bayes' rule, imposing posterior regularization is arguably more direct and in some cases more natural and general. In this paper, we present regularized Bayesian inference (RegBayes), a novel computational framework that performs posterior inference with a regularization term on the desired post-data posterior distribution under an information theoretical formulation. RegBayes is more flexible than the procedure that elicits expert knowledge via priors, and it covers both directed Bayesian networks and undirected Markov networks whose Bayesian formulation results in hybrid chain graph models. When the regularization is induced from a linear operator on the posterior distributions, such as the expectation operator, we present a general convex-analysis theorem to characterize the solution of RegBayes. Furthermore, we present two concrete examples of RegBayes, infinite latent support vector machines (iLSVM) and multi-task infinite latent support vector machines (MT-iLSVM), which explore the large-margin idea in combination with a nonparametric Bayesian model for discovering predictive latent features for classification and multi-task learning, respectively. We present efficient inference methods and report empirical studies on several benchmark datasets, which appear to demonstrate the merits inherited from both large-margin learning and Bayesian nonparametrics. Such results were not available until now, and contribute to push forward the interface between these two important subfields, which have been largely treated as isolated in the community.", "text": "ning chen state laboratory intelligent technology systems tsinghua national laboratory information science technology department computer science technology tsinghua university eric xing school computer science carnegie mellon university existing bayesian models especially nonparametric bayesian methods rely specially conceived priors incorporate domain knowledge discovering improved latent representations. priors aﬀect posterior distributions bayes’ rule imposing posterior regularization arguably direct cases natural general. paper present regularized bayesian inference novel computational framework performs posterior inference regularization term desired postdata posterior distribution information theoretical formulation. regbayes ﬂexible procedure elicits expert knowledge priors covers directed bayesian networks undirected markov networks whose bayesian formulation results hybrid chain graph models. regularization induced linear operator posterior distributions expectation operator present general convex-analysis theorem characterize solution regbayes. furthermore present concrete examples regbayes inﬁnite latent support vector machines multi-task inﬁnite latent support vector machines explore largemargin idea combination nonparametric bayesian model discovering predictive latent features classiﬁcation multi-task learning respectively. present eﬃcient inference methods report empirical studies several benchmark datasets appear demonstrate merits inherited large-margin learning bayesian nonparametrics. results available contribute push forward interface important subﬁelds largely treated isolated community. past decade nonparametric bayesian models gained remarkable popularity machine learning ﬁelds partly owing desirable utility nonparametric prior distribution wide variety probabilistic models thereby turning largely heuristic model selection practice determining unknown number components mixture model unknown dimensionality latent features factor analysis model bayesian inference problem unbounded model space. popular examples include gaussian process dirichlet process beta process often described chinese restaurant process metaphor similarly often described indian buﬀet process metaphor nonparametric bayesian approaches allow model complexity grow data observed factor diﬀering traditional parametric bayesian models. recent development practicing bayesian nonparametrics relax unrealistic assumptions data homogeneity exchangeability. example handle heterogenous observations predictor-dependent processes proposed; relax exchangeability assumption stochastic processes various correlation structures hierarchical structures temporal spatial dependencies stochastic ordering dependencies introduced. common principle shared approaches rely deﬁning unusual cases learning nonparametric bayesian prior encoding special structures indirectly inﬂuences posterior distribution interest interplay likelihood model according bayes’ rule paper explore diﬀerent principle known posterior regularization oﬀers additional arguably richer ﬂexible means augment posterior distribution rich side information predictive margin structural bias etc. harder possible captured bayesian prior. denote model parameters denote hidden variables. given observed data posterior regularization generally deﬁned solving regularized maximum likelihood estimation problem marginal likelihood regularization function model posterior latent variables necessarily corresponding bayesian posterior must induced bayes’ rule). regularizer deﬁned kl-divergence desired distribution certain properties latent variables model posterior question constraints model posterior used generalized expectation constraint-driven semi-supervised learning em-type procedure applied solve approximately obtain augmented although likelihood another dimension incorporate domain knowledge existing work bayesian nonparametrics mainly focusing priors. following convention paper assumes common likelihood model given. hidden variable model distribution model parameter interest going beyond classical bayesian theory recent attempts toward learning regularized posterior distribution model parameters include learning measurements maximum entropy discrimination maximum entropy discrimination latent dirichlet allocation methods parametric give rise distributions ﬁxed ﬁnite-dimensional parameter space. best knowledge attempts made impose posterior regularization nonparametric setting model complexity depends data case nonparametric bayesian latent variable models. general formalism bayesian inference posterior regularization seems available apparent. paper present formalism call regularized bayesian inference regbayes built convex duality theory distribution function spaces; apply formalism learn regularized posteriors under indian buﬀet process conjoining powerful machine learning paradigms nonparametric bayesian inference svm-style max-margin constrained optimization. unlike regularized formulation traditional formulation bayesian inference directly optimizing objective respect posterior. enable regularized optimization formulation regbayes begin variational reformulation bayes’ theorem deﬁne kl-divergence desired post-data posterior model standard bayesian posterior regbayes solves following optimization problem regularization function post-data posterior pprob feasible space well-deﬁned distributions. appropriately deﬁning model prior distribution regbayes instantiated perform either parametric nonparametric regularized bayesian inference. particularly interesting derive posterior regularization impose posterior constraints. denote slack variables ppost denote general soft posterior constraints then express regularization term variationally normally deﬁned convex penalty function. regbayes formalism deﬁned applies wide spectrum models including directed graphical models undirected markov networks. undirected models performing bayesian inference resulting posterior takes form hybrid chain graphical model usually much challenging regularize bayesian inference directed gms. regularization term convex induced linear operator posterior distributions regbayes solved convex analysis theory. allowing direct regularization posterior distributions regbayes provides signiﬁcant source extra ﬂexibility post-data posterior inference applies parametric nonparametric bayesian learning paper focus applying technique later case illustrate regbayes facilitate integration bayesian nonparametrics large-margin learning complementary advantages largely treated disjoint subﬁelds. previously shown that core ideas support vector machines maximum entropy discrimination well structured extensions max-margin markov networks maximum entropy discrimination markov networks successful outcomes many scenarios. large-margin model rarely ﬂexibility nonparametric bayesian models automatically handle model complexity data especially latent variables present paper intend bridge using regbayes principle. speciﬁcally develop inﬁnite latent support vector machines multitask inﬁnite latent support vector machines explore discriminative large-margin idea learn inﬁnite latent feature models classiﬁcation multi-task learning respectively. show models readily instantiated regbayes master equation deﬁning appropriate posterior regularization using large-margin principle employing appropriate prior. ilsvm prior allow model unbounded number latent features priori. mt-ilsvm similar prior infer latent projection matrix capture correlations among multiple predictive tasks avoiding pre-specifying dimensionality projection matrix. regularized inference problems eﬃciently solved iterative procedure leverages existing high-performance convex optimization techniques. rest paper organized follows. section discusses related work. section presents regularized bayesian inference together convex duality results needed latter sections. section concretizes ideas regbayes presents inﬁnite latent feature models large-margin constraints classiﬁcation multi-task learning. section presents preliminary experimental results. finally section concludes discusses future research directions. expectation regularization expectation constraints considered regularize model parameter estimation context semi-supervised learning learning weakly labeled data. mann mccallum summarized recent developments generalized expectation criteria training discriminative probabilistic model unlabeled data. providing appropriate side information labeled features estimates label distributions ge-based penalty function deﬁned regularize model distribution e.g. distribution class labels. commonly used function kl-divergence empirical expectation model expectation feature functions expectations normalized general bregman divergence unnormalized expectations. although criteria used alone scoring function estimate unknown parameters discriminative model usually used regularization term estimation method maximum likelihood estimation. bellare presented diﬀerent formulation using expectation constraints semi-supervised learning introducing auxiliary distribution together alternating projection algorithm eﬃcient. liang proposed general notion measurements encapsulate variety weakly labeled data learning exponential family models. measurements labels partial labels constraints model predictions. framework posterior constraints used modify e-step algorithm project model posterior distributions onto subspace distributions satisfy auxiliary constraints. dudik studied generalized maximum entropy principle rich form expectation constraints using convex duality theory standard moment matching constraints maximum entropy relaxed inequality constraints. analysis restricted kl-divergence minimization ﬁnite dimensional space observations. later altun smola presented general duality theory family divergence functions banach spaces. drawn inspiration papers develop regularized bayesian inference framework using convex duality theory. using large-margin posterior regularization regbayes generalizes previous work maximum entropy discrimination present paper provides full extension preliminary work max-margin nonparametric bayesian models example inﬁnite latent class model data example assigned single mixture component ilsvm mt-ilsvm extend ideas inﬁnite latent feature models. multi-task learning nonparametric bayesian models developed learning features shared multiple tasks. however methods based standard bayesian inference without posterior regularization using example large-margin constraints. finally mt-ilsvm also regarded nonparametric bayesian formulation popular multi-task learning methods ﬁrst derive optimization-theoretic reformulation bayes’ theorem. denote space feasible models represents atom space. assume complete separable metric space endowed borel σ-algebra distribution measurable space assume absolutely continuous respect background measure exists density πdµ. {xn}n collection observed data assume i.i.d. given model. likelihood distribution assumed dominated σ-ﬁnite measure positive density exists density pdλ. then bayes’ conditionalization rule gives posterior distribution density reasons clear shortly introduce variational formulation bayes’ theorem. arbitrary distribution measurable space assume absolutely continuous respect denote density respect background measure shown posterior distribution bayes’ theorem equivalent optimum solution following convex optimization problem divergence pprob represents feasible space density functions respect measure proof straightforward noticing objective become klkp) adding constant noteworthy represents density general post-data posterior distribution sense necessarily corresponding bayesian posterior induced bayes’ rule. shall soon later introduce additional constraints post-data posterior diﬀerent bayesian posterior moreover could even obtainable bayesian conditionalization diﬀerent model. sequel order distinguish bayesian posterior call post-data distribution short post-data posterior distribution full. notation simplicity omitted condition post-data posterior distribution remark optimization formulation implies bayes’ rule information projection procedure projects prior density post-data posterior taking account observed data. general bayes’s rule special case principle minimum information figure illustration hard soft constraints simple setting three possible models. hard constraints feasible subspace. contrast many feasible subspaces soft constraints associated diﬀerent complexity penalty measured function. variational formulation bayes’ rule constraints ensure well-normalized objective well-deﬁned i.e. pprob capture domain knowledge structures model data. previous eﬀorts devoted eliciting domain knowledge constraining prior base measure shall constraints without considering data special cases regbayes presented. speciﬁcally optimization-based formulation bayes’ rule makes straightforward generalize bayesian inference richer type posterior inference replacing standard normality constraint wide spectrum knowledge-driven and/or datadriven constraints regularization. unconstrained unregularized.) formally deﬁne regularized bayesian inference generalized posterior inference procedure solves constrained optimization problem additional regularization imposed ppost subspace distributions satisfy additional constraints besides standard normality constraint probability distribution. using variational formulation problem rewritten form master equation obviously formulation enables diﬀerent types constraints employed practice. paper focus expectation constraints function expectation operator. instance vector feature functions deﬁned possibly data dependent. subspace feasible post-data distributions deﬁned following form feature function function form theory though simple function make optimization problem easy solve. auxiliary parameters usually nonnegative interpreted slack variables. constraints non-trivial soft constraints illustrated figure emphasize deﬁning indicator function formulation covers case hard constraints imposed. instance deﬁne indicator function equals condition satisﬁed; otherwise expectation constraints hard constraints. illustrated figure hard constraints deﬁne single feasible subspace general assume convex function represents penalty size feasible subspaces illustrated figure larger subspace typically leads models higher complexity. classiﬁcation models presented corresponds surrogate loss e.g. hinge loss prediction rule shall see. point t-th coordinate function deﬁned before. assume real-valued function convex lower semi-continuous. induce function taking inﬁmum posterior constraints; vice versa. hard constraints similar regularized maximum entropy density estimation indicator function could obtain ‘prior’ expectations domain/expert knowledge normalize empirical expectations functions denote discrete distribution natural regularization function would kl-divergence prior expectations expectations computed general bregman divergence used unnormalized expectations. kind regularization function used label regularization context semi-supervised learning. choices regularization function include penalty indicator function equality constraints summary). remark focused regbayes context full bayesian inference. indeed regbayes generalized apply empirical bayesian inference model parameters need estimated. generally regbayes applies directed bayesian networks undirected markov random ﬁelds. undirected models regbayes treatment deal chain graph resultant bayesian inference challenging existence normalization factors. discuss details examples appendix depending several factors including size model space data likelihood model prior distribution regularization function regbayes problem general highly non-trivial solve either constrained unconstrained form seen several concrete examples regbayes models present next section appendix section present representation theorem characterize solution convex regbayes problem expectation regularization. theoretical results used later developing concrete regbayes models. make subsequent statements general consider following problem convex function; bounded linear operator; also convex. introduce tools convex analysis theory study problem. begin formulating primal-dual space relationships convex optimization problems general settings assume banach spaces. important result build fenchel duality theorem. banach space vector space metric allows computation vector length distance vectors. moreover cauchy sequence vectors always converges well deﬁned limit space. fenchel duality theorem applied solve divergence minimization problems def= density estimation vector feature functions. feature function mapping therefore product space simple banach space. banach space ﬁnite signed measures absolutely continuous respect measure expectation operator feature functions respect distribution def= em∼q] reference point density estimation observations here apemp] pemp empirical distribution. then function kl-divergence constraints relaxed moment matching constraints following result proven. note lemma ones presented hold problems need meet regularity conditions theorem practice diﬃcult check whether constraint qualiﬁcations hold. solution solve dual optimization problem examine conditions hold depending whether solution diverge theorem. examples posterior constraints found discussed section paper consider general soft constraints deﬁned regbayes problem furthermore assume existence fully observed dataset compute empirical expectation speciﬁcally following similar line reasoning though time un-normalized following result. detailed proof deferred appendix optimum solution form regbayes posterior symbolically similar bayesian posterior; instead multiplying likelihood term prior distribution regbayes introduces extra term expi whose coeﬃcients derived constrained optimization problem resultant constraints posterior. make following remarks. remark feature function depend model only extra term contributes deﬁne prior expi ˆφ). example constrain model space subset priori constraint incorporated regbayes deﬁning expectation constraint only. speciﬁcally deﬁne single feature function otherwise deﬁne simple posterior regularization then theorem constrained prior. therefore constraint lets regbayes cover widely used truncated priors truncated gaussian remark general case depends term expi) implicitly deﬁnes joint distribution ﬁnite measure. case regbayes implicit bayesian conditionalization posterior obtained bayes’ rule well-deﬁned prior likelihood. however could integral expi) respect ﬁnite varies case implicit prior likelihood give back bayesian conditionalization. therefore regbayes ﬂexible standard bayesian inference prior likelihood model explicitly deﬁned additional constraints regularization systematically incorporated. recent work presents example. speciﬁcally show incorporating domain knowledge posterior regularization lead ﬂexible framework automatically learns importance piece knowledge thereby allowing robust incorporation important scenarios noisy knowledge collected crowds. contrast eliciting expert knowledge ﬁtting priors generally hard especially high-dimensional spaces experts normally good perceiving low-dimensional well-behaved distributions perceiving high-dimensional skewed distributions worth mentioning although theorem provides generic representation solution regbayes practice usually need make additional assumptions order make either primal dual problem tractable solve. since assumptions could make feasible space non-convex additional cautions need paid. instance mean-ﬁeld assumptions lead non-convex feasible space apply convex analysis theory deal convex sub-problems within em-type procedure. concrete examples provided later along developments various models. also note modeling ﬂexibility regbayes comes risks. example might lead inconsistent posteriors paper focuses presenting several practical instances regbayes leave systematic analysis bayesian asymptotic properties future work. derive conjugate functions three examples used shortly developing inﬁnite latent models intended. defer proof appendix speciﬁcally ﬁrst conjugate simple function used binary latent classiﬁcation model. given general theoretical framework regbayes introduced section ready present application development interesting nonparametric regbayes models. models conjoin ideas behind nonparametric bayesian inﬁnite feature model known indian buﬀet process large margin classiﬁer known support vector machines build class models simultaneous single-task classiﬁcation feature learning. parametric bayesian model presented appendix speciﬁcally illustrate develop latent large-margin classiﬁers automatically resolve unknown dimensionality latent features data demonstrate choose/deﬁne three elements regbayes prior distribution likelihood model posterior regularization. ﬁrst present single-task classiﬁcation model. basic setup project data example latent feature vector here consider binary features. real-valued features easily considered elementwisely multiplying guassian vector given data examples matrix binary vector associated data sample instead pre-specifying ﬁxed dimension resort nonparametric bayesian methods inﬁnite number dimensions. make expected number active latent features ﬁnite employ prior binary feature matrix reviewed below. indian buﬀet process proposed griﬃths ghahramani successfully applied various ﬁelds link prediction multi-task learning make stick-breaking construction good developing eﬃcient inference methods. parameter associated column binary matrix given column sampled independently bernoulli. parameter generated stick-breaking process several properties. ﬁnite number rows prior gives zero mass matrices inﬁnite number ones total number columns thus almost surely ﬁnite number non-zero entries though number unbounded. second property number features possessed data point follows poisson distribution. therefore expected number non-zero entries consider single-task multi-way classiﬁcation training data provided categorical label document given deﬁne latent discriminant function linear vector stacking subvectors others zero; corresponding inﬁnite-dimensional vector feature weights. since bayesian inference need maintain entire distribution proﬁle latent feature matrix however order make prediction observed data need remove uncertainty here deﬁne eﬀective discriminant function expectation latent discriminant function. fully explore ﬂexibility oﬀered bayesian inference also treat random infer posterior distribution given data. prior assume dimensions independent dimension follows standard normal distribution. fact gaussian process prior inﬁnite dimensional. formally eﬀective discriminant function post-data posterior distribution want infer. included place holder variables deﬁne e.g. variables arising data likelihood model. since taking expectation variables appear feature marginalized out. although choices taking mode possible choice could lead computationally easy problem expectation linear functional distribution expectation taken. moreover expectation robust taking mode widely used problem also zero mass inﬁnite number non-zero entries properties prior. sparsity essential ensure dot-product expectation well deﬁned i.e. ﬁnite values. moreover practice make problem computationally feasible usually ﬁnite upper bound number possible features suﬃciently large known truncation level shown ℓ-distance truncation error marginal distributions decreases exponentially increases. ﬁnite truncation level expectations deﬁnitely ﬁnite. margin favored true label arbitrary label superscript used distinguish posterior constraints multi-task ilsvm presented. deﬁne penalty function classiﬁcation surrogate loss squared ℓ-loss. clarity consider hinge loss. non-negative cost function measures cost predicting true label index training data. besides performing prediction task also interested explaining observed data using latent factors done deﬁning likelihood model here deﬁne common linear-gaussian likelihood model real-valued data random loading matrix. assume follows independent gaussian prior entry prior distribution hyperparameters priori estimated observed data figure shows graphical structure ilsvm deﬁned above plate means replicates. rigorous derivation ﬁniteness quantities beyond scope work could require additional technical conditions refer readers generic deﬁnition bregman divergence banach spaces case second measure unnormalized. directly solving ilsvm problems easy either posterior constraints non-smooth regularization function hard deal with. thus resort convex duality theory useful developing approximate inference algorithms. either solve constrained form using lagrangian duality theory solve unconstrained form using fenchel duality theory. here take second approach. case linear operator expectation operator denoted pprob r|itr|×l element evaluated example testing make prediction test examples training test data together regularized bayesian inference. training data impose large-margin constraints awareness true labels test data inference without large-margin constraints since know true labels. therefore classiﬁer learned training data only training testing data inﬂuence posterior distributions likelihood model inference make prediction rule note ability generalize test data relies fact data examples share prior. also cast problem transductive inference problem imposing additional large-margin constraints test data however resulting problem generally harder solve needs resolve unknown labels testing examples. also note testing diﬀerent standard inductive setting latent features data example approximately inferred given training data. empirical study shows little diﬀerence performance setting standard inductive setting. diﬀerent classiﬁcation typically formulated single learning task multitask learning aims improve related tasks sharing statistical strength among tasks performed jointly. many diﬀerent approaches developed multi-task learning review). particular learning common latent representation shared related tasks proven eﬀective capture task relationships below present multi-task inﬁnite latent learning common binary projection matrix capture relationships training data task consider binary classiﬁcation tasks extension multi-way classiﬁcation regression easily done. na¨ıve solve learning problem multiple tasks perform multiple tasks independently. order make multiple tasks coupled share statistical strength mt-ilsvm introduces latent projection matrix latent matrix given deﬁne latent discriminant function task data example vector parameters task dimension number columns latent projection matrix unbounded nonparametric setting. deﬁnition provides views tasks related. such method viewed nonparametric bayesian treatment alternating structure optimization learns single projection matrix pre-speciﬁed latent dimension. moreover diﬀerent learns binary vector known dimensionality select features kernels learn unbounded projection matrix using nonparametric bayesian techniques. ilsvm employ bayesian treatment view random variables. assume fully-factorized gaussian prior i.e. then deﬁne eﬀective discriminant function task expectation place holder variables possibly arise parts model. ilsvm since taking expectation variables appear feature marginalized out. then prediction rule task naturally deﬁning testing strategy ilsvm bayesian inference training test data. diﬀerence training data subject large-margin constraints test data not. similarly hyper-parameters priori estimated data discuss perform regularized bayesian inference large-margin constraints ilsvm mt-ilsvm. primal-dual formulations obvious basically methods perform regularized bayesian inference. directly solve primal problem posterior distribution ﬁrst solve dual problem optimum infer posterior distribution. however primal dual problems intractable ilsvm mt-ilsvm. intrinsic hardness mutual dependency among latent variables desired posterior distribution. therefore natural approximation method mean ﬁeld breaks mutual dependency assuming factorization form. method approximates original problems imposing additional constraints. alternative method apply approximate methods infer true posterior distributions derived convex conjugates above iteratively estimate dual parameters using approximate statistics below mt-ilsvm example illustrate idea ﬁrst strategy. full discussion second strategy beyond scope paper. ilsvm similar procedure applies defer details appendix make problem easier solve stick-breaking representation includes auxiliary variable infer augmented posterior joint model distribution furthermore impose truncated mean-ﬁeld constraint infer since directly involved posterior constraints solve using standard bayesian inference i.e. minimizing kl-divergence. speciﬁcally since prior also normal easily derive update rules update rules defer details appendix directly involved posterior constraints. need solve together using conjugate theory. however intractable. here adopt alternating strategy ﬁrst infers dual parameters ﬁxed infers solves speciﬁcally since large-margin constraints linear mean-ﬁeld update equation vj)] last term large-margin posterior constraints deﬁned therefore equation large-margin constraints regularize procedure inferring latent matrix present empirical results classiﬁcation multi-task learning. results appear demonstrate merits inherited bayesian nonparametrics largemargin learning. evaluate inﬁnite latent classiﬁcation real trecvid flickr image datasets extensively evaluated context learning ﬁnite latent feature models trecvid consists video key-frames belong categories including airplane scene basketball scene weather news baseball scene hockey scene. data example types features -dimension binary vector text features -dimension color histogram. flickr image dataset consists natural scene images types animals including squirrel zebra tiger lion elephant whales rabbit snake antlers hawk wolf downloaded flickr website. also example types features including -dimension sift bag-of-words -dimension real-valued features here consider real-valued features deﬁning gaussian likelihood distributions deﬁne discriminant function using latent features follow training/testing splits compare ilsvm large-margin harmonium shown outperform many latent feature models decoupled approaches efh+svm ibp+svm. efh+svm uses exponential family harmonium discover latent features learns multi-way classiﬁer. ibp+svm similar uses factor analysis model discover latent features. initialize learning algorithms models found using factors input feature matrix initial weights produce better results. here also factors initial mean weights likelihood models ilsvm. efh+svm ﬁnite models need pre-specify dimensionality latent features. report results classiﬁcation accuracy score achieved best dimensionality table figure illustrates performance change using diﬀerent number latent features produces best performance either increasing decreasing could make performance worse. ilsvm ibp+svm mean-ﬁeld inference method present average performance randomly inifigure overall average values latent features standard deviation diﬀerent classes; per-class average values latent features learned ilsvm trecvid dataset. tialized runs perform -fold cross-validation training data select hyperparameters e.g. ilsvm achieve comparable performance nearly optimal without needing pre-specify latent feature dimension much better decoupled approaches stage methods don’t clear winner ibp+svm performs worse efh+svm trecvid dataset outperforms efh+svm ﬂickr dataset. reason diﬀerence initialization diﬀerent properties data. also interesting examine discovered latent features. figure shows overall average values latent features per-class average feature values ilsvm trecvid dataset. average features active trecvid dataset. overall average also present standard deviation categories. larger deviation means corresponding feature discriminative predicting diﬀerent categories. example feature feature generally less discriminative many features feature feature figure shows overall average feature values together standard deviation flickr dataset. omitted per-class average ﬁgure crowded categories. increases probability feature active decreases. reason features stable values initialization strategy initializing exponentially decreasing leads faster decay many features inactive. examine semantics feature figure presents example features discovered flickr animal dataset. feature present top-ranked images large values particular feature. features semantically interpretable. instance feature squirrel; feature ocean animal whales flickr dataset; feature hawk. also features diﬀerent aspects category. example feature feature whales diﬀerent background. scene yeast data datasets repository data example multiple labels. treat multi-label classiﬁcation multi-task learning problem label assignment treated binary classiﬁcation task. yeast dataset consists training test examples features number labels example scene dataset consists training test examples features number labels example dataset school data dataset comes inner london education authority used study eﬀectiveness schools. consists examination records students secondary schools years dataset publicly available extensively evaluated various multi-task learning methods task deﬁned predicting exam scores students belonging speciﬁc school based four student-dependent features four school-dependent features order compare methods follow setup described similarly create dummy variables features categorical forming total student-dependent features school-dependent features. random splits data examples school belong training test set. average training includes students school test students school. scene yeast data compare closely related nonparametric bayesian methods including kernel stick-breaking basic augmented inﬁnite predictor subspace models nonparametric bayesian models shown outperform independent bayesian logistic regression single-task pooling approach also compare decoupled method mt-ibp+svm uses factor analysis model shared latent features among multiple tasks builds separate classiﬁers diﬀerent tasks. mt-ilsvm mt-ibp+svm mean-ﬁeld inference method report average performance randomly initialized runs comparison overall classiﬁcation accuracy f-macro f-micro performance measures. table shows results. datasets mt-ilsvm needs less latent features average. large-margin mt-ilsvm performs much better nonparametric bayesian methods mt-ibp+svm separates inference latent features learning classiﬁers. school data percentage explained variance measure regression performance deﬁned total variance data minus sum-squared error test percentage total variance. since settings compare state-of-the-art results latent features original input features vector concatenation denote corresponding methods mt-ilsvmf mt-ibp+svmf respectively. average multi-task latent needs latent features suﬃciently good robust performance. results figure mt-ilsvm achieves better results existing methods tested previous studies. again joint mt-ilsvm performs much better decoupled method mt-ibp+svm separates latent feature inference training large-margin classiﬁers. finally using latent features original input features boost performance slightly mt-ilsvm much signiﬁcantly decoupled mt-ibp+svm. figure shows performance mt-ilsvm changes hyper-parameter regularization constant yeast school datasets. yeast dataset mt-ilsvm insensitive school dataset mt-ilsvm insensitive stable figure shows training size aﬀects performance running time mtilsvm school dataset. ﬁrst training data random splits training corresponding test data test set. training size increases performance running time generally increase; mt-ilsvm achieves state-of-art performance using training data. running time also mt-ilsvm generally quite eﬃcient using mean-ﬁeld inference. finally investigate performance mt-ilsvm changes hyperparameters observed data. estimate maximizing objective function performance change much similar observations ilsvm. figure sensitivity study mt-ilsvm classiﬁcation accuracy diﬀerent yeast data; classiﬁcation accuracy diﬀerent yeast data; percentage explained variance diﬀerent school data; percentage explained variance diﬀerent school data. present regularized bayesian inference computational framework perform post-data posterior inference rich regularization/constraints desired post-data posterior distributions. regbayes formulated information-theoretical optimization problem applicable directed undirected graphical models. present general theorem characterize solution regbayes posterior regularization induced linear operator furthermore particularly concentrate developing large-margin nonparametric bayesian models regbayes framework learn predictive latent features classiﬁcation multi-task learning exploring large-margin principle deﬁne posterior constraints. models allow latent dimension automatically resolved data. empirical results several real datasets appear demonstrate methods inherit merits bayesian nonparametrics large-margin learning. regbayes oﬀers ﬂexible framework considering posterior regularization performing parametric nonparametric bayesian inference. future work plan study posterior regularization beyond large-margin constraints posterior constraints deﬁned manifold structures represented form ﬁrst-order logic investigate posterior regularization used interesting nonparametric bayesian models diﬀerent contexts link prediction social network analysis low-rank matrix factorization collaborative prediction. preliminary results shown great promise. interesting investigate carefully along direction. moreover stated regbayes developed undirected mrfs. inference would even harder. plan systematic investigation along direction too. preliminary results presented room improve. finally regularized bayesian inference general leads highly nontrivial inference problem. although general solution derived convex analysis theory normally intractable infer directly. therefore approximate inference techniques truncated mean-ﬁeld approximation used. current truncated inference methods limit pre-specify truncation level. conservative truncation level could lead waste computing resources. important develop inference algorithms could adaptively determine number latent features monte carlo methods. preliminary progress along direction reported thank anonymous reviewers editors many helpful comments improve manuscript. supported national foundation projects national natural science foundation china tsinghua university initiative scientiﬁc research program china postdoctoral science foundation grant supported afosr career dbi- alfred sloan research fellowship. standard bayesian inference proposed regularized bayesian inference implicitly make assumption model graphically drawn bayesian network illustrated figure here consider general formulation could cover directed undirected latent variable models well-studied boltzmann machines well case model could unknown parameters need estimation procedure maximum likelihood estimation besides posterior inference. latter also known empirical bayesian methods frequently employed practitioners. extension empirical bayesian inference unknown parameters illustrated figure cases need perform empirical bayesian inference presence unknown parameters. instance linear-gaussian bayesian model choose estimate covariance matrix using mle; latent dirichlet allocation model choose estimate unknown topical dictionary although principle treat parameters random variables perform full bayesian inference. cases need mechanisms estimate unknown parameters bayesian inference. model parameters. formulate empirical bayesian inference solving although problem convex ﬁxed jointly convex general. natural algorithm solve problem well-known procedure converges local optimum. speciﬁcally following result. structure within arbitrary either directed undirected hybrid chain graph. objective derived using variational techniques. fact variational upper bound figure illustration graphs three diﬀerent types models involve bayesian inference bayesian generative model; bayesian generative model unknown parameters chain graph model. extension chain graph cases assumed observed data generated model directed causal sense. assumption holds directed latent variable models. however many cases choose alternative formulations deﬁne joint distribution model observed data. figure illustrates scenario model consists subsets random variables. subset connected observed data undirected graph subset connected observed data using directed edges. graph known chain graph. markov properties chain graph know joint distribution factorization form markov random ﬁeld concrete example hybrid chain model bayesian boltzman machines treat parameters boltzmann machine random variables perform bayesian inference mcmc sampling methods. joint distribution. note given distribution non-normalized abused notation non-normalized distributions formula. directed bayesian networks naturally undirected models deﬁne joint distribution objective function problem formulations equivalent. call former constrained formulation call latter unconstrained formulation ignoring standard normalization constraints easy deal with. section presents interpretation medlda framework regularized bayesian inference. medlda max-margin supervised topic model extension latent dirichlet allocation supervised learning tasks. medlda data example projected point ﬁnite dimensional latent space feature corresponds topic i.e. unigram distribution terms vocabulary. medlda represents data probability distribution features results conservation constraint inﬁnite latent feature models discussed section constraint. without loss generality consider medlda regression model example whose graphical structure shown figure assume data examples length notation simplicity. document associated response variable observed training phase unobserved testing. denote instance value number topics dimensionality latent topic space. medlda builds model describe observed words. generating process document mixing proportion dirichlet; word associated topic indexes topic generates word i.e. βznm. deﬁne average topic assignment document denote unknown model parameters wnm} training set. medlda deﬁned solving posterior constraints imposed following large-margin principle correspond quality measure prediction results training data. fact easy constraints equivalent practically learn medlda model since problem intractable variational methods used introducing auxiliary distribution approximate true posterior replacing negative data likelihood variational upper-bound negative data log-likelihood. upper bound tight restricting constraints made variational distribution practice additional assumptions made derive practical approximate algorithm. based previous discussions extensions regbayes duality lemma reformulate medlda regression model example regbayes. speciﬁcally medlda regression model according easily show medlda problem lagrangian methods solve constrained formulation. alternatively also convex duality theorem solve equivalent unconstrained form. variational medlda ǫ-insensitive loss rǫ). conjugate derived using results lemma speciﬁcally following result whose proof deferred appendix note although general either primal dual problem hard solve exactly conjugate results still useful developing approximate inference algorithms. instance impose additional mean-ﬁeld assumptions primal formulation iteratively solve factor; process convex conjugates useful deal large-margin constraints alternatively apply approximate methods infer based solution iteratively solves dual parameters using approximate statistics discuss presenting inference algorithms ilsvm mt-ilsvm. discussions treated topics ﬁxed unknown parameters. fully bayesian formulation would treat random variables e.g. dirichlet prior regbayes interpretation easily extension medlda simply moving constant. denote kl-divergence klkp). following proof similar proof fenchel duality theorem denote primal value dual value respectively. lemma appropriate regularity conditions terms upper bound implement general procedure outlined algorithm solve mt-ilsvm problem. speciﬁcally inference procedure iteratively solves following steps summarized algorithm ]ψnk. again computational tractability need lower bound vj)]. using lower bound upper bound kl-divergence term. then inference procedure iteratively solves following steps", "year": 2012}