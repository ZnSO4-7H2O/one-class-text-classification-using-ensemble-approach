{"title": "Safer Classification by Synthesis", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "The discriminative approach to classification using deep neural networks has become the de-facto standard in various fields. Complementing recent reservations about safety against adversarial examples, we show that conventional discriminative methods can easily be fooled to provide incorrect labels with very high confidence to out of distribution examples. We posit that a generative approach is the natural remedy for this problem, and propose a method for classification using generative models. At training time, we learn a generative model for each class, while at test time, given an example to classify, we query each generator for its most similar generation, and select the class corresponding to the most similar one. Our approach is general and can be used with expressive models such as GANs and VAEs. At test time, our method accurately \"knows when it does not know,\" and provides resilience to out of distribution examples while maintaining competitive performance for standard examples.", "text": "discriminative approach classiﬁcation using deep neural networks become de-facto standard various ﬁelds. complementing recent reservations safety adversarial examples show conventional discriminative methods easily fooled provide incorrect labels high conﬁdence distribution examples. posit generative approach natural remedy problem propose method classiﬁcation using generative models. training time learn generative model class test time given example classify query generator similar generation select class corresponding similar one. approach general used expressive models gans vaes. test time method accurately knows know provides resilience distribution examples maintaining competitive performance standard examples. famous quote richard feynman stands stark contrast majority image classiﬁcation breakthroughs last decade. prevalent deep learning approach discriminative deep network maps observation probability decisions providing little understanding particular decision chosen approach demonstrated remarkable empirical results opaque nature raises questions safety trust example much attention recently focused dealing adversarial perturbations discriminative image classiﬁers make small image modiﬁcations result misclassiﬁcation work complement view showing discriminative models also easily fooled give erroneous predictions high conﬁdence out-of-distribution examples signiﬁcantly different examples data. motivated safety issues discriminative classiﬁers work propose safer generative image classiﬁcation paradigm. build recent breakthroughs deep generative modelling variational autoencoders generative adversarial nets shown convincing results generating complex observations images directly data. idea labeled training data building generative models images class. models random input vector transformed deep neural network image. test-time given image classify search across input vectors image sufﬁciently similar test image across generators. corresponding class best image taken classiﬁcation result. immediate beneﬁt approach interpretability class decision made know exactly chosen since readily know image model imagined representative another beneﬁt show signiﬁcant safety. traditional discriminative model trained classify road signs idea would faced out-of-distribution example elephant. poses severe problem safety critical systems self driving cars. model deﬁnition would never generate image elephant elephant images training data. thus distinguishing model know becomes straightforward. brings back premise feynman quote. objects understand therefore reliably generate model provides reliable classiﬁcation. generative classiﬁcation idea. using shallow architectures jordan compared discriminative generative learning investigating logistic regression naive bayes observed improved performance generative models low-data regimes. jaakkola haussler used generative model extract kernels discriminative logistic regression classiﬁer. seminal work hinton deep belief networks deep generative models images matching class labels learned. classiﬁcation using dbns image used calculate activations restricted boltzman machine image label clamped mcmc sampling used generate corresponding label. last decade dbns outperformed discriminative models trained using backpropagation recently generative models trained using backpropagation become popular. vaes gans seen extensions helmholtz machine model random vector known distribution mapped neural network generate data distribution. vaes trained using variational lower bound gans trained using adversarial method. generative models used success semi-supervised learning explored context supervised learning. given test image recognition network used sample distribution latent variables generate similar image. gans models learn inference network proposed work describe alternative inference approach gans require change training objective. selective classiﬁcation established approach improving classiﬁcation accuracy rejecting examples fall conﬁdence threshold. work method rejecting out-of-distribution examples based conﬁdence score. build recent work geifman el-yaniv investigated suitable conﬁdence scores deep neural networks. recently mandelbaum weinshall studied different distance metrics conﬁdence score. concurrent work authors generative models used detect examples outliers outside training distribution. anonymous uses similar technique searching latent space discover outliers technique classiﬁcation. anonymous trains gans feature matching loss develop model capable simultaneous classiﬁcation novelty detection. curves benchmark model context classiﬁcation believe risk-coverage analysis appropriate evaluate conﬁdence classiﬁcation performance novelty detection. analysis novelty detection hand ignores information classiﬁcation accuracy. consider classiﬁcation problem classes. inputs denoted outputs denoted data consists pairs discriminative classiﬁcation learns model parameters typically maximizing popular model classiﬁcation softmax data log-likelihood maxθ deterministic functions e.g. outputs neural network weights central component method generative model takes input random m-dimensional latent vector distribution learns transform sample learning unsupervised learning task recently several efﬁcient training methods backpropagation proposed gans vaes gans adversarially trained networks consisting generator discriminator. generator network takes latent vector produces image discriminator network takes image outputs predicted probability image came distribution training set. training consists two-player game generator tries produce images discriminator unable distinguish training distribution discriminator improves ability discern real data generated images. vaes trained using variational lower bound learning encoder network maps training image corresponding distribution latent vector decoder network maps latent vector back image. training balances reconstruction loss decoder kullback-leibler distance encoded latent vector distribution central motivation approach accurately identifying classiﬁer ‘does know’ correct class. problem typically explored context discriminative classiﬁcation known selective classiﬁcation suppose classiﬁer takes input image outputs predicted class along measure conﬁdence prediction. example could maximum softmax output conﬁdence. selective classiﬁers abstain prediction conﬁdence score certain threshold threshold parameter thus offers balance proportion data classiﬁed accuracy portion dataset. coverage selective classiﬁer threshold deﬁned proportion test observations classiﬁed conﬁdence greater empirical risk given deﬁned error rate subset test classiﬁed conﬁdence greater principled method comparing selective classiﬁers examine risk-coverage plots exempliﬁed figure classiﬁers meaningful measures conﬁdence predict difﬁcult distribution images lower conﬁdence thus coverage decreased risk shrink zero. recently geifman el-yaniv showed discriminatively-trained cnns thresholding softmax output provides state-of-the-art selective classiﬁcation surpassing alternative conﬁdence measures mc-dropout figure risk-coverage plot trained mnist. observe choose conﬁdence threshold around data covered data portion classiﬁcation perfect. increase desired coverage accuracy decreases choosing threshold covers percent data result sharp decrease accuracy indicates low-conﬁdence predictions generally correspond misclassiﬁed data would expect. begin discussion showing discriminatively trained easily fooled give high-conﬁdence predictions out-of-distribution examples examples signiﬁcantly different example training data correspond particular class trained predict. present results selective classiﬁcation paradigm principled evaluation conﬁdence accuracy. consider classiﬁcation task selective classiﬁer trained data containing certain classes tested data contains data classes seen training contains data match classes seen training set. obviously classiﬁer unable correctly classify points expect abstain prediction points. concretely would like pick threshold points conﬁdence less left unclassiﬁed points conﬁdence greater classiﬁed correctly. data signiﬁcantly different conﬁdence measure reliable able determine threshold. unfortunately show next discriminative classiﬁers easily fooled give highconﬁdence predictions out-of-distribution examples wildly different training data. train standard well-known mnist dataset test selectivity running predictions mnist augmented rescaled images omniglot dataset. omniglot dataset compiled lake contains handwritten characters different alphabets figure images resized nearest neighbor interpolation order size mnist. removed ambiguous images figure characters different languages resembled digits point even human would categorize numerical digits. augmented dataset composed percent mnist data percent omniglot. omniglot images chose include resemble images mnist training expect predict images conﬁdence. thus expect risk-coverage plot similar figure starts increases images uncertain included classiﬁcation. mnist composes percent data performs well mnist dataset expect line begins rise monotonically percent coverage mark. further expect risk decrease towards decrease coverage vanishing risk property reﬂects idea increased conﬁdence associated increased classiﬁcation accuracy observed mnist experiment figure however risk-coverage plot actually obtained experiment shown figure exhibit vanishing risk property. lowest risk attainable using maximum possible conﬁdence threshold case images classiﬁed threshold conﬁdence omniglot images. images displayed figure attain highest level conﬁdence despite resembling digit mnist training thus choice threshold allow abstain classifying images. combining risk-coverage results mnist augmented mnist datasets image classiﬁed conﬁdence likely incorrect classiﬁcation converse true image would incorrectly classiﬁed still good chance prediction conﬁdence high. thus conﬁdence metric accurately reﬂect ability classiﬁer make precise prediction given out-of-distribution examples. results example seem innocuous ﬁrst glance. however easily imagine scenario performance would lead dire consequences. example realistic scenario self-driving cars defer decision road signs human based conﬁdence prediction. learned example conﬁdence cannot trusted figure illustration out-of-distribution example discriminative classiﬁer. here classiﬁer discriminates square circle examples. relate conﬁdence classiﬁer distance decision boundary. shown out-of-distribution example away boundary classiﬁed high conﬁdence belong case class circles. principle fact discriminative approach fooled out-of-distribution examples surprising. figure provide explanation result simple binary classiﬁcation task. intuitively conﬁdence discriminative classiﬁer related distance decision boundary. therefore imagine exists examples different data still away decision boundary therefore high conﬁdence value. immediate conclusion example carries high-dimensional problems expressive classiﬁers results suggest indeed case. figure also clear viable solution out-of-distribution detection problem identify examples suitable distance metric training data problem becomes identify suitable distance metric compute distance efﬁciently typically distance computation scales amount training data. following propose alternative approach based generative models. idea learning generate samples low-dimensional latent vector would effectively learn manifold class. distance manifold expected reliable measure classiﬁcation conﬁdence. generative classiﬁer consists class-conditional generative models similarity measure generator takes input latent variable outputs generated image respective class. similarity measure could complex function could negative distance siamese network predicts probability images class. work consider negative distance allows fair comparison conventional novelty detection approaches similarity metrics. future investigate using alternative measures similarity exclusive metrics. classify test image class generator solve following optimization problem practice non-convex optimization problem. optimization heuristic perform monte carlo sampling evaluate similarity generated image latent variable optimal latent vector starting point non-linear optimization method l-bfgs generative model gaussian latent model also option feeding test image encoder network obtain parameter estimate mean gaussian heuristic solve optimization. method works better practice goal classiﬁcation accuracy. approach parallels -nearest neighbor classiﬁer methods classify test point optimization performed similar match images known classes. nearest neighbors training generative classiﬁer images range generators. generators presumably capable reproducing training would expect method outperform nearest neighbors classiﬁcation accuracy long generator spaces class intersect. nearest neighbors improved using different distance metrics example distance known outperform distance mnist. similarly would possible distance metrics generative classiﬁer. runtime nearest neighbors increases number training samples method runtime controlled hyperparameters optimization routine. nearest neighbors desirable property interpretability prediction rationale prediction form closest image test point metric score. using score conﬁdence value view selective classiﬁer would expect higher conﬁdence thresholds would lead accurate predictions. generative classiﬁer retains properties example visualize optimization procedure yielding images explains classiﬁer made prediction also take maximum similarity conﬁdence measure purpose selective classiﬁcation intuitively accurately reﬂect ability classiﬁer accurately make prediction. figure mnist classiﬁcation performed generative classiﬁer test image left accompanied generated images distances test point. generators best match test image predicted class class image closely matches test image distance. figure mistakes made generative classiﬁer interpretable left model learned reproduce test image resulting unconﬁdent incorrect prediction. right generator incorrect class capable reproducing test image extent pointing degeneracies generator space. generative models trained dcgans -dimensional latent space vaes -dimensional latent space dimensions chosen crossvalidation. generative models used similarity measure. train dcgans employed techniques label smoothing noise injection weight normalization helped improve quality images stabilize training. using dcgan generative model achieved accuracy percent mnist. using encoder output place iterative optimization achieved accuracy percent. baseline nearest neighbor achieves accuracy percent. addition mistakes made generative classiﬁcation method readily interpretable figure models unable produce test demonstrates test dissimilar seen training thus difﬁcult correctly classify. indeed conﬁdence predictions within bottom percentiles conﬁdences respective models. however issue classiﬁcation method highly depends regularity images produced generators test image generator capable producing something looks like possible method misclassify figure observed similar results using various recent generative model formulations known produce high quality images wasserstein possible alternative similarity measures images produced generators figure similar test image distance close respect another similarity measure. hand optimization procedure exploit complex neural network similarity metrics ﬁnding images visually resemble test image still produce high similarity scores. another approach would train generators search space certain conditional generator contain examples classes. example began variation hyperparameter controls trade-off image quality image diversity possible emphasizing image quality would remove out-of-distribution examples conditional gan’s search space. figure mnist images misclassiﬁed correctly classiﬁed vae-based using distance test image. horizontal triplet images shows test image nearest neighbor data generated image. able produce images similar test image images correct class exist training misclassiﬁes data. distribution experiment omniglot-augmented mnist using generative classiﬁer models described above. although generative classiﬁer distance lower baseline performance original test possess vanishing risk property even inclusion out-of-distribution examples demonstrated risk-coverage plot figure means contrast risk driven zero increasing selectivity thus conﬁdence measure really reﬂect classiﬁer’s inherent ability classify image. example figure displays optimal images produced generators omniglot image classiﬁed highest possible conﬁdence. generators unable match image closest distance achieved order opposed distances order mnist images seen figure lower conﬁdence score generative classiﬁer omniglot reﬂects fact cannot actually classify images correctly high conﬁdence images misleads believe capable order improve accuracy mnist preserving desirable selectivity properties ﬁrst generator conﬁdence determine coverage thresholding appropriately make ﬁnal classiﬁcation data. procedure generative novelty detection classiﬁcation results risk-coverage curve lies entirely curve thus able maintain performance mnist achieving desired selectivity properties. future work improving generative models selecting appropriate similarity measure result generative classiﬁer simultaneously outperform classiﬁcation serve novelty detector. proposed general method classiﬁcation using generative models used various models. shown generative approach offers resilience misclassiﬁcation out-of-distribution examples provides reliable measure classiﬁcation conﬁdence. much work required scale approach challenging image recognition domains. present aware generative models reliably capture latent manifold figure risk-coverage generative classiﬁer. left results generative classiﬁers discriminative cnn. note coverage generative classiﬁers perform better higher coverage outperforms generative models higher accuracy in-distribution examples. right using generative models novelty detection classiﬁcation obtain models outperform discriminative coverage values. figure omniglot image classiﬁed high conﬁdence classiﬁed conﬁdence gan-based generative classiﬁer. distance closest image order magnitude higher mnist images figure complex realistic images imagenet dataset although considerable progress made believe work provides additional motivation improving performance generative models. work also offers possible principled evaluating generative model performance used generative classiﬁer. future research fundamentally understanding controlling behavior generative models increase effectiveness applied problems safe prediction. vincent dumoulin ishmael belghazi poole alex lamb martin arjovsky olivier mastropietro aaron courville. adversarially learned inference. proceedings international conference learning representations yarin zoubin ghahramani. dropout bayesian approximation representing model uncertainty deep learning. international conference machine learning pages goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems pages kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition pages diederik kingma shakir mohamed danilo jimenez rezende welling. semi-supervised learning deep generative models. ghahramani welling cortes lawrence weinberger editors advances neural information processing systems pages gregory koch richard zemel ruslan salakhutdinov. siamese neural networks one-shot image recognition. proceedings international conference machine learning. alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems pages andrew michael jordan. discriminative generative classiﬁers comparison logistic regression naive bayes. advances neural information processing systems pages salimans diederik kingma. weight normalization simple reparameterization accelerate training deep neural networks. advances neural information processing systems pages salimans goodfellow wojciech zaremba vicki cheung alec radford chen. improved techniques training gans. advances neural information processing systems pages christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. proceedings ieee conference computer vision pattern recognition pages pascal vincent hugo larochelle isabelle lajoie yoshua bengio pierre-antoine manzagol. stacked denoising autoencoders learning useful representations deep network local denoising criterion. journal machine learning research", "year": 2017}