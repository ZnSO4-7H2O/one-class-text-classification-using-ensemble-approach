{"title": "Low-Cost Learning via Active Data Procurement", "tag": ["cs.GT", "cs.AI", "cs.LG", "stat.ML", "J.4; I.2.6"], "abstract": "We design mechanisms for online procurement of data held by strategic agents for machine learning tasks. The challenge is to use past data to actively price future data and give learning guarantees even when an agent's cost for revealing her data may depend arbitrarily on the data itself. We achieve this goal by showing how to convert a large class of no-regret algorithms into online posted-price and learning mechanisms. Our results in a sense parallel classic sample complexity guarantees, but with the key resource being money rather than quantity of data: With a budget constraint $B$, we give robust risk (predictive error) bounds on the order of $1/\\sqrt{B}$. Because we use an active approach, we can often guarantee to do significantly better by leveraging correlations between costs and data.  Our algorithms and analysis go through a model of no-regret learning with $T$ arriving pairs (cost, data) and a budget constraint of $B$. Our regret bounds for this model are on the order of $T/\\sqrt{B}$ and we give lower bounds on the same order.", "text": "design mechanisms online procurement data held strategic agents machine learning tasks. study model agents cannot fabricate data cost furnishing data. challenge past data actively price future data order obtain learning guarantees even agents’ costs depend arbitrarily data itself. show convert large class no-regret algorithms online posted-price learning mechanisms. results parallel classic sample complexity guarantees resource constraint money rather quantity data available. budget constraint rising interest ﬁeld machine learning strongly driven potential generate economic value. firms seeking revenue optimizations gather abundant data cost apply inexpensive algorithmic tools produce highaccuracy predictors massively improve future decision making. extent potential value created leveraging data prediction apparent multi-million dollar competition bounties oﬀered companies like netﬂix heritage health foundation perhaps even aggressive hiring many experts companies like google facebook. much theoretical results measure least implicitly economic eﬃciency learning problems. example certain settings reasonably thorough understanding sample complexity gives precise tradeoﬀ quantity data disposal error loss rate want achieve. reducing error always beneﬁcial course must weighed marginal cost increasing measures eﬃciency broadened recent years particular gathering data typically orders magnitude cheaper labeling emergence active learning paradigm here imagine interface learner label provider learner make label queries data points online fashion. sequentially choosing data label learner greatly reduce number labels required learn problem received little attention learning theory literature monetary eﬃciency learning data diﬀering costs. indeed real-world prediction tasks often require obtaining examples held self-interested strategic agents; agents must incentivized provide data hold heterogeneous costs world data held self-interested agents heterogeneous costs providing particular costs arbitrarily correlated underlying data design mechanisms incentivecompatible robust learning guarantees optimize cost-eﬃciency tradeoﬀs inherent learning problem? development certain drug pharmaceutical company wishes train disease classiﬁer based data obtained hospitals stored patients’ medical records. data public company oﬀer hospital patients ﬁnancial incentives contribute private records. note potential cost heterogeneity compensation required patients correlated content medical data online retailers generally hope know website visitors order better target products customers. retailer oﬀer customers’ demographic social data form access facebook proﬁle. again customers’ willingness sell covary demographics data unknown way. loss function write known risk expectation random draw goal produce hypothesis whose risk much optimal member example binary classiﬁcation data point consists pair encodes features research statistical learning theory attempts characterize well tasks performed terms resources available inherent diﬃculty problem. resource usually quantity data binary classiﬁcation instance diﬃculty richness problem captured vc-dimension famous result algorithm achieving bound present work consider alternative scenario learner ﬁxed budget budget purchase examples. precisely round sequence rounds agent arrives data point sampled i.i.d. cost cost known agent depend arbitrarily learning mechanism oﬀer menu take-it-or-leave-it prices possibly diﬀerent price data point arriving agent observes price oﬀered data accepts long case mechanism pays agent learns goal actively select prices oﬀer diﬀerent datapoints subject budget order minimize risk ﬁnal output figure algorithmic analytic approach. first convert follow-the-regularizedleader online no-regret algorithms mechanisms purchase data regret-minimization setting introduce purposes analysis. then convert mechanisms solve main problem statistical learning. mechanisms interact online learning algorithms black boxes analysis relies opening box. mean arriving costs main result continues hold signiﬁcantly smaller particular correlations costs examples; indeed even stays constant. indicates case average cost general idea attacking problem utilize online learning algorithms regret minimization algorithms output hypothesis prediction step performance measured summed loss predictions steps. idea hypotheses produced step used determine value data procurement process generate ﬁnal prediction. section tools need pricing learning mechanism interact olas. ﬁrst high-level problem that budget constraint small subset data sequence. tool importance-weighting give good regret-minimization guarantees even entire data sequence. second problem aggregate hypotheses convert regret guarantee risk guarantee statistical learning setting. achieved standard online-to-batch conversion given tools section remaining challenge develop pricing learning strategy achieves regret. address question section formally deﬁne model online learning regret minimization purchased data mechanism must output hypothesis time step perform well hindsight entire data sequence enough budget purchase observe fraction arriving data. defer later detailed analysis setting derivation pricing strategy lower bounds. point present pricing strategy regret guarantees setting. section give main results risk guarantees learner budget access arriving agents. bounds follow directly using tools section regret-minimization results section section develop deeper understanding regret minimization setting. derive pricing strategy in-depth analysis analytically tractable variant problem at-cost setting mechanism required cost arriving data point rather price posted. setting able derive optimal pricing strategy minimizing regret bound class learning algorithms subject expected budget constraint. also complement upper bounds proving lower bounds data-purchasing regret minimization. show mechanisms easier at-cost setting order-optimal regret guarantee γta. small mechanisms b√γta main regret minimization setting guarantee order weaker guarantee). dependence approaches classic regret bound large small still batch settings agents oﬀered price simultaneously pricing schemes obtaining data appeared recent work especially roth schoenebeck considered design mechanisms eﬃcient estimation statistic. however work others related settings consider oﬄine solutions e.g. drawing posted price independently data points. focus active approach marginal value individual examples estimated according current learning progress budget. data-dependent approach pricing data appear horel paper focuses quite diﬀerent learning setting model regression noisy samples budget-feasible mechanism design approach. many ideas present work draw recent advances using importance weighting active learning problem wealth theoretical research active learning including balcan beygelzimer hanneke many others. section formally deﬁne problem setting. body paper consist series steps deriving mechanisms setting provable guarantees ﬁnally appear section objects given hypothesis class assume parameterized vectors broadly hilbert space endowed norm convenience treat elements vectors added scaled etc. also given loss function convex assume throughout paper loss function -lipschitz many common scenarios space pairs cross product feature input label though setting generic object. example canonical problem linear regression hypothesis class vectors loss function deﬁned according squared error data-purchasing statistical learning problem parameterized data space hypothesis space loss function number arriving data points expected budget constraint problem instance consists distribution private cost associated data point. costs arbitrarily chosen i.e. consider worst-case model costs. note mechanism given parameters problem instance completely unknown mechanism prior arrivals. design problem mechanism choose pricing function post time update based receiving data choose ﬁnal prediction. risk predictive error hypothesis agent-mechanism interaction. model agent arrival posted prices contains several assumptions. first agents cannot fabricate data; report data actually mechanism. second agents rational accept posted price higher cost reject otherwise. third implementation mechanism obtain agent’s cost transaction occurs. emphasize purpose paper focused implementation setting instead developing active learning pricing techniques guarantees. also intended simple clean model begin developing techniques. however brieﬂy note possible implementations. straightforward mechanism posts prices directly agent responds directly. would weakly truthful implementation agents incentive misreport costs choose accept transaction. strictly truthful implementation uses trusted third party facilitate transactions example could imagine attempting learn classify disease could rely hospital broker allowing negotiate patients data. ttp/agent interaction could proceed follows section begin classic regret-minimization problem broad class algorithms problem. show apply techniques convert algorithms form useful solving statistical learning problem purchased data. missing ingredient price-posting strategy presented section regret objective typically studies adversarial settings want discount loss incurred algorithm loss suﬀered best possible chosen knowledge sequence ft’s. often consider randomized algorithms generally consider expected loss regret expectation randomness algorithm input sequence loss functions. algorithm said guarantee regret latter provides upper bound regret every sequence loss functions weights others. ftrl algorithm speciﬁed convex function known regularizer usually strongly convex respect norm. example multiplicative weights follows using negative entropy function regularizer strongly-convex respect norm online gradient descent follows using regularizer strongly-convex respect norm. special cases eﬃcient closed-form solutions update rule computing ht+. arrival sequence adversarially chosen good algorithm randomly choose sample arrivals. section abstract away decision randomly sample. section suppose time posting hypothesis probability speciﬁed external means function preceding time steps. probability goal modify ftrl algorithm setting obtain modiﬁed regret guarantee. notice crucially deﬁnition loss regret unchanged still suﬀer loss regardless whether observe technique importance weighting. idea that observe sequence values probability unbiased estimate taking observe. check fact indicator variable event observe note expectation called importance-weighting observations regularizer strongly convex respect expected regret respect loss sequence good convert online regret-minimization algorithm smaller amounts data postpone question price data till section address statistical learning problem generate accurate predictions based online learning process. address standard tool known online-to-batch conversion leverage online learning algorithm batch setting. sketch technique follows details found e.g. shalev-shwartz given batch i.i.d. data points feed one-by-one no-regret algorithm. algorithm regret hypotheses predicted well average. since data point drawn i.i.d. means hypotheses average predict well i.i.d. draw distribution. thus suﬃces take mean hypotheses obtain risk. lemma suppose sequence convex loss functions drawn i.i.d. distribution online learning algorithm hypotheses achieves expected regret ef∼f minh∈h note conversion continue hold data-purchasing no-regret setting deﬁne next since required algorithm output hypothesis step regret bound hypotheses. setting deﬁne problem regret minimization purchased data. design mechanisms good regret guarantees problem translate aforementioned online-to-batch conversion guarantees original problem statistical prediction. essence data-purchasing no-regret learning setting online algorithm asked perform well sequence data default mechanism ability data. rather mechanism purchase right observe data points using limited budget. mechanism still expected regret compared optimal hypothesis hindsight entire data sequence space number arriving data points expected budget constraint problem instance sequence pairs convex loss function cost associated data point. assume -lipschitz loss functions. problem design mechanism implementing operations post receive deﬁnition regret also classical setting note suﬀer loss time regardless whether purchase not. mechanism must also guarantee that every problem instance spends expectation internal randomness. recall that section introduced importance-weighting technique online learning. gave regret guarantees learning algorithm arrival observed probability therefore entire problem reduced choosing posted-price strategy time step. posted-price strategy attempt minimize regret bound satisfying expected budget constraint. brief sketch proof arguments follows. choose posted price strategy determined function thus apply lemma stated induced probabilities expected regret learning algorithm bulk analysis no-regret data-purchasing problem actually focuses slightly easier variant setting arriving agent accepts transaction mechanism cost rather posted price call at-cost variant problem. setting turns much analytically tractable derive optimal regret bounds mechanisms matching lower bounds. take approach insights derived variant apply produce solution main no-regret data purchasing problem. order keep story moving forward summarize results at-cost setting explore obtained section at-cost setting able solve directly pricing strategy minimizes importance-weighted regret bound lemma ﬁrst deﬁne important quantity state strategy result theorem diﬃculty data also value beneﬁt explain diﬃculty aspect examining regret bound ftrl learning algorithms observes ∆htft small excellent regret bound learning algorithm; problem easy. explain value aspect concreteness take online gradient descent algorithm; larger gradient larger update step ∆htft expect good prior knowledge γta? although general domain-speciﬁc several reasons optimism. first compresses information data costs single scalar parameter second need exact estimates order-optimal regret bounds need estimate within constant factor γta. third directly proportional normalization constant pricing distribution increase probability obtaining given data point decreases vice versa. fact best choice normalization constant budget precisely last arrival leaves. thus estimated adjusted online tracking burn rate algorithm. simulations observed success simple approach estimating based average correlation along burn rate i.e. current estimated ˆγta steps remaining budget remaining spend ˆγta graceful degradation continue true main setting. idea follow optimal form pricing strategy choosing normalization constant γta. longer optimal ensure satisfy budget give guarantees depending magnitude need approximate estimate value larger γta. guaranteed upper-bound used pick satisfying budget. depends algorithm—what implications? ﬁrst note upper-bounded instance average arriving costs. bound containing imply nontrivial algorithm-independent bounds. purpose capture cases signiﬁcantly better bounds algorithm good problem. this note running ftrl algorithm entire data sequence gives regret bound regret bound. case algorithm small average ∆htft algorithm enjoys better regret bound also hope improvement reﬂected γta. however might hope algorithm-independent quantity that analogy vc-dimension captures diﬃculty purchasing learning problem instance. leads question remove algorithm-dependence bound? might hope achieve bound depending algorithm-independent quantity captures correlations data cost. natural candidate cases achieve bound terms expect approximate regret guarantee mechanism good algorithm even initialized weak knowledge diﬀerence losses time step implying deeper investigation phenomenon good candidate future work. problem unlike at-cost variant cannot general solve form optimal pricing strategy. intuitively because must price post optimal strategy depends algorithm cannot condition purchasing decision directly private information arriving agent. min{ ∆htf /k}. interval density function ∆htf /kx/. figure pricing distribution. illustrates distribution draw posted prices time ﬁxed arrival quantity ∆htf captures beneﬁt obtaining normalization parameter. distribution’s support lowest price form pricing distribution given figure strategy gives mechanism known-costs case regret bounds depend upon prior knowledge algorithm. turn helpful prior knowledge following parameter interpreted costs upper lower bounds general data purchasing regret problem. immediate open problem paper close gap. intuitively lower bound take advantage strategic behavior posted-price mechanism often signiﬁcantly data actually costs meaning obtains less data long run. meanwhile possible improve upper-bound strategy drawing prices diﬀerent distribution. section give ﬁnal mechanism mechanism data purchasing statistical learning problem. idea simply regret-minimization mechanism arriving agents. stage mechanism posts hypothesis aggregate hypothesis averaging obtain ﬁnal prediction. section stated results easier at-cost variant regret minimization purchased data problem. included posted-price distribution main results. section show results distribution derived. at-cost variant formally deﬁned exactly main setting except ﬁrst show posted-price strategy derived optimal solution problem minimizing regret subject budget constraint. resulting upper bounds at-cost variant given theorem then give fundamental lower bounds regret showing general upper bounds cannot improved upon here. lower bounds also hold main no-regret data purchasing problem small upper bounds. begin asking seems even easier question. suppose every pair arrives could ﬁrst choose probability obtain would optimal probability take data? insight actually achieve sampling probabilities dictated lemma using randomized posted-price mechanism. notice optimal sampling probabilities decreasing general drawing price distribution probability exceeds decreasing remains posted-price distribution actually induces sampling probabilities want simultaneously. randomly drawing posted prices according distribution choose purchase exactly probability stated lemma possible value without knowing thus ﬁnal mechanism at-cost variant simply apply mechanism cost arrival rather price posted. γta. note choice normalization constant diﬀerent main setting average less at-cost setting; leads diﬀerence regret bounds. main bound at-cost variant given theorem open problem setting whether obtain regret bounds without prior knowledge arriving costs data. at-cost setting match upper bounds. open problem obtain larger-order lower bound main setting mechanism pays posted price. would show separation at-cost variant main problem. first give might considered sample complexity lower bound no-regret learning specializes setting case costs equal question regret achievable algorithm observes arrivals. next extend idea case heterogeneous costs. idea simple begin problem label-complexity lower bound introduce useless data points heterogeneous costs. worst hardest case given average cost cost perfectly correlated beneﬁt useful data points expensive. figure dataset. data points images handwritten digits data point consisting feature vector grayscale pixels label digit depicts. mnist handwritten digit dataset algorithm asked distinguish categories digits positive examples digits negative examples number training examples task allows adjust correlations drawing costs diﬀerently diﬀerent digits. comparison mechanisms. naive oﬀers maximum price every arrival budget. ours mechanism initialized adjusted online according estimated average data far. baseline obtains every data point costs distributed uniform independently. datapoint average trials standard error illustration role cost-data correlations. marginal distribution costs probability free otherwise correlation cost data changes. performance naive baseline change correlations. largerγta case high-cost points consisting smaller costs data independent. datapoint average trials standard error baseline mechanism budget purchases every data point. naive mechanism oﬀers maximum price every data point budget. ours implementation mechanism prior knowledge costs initialize adjust online estimating data purchased far. examples shown figure model interaction perhaps simplest initial starting point involves subtleties interesting address future. property need obtain arriving agent’s data point cost reason cost used importance-weight data based probability picking price larger cost. reason.) discussed section na¨ıve implementation model incentive-compatible strictly exploring implementations trusted third party approach mentioned interesting direction. instance strictly truthful implementation arriving agent cryptographically commit e.g. submitting cryptographic hash cost. contribution work propose active scheme learning pricing data arrives online held strategic agents. active approach allows learning past data selectively pricing future data. mechanisms interface existing no-regret algorithms essentially black-box fashion analysis relies showing good guarantees model no-regret learning purchased data. no-regret setting interest future work either achieve good guarantees foreknowledge maximum cost propose variants model. no-regret analysis means mechanisms robust adversarial input. nicer settings might hope improve guarantees. direction assume costs drawn according known marginal distribution combination approach posted-price distributions roth schoenebeck fruitful here. broadly problem purchasing data learning many potential models directions study. motivating setting closer crowdsourcing active problem data points consist pairs mechanism oﬀer price anyone obtains label given example. online arrival scheme mechanism could build importance-weighted active learning paradigm authors thank mike ruberry discussion formulation problem. thanks organizers participants indo-us lectures week machine learning game theory optimization bangalore.", "year": 2015}