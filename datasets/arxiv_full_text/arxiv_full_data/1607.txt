{"title": "Thinking Required", "tag": ["cs.LG", "cs.AI", "cs.CL"], "abstract": "There exists a theory of a single general-purpose learning algorithm which could explain the principles its operation. It assumes the initial rough architecture, a small library of simple innate circuits which are prewired at birth. and proposes that all significant mental algorithms are learned. Given current understanding and observations, this paper reviews and lists the ingredients of such an algorithm from architectural and functional perspectives.", "text": "exists theory single general-purpose learning algorithm could explain principles operation. assumes initial rough architecture small library simple innate circuits prewired birth. proposes signiﬁcant mental algorithms learned. given current understanding observations paper reviews lists ingredients algorithm architectural functional perspectives. classical also known good old-fashioned artiﬁcial intelligence concerning well deﬁned tasks including optimization rulebased inference eﬃcient knowledge representation expert systems planning graph search algorithms evolutionary algorithms among others. data distribution ﬁtted model describing prior knowledge problem. includes limited deep learning algorithms– reinforcement learning– well non-neural machine learning methods support vector machines despite fundamentally novel deep models become successful recently solving variety hard problems–. sudden popularity computational tractability creation large labeled data sets emergence fast programmable hardware parallel processing usage alternative nonlinearities rectiﬁers– advances numerical optimization–. artiﬁcial general intelligence another branch research distinguished– ambitious theoretical description intelligence might however lacking practical algorithms exist machine learning. cerned building large anatomically accurate possible computational models– order improve understanding brain works bottom-up way. results research inspirational machine learning community neuromorphic brain-inspired algorithm-agnostic hardware– focusing modeling physical properties neurons synapses order replicate brain’s learning ability. recently much progress made area supervised learning however greatest challenges remaining artiﬁcial intelligence research make steps towards advancements ﬁeld unsupervised learning algorithms especially autonomous learning complex spatiotemporal patterns main motivation work presented paper observation human experiences inherently spatiotemporal predictions context-dependent. paper postulates need another intensiﬁed research eﬀort spanning ﬁelds neuroscience machine learning neuromorphic computing order design algorithms build machines think machine intelligence. focuses neocortex intelligence part addressing aspects related brain consciousness emotions reinforcement long-term memory. neocortex attributed mammals deemed place intelligent behavior resides. studied extensively past decades date still consensus works. exists theory single learning algorithm explains intelligence considered ever since mountcastle’s discovery simple uniform architecture cortex might suggest brain regions perform similar operations regionspeciﬁc algorithms. another famous experiment reignited idea showed rewiring auditory part brain ferrets able learn interpret visual inputs. however alternative theories structural arrangement exist propose brain might collection microcircuits basically look same however perform diﬀerent functions. paper consider latter view. despite fact still much discovered many facts either already known likely true. assuming general purpose learning procedure indeed exist paper lists aspects could agree pieces evidence gathered far. knowledge necessary ingredients algorithm shaped neuroscientiﬁc discoveries empirical evaluation eﬀectiveness algorithms metacognition observations. points considered general assumptions reverse-engineering learning algorithm. real world almost data unlabeled. although best knowledge nobody discovered precise rules used human brain learning assume learn mostly unsupervised way. speciﬁcally newborn learns world diﬀerent objects interact might even provide supervised signal him/her because appropriate sensory representations need developed ﬁrst. another piece evidence supervised learning obtained simple calculation assuming approximately synapses seconds human lifetime enough capacity store memories rate bits/second would simply enough information labels alone. motivates hypothesis predominance unsupervised learning since acquiring much information absorbing data perceptual inputs. even teacher present learning must done learning associations events without supervision. ﬁrst learning concepts forming internal representations experiences associating internal representations names afterwards separated unsupervised learning researched extensively found closely connected process entropy-maximization regularization compression means evolution brains adapted data compactors. particular goal unsupervised learning might codes disentangle input sources describe original information less redundant interpretable throwing much data possible without losing information. example operation observed visual cortex learns patterns appearing natural environment assigns high probability those whereas probability random combinations. real world data said near nonlinear manifold within higher-dimensional space manifold shape deﬁned data probability distribution. clustering equivalent learning manifolds able separate well enough given task. humans learn concepts sequential order ﬁrst making sense simple patterns representing complex ones terms previously learned abstractions. ability read might serve example. first learn recognize strokes letters words able understand complex sentences whereas non-hierarchical approach would attempt read straight patterns piece paper. brain might adapted reﬂect fact world inherently hierarchical. observation also inspired deep learning movement used hierarchical approach model real world data achieving unprecedented performance many tasks. deep learning algorithms automatically compose multiple layers representations data gives rise models yield increasingly abstract associations concepts main distinction deep approach previous generation machine learning structure data discovered automatically general-purpose learning procedure without need hand-engineer feature detectors scheme agrees well idea unsupervised learning mentioned above. abstract hierarchical representations might natural by-products data compression. upon theoretical empirical evidence favor deep representation learning could formulate requirement type brain-like architecture deep. question however nature learning procedure. existence cortical columns neocortex linked functional importance arrangement. column typically responds sensory stimulus representing certain body part region sound vision cells belonging cell excited simultaneously therefore acting feature detector. time column active prohibit nearby columns becoming active. lateral inhibition mechanism leads sparse activity patterns. fact strongly active columns inhibited forces learned patterns invariant possible giving rise independent feature detectors cortex. might expected sparse distributed representations brain coincidental since possess important properties information-theoretical perspective. distributed aspect makes factored codes possible important order disentangle underlying causes sparsity aﬀects elements learning good features. proven given certain sparsity signal correctly reconstructed even fewer samples sampling theorem requires comparison dense binary representation space-eﬃcient sense given bits capable storing diﬀerent values. domain learning complex patterns real environment fundamental problem almost random bits produce value diﬀerent original given noisy version pattern would possible recover correct version. sparse distributed represenfigure example factored representation composite object/scene represented objects comprising scene addition that shapes described reusable properties color shape. tations hand assume every meaning small number bits active moment time object collection simples patterns properties make sdrs signiﬁcantly noise-resistant. another important property distributed representations number distinguishable regions scales exponentially number parameters used describe true non-distributed representations. sparse distributed representations combinatorially much expressive. given observation simple discriminative point view higher levels abstractions sdrs preferred representing inputs since learning procedure produces form preserves much information possible making code short/simple possible in-line occam’s razor minimum description length rules postulate simple solution chosen complex ones allows manipulating sparse representations throughlarge network simpliﬁes learning higher level concepts ever since discovery selective features detectors edge detectors center-surround receptive ﬁelds hubel wiesel learning biologically plausible sparse distributed representations input patterns research topic seen instantiation unsupervised learning many algorithms invented convolutional neural networks hand supervised learning architectures based principle learning hierarchy sdrs currently provide state-ofthe-art image recognition proving discriminative value learning representations. sparsity also link quantiﬁably better performance discriminative tasks explained fact sparse representations simplify optimization objective. gates backward turn rely objective function depends task definition performance criterion assumptions. deﬁnitely problem causes generality/performance tradeoﬀ requires priori knowledge task. another issue related scalability procedure propagating error derivatives backwards single place network. next standard backpropagation assumes objective function intermediate activations differentiable. figure eﬃcient learning sdrs; sparse distributed representations simplify learning temporal dependencies; provide mechanism generalization outof-domain prediction another desirable property resulting factored representation generalization capability meaning similar input patterns produce similar outputs. might imply sdrs plausible candidate alphabet used neocortex means machine intelligence. example advantage compared dense representation becomes obvious considering learning temporal dependencies spatial patterns assuming learning procedure disentangled underlying sources variation learning complex sequences decomposed ﬁnding relationships sources. backpropagation algorithm lies heart deep architectures. speciﬁes internal parameters model levels changed order improve given certain problems† following occam’s razor rule paper questions whether really needs backpropagation learn non-trivial concepts. usually algorithm computes derivatives outputs propa†this again author’s view; backpropagation evil; might diﬀerent path leading goal figure images unrecognizable humans classiﬁed trained imagenet percent certainty. result highlights diﬀerences between statistical methods humans recognize objects ﬁgure borrowed then addition diﬃculty handengineer task deﬁnition objective function exists problem discovered recently fact exist images classiﬁed almost object great conﬁdence despite fact might resemble know object humans. existence images suggests models fails really understand concepts instead situation bears resemblance chinese room argument. adversarial examples neighborhood data manifold possess similar statistics therefore algorithm thinks same however drastically diﬀerent humans. hypothesis explaining phenomenon objective problem itself. could hypothesized following gradient objective function prohibit learning procedure discovering unknown state-space progress learning equivalent close objective fooling images problem limited particular algorithm architecture dataset. fact shown many areas undesirable properties even transferred other. striking example contrast accurate generating image captions type mistakes makes shown figure problem lack understanding grammar complex concepts applies machine translation. brain comprises approximately neurons synapses large network having single learning objective propagating error derivates backwards might best choice instead might reasonable separate local learning adjusting higher level connections layers/regions functional distinction would reﬂect structural hierarchy predominant deep learning methods described real world. biological technological social networks types real-world networks neither completely random deﬁnitely regular. instead topology lies somewhere between. called small world networks nature’s solution hierarchical structure allows separate parallel local global updates synapses scalability unsupervised learning lower levels goal-oriented ﬁne-tuning higher regions. finally problem attributed gradient-based learning called catastrophic forgetting means model forget previously learned knowledge upon presentation data re-adjusting parameters according gradients. studying neocortex indeed reveals case columnar organization reﬂects local connectivity cerebral cortex. anpiece evidence comes success convolutional networks sharing connections imposes local/global structure learning. usage sparse distributed representation also important scalability point view since representations inherently fault tolerant. moreover sparse activations stored compact non-zero elements could processed instead all. finally shown recently neural networks able learn even limited computational precision stochastic approximations noise–. fact networks even better generalization capability. brain inherently parallel machine without separate instruction-issuing memory storage areas. instead parts neocortex participate both. diﬀerence compared von-neumann architecture describing majority computing systems organized. main bottleneck current systems concerns data movement implies additional bandwidth power latency requirements. cpus typically optimized serial tasks mitigating negative eﬀects architecture deep cache hierarchy losing parallelism involved. gpus brain-like layout equal processing units having private memory actually operate parallel without colliding. however problem moving data still exists either inside gpu. problem persists. fact quite easy show virtually impossible achieve peak performance processors data cannot fast enough. moreover data transfers major energy consumption factors parallel gpu-like devices. therefore radical approach needed order improve performance significantly. von-neumann architecture needs changed memory compute. hardware allows functionality already appeared. concept in-place processing assumes however diﬀerent approach also needed thinking algorithms. process communication-aware algorithm design already started advent multi-core cpus gpus fpgas. next step design communication-less algorithms. ongoing eﬀort supercomputing community noticed signiﬁcant progress made without reducing information transfer-overhead. given views expressed above could facilitate learning? successful learning algorithms moment based global gradient descent algorithm however several reasons preferred solution problem avoid specifying global objective. idea look inspiration nature study brains evolved order build rich internal representations environment. provides shortcut might safest general time potentially solving many problems even though yet. major goal current research. although task decoding algorithm used brain diﬃcult clues might happening inside heads. start with roughly brain divided separate subsystems neocortex main information processing workhorse therefore considered separated lower-level actions reward/value-like inputs long-term memory access/formation serve reason paper skips parts justify unreasonable think single learning algorithm reinforcement-based. shown tasks performed visual cortex brain spatial pattern detection forming sparse representations inputs found work modeled algorithmically shown biologically plausible features learned using simple hebbian-like learning rules. then observed exist called simple cells speciﬁc pattern detector oriented edge complex cells extent invariant transformations inputs react general group stimuli discoveries served inspiration groundbreaking performance deep convolutional neural networks image recognition time successes served feedback loop study create even accurate models processing brain feedforward hmax model whose neurophysiologically plausible topology similar cresceptron similarities auditory cortex individual phonemes activated diﬀerent subsets auditory neurons. comes learning still much discovered. thing already pointed algorithm/learning process mostly unsupervised nature. speciﬁcally shown hypothesized main function brain unsupervised learning temporal sequences. general level means constantly anticipates outcome acts observes world compares observations previous expectations adjusts synapses internal model world makes accurate predictions. formal approaches expressing idea formulated hinton fritz wiskott sejnowski local contrastive divergence-like learning target propagation much plausible method backpropagation; implementation side things discovered least types connections integrated pyramidal cell likely serving feed-forward sequence feedback roles accordingly using simple local hebbian-like learning rules might resemble local backpropagation algorithm tiny scale. fact observed brain figure neocortical pyramidal neuron thousands excitatory synapses located dendrites three sources input cell. feedforward inputs form synapses proximal soma directly lead action potentials. nmda spikes generated distal basal apical dendrites depolarize soma typically suﬃciently generate somatic action potential. figure borrowed hypothesized pyramidal cells neocortex might integrate incoming bottom-up topsignals simple instead operation gate-like proximal dendrites responsible feed-forward stimuli distal dendrites immediate prediction apical dendrites acting ﬁlter/gate disambiguation mechanism sense recently popular approach including gating means memory access control attention might biologically plausible probably anatomically accurate model networks spiking neurons. learning networks also using local plasticity rules i.e. spike timing dependent plasticity temporal form hebbian-like learning based temporal correlations spikes prepostsynaptic neurons. believed underlie learning information storage biological neural networks. might suggest fact local learning rules exist brain however asynchronous asymmetric version. core operation deﬁned kind networks detecting occurrence temporally close spatially distributed input signals coincidence detection. sake simplicity‡ paper focus aspect assume simpler symmetric local learning approach described hinton. important fact neocortex connections neurons created rewords number parameters moved model ﬁxed. however majority current approaches focus changes strengths connections neurons meaning topology network ﬁxed reﬂects prior knowledge. given sparse connectivity approach closer biological reality would oﬀer great advantage terms increase number combinations neurons/synapses available encode learned information. interesting aspect research connecting mechanisms described theoretical concepts machine intelligence given low-level properties learning algorithm would overall goal learning learning path look like? kind behavior would considered stepping stone towards machine intelligence describe precise way? basic question means machine algorithm intelligent needs clariﬁcation. according some goal-directed behavior considered essence intelligence. however implies necessary suﬃcient condition intelligent behavior rationality paper questions statement. humans often rational. creativity fall deﬁnition risk-taking might rational it’s essential innovation. therefore appealing theories universal intelligence broader priors theory curiosity creativity beauty described schmidhuber. previous section introduces problems arise objective based learning chinese room argument algorithm interested inputs outputs without motivation learn anything beyond task given. intelligent algorithm able reveal hidden knowledge might even discoverable humans. despite speciﬁc task section point functional ingredients learning procedure would violate generality assumption. informationlearning likened formal theory based concept information compression. assuming goal build compact useful representations environment interpretation relates representation learning analogy building compression scheme neocortex. looking task considering general artiﬁcial intelligence general purpose compressor able discover probability distribution source. however free lunch theorem states completely general-purpose learning algorithm exist words every compressor exists data distribution perform poorly. implies must exist restrictions class problems work well. previous section already mentioned them fortunately general plausible smoothness prior depth prior complete list sensible assumptions). whereas smoothness prior considered spatial coherence assumption world mostly predictable corresponds temporal generally spatiotemporal coherence. probably important ingredient general-purpose learning procedure. words states things close time close space vice versa. purely spatial analogy huge image space tiny fraction possible real images. occam’s razor rule principle state simple solutions favored complex ones therefore learning better representations goal itself even without objective. assumed task given priori best observe learn predict. ﬁrst working examples principle history compression employed recurrent architecture proposed schidmuber. ability predict equivalent understanding since given moment cause prediction could inferred given state context. therefore learning predict general requirement intelligent behavior. fact postulated brain constantly predicting future states comparing predictions sensory inputs readjusting accordingly. might seem equivalent backpropagating error entire network however biological perspective prediction/expectation readjustment neurons likely operating locally. scientists demonstrated brain predicts consequences movements based next. ﬁndings implications understanding human attention applications robotics. despite fact that practice experienced perceived twice human brains able form stable representation abstract make accurate predictions despite changes context. example mental representations present observed explaining rapid movements known saccades eyes move rapidly approximately three times second order capture visual information. jump image falls onto retina. however experience quickly-changing sequence images instead stable image. brain uses mechanism order redirect attention since approximately retina provides sharp image operation extensively researched neuroscientiﬁc perspective provides visible brain activities well provided inspiration algorithms mimicking behavior.– sensorimotor connections needed order know changes image result internal movement. assumed motor command subtracted inputs order provide invariant representation concept. implies actually every part neocortex must performing function given uniformity. hypothesis basic repeating functional unit neocortex sensorimotor model every part brain performs sensory motor processing extent. complex cells visual cortex invariant small changes inputs patterns invariant activations might mapped purely spatially represent spatiotemporal patterns experiments support claim showing similar mechanism operating diﬀerent type sensory inputs. implementation perspective sensorimotor integration understood top-down connections mentioned previous section figure feedforward feedback connections’ roles concept understanding. despite rapidly changing sensory inputs diﬀerent order observations brain capable maintaining stable representations higher levels; low-level predictions depends context ject additional context fig. paper assumes predictions associated uncertainty bayesian approach instead assuming single point prediction distribution highly multimodal. additional context equivalent integrating evidence makes predictions speciﬁc. figure face example spatiotemporal concept micro-saccades sequences low-level spatial patterns fovea pooled temporally mid-level concept nose; macro-saccades taskoriented movement moving nose eyes mouth necessity means manipulating spatiotemporal concept illustrated simple example. given images fig. obvious unnatural classiﬁcation based purely spatial aspect pattern. much natural putting objects category function requires ability imagine whether particular object used used certain applies objects chairs. much natural learn concepts spatiotemporal ideas rather predominant purely spatial machine-learning methods considering ability imagine/dream/hallucinate commonness sensorimotor functionality brain surprising. concept manipulating compact spatiotemporal thought might necessary reasoning perspective transfer learning majority analogies make temporal nature. importance learning transformations real-world recognized research community still needs attention. last functional component postulated paper states exists inﬁnite loop bottom-up predictions top-down context. hypothesis interconnectedness enables perceptual ﬁlling higher layers make hypotheses inferences coming lower layers predictions iteratively reﬁned based hypotheses. likened working memory theory non-episodic memories being held analogy expectation maximization learning procedure commonly used boltzmann machines samples obtained iteratively alternating between unit activations connected layers real-world analogy process solving crossword sudoku puzzle ﬁlling missing words sentence. problems require iterative procedure reﬁning solution using intermediate hypotheses. addition links ideas objective-less learning imagination novel pattern discovery. partial support work provided defense advanced research projects agency would like thank members machine intelligence group research numenta suggestions many interesting discussions.", "year": 2015}