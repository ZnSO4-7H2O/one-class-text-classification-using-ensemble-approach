{"title": "E-swish: Adjusting Activations to Different Network Depths", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Activation functions have a notorious impact on neural networks on both training and testing the models against the desired problem. Currently, the most used activation function is the Rectified Linear Unit (ReLU). This paper introduces a new and novel activation function, closely related with the new activation $Swish = x * sigmoid(x)$ (Ramachandran et al., 2017) which generalizes it. We call the new activation $E-swish = \\beta x * sigmoid(x)$. We show that E-swish outperforms many other well-known activations including both ReLU and Swish. For example, using E-swish provided 1.5% and 4.6% accuracy improvements on Cifar10 and Cifar100 respectively for the WRN 10-2 when compared to ReLU and 0.35% and 0.6% respectively when compared to Swish. The code to reproduce all our experiments can be found at https://github.com/EricAlcaide/E-swish", "text": "activation functions notorious impact neural networks training testing models desired problem. currently used activation function rectiﬁed linear unit paper introduces novel activation function closely related activation swish sigmoid generalizes call activation swish sigmoid. show e-swish outperforms many well-known activations including relu swish. example using e-swish provided accuracy improvements cifar cifar respectively compared relu respectively compared swish. code reproduce experiments found https //github.com/ericalcaide/e-swish election activation function notorious impact training testing dynamics neural network. correct choice activation function speed learning phase also lead better convergence resulting improvement metrics and/or benchmarks. initially since ﬁrst neural networks shallow sigmoid tanh nonlinearities used activations. problem came depth networks started increase became difﬁcult train deeper networks functions introduction rectiﬁer linear unit jarrett nair hinton allowed training deeper networks providing improvements allowed accomplishment state-of-theart results since then variety activations proposed clevert klambauer however none managed replace relu default activation majority models inconstant gains computational complexity paper introduce activation function closely related recently proposed swish function call e-swish. eswish generalization swish activation function multiplied parameter experiments show e-swish systematically outperforms well-known activation function providing better overall accuracy relu swish sometimes matching speed elu. experiments show e-swish outperforms relu swish even hyperparameters designed relu. example wide resnet e-swish provided improvement relative relu relative swish cifar dataset. ones swish. fact e-swish reverts swish. like relu swish e-swish unbounded bounded below. like swish also smooth non-monotonic. property non-monotonicity almost exclusive swish e-swish. another exclusive feature both swish e-swish region derivative greater seen image above maximum slope function therefore derivative grows linearly therefore speculate large coefﬁcient cause gradient exploding problems small cause gradient vanishing. experimentally show great choice parameter might derivative swish sigmoid seen derivatives similar change parameter instead inferred formulas figure seen figure derivative e-swish often bigger regions especially parameter high. confusing provides faster learning show experimentally e-swish swish able train deeper networks relu using batch normalization e-swish implemented custom activation popular deep learning libraries using keras β*tf.nn.swishwhen using tensorﬂow). implementations e-swish widely used deep learning frameworks provided together code reproduce experiments performed paper. paper takes reference work original swish paper reason compare e-swish relu swish consider basis work compare dont provide extensive comparisons activation functions since theyre provided original swish paper. difﬁcult determine functions perform better others given presence confounding factors. despite this believe nonmonotonicity e-swish favours performance. fact gradients negative part function approach zero also observed swish relu softplus activations. however believe particular shape curve described negative part gives swish e-swish non-monotonicity property improves performance since output small negative numbers unlike relu softplus. also believe multiplying original swish function accentuates properties. however choosing large cause gradient exploding problems. reason conduct experiments values parameter range inspired original swish paper plot output landscape random network result passing grid form points coordinates. network composed layers neurons initialized small values using glorot uniform initialization original swish paper improvements associated swish function supposed caused smoothness swish output landscape directly impacts loss landscape. seen figure e-swish output landscapes also smoothness property. plot output landscapes seen figure observed slope e-swish landscape higher relu swish. found higher parameter higher slope loss landscape. therefore infer that probably e-swish show faster learning relu swish parameter increases. reference also plot landscape activation function since proved provide faster learning relu slope higher too. figure projection output landscape random network. values obtained ﬁtting coordinates point grid -layer neurons each randomly initialized neural network. best viewed color. section provide detailed explanation tests e-swish activation function. measure performance e-swish relative activations benchmark results obtained following datasets mnist cifar cifar. experiments nvidia geforce memory. first want prove e-swish also ability train deeper networks relu shown swish ramachandran conduct similiar experiment performed paper mentioned before. train fully connected networks different depths mnist neurons layer. optimizer initial learning rate momentum multiply learning rate whenever epochs improvement validation accuracy. train network epochs although terminate training earlier case improvement validation accuracy epochs. glorot uniform initialization batch size dropout applied. dont residual connections since would allow train deeper networks. batch normalization last layers index satisﬁes index modulus operator. experiment shows e-swish performs similar swish deep networks speculate goes line hypothesis large values would cause gradient exploding. experiments three activations perform almost equally layers. then clear divergence relu swish e-swish swish e-swish outperform relu large margin interval layers. seen figure above performance e-swish affected parameter changed. experiment proves swish e-swish outperform relu training deep networks since achieve better test accuracies. first conduct small experiments show improvements provided eswish range models architectures. experiments showed e-swish consistently outperforms relu swish swish showed inconsistent improvements relu. also noticed eswish provides faster learning swish. conducted experiments single model performance median runs. mnist ﬁrst compare different activations single model performance. train fully connected network layers following number neurons dropout learning rate momentum also glorot uniform initialization batch size train network epochs. cifar datasets consist colored small images pixels one. cifar datasets composed images training testing purposes. cifar dataset contains images different classes boat plane. cifar datset contains images different classes requires much ﬁne-grained recognition compared cifar given classes similar. cifar measure performance e-swish relative activations different models. first compare different activations simple model show difference performance different activations. model achieves accuracy cifar relu activation function original paper implementation) parameters. optimizer original implementation change learning rates order speed training epochs initial learning rate multiply epochs. expect learning rate setting provide faster learning also little effect performance especially swish e-swish. figure value parameter e-swish respect relu swish conﬁdence intervals tests cifar- cifar- datasets using wrn-- standard data augmentation. color blue represents relu represents swish green represents e-swish. best viewed color compare e-swish relu swish simplenet model deeper composed convolutional layers. designed achieve good trade-off number parameters accuracy achieving accuracy less parameters. implemented model tensorﬂow replaced relu activation function either swish e-swish. used data augmentation batch size proposed original paper used instead adadelta optimizer used original implementation. model epochswith initial learning rate multiplied every epochs. since speculate values larger cause gradient exploding problems train models e-swish values parameter. computational constraints able benchmark e-swish relu swish single model performance. results obtained provided below table shows e-swish outperforms relu swish. improvement provided e-swish relative relu relative swish. considering hyperparameters designed relu e-swish/swish introduced drop-in replacement really signiﬁcative. finally compare e-swish relu wide residual network model chosen dropout since able train bigger models computational constraints. implemented model tensorﬂow framework replaced original relu e-swish activation function. used optimizer original implementation trained models epochs learning rate schedule plus another drop learning rate epochs. shown table e-swish provided improvement relative relu compared swish. also observe larger values parameter diminished improvements provided e-swish. work weve referred paramenter fact considered hyperparameter since non-learnable choice directly affects performance. empirically found values around worked well shallow mid-scale networks. large cnns found worked better however ﬁndings affected presence residual connections lower difference different values experiments could performed resource-intensive nature training deep networks computational constraints time publication paper. reason would like invite everyone perform experiments e-swish order extensive proof superiority activations. especially interested testing e-swish wide residual networks densenets resnets gans autoencoders rnns nasnets sigmoid. also showed parameter depended network depth proving choosing either large small harmed performance. experiments used models hyperparameters designed relu replaced relu swish e-swish another activation function. although suboptimal technique e-swish outperformed swish relu variety problems. although unable achieve state-of-the-art results resourceintensive nature training huge models shown e-swish consistently outperformed activation functions range different problems depths architectures managed improve existing result relatively recent model changing original activation e-swish.", "year": 2018}