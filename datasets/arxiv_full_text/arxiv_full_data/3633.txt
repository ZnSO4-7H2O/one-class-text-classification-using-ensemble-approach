{"title": "Deep Component Analysis via Alternating Direction Neural Networks", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Despite a lack of theoretical understanding, deep neural networks have achieved unparalleled performance in a wide range of applications. On the other hand, shallow representation learning with component analysis is associated with rich intuition and theory, but smaller capacity often limits its usefulness. To bridge this gap, we introduce Deep Component Analysis (DeepCA), an expressive multilayer model formulation that enforces hierarchical structure through constraints on latent variables in each layer. For inference, we propose a differentiable optimization algorithm implemented using recurrent Alternating Direction Neural Networks (ADNNs) that enable parameter learning using standard backpropagation. By interpreting feed-forward networks as single-iteration approximations of inference in our model, we provide both a novel theoretical perspective for understanding them and a practical technique for constraining predictions with prior knowledge. Experimentally, we demonstrate performance improvements on a variety of tasks, including single-image depth prediction with sparse output constraints.", "text": "despite lack theoretical understanding deep neural networks achieved unparalleled performance wide range applications. hand shallow representation learning component analysis associated rich intuition theory smaller capacity often limits usefulness. bridge introduce deep component analysis expressive multilayer model formulation enforces hierarchical structure constraints latent variables layer. inference propose diﬀerentiable optimization algorithm implemented using recurrent alternating direction neural networks enable parameter learning using standard backpropagation. interpreting feedforward networks single-iteration approximations inference model provide novel theoretical perspective understanding practical technique constraining predictions prior knowledge. experimentally demonstrate performance improvements variety tasks including single-image depth prediction sparse output constraints. deep convolutional neural networks achieved remarkable success ﬁeld computer vision. increasing availability extremely large labeled datasets along modern advances computation specialized hardware resulted state-of-the-art performance many problems including essentially visual learning tasks. examples include image classiﬁcation object detection semantic segmentation despite rich history practical theoretical insights problems modern deep learning techniques typically rely task-agnostic models poorly-understood heuristics. however recent work shown specialized architectures incorporating classical domain knowledge increase parameter eﬃciency relax training data requirements improve performance. prior advent modern deep learning optimization-based methods like component analysis sparse coding dominated ﬁeld representation learning. techniques structured matrix factorization decompose data linear combinations shared components. latent representations inferred minimizing reconstruction error subject constraints enforce properties like uniqueness interpretability. unlike feed-forward alternatives construct representations closed-form independent feature detectors optimization-based approach naturally introduces conditional dependence features order best explain data useful phenomenon commonly referred explaining away within context graphical models example eﬀect shown fig. compares sparse representations constructed using feed-forward soft thresholding given optimization-based inference penalty. many components overcomplete features high-correlation image constrained optimization introduces competition components resulting parsimonious representations. component analysis methods also often guided intuitive goals incorporating prior knowledge learned representations. example statistical independence allows separation signals distinct generative sources non-negativity leads parts-based decompositions objects sparsity gives rise locality frequency selectivity diﬃculty enforcing intuitive constraints like feed-forward computations deep learning architectures instead often motivated distantly-related biological systems poorly-understand internal mechanisms covariate shift gradient furthermore theoretical understanding deep learning fundamentally lacking even non-convex figure example explaining away conditional dependence provided optimization-based inference. sparse representations constructed feed-forward nonnegative soft thresholding many non-zero elements redundancy spurious activations hand sparse representations found -penalized nonnegative least-squares optimization yield parsimonious components optimally reconstruct approximations data. figure comparison feed-forward neural networks proposed deep component analysis model. standard deep networks construct learned representations feed-forward compositions nonlinear functions deepca instead treats unknown latent variables inferred constrained optimization accomplish this propose diﬀerentiable inference algorithm expressed alternating direction neural network recurrent generalization feed-forward networks unrolled ﬁxed number iterations learning backpropagation order unify intuitive theoretical insights component analysis practical advances made possible deep learning introduce framework deep component analysis novel model formulation interpreted multilayer extension traditional component analysis multiple layers learned jointly intuitive constraints intended encode structure prior knowledge. deepca also motivated perspective deep neural networks relaxing implicit assumption input layer constrained output previous layer shown below. feed-forward network output layer denoted given closed-form nonlinear function aj−. deepca instead takes generative approach latent variables associated layer inferred optimally reconstruct linear combination learned components subject constraints perspective intermediate network activations cannot found closed-form instead require explicitly solving optimization problem. variety diﬀerent techniques could used performing inference propose alternating direction method multipliers importantly demonstrate proper initialization single iteration algorithm equivalent pass associated feed-forward neural network nonlinear activation functions interpreted proximal figure demonstration deepca applied single-image depth prediction using images concatenated sparse sets known depth values input. baseline feed-forward networks guaranteed produce outputs consistent given depth values. adnns increasing number iterations learn satisfy sparse output constraints resolving ambiguities accurate predictions without unrealistic discontinuities. operators corresponding penalties constraints coeﬃcients. full inference procedure thus implemented using alternating direction neural networks recurrent generalizations feed-forward networks allow parameter learning using backpropagation. comparison standard neural networks deepca shown fig. experimentally demonstrate recurrent passes convolutional neural networks enable better sparsity control resulting consistent performance improvements supervised unsupervised tasks without introducing additional parameters. importantly deepca also allows constraints would impossible eﬀectively enforce single feed-forward pass network. example consider task single-image depth prediction diﬃcult problem absence three-dimensional information scale perspective. many practical scenarios however sparse sets known depth outputs available resolving ambiguities improve accuracy. prior knowledge come additional sensor modalities like lidar reconstruction algorithms provide sparse depths around textured image regions. feedforward networks proposed problem concatenating known depth values additional input channel however provides useful context predictions guaranteed consistent given outputs leading unrealistic discontinuities. comparison deepca enforces constraints treating predictions unknown latent variables. examples behavior resolve ambiguities shown fig. adnns additional iterations learn propagate information given depth values produce accurate predictions. addition practical advantages model also provides novel perspective conceptualizing deep learning techniques. decoupling layers provided relaxing feed-forward function composition constraints deepca equivalently expressed shallow model augmented architecture-dependent structure imposed model parameters. case rectiﬁed linear unit activation functions allows direct application results sparse approximation theory suggesting insights towards better understanding deep neural networks eﬀective. data linear combinations learned components rd×k. typically accomplished minimizing reconstruction error subject constraints coeﬃcients serve resolve ambiguity incorporate prior knowledge low-rank structure sparsity. examples include principal component analysis dimensionality reduction sparse dictionary learning accommodates overcomplete representations enforcing sparsity. component analysis problems typically non-convex structure naturally suggests simple alternating minimization strategies often guaranteed converge however unlike backpropagation stochastic gradient descent techniques typically require careful initialization order avoid poor local minima. alternatively consider nested optimization problem separates learning inference corresponding representations solving optimization problem ﬁxed parameters. unconstrained orthogonal components inference problem simple closed-form solution given linear transformation btx. substituting results linear autoencoder hidden layer tied weights unique global minimum trained backpropagation general constraints inference typically cannot accomplished closed form must instead rely iterative optimization algorithm. however algorithm composed ﬁnite sequence diﬀerentiable transformations model parameters still learned backpropagating gradients steps inference algorithm. extend idea representing algorithm inference deepca model recurrent neural network unrolled ﬁxed number iterations. recently deep neural networks emerged preferred alternative component analysis representation learning visual data. ability jointly learn multiple layers abstraction shown allow encoding increasingly complex features textures object parts unlike component analysis inference given closed-form design. speciﬁcally representation constructed passing image composition alternating linear transformations parameters ﬁxed nonlinear activation functions layers follows instead considering forward pass neural network arbitrary nonlinear function interpret method approximate inference unsupervised generative model. follows previous work shown equivalent bottom-up inference probabilistic graphical model approximate inference multi-layer convolutional sparse coding model however approaches limited practical applicability reliance careful hyperparameter selection specialized optimization algorithms. admm proposed gradient-free alternative backpropagation parameter learning inference allows simpler learning using backpropagation arbitrary loss functions. aside adnns recurrent feedback proposed models improve performance iteratively reﬁning predictions especially applications human pose estimation image segmentation outputs complex correlation patterns methods also implement feedback directly unrolling iterative algorithms often geared towards speciﬁc applications graphical model inference solving under-determined inverse problems image alignment similar deepca provides general mechanism feedback arbitrary neural networks motivated interpretable goal minimizing reconstruction error subject constraints network activations. inﬁnity otherwise. pre-multiplication weight matrix simplify notation method also supports linear transformation replacing transposed weight matrix multiplication corresponding adjoint operator. example adjoint convolution transposed convolution popular approach upsampling convolutional networks penalty functions convex problem also convex solved using standard optimization methods. appears diﬀer substantially inference deep neural networks later show seen generalization feed-forward inference function remainder section justify penalty functions lieu explicit nonlinear activation functions drawing connections non-negative regularization relu activation functions. propose general algorithm solving unknown coeﬃcients formalize relationship deepca traditional deep neural networks enables parameter learning backpropagation. introducing inference algorithm ﬁrst discuss connection penalties nonlinear proximal operators forms basis close relationship deepca traditional neural networks. ubiquitous within ﬁeld convex optimization proximal algorithms methods solving nonsmooth optimization problems. essentially techniques work breaking problem sequence smaller problems often solved closed-form proximal operators associated penalty functions given solution following optimization problem within framework deepca interpret nonlinear activation functions deep networks proximal operators associated convex penalties latent coeﬃcients layer. connection cannot used generalize nonlinearities many naturally interpreted proximal operators. example sparsemax activation function projection onto probability simplex similarly relu activation function projection onto nonnegative orthant. used negative bias equivalent equivalence noted previously means theoretically analyze convolutional neural networks deepca supports optimizing bias penalty hyperparameter backpropagation adaptive regularization results better control representation sparsity. addition standard activation functions deepca also allows enforcing additional constraints encode prior knowledge. example single-image depth prediction sparse known outputs provided prior knowledge penalty function ﬁnal output selector matrix extracts indices corresponding known outputs associated proximal operator projects onto constraint simply correcting outputs disagree known constraints. note would eﬀective output nonlinearity feed-forward network because constraints would technically satisﬁed nothing enforce consistent neighboring predictions leading unrealistic discontinuities. contrast deepca inference minimizes reconstruction error layer subject constraints taking multiple iterations network. model parameters ﬁxed solve deepca inference problem using alternating direction method multipliers general optimization technique successfully used wide variety applications derive algorithm applied problem ﬁrst modify objective function introducing auxiliary variables constrain equal unknown coeﬃcients shown below. others ﬁxed breaking full inference optimization problem smaller pieces solved closed form. decoupling layers deepca model latent activations updated incrementally stepping layer succession resulting faster convergence updates mirror computational structure deep neural networks. layer objective function separable algorithm reduces classical two-block admm extensive convergence guarantees multiple layers however algorithm seen instance cyclical multiblock admm quadratic coupling terms. experiments shown approach eﬀective applications theoretical analysis convergence properties still active area research process repeated convergence. though available closed-form expression next section demonstrate algorithm posed recurrent generalization feed-forward neural network. inference algorithm essentially follows pattern deep neural network layer learned linear transformation applied current output followed ﬁxed nonlinear function. building upon observation implement using recurrent network standard layers thus allowing model parameters learned using backpropagation. recall update requires solving linear system equations. diﬀerentiable introduces additional computational complexity present standard neural networks. overcome this implicitly assume parameters over-complete layers parseval tight frames i.e. bjbt property theoretically advantageous ﬁeld sparse approximation used constraint encourage robustness deep neural networks however experiments found unnecessary explicitly enforce assumption training; appropriate learning rates backpropagating inference algorithm enough ensure repeated iterations result diverging sequences variable updates. thus assumption simplify update using woodbury matrix identity follows involves simple linear transformations admm algorithm solving optimization expressed recurrent neural network repeatedly iterates problem inference function convergence. practice however unroll network ﬁxed number iterations approximation thus iteration inference algorithm equivalent standard feed-forward neural network given i.e. nonlinear activation functions interpreted proximal operators corresponding penalties deepca model. additional iterations network lead accurate inference approximations explicitly satisfying constraints latent variables. deepca inference approximated diﬀerentiable adnns model parameters learned standard feed-forward networks. extending nested component analysis optimization problem inference function used generalization feed-forward network inference backpropagation arbitrary loss functions encourage output consistent provided supervision shown below. here latent coeﬃcients last layer shown loss function intermediate outputs could also included. agnostic perspective adnn thus seen end-to-end deep network architecture particular sequence linear nonlinear transformations tied weights. iterations result networks greater eﬀective depth potentially allowing representation complex nonlinearities. however network architecture derived algorithm inference deepca model instead arbitrary compositions parameterized transformations greater depth requires additional parameters serves speciﬁc purpose satisfying constraints latent variables enforcing consistency model parameters. addition practical advantages recurrent adnns constraint satisfaction deepca model provides useful theoretical tool better understanding traditional neural networks. prior work papyan analyzed feed-forward networks method approximate inference multilayer convolutional sparse coding model seen special case deepca overcomplete dictionaries ﬁxed constraints sparsity coeﬃcients subject exact reconstructions layer. speciﬁcally provide conditions activations feed-forward convolutional network relu nonlinearities approximate true coeﬃcients model bounded error exact support recovery. conditions likely strict satisﬁed practice theoretical connection emphasizes importance sparsity demonstrates potential analyzing deep neural networks perspective sparse approximation theory. general deepca model introduces penalties relax requirement exact reconstruction layer eﬀectively introducing errors break standard compositional structure neural networks. importantly decoupling layers allows original multilayer inference function expressed equivalent shallow learning problem nonnegative penalties corresponding biased relu activation functions simply augmented higher-dimensional nonnegative sparse coding problem dictionary constrained particular block structure relates architecture corresponding neural network. suggests nonlinear function composition necessary eﬀective deep learning instead implicit structure enforces. connection shallow learning allows direct application results ﬁeld sparse approximation theory. example despite able exactly reconstruct datapoint overcomplete dictionaries satisfy certain incoherence properties sparsest reconstruction coeﬃcients actually unique uniqueness important characteristic learned representations able memorize data prominent feature many eﬀective deep architectures standard shallow models typically incapable satisfying properties required uniqueness high-dimensional datasets like common ﬁeld computer vision deepca suggests increased capacity deep networks explained structure imposed augmented dictionaries higher-dimensionality allows richer unique representations greater number non-zero elements block structure figure demonstration eﬀects ﬁxed learnable bias parameters reconstruction error activation sparsity comparing feed forward networks deepca models consist three layers components. conditional dependence provided recurrent feedback deepca learns better control sparsity level order improve reconstruction error. regularization weights biases converge towards zero resulting denser activations higher network capacity reconstruction. figure eﬀect increasing model size training testing classiﬁcation error demonstrating consistently improved performance adnns feed-forward networks especially larger models. base reduces number free parameters need learned. uniqueness guarantees also apply solutions sparse nonnegative least squares problems even without explicitly enforcing sparsity help explain success deep networks without explicit bias terms broadly believe deepca opens fruitful direction future research towards principled design neural network architectures optimize capacity sparse representation enforcing implicit structure augmented dictionaries associated shallow models. section demonstrate practical advantages accurate inference approximations deepca model using recurrent adnns feed-forward networks. even without additional prior knowledge standard convolutional networks relu activation functions still beneﬁt additional recurrent iterations demonstrated consistent improvements supervised unsupervised tasks cifar- dataset speciﬁcally unsupervised reconstruction loss function fig. shows conditional dependence features provided additional iterations allows better sparsity control resulting higher network capacity denser activations lower reconstruction error. suggests recurrent feedback allows adnns learn richer representation spaces explicitly penalizing activation sparsity. supervised classiﬁcation loss function backpropagating inference allows biases used adaptive regularization providing freedom modulating activation sparsity increase model capacity ensuring uniqueness representations across semantic categories. results improved classiﬁcation performance shown fig. especially wider models components layer. experiments emphasize importance sparsity deep networks justify deepca model formulation eﬀectiveness feed-forward soft thresholding approximation explicit regufigure quantitative results demonstrating improved generalization performance adnn inference. training testing reconstruction errors throughout optimization show iterations substantially reduce convergence time give much lower error held-out test data. suﬃciently large number iterations even lower-capacity models encoders consisting fewer residual blocks achieve nearly level performance small discrepancies training testing errors. larization limits amount additional capacity achieved iterations. such adnns provide much greater performance gains prior knowledge available form constraints cannot eﬀectively approximated feed-forward nonlinearities. exempliﬁed application output-constrained single-image depth prediction simple feed-forward correction known depth values results inconsistent discontinuities. demonstrate nyu-depth dataset sample training images testing images held-out scenes. facilitate faster training known measurements. following model architecture uses resnet encoder feature extraction image concatenated known depth values additional input channel. followed adnn decoder composed three transposed convolution upsampling layers biased relu nonlinearites ﬁrst layers constraint correction proximal operator last layer. fig. shows mean absolute prediction errors model increasing numbers iterations diﬀerent encoder sizes. models similar prediction error training data adnns iterations achieve signiﬁcantly improved generalization performance reducing test error feed-forward baseline iterations even low-capacity encoders. qualitative visualizations fig. show improvements result consistent constraint satisfaction serves resolve depth ambiguities. deepca novel deep model formulation extends shallow component analysis techniques increase representational capacity. unlike feed-forward networks intermediate network activations interpreted latent reconstruction coeﬃcients inferred using iterative constrained optimization algorithm. implemented using recurrent adnns allow model parameters learned arbitrary loss functions. addition provide tool consistently integrating prior knowledge form constraints regularization penalties. close relationship feed-forward networks equivalent iteration algorithm proximal operators replacing nonlinear activation functions deepca also provides novel theoretical perspective interpret deep networks. suggests sparse approximation theory tool analyzing designing network architectures optimize capacity learning unique representations data. figure qualitative depth prediction results given single image sparse known depth values input. outputs baseline feed-forward model inconsistent constraints evidenced unrealistic discontinuities. adnn iterations learns enforce constraints resolving ambiguities detailed predictions better agree ground truth depth maps depending diﬃculty additional iterations little eﬀect output insuﬃcient consistently integrate known constraint values", "year": 2018}