{"title": "PDE-Net: Learning PDEs from Data", "tag": ["math.NA", "cs.LG", "cs.NE", "stat.ML"], "abstract": "In this paper, we present an initial attempt to learn evolution PDEs from data. Inspired by the latest development of neural network designs in deep learning, we propose a new feed-forward deep network, called PDE-Net, to fulfill two objectives at the same time: to accurately predict dynamics of complex systems and to uncover the underlying hidden PDE models. The basic idea of the proposed PDE-Net is to learn differential operators by learning convolution kernels (filters), and apply neural networks or other machine learning methods to approximate the unknown nonlinear responses. Comparing with existing approaches, which either assume the form of the nonlinear response is known or fix certain finite difference approximations of differential operators, our approach has the most flexibility by learning both differential operators and the nonlinear responses. A special feature of the proposed PDE-Net is that all filters are properly constrained, which enables us to easily identify the governing PDE models while still maintaining the expressive and predictive power of the network. These constrains are carefully designed by fully exploiting the relation between the orders of differential operators and the orders of sum rules of filters (an important concept originated from wavelet theory). We also discuss relations of the PDE-Net with some existing networks in computer vision such as Network-In-Network (NIN) and Residual Neural Network (ResNet). Numerical experiments show that the PDE-Net has the potential to uncover the hidden PDE of the observed dynamics, and predict the dynamical behavior for a relatively long time, even in a noisy environment.", "text": "dong beijing international center mathematical research peking university center data science peking university beijing institute data research beijing china dongbinmath.pku.edu.cn partial differential equations play prominent role many disciplines applied mathematics physics chemistry material science computer science etc. pdes commonly derived based physical laws empirical observations. however governing equations many complex systems modern applications still fully known. rapid development sensors computational power data storage past decade huge quantities data easily collected efﬁciently stored. vast quantity data offers opportunities data-driven discovery hidden physical laws. inspired latest development neural network designs deep learning propose feed-forward deep network called pde-net fulﬁll objectives time accurately predict dynamics complex systems uncover underlying hidden models. basic idea proposed pde-net learn differential operators learning convolution kernels apply neural networks machine learning methods approximate unknown nonlinear responses. comparing existing approaches either assume form nonlinear response known certain ﬁnite difference approximations differential operators approach ﬂexibility learning differential operators nonlinear responses. special feature proposed pde-net ﬁlters properly constrained enables easily identify governing models still maintaining expressive predictive power network. constrains carefully designed fully exploiting relation orders differential operators orders rules ﬁlters also discuss relations pde-net existing networks computer vision network-in-network residual neural network numerical experiments show pde-net potential uncover hidden observed dynamics predict dynamical behavior relatively long time even noisy environment. differential equations especially partial differential equations play prominent role many disciplines describe governing physical laws underlying given system interest. traditionally pdes derived based simple physical principles conservation laws minimum energy principles based empirical observations. important examples include navierstokes equations ﬂuid dynamics maxwell’s equations electromagnetic propagation schr¨odinger’s equations quantum mechanics. however many complex systems modern applications still eluded mechanisms governing equations systems partially known. rapid development sensors computational power data storage last decade huge quantities data easily collected efﬁciently stored vast quantity data offers opportunities data-driven discovery potentially physical laws. then following interesting intriguing question learn model given data perform accurate efﬁcient predictions using learned model? earlier attempts data-driven discovery hidden physical laws bongard lipson schmidt lipson main idea compare numerical differentiations experimental data analytic derivatives candidate functions apply symbolic regression evolutionary algorithm determining nonlinear dynamical system. recently brunton schaeffer rudy zhang propose alternative approach using sparse regression. construct dictionary simple functions partial derivatives likely appear unknown governing equations. then take advantage sparsity promoting techniques select candidates accurately represent data. form nonlinear response known except scalar parameters raissi karniadakis presented framework learn unknown parameters introducing regularity consecutive time step using gaussian process. recently raissi introduced class universal function approximators called physics informed neural networks capable discovering nonlinear pdes parameterized scalars. recent work greatly advanced progress problem. however symbolic regression expensive scale well large systems. sparse regression method requires certain numerical approximations spatial differentiations dictionary beforehand limits expressive predictive power dictionary. although framework presented raissi karniadakis raissi able learn hidden physical laws using less data approach sparse regression explicit form pdes assumed known except scalar learnable parameters. therefore extracting governing equations data less restrictive setting remains great challenge. main objective paper accurately predict dynamics complex systems uncover underlying hidden models time minimal prior knowledge systems. inspiration comes latest development deep learning techniques computer vision. interesting fact popular networks computer vision resnet close relationship pdes furthermore deeper network expressive power network possesses enable learn complex dynamics arose ﬁelds computer vision. however existing deep networks designed deep learning mostly emphasis expressive power prediction accuracy. networks transparent enough able reveal underlying models although perfectly observed data perform accurate predictions. therefore need carefully design network combining knowledge deep learning applied mathematics learn governing pdes dynamics make accurate predictions time. note work closely related chen authors designed network based discretization quasilinear parabolic equations. however clear dynamics image denoising governed pdes authors attempt recover objective pde-net learn form nonlinear response perform accurate predictions. unlike existing work proposed network requires minor knowledge form nonlinear response function requires knowledge involved differential operators associated discrete approximations. nonlinear response function learned using neural networks machine learning methods discrete approximations differential operators learned using convolution kernels jointly learning response function prior knowledge form response function easily adjust network architecture taking advantage additional information. simplify training improve results. also discuss relations pde-net existing networks computer vision network-in-network resnet. details given section section section conduct numerical experiments linear nonlinear generate data using high precision numerical methods gaussian noise mimic real situations. numerical results show pde-net uncover hidden equations observed dynamics predict dynamical behavior relatively long time even noisy environment. particular novelty approach impose appropriate constraints learnable ﬁlters order easily identify governing models still maintaining expressive predictive power network. makes approach different existing deep convolutional networks mostly emphasis prediction accuracy networks well existing approaches learning pdes data assume either form response function known ﬁxed approximations differential operators. words proposed approach vast ﬂexibility ﬁtting observed dynamics able accurately predict future behavior also able reveal hidden equations driving observed dynamics. constraints ﬁlters motivated earlier work dong general relations wavelet frame transforms differential operators established. particular observed dong relate ﬁlters ﬁnite difference approximation differential operators examining orders rules ﬁlters constraints ﬁlters also useful network designs machine learning tasks computer vision. given series measurements physical quantities t···} spatial domain want discover governing pdes data. assume observed data associated takes following general form objective design feed-forward network named pde-net approximates that predict dynamical behavior equation long time possible; able reveal form response function differential operators involved. main components pde-net combined together network automatic determination differential operators involved discrete approximations; approximate nonlinear response function section start discussions relation convolutions differentiations discrete setting. comprehensive analysis relations convolutions differentiations within variational framework laid dong authors established general connections based approach wavelet frame based approach image restoration problems. demonstrate observations work using simple example. consider -dimensional haar wavelet frame ﬁlter bank contains low-pass ﬁlter three high pass ﬁlters circular convolution. easy verify using taylor’s expansion high frequency coefﬁcients haar wavelet frame transform discrete approximations differential operators here represent horizontal vertical spatial grid size respectively. simplicity notation regular character denote discrete continuum functions since confusion within context. profound relationship convolutions differentiations presented dong authors discussed connection order rules ﬁlters orders differential operators. note order rules closely related order vanishing moments wavelet theory ﬁrst recall deﬁnition order rules. deﬁnition ﬁlter rules order according proposition order differential operator approximated convolution ﬁlter order rules. furthermore according obtain high order approximation given differential operator corresponding ﬁlter order total rules example ﬁlter haar wavelet frame ﬁlter bank rules order total rules order \\{}. thus constant proper scaling corresponds discretization ﬁrst order approximation. ﬁler rules order total rules order \\{}. thus constant proper scaling corresponds discretization ∂x∂y ﬁrst order approximation. finally consider ﬁlter shall call -element -moment simplicity. combining proposition easily ﬁlter designed approximate differential operator given approximation order imposing constraints convolution example want approximate ﬁlter consider following constrains here means constraint corresponding entry. constraints described moment matrix left guarantee approximation accuracy least ﬁrst order ones right guarantee approximation least second order. particular entries constrained e.g. corresponding ﬁlter uniquely determined case call frozen ﬁlter. pde-net shall introduced next subsection ﬁlters learned subjected partial constraints associated moment matrices. worth noticing approximation property ﬁlter limited size. generally speaking large ﬁlters approximate higher order differential operators lower order differential operators higher approximation orders. taking -dimensional case example -element ﬁlters cannot approximate ﬁfth order differential operator whereas -element ﬁlters can. words larger ﬁlters stronger representation capability ﬁlters. however larger ﬁlters lead memory overhead higher computation cost. wisdom balance trade-off practice. given evolution consider forward euler temporal discretization. consider sophisticated temporal discretization leads different network architectures. simplicity focus forward euler paper. here operators convolution operators underlying ﬁlters denoted i.e. diju operators etc. approximate differential operators i.e. diju ∂i+j ∂ix∂j operators average operators. purpose introducing average operators stead using identity improve stability network enables capture complex dynamics. assumption observed dynamics governed form assume highest order less positive integer. then task approximating equivalent multivariate regression problem approximated point-wise neural network classical machine learning methods. combining approximation differential operators nonlinear function achieve approximation framework referred δt-block note prior knowledge form response function easily adjust network architecture taking advantage additional information. simplify training improve results. δt-block guarantees accuracy one-step dynamics take error accumulation consideration. cause severe instability prediction. improve stability network enable long-term prediction stack multiple δt-blocks deep network call network pde-net importance stacking multiple δt-blocks demonstrated section pde-net easily described stacking δt-block multiple times; sharing parameters δt-blocks. given input data training pde-net δt-blocks needs minimize accumulated error ˜u|| output pde-net input thus pde-net bigger owns longer time stability. note sharing parameters common practice deep learning decreases number parameters leads signiﬁcant memory reduction loss function constraints consider data indicates j-th solution path certain initial condition unknown dynamics. would like train pde-net δt-blocks. given every pair data training sample input label need match output pde-net. select following simple loss function training output pde-net input. ﬁlters involved pde-net properly constrained using associated moment matrices. underlying ﬁlters dij. impose following constrains demonstrate necessity learnable ﬁlters compare pde-net aforementioned constrains ﬁlters pde-net frozen ﬁlters. differentiate cases shall call pde-net frozen ﬁlters frozen-pde-net. increase expressive power ﬂexibility pde-net associate multiple ﬁlters approximate given differential operator. however order mess identiﬁability underlying model select ﬁlters provide correct approximation given differential operator described above. rest ﬁlters constrained contribute modify local truncation errors. example consider ﬁlters constrain moment matrices follows different ﬁxing numerical approximations differentiations advance sparse regression methods using learnable ﬁlters makes pde-net ﬂexible enables robust approximation unknown dynamics longer time prediction furthermore speciﬁc form response function also approximated data rather assumed known advance hand inﬂicting constrains moment matrices identify differential operators included underlying helps identifying nonlinear response function grants transparency pde-net potential reveal hidden physical laws. therefore proposed pde-net distinct existing learning based method discover pdes data well networks designed deep learning computer vision tasks. parameters point-wise neural network shared across computation domain initialized random sampling gaussian distribution. ﬁlters initialize freezing corresponding differential operators. example ﬁlter approximate freeze constraining -moment moments training process release ﬁlters switching constrains described section instead training n-layer pde-net directly adopt layer-wise training improves training speed. precise start training pde-net ﬁrst δt-block results ﬁrst δt-block initialization restart training pde-net ﬁrst δt-blocks. repeat complete blocks. note parameters δt-block shared across layers. addition warm-up step training ﬁrst δt-block. warm-up step obtain good initial guess parameters point-wise neural network approximates using frozen ﬁlters. recent years variety deep neural networks introduced great success computer vision. structure proposed pde-net similar existing networks network-in-network deep residual neural network improvement traditional convolutional neural networks. special designs multilayer perceptron convolution layers instead ordinary convolution layers. mlpconv layer contains convolutions small point-wise neural networks. design improve ability network extract nonlinear features shallow layers. inner structure δt-block pde-net similar mlpconv layer multiple δt-blocks structure similar structure except pooling relu operations. hand δt-block pde-net paths averaged quantity increment structure coincides residual block introduced resnet. fact substantial study relation resnet dynamical systems recently convection-diffusion equations classical pdes used describe physical phenomena particles energy physical quantities transferred inside physical system processes diffusion convection convection-diffusion equations widely applied many scientiﬁc areas industrial ﬁelds pollutants dispersion rivers atmosphere solute transferring porous medium reservoir simulation. practical situations usually physical chemical properties different locations cannot thus reasonable convection coefﬁcients diffusion coefﬁcients variables instead constants. computation domain discretized using regular mesh. data generated solving problem using high precision numerical scheme pseudo-spectral method spatial discretization order runge-kutta temporal discretization assume periodic boundary condition initial value generated |k||l|≤n chosen randomly. order mimic real world scenarios noise generated data. sample sequence noise added maxxyt{u} represents standard normal distribution. convolution operators {cij arrays approximate functions approximation achieved using piecewise quadratic polynomial interpolation smooth transitions boundaries piece. ﬁlters associated convolution operators coefﬁcients piecewise quadratic polynomials trainable parameters network. training testing data generated on-the-ﬂy i.e. generate data needed following aforementioned procedure training testing pde-net. experiments size ﬁlters used total number trainable parameters δt-block approximately training lbfgs instead optimize parameters. data samples batch train layer construct pde-net layers requires totally data samples batch. note pde-net designed assumption approximates nonlinear evolution pdes relatively stronger assumption networks deep learning. therefore require less training data lbfgs performs better furthermore shown numerical results learned pde-net generalizes well. pde-net accurately predict dynamics even initial data come distribution training process. section presents numerical results training pde-net using data described previous subsection. speciﬁcally observe learned pde-net performs terms prediction dynamical behavior identiﬁcation underlying model. furthermore investigate effects hyper-parameters learned pde-net. demonstrate ability trained pde-net prediction language machine learning ability generalize. pde-net δt-blocks trained randomly generate initial guesses based feed pde-net measure normalized error predicted dynamics actual dynamics using high precision numerical scheme). normalized error true data predicted data deﬁned spatial average error plots shown figure results longer prediction pde-net learnable ﬁlters shown figure images predicted dynamics presented figure results that pde-net ﬁlters signiﬁcantly outperforms pde-net ﬁlters terms length reliable predictions reach error length prediction pde-net ﬁlters times pde-net ﬁlters. figure prediction errors pde-net frozen-pde-net ﬁlters. plot horizontal axis indicates time prediction interval vertical axis shows normalized errors. banded curves indicate percentile normalized errors among test samples. discovering hidden equation linear problem identifying amounts ﬁnding coefﬁcients {cij approximate {fij coefﬁcients {cij trained pde-net shown figure note {fij absent corresponding coefﬁcients learned pde-net indeed close zero. order concise demonstration results show image {cij figure figure images true dynamics predicted dynamics. ﬁrst shows images true dynamics. second shows images predicted dynamics using pde-net δt-blocks ﬁlters. time step comparing ﬁrst three rows figure coefﬁcients {cij} learned pde-net close true coefﬁcients {fij} except oscillations presence noise training data. furthermore last figure indicates multiple δt-blocks helps estimation coefﬁcients. however larger ﬁlters seem improve learning coefﬁcients though helps tremendously prolonging predictions pde-net. figure first true coefﬁcients equation. left right coefﬁcients uyy. second learned coefﬁcients pde-net δt-blocks ﬁlters. third learned coefﬁcients pde-net δt-blocks ﬁlters. last errors true learned coefﬁcients v.s. number δt-blocks different sizes ﬁlters demonstrate well learned pde-net generalizes generate initial values following highest frequency equal followed adding noise note maximum allowable frequency training results long-time prediction estimated dynamics shown figure although oscillations observed prediction estimated dynamic still captures main pattern true dynamic. second order. previous experiments assumed exceed order. know second order able accurate estimation variable coefﬁcients convection diffusion terms. however prediction errors slightly higher since fewer trainable parameters. nonetheless since using accurate prior knowledge unknown variance prediction demonstrate importance moment constraints ﬁlters pde-net trained network without moment constraints skipped steps utilize knowledge relation ﬁlters differential operators simplicity call pde-net train freed-pde-net. prediction errors freed-pde-net shown curves figure since without moment constraints know correspondence ﬁlters differential operators. therefore cannot identify correspondence learned variable coefﬁcients either. plot variable coefﬁcients figure freed-pde-net better prediction pde-net since trainable parameters pde-net. however unable identify free-pde-net. figure prediction errors pde-net assuming underlying order order freed-pde-net ﬁlters. plot horizontal axis indicates time prediction interval vertical axis shows normalized errors. banded curves indicate percentile normalized errors among test samples. summary numerical experiments show pde-net able conduct accurate prediction identify underlying model time even noisy environment. multiple δt-blocks i.e. deeper structure pde-net makes pde-net stable enables longer time prediction. furthermore using larger ﬁlters helps stability prolong reliable predictions. comparisons pde-net frozen-pde-net freed-pde-net demonstrate importance using learnable partially constrained ﬁlters literature. figure first true coefﬁcients equation. left right coefﬁcients uyy. second learned coefﬁcients pde-net assuming order third learned coefﬁcients pde-net assuming order last errors true learned coefﬁcients v.s. number δt-blocks pde-net assuming order modeling physical processes like particle transportation energy transfer addition convection diffusion consider source/sink terms. problems source/sink plays important role. example convection-diffusion equations used describe distribution pollutants water atmosphere identifying intensity pollution source equivalent ﬁnding source term important environmental pollution control problems. sin. computation domain discretized using regular mesh. data generated solving problem using forward euler temporal discretization central differencing spatial discretization mesh restricted mesh. assume zero boundary condition initial value generated obtained maximum allowable frequency numerical setting section gaussian noise added sample sequence described suppose know priori underlying convection-diffusion equation order nonlinear source depending variable then response function takes following form δt-block pde-net written convolution operators {cij arrays approximate functions approximation achieved using piecewise quadratic polynomial interpolation smooth transitions boundaries piece. approximation obtained piecewise order polynomial approximation regular grid interval grid points. training testing strategy exactly experiments size ﬁlters total number section trainable parameters δt-block approximately section presents numerical results trained pde-net using data described section observe trained pde-net performs terms prediction dynamical behavior identiﬁcation underlying model. demonstrate ability trained pde-net prediction language machine learning ability generalize. testing method exactly method described section comparisons pde-net frozen-pde-net shown figure clearly advantage learning ﬁlters. long-time predictions pde-net shown figure visualize predicted dynamics figure demonstrate well learned pde-net generalizes generate initial values following highest frequency equal followed adding noise note maximum allowable frequency training results long-time prediction estimated dynamics shown figure results show learned pde-net performs well prediction. discovering hidden equation identifying amounts ﬁnding coefﬁcients {cij approximate {fij approximates computed coefﬁcients {cij trained pde-net shown figure computed shown figure note ﬁrst order terms absent corresponding coefﬁcients learned pde-net indeed close zero. approximation accurate near center interval near boundary. value data mostly distributed near center figure prediction errors pde-net frozen-pde-net ﬁlters. plot horizontal axis indicates time prediction interval vertical axis shows normalized errors. banded curves indicate percentile normalized errors among test samples. paper designed deep feed-forward network called pde-net discover hidden model observed dynamics predict dynamical behavior. pde-net consists major components jointly trained approximate differential operations convolutions properly constrained ﬁlters approximate nonlinear response deep neural networks machine learning methods. pde-net suitable learning pdes general however prior knowledge form response function easily adjust network architecture taking advantage additional information. simplify training improve results. example considered linear variable-coefﬁcient convection-diffusion equation. results show pde-net uncover hidden equation observed dynamics predict dynamical behavior relatively long time even noisy environment. furthermore deep structure larger learnable ﬁlters improve pde-net terms stability prolong reliable predictions. part future work proposed framework real data sets. important directions uncover hidden variables cannot measured sensors directly data assimilation. another interesting direction worth exploring learn stable consistent numerical schemes given model based architecture pde-net. dong supported part nsfc zichao long supported part national research development program china yfc. yiping supported elite undergraduate training program school mathematical sciences peking university. steven brunton joshua proctor nathan kutz. discovering governing equations data sparse identiﬁcation nonlinear dynamical systems. proceedings national academy sciences figure images true dynamics predicted dynamics. ﬁrst shows images true dynamics. second shows images predicted dynamics using pdenet δt-blocks ﬁlters. here kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition", "year": 2017}