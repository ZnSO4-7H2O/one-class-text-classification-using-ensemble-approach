{"title": "Missing Value Imputation With Unsupervised Backpropagation", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "Many data mining and data analysis techniques operate on dense matrices or complete tables of data. Real-world data sets, however, often contain unknown values. Even many classification algorithms that are designed to operate with missing values still exhibit deteriorated accuracy. One approach to handling missing values is to fill in (impute) the missing values. In this paper, we present a technique for unsupervised learning called Unsupervised Backpropagation (UBP), which trains a multi-layer perceptron to fit to the manifold sampled by a set of observed point-vectors. We evaluate UBP with the task of imputing missing values in datasets, and show that UBP is able to predict missing values with significantly lower sum-squared error than other collaborative filtering and imputation techniques. We also demonstrate with 24 datasets and 9 supervised learning algorithms that classification accuracy is usually higher when randomly-withheld values are imputed using UBP, rather than with other methods.", "text": "many data mining data analysis techniques operate dense matrices complete tables data. realworld data sets however often contain unknown values. even many classiﬁcation algorithms designed operate missing values still exhibit deteriorated accuracy. approach handling missing values missing values. paper present technique unsupervised learning called unsupervised backpropagation trains multi-layer perceptron manifold sampled observed point-vectors. evaluate task imputing missing values datasets show able predict missing values signiﬁcantly lower sum-squared error collaborative ﬁltering imputation techniques. also demonstrate datasets supervised learning algorithms classiﬁcation accuracy usually higher randomly-withheld values imputed using rather methods. words imputation; manifold learning; missing values; neural networks; unsupervised learning. many effective machine learning techniques designed operate dense matrices complete tables data. unfortunately real-world datasets often include samples observed values mixed many missing unknown elements. missing values occur human impatience human error data entry data loss faulty sensory equipment changes data collection methods inability decipher handwriting privacy issues legal requirements variety practical factors. thus improvements methods imputing missing values far-reaching impact improving effectiveness existing learning algorithms operating real-world data. present method imputation called unsupervised backpropagation trains multilayer perceptron manifold represented known features dataset. demonstrate algorithm task imputing missing values show signiﬁcantly effective methods imputation. backpropagation long popular method training neural networks typical supervised approach trains weights multilayer perceptron training examples consisting feature vectors corresponding label vectors many interesting problems however training data available form. paper consider signiﬁcantly different problem training estimate impute missing attribute values represented matrix attributes continuous categorical. missing elements must predicted becomes output rather input. latent vectors inputs mlp. however examples given training data. thus must trained using accomplishes task training using known attribute values on-line backpropagation. presentation known value attribute instance simultaneously computes gradient vector update weights gradient vector update input vector paper demonstrate method imputing missing values show outperforms approaches task. compare imputation methods data sets. values removed data sets completely random. show predicts missing values signﬁcantly lower error approaches. also evaluated learning algorithms compare classiﬁcation accuracy using imputed data sets. learning algorithms using imputed data usually achieve higher classiﬁcation accuracy methods. increase signiﬁcant data missing. remainder paper organized follows. section reviews related work missing value imputation. described section section presents results comparing imputation methods. provide conclusions discussion future directions section algorithm falls intersection several different paradigms neural networks collaborative ﬁltering data imputation manifold learning. neural networks extension generative backpropagation generative backpropagation adjusts inputs neural network holding weights constant. contrast computes weights input values simultaneously. related approaches used generate labels images natural language although techniques used labeling images documents knowledge used application imputing missing values. differs generative backpropagation trains weights simultaneously inputs instead training pre-processing step. also classiﬁed manifold learning algorithm. like common non-linear dimensionality reduction algorithms isomap mlle manifold sculpting reduces high-dimensional vectors corresponding low-dimensional vectors unlike algorithms however also learns model manifold. also unlike algorithms designed operate incomplete observations. collaborative ﬁltering viewed non-linear generalization matrix factorization linear dimensionality reduction technique effective collaborative ﬁltering well imputation. method become popular technique part effectiveness data used netflix competition involves factoring data matrix much-smaller matrices. smaller matrices combined predict missing values original dataset. equivalent using linear regression project data onto ﬁrst principal components. unfortunately well-suited data exhibits non-linearities. previously shown matrix factorization could represented neural network model involving hidden layer linear activation functions comparison approach uses standard arbitrary number hidden layers non-linear activation functions instead network structure previously proposed matrix factorization. produces good results task imputation demonstrate better. imputation technique reﬁnement nonlinear shown effective imputation. approach also uses gradient descent train high-dimensional space. training weights used represent non-linear components within data. components extracted one-at-a-time data principal components nlpca becomes non-linear generalization pca. typically however components learned together would properly termed non-linear generalization nlpca evaluated task missing value imputation relationship recognized time compared contributions paper show nlpca signiﬁcant improvement task imputation. also demonstrate achieves even better results nlpca task best algorithm imputation aware. primary difference nlpca utilizes three-phase training approach makes robust falling local optimum training. comparable latter-half autoencoder autoencoders create dimensional representation training using training examples input features well target values. ﬁrst half encoder reduces input features dimensional space nodes middle layer less number input features. latter-half autoencoder maps dimensional representation training back original input features. however capture non-linear dependencies data autoencoders require deep architectures allow layers inputs dimensional representation data dimensional representation data output. deep architecture makes training autoencoder difﬁcult computationally expensive generally requiring unsupervised layer-wise training trains network half depth corresponding autoencoder practical many problems autoencoders computationally expensive. since demonstrate application imputing missing values data also relevant consider approaches classically used task. simple methods dropping patterns contain missing values randomly drawing values replace missing values often used based simplicity implementation. methods however signiﬁcant obvious disadvantages data scarce. another common approach treat missing elements unique value. approach however shown bias parameter estimates multiple linear regression models cause problems inference many models take granted better accuracy desirable methods generally used better methods exist. simple improvement compute separate centroid output class. disadvantages method suitable regression problems cannot generalize unlabeled data since depends labels impute. methods based maximum likelihood long studied statistics also depend pattern labels. since common unlabeled data labeled another well-studied approach involves training supervised learning algorithm predict missing values using non-missing values inputs unfortunately case multiple values missing pattern present difﬁculty approaches. either learning algorithm must used implicitly handles missing values manner exponential number models must trained handle combination missing values. further also shown results methods tend poor high percentages missing values effective collaborative ﬁltering method imputation cluster data make predictions according centroid cluster point falls luengo compared several imputation methods evaluating effect classiﬁcation accuracy found cluster-based imputation fuzzy k-means using manhattan distance outperform methods including involving state machine learning methods methods traditionally used imputation. analysis however ﬁnds methods compared outperform fkm. related imputation method called instance-based imputation combine non-missing values k-nearest neighbors point replace missing values. evaluate similarity points cosine correlation often used tends effective presence missing values well aforementioned imputation techniques considered single imputation techniques imputation missing value made. single imputation disadvantage introducing large amounts bias since imputed values reﬂect added uncertainty fact values missing. overcome this rubin proposed multiple imputation estimates added variance combining outcomes imputed data sets. similarly ensemble techniques also shown effective imputing missing values paper compare ensemble methods involves single model included ensemble well imputation method. latent matrix element column ˆxrc value predicted weight feeds unit unit mlp. network unit hidden layer input unit output activation value unit error term associated unit. component gradient used reﬁne wij. component gradient used reﬁne using backpropagation compute gradient respect weights common operation training mlps using backpropagation compute gradient respect inputs however much less common provide derivation here. deriviation compute presentation single element could also derived presentation full presentation since assume high-dimensional missing many values signiﬁcantly efﬁcient train presentation known element individually. begin deﬁning error signal express gradient partial derivative error signal respect inputs represent output values values units hidden layer. part error term units layer backpropagation calculates error term associated network unit. thus additional calculation equation strict generalization equation equation considers output unit known target value presented whereas equation sums unit intrinsic value feeds. trains three phases ﬁrst phase computes initial estimate intrinsic vectors second phase computes initial estimate network weights third phase reﬁnes together. three phases train using stochastic gradient descent derive classic backpropagation algorithm. brieﬂy give intuitive justiﬁcation approach. initial experimentation used simpler approach training single phase. several problems observed early training intrinsic point vectors tended separate clusters. points cluster appeared unrelated arbitrarily assigned clusters random initialization. training continued effectively created separate mapping cluster intrinsic representation corresponding values effectively places unnecessary burden must learn separate mapping cluster forms expected target values. phase give intrinsic vectors chance self-organize hidden layers form nonlinear separations among them. likewise phase gives weights chance organize without train moving inputs. preprocessing phases initialize system good initial starting point gradient descent likely local optimum higher quality. empirical results validate theory showing produces accurate imputation results nlpca reﬁnes together. pseudo-code algorithm trains three phases given algorithm algorithm calls algorithm performs single epoch training. detailed description algorithm follows. matrix containing known data values passed returns matrix low-dimensional representation corresponding ragged matrix containing weight values maps approximation forwardpropagated estimate values missing elements line sets heuristic values used detect convergence. note many techniques could used detect convergence. implementation used simple approach dividing half training data validation set. decay learning rate whenever predictions fail improve sufﬁcient amount validation data. simple approch always stops yielded better empirical results variations tried. speciﬁes initial learning rate. convergence detected learning rate falls speciﬁes amount improvement expected epoch else learning rate decayed. regularization term used ﬁrst phases ensure weights become excessively saturated ﬁnal phase training. regularization used ﬁnal phase training want ultimately data closely possible. used default heuristic values speciﬁed line experiments tuning seemed little impact ﬁnal results. believe values well-suited problems could possibly tuned necessary. line sets learning rate initial value. value used store previous error score. error measured initialized lines train convergence detected. discarded. lines perform second phase training. phase differs ﬁrst phase ways used instead temporary single-layer perceptron held constant phase. next describe algorithm performs single epoch training stochastic gradient descent. algorithm similar epoch traditional backpropagation except presents element individually instead presenting vector conditionally reﬁnes intrinsic vectors well weights order enable process nominal attributes convert values vector representing membership weight category. example given value value {mousecatdog} represented vector unknown values attribute converted unknown real values requiring algorithm make predictions. missing values imputed convert data back original form ﬁnding mode categorical distribution. example predicted vector would converted prediction mouse. well-known techniques comparisons made. matrix factorization generally considered efﬁcient imputation technique nonlinear nonlinear generalization matrix factorization. linear activation function used equivalent matrix factorization additional computational overhead computing activation function. hidden layers added computational complexity increased remains proportional number weights network. adds additional phases training nonlinear pca. thus worst case times slower nonlinear pca. practice however ﬁrst phases tend fast preprocessing phases even cause third phase much faster running time signiﬁcant issue empirical validation focuses imputation accuracy. compared imputation algorithms. imputation methods examined well algorithmic parameter values mean/mode baseline establish baseline comparison compare method replacing missing values continuous attributes mean non-missing values attribute replacing missing values nominal attributes common value non-missing values attribute. parameters expected reasonable algorithm outperform baseline algorithm problems. nlpca used logistic function activation function summed squared error objective function. thus imputed missing values total dataset scenarios. algorithm found parameters yielded best results compared best results algorithm averaged runs differing random seeds. facilitate reproduction results assist related research efforts integrated implementation competitor algorithms wafﬂes machine learning toolkit repository mldata.org {abalone arrhythmia bupa colic credit-g desharnais diabetes ecoli eucalyptus glass hypothyroid ionosphere iris nursery ozone pasture sonar spambase spectrometer teaching assistant vote vowel waveform- yeast}. ensure objective evaluation collection determined evaluation performed modiﬁed emphasize favorable results. ensure results would applicable tasks require generalization removed class labels dataset input features could used imputing missing values. normalized real-valued attributes fall within range every attribute would carry approximately equal weight evaluation. removed completely random values dataset dataset generated datasets missing values using different random number seed make total tasks evaluation. task imputation algorithm restore missing values. measured error comparing predicted value corresponding original normalized value summed attributes dataset. nominal values used hamming distance real values used squared difference original predicted values. average error computed patterns dataset. figure shows representative comparisons error scores obtained algorithm varying levels sparsity. comparisons datasets generally exhibited similar trends. nlpca much better algorithms values missing. algorithm best every case achieved best score cases algorithm. table summarizes results comparisons. achieved lower error algorithm pair-wise comparisons comparing imputation scores datasets averaged runs different random seeds. pair-wise comparisons better sufﬁcient number table high-level summary comparisons imputation techniques. results shown sparsity values. table summarizes comparison competitor imputation algorithm predicting missing values. help understand might lead better classiﬁcation results also performed experiments involving classiﬁcation imputed datasets. restored class labels datasets imputed feature values. used repetitions -fold cross validation learning algorithms weka machine learning toolkit backpropagation nearest neighbor locally weighted learning na¨ıve bayes nearest neighbor generalization random forest ripple rule learner ripper learning algorithms chosen intent diverse another diversity determined using unsupervised meta-learning table results classiﬁcation tests sparsity level upper numbers cell indicate wins ties losses pair-wise comparison. lower number cell indicates wilcoxon signed ranks test p-value evaluated datasets. evaluated learning algorithm previously mentioned sparsity levels using imputation algorithm parameters resulted lowest error score imputation. results experiment summarized sparsity levels tables cell tables summarizes experiments imputation algorithm classiﬁcation algorithm pair. three upper numbers cell indicate number times lead higher equal lower classiﬁcation accuracy. left-most three numbers biggest indicates classiﬁcation accuracy higher cases used imputation algorithm. lower number cell indicates p-value obtained wilcoxon signed ranks test performing pair-wise comparison imputation algorithm column. value better sufﬁcient number pair-wise comparisons demonstrate better statistical signiﬁcance. cases indicated classiﬁcation algorithms responsive improved imputation accuracy offered others. example appears beneﬁcial effect classiﬁcation results used random forest na¨ıve bayes. overall demonstrated beneﬁcial imputation algorithm imputation algorithms. expected nlpca closest competitor. better nlpca larger number cases able demonstrate better statistical signiﬁcance sparsity level appears results improvements nlpca generally bigger impact missing values data. signiﬁcant difﬁculty effective imputation increases sparsity data. demonstrate difference nlpca brieﬂy examine manifold learning technique. trained nlpca using data mnist dataset handwritten digits. used multilayer perceptron topology. inputs treated latent values inputs used specify coordinates image. output trained predict grayscale pixel value speciﬁed coordinates. trained separate network digits dataset. training multilayer perceptrons manner uniformly sampled two-dimensional latent inputs order visualize multilayer perceptron organized digits learned. matrix factorization suitable application linear would predict linear gradient instead image. imputation algorithms suitable task designed used generative manner. figure shows sample generated nlpca ubp. algorithms approximately equally well generating digits appear natural range styles. primary difference results ultimately formed order intrinsic values represent various styles. case somewhat better intrinsic organization formed. three distinct styles observed three horizontal stripes figure boxey digits rows slanted digits middle rows digits closed bottom rows. height horizontal digit varies clearly left-to-right. along left horizontal crosses point along right horizontal crosses high point. case nlpca similar styles organized clusters exhibit degree organization apparent ubp. example appear able vary height horizontal three main styles digit. occurs nlpca extra step designed explicitly promote organization intrinsic values. demonstration also serves show techniques imputation techniques manifold learning merging. case handwritten digits multilayer perceptron ﬂexible enough generate visibly appealing digits even intrinsic organization poor. however better generalization expected mapping simplest occurs intrinsic values better organized. hence imputation manifold learning applied increasingly complex problems importance ﬁnding good organization intrinsic values increase. future work area explore techniques promoting organization within intrinsic values contrast simple approach proposed ubp. generate high-quality digits able organize various styles effectively. note height horizontal varies left-to-right. results nlpca provide ability vary height possible styles digit. since netflix competition matrix factorization related linear techniques generally considered state-of-the-art imputation. unfortunately focus imputing large datasets caused community largely overlook value nonlinear imputation techniques. contributions paper observation nonlinear presented almost decade consistently outperforms matrix factorization across diversity datasets. perhaps recognized thorough comparison techniques across diversity datasets previously performed. primary contribution paper however improvement nonlinear improves accuracy even more. contribution consists -phase training process initializes weights intrinsic vectors better starting positions. theorize leads better results bypasses many local optima network could otherwise settle places closer global optimum. datasets across range parameters algorithm. predicts missing values lower error methods majority cases. also demonstrated using impute missing values leads better classiﬁcation accuracy imputation techniques especially higher levels sparsity important case imputation. demonstrated also bettersuited manifold learning nlpca problem involving handwritten digits. ongoing research seeks demonstrate better organization intrinsic variables naturally leads better generalization. signiﬁcant potential application unsupervised learning tasks automation. edgar caroline rodriguez. treatment missing values effect classiﬁer accuracy. classiﬁcation clustering data mining applications. springer berlin heidelberg figure trained nlpca digits mnist dataset handwritten digits. training digits shown generated uniformly sampling space intrinsic values generating image sampled intrinsic point.although nlpca able generate high-quality digits able organize various styles effectively. note height horizontal varies left-to-right. results nlpca provide ability vary height possible styles digit. erhan dumitru pierre-antoine manzagol yoshua bengio samy bengio pascal vincent. difﬁculty training deep architectures effect unsupervised pre-training. journal machine learning research proceedings track jitender deogun william spaulding bill shuart. towards missing data imputation study fuzzy k-means clustering method. rough sets current trends computing volume lecture notes computer science. springer berlin heidelberg y.p. chen lin. filtering techniques selection contents products. personalization interactive multimedia services research development perspective nova science publishers. isbn ----. quinlan ross. programs machine learning. morgan kaufmann mateo usa. rubin multiple imputation nonresponse surveys. wiley. rumelhart d.e. g.e. hinton r.j. williams. learning representations back-propagating sarwar karypis konstan reidl. item-based collaborative ﬁltering recommendation algorithms. proceedings international conference world wide acm. isbn sayyad shirabad t.j. menzies. promise repository software engineering databases. school information technology engineering university ottawa canada. http //promise.site.uottawa.ca/serepository.", "year": 2013}