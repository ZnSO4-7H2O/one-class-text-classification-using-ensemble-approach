{"title": "Bootstrap Model Aggregation for Distributed Statistical Learning", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "In distributed, or privacy-preserving learning, we are often given a set of probabilistic models estimated from different local repositories, and asked to combine them into a single model that gives efficient statistical estimation. A simple method is to linearly average the parameters of the local models, which, however, tends to be degenerate or not applicable on non-convex models, or models with different parameter dimensions. One more practical strategy is to generate bootstrap samples from the local models, and then learn a joint model based on the combined bootstrap set. Unfortunately, the bootstrap procedure introduces additional noise and can significantly deteriorate the performance. In this work, we propose two variance reduction methods to correct the bootstrap noise, including a weighted M-estimator that is both statistically efficient and practically powerful. Both theoretical and empirical analysis is provided to demonstrate our methods.", "text": "distributed privacy-preserving learning often given probabilistic models estimated different local repositories asked combine single model gives efﬁcient statistical estimation. simple method linearly average parameters local models which however tends degenerate applicable non-convex models models different parameter dimensions. practical strategy generate bootstrap samples local models learn joint model based combined bootstrap set. unfortunately bootstrap procedure introduces additional noise signiﬁcantly deteriorate performance. work propose variance reduction methods correct bootstrap noise including weighted m-estimator statistically efﬁcient practically powerful. theoretical empirical analysis provided demonstrate methods. modern data science applications increasingly involve learning complex probabilistic models massive datasets. many cases datasets distributed multiple machines different locations communication expensive restricted; either data volume large store process single machine privacy constraints healthcare ﬁnancial systems. recent growing interest developing communication-efﬁcient algorithms probabilistic learning distributed datasets; e.g. boyd zhang dekel ihler rosenblatt nadler reference therein. work focuses one-shot approach distributed learning ﬁrst learn local models local machines combine fusion center form single model integrates information local models. approach highly efﬁcient computation communication costs casts challenge designing statistically efﬁcient combination strategies. many studies focused simple linear averaging method linearly averages parameters local models although nearly optimal asymptotic error rates achieved simple method tends degenerate practical scenarios models non-convex log-likelihood non-identiﬁable parameters applicable models non-additive parameters better strategy overcomes practical limitations linear averaging kl-averaging method ﬁnds model minimizes kullback-leibler divergence local models. directly combine models instead parameters. exact kl-averaging computationally tractable intractability calculating divergences; practical approach draw samples given local models learn combined model based bootstrap data. unfortunately bootstrap noise easily dominate approach need large bootstrap sample size obtain accurate results. section show estimator obtained naive total size observed data bootstrap sample size local model number machines. means ensure guaranteed centralized method simple linear averaging need unsatisfying since usually large assumption. work variance reduction techniques cancel bootstrap noise better kl-averaging estimates. difﬁculty task ﬁrst illustrated using relatively straightforward control variates method unfortunately suffers practical drawback linear averaging method linear correction term. propose better method based weighted m-estimator inherits practical advantages kl-averaging. theoretical part show methods give signiﬁcantly improves original bootstrap estimator. empirical studies provided verify theoretical results demonstrate practical advantages methods. paper organized follows. section introduces background section introduces methods analyze theoretical properties. present numerical results section conclude paper section detailed proofs found appendix. background problem setting suppose dataset size i.i.d. drawn probabilistic model require iterative communication local machines fusion center time consuming distributed settings number communication rounds forms main bottleneck instead consider simpler one-shot approach ﬁrst learns local models based subset send fusion center combined global model captures information. assume local models estimated using based subset k-th machine comprehensive theoretical analysis done ˆθlinear show asymptotic ˆθlinear fact equivalent global ˆθmle ﬁrst order several improvements developed improve second order term unfortunately linear averaging method easily break practice even applicable underlying model complex. example work poorly likelihood multiple modes exist non-identiﬁable parameters different parameter values correspond model models kind include latent variable models neural networks appear widely machine learning. addition linear averaging method obviously applicable local models different numbers parameters parameters simply additive discussions practical limitaions linear averaging method found ihler problems linear averaging well addressed kl-averaging method averages model ﬁnding geometric center local models terms divergence speciﬁcally ﬁnds model studied theoretical properties kl-averaging method showed ˆθmle distribution family full exactly recovers global exponential family achieves optimal asymptotic error rate among possible combination methods ˆθk}. despite attractive properties exact kl-averaging computationally tractable except simple models. ihler suggested naive bootstrap method approximation local model unfortunately show sequel accuracy ˆθkl critically depends bootstrap sample size would need nearly large original data size make ˆθkl achieve baseline asymptotic rate simple linear averaging achieves; highly undesirably since often assumed large distributed learning settings. propose variance reduction techniques improving kl-averaging estimates discuss theoretical practical properties. start concrete analysis kl-naive estimator missing ihler ˆθkl equal achieved simple linear make ˆθkl averaging need draw bootstrap data points total undesirable since often assumed large assumption distributed learning setting therefore critical task develop accurate methods reduce noise introduced bootstrap process. sequel introduce variance reduction techniques achieve goal. based control variates method improves ˆθkl using linear correction term another multiplicative control variates method modiﬁes m-estimator assigning bootstrap data point positive weight cancel noise. show method achieves higher rate mild assumptions second method attractive practical advantages. control variates method technique variance reduction monte carlo estimation introduces correlated auxiliary random variables known expectations asymptotics balance variation original estimator. know drawn local empirical fisher information matrix local model note differentiates method typical control variates methods instead estimated using empirical covariance control variates original estimator shares similarity one-step estimator huang huang focuses improving linear averaging estimator different setting. drawbacks linear averaging estimator discuss section motivates develop another kl-weighted method shown next section achieves asymptotical efﬁciency ˆθkl−c still inherits practical advantages kl-averaging. probability ratio acts like multiplicative control variate advantage positive applicable non-identiﬁable non-additive parameters. estimator deﬁned estimator motivated henmi idea applied reduce asymptotic variance importance sampling. similar result also found hirano shown similar weighted estimator estimated propensity score efﬁcient estimator using true propensity score estimating average treatment effects. although powerful tool results type seem widely known machine learning except several applications semi-supervised learning off-policy learning proof appendix result parallel theorem linear control variates estimator ˆθkl−c. similarly reduces rate take meanwhile unlike linear control variates estimator ˆθkl−w inherits practical advantages kl-averaging applied whenever kl-naive estimator applied including models non-identiﬁable parameters different numbers parameters. implementation θkl−w also much convenient since need calculate fisher information matrix required algorithm study empirical performance methods simulated real world datasets. ﬁrst numerically verify convergence rates predicted theoretical results using simulated data demonstrate effectiveness methods challenging setting number parameters local models different decided bayesian information criterion finally conclude experiments testing methods real world datasets. models tested include probabilistic principal components analysis mixture ppca model deﬁned help hidden variable αsps θs}m ppca model. models latent variable models unidentiﬁable parameters direct linear averaging method applicable. still possible matched linear averaging matches mixture components different local models minimizing symmetric divergence; idea used linear control variates method make applicable gmm. hand parameters ppca-based models unidentiﬁable arbitrary orthonormal transforms linear averaging linear control variates longer applied easily. expectation maximization learn parameters three models. numerical veriﬁcation convergence rates different start verifying convergence rates estimators estimating true parameters. also non-identiﬁability problem calculating symmetric divergence match mixture components evaluate avoid non-identiﬁability w.r.t. orthonormal transforms. verify convergence rates w.r.t. total dataset large negligible. figure shows results vary kl-naive kl-control ˆθkl−c kl-weighted ˆθkl−w consistent results figure increase number local machines using large ˆθkl ˆθkl−w scales expected. note since total observation data size ﬁxed number data local machine decreases increase interesting performance kl-based methods actually increases partitions; course cost increasing total bootstrap sample size increases. figure considers different setting increase ﬁxing total observation data size total bootstrap sample size ntot according mses ˆθkl ˆθkl−w respectively large consistent results figure interesting note ˆθkl independent ˆθkl−w increases linearly conﬂict fact ˆθkl−w better ˆθkl since always ntot. figure shows result vary ˆθkl−w quickly converges global increases kl-naive estimator ˆθkl converges signiﬁcantly slower. figure demonstrates case increase kl-weighted estimator ˆθkl−w matches closely except large case term starts dominate kl-naive much worse. also linear averaging estimator performs poorly scale theoretical rate claims; unidentiﬁable orthonormal transform ppca model test figure results different models simulated data change bootstrap sample size ﬁxed dimensions ppca models numbers mixture components linear averaging kl-control applicable ppca-based models shown apply methods challenging setting distributed learning number mixture components unknown. case ﬁrst learn local model decide number components using selection. linear averaging kl-control θkl−c applicable setting test kl-naive ˆθkl kl-weighted θkl−w since also computable different dimensions evaluate ˆθkl ˆθkl−w using log-likelihood hold-out testing dataset shown figure ˆθkl−w generally outperforms ˆθkl expect relative improvement increases figure number mixture components estimated bic. true number mixtures cases. vary total data size dimension respectively. varies dimension data ﬁxed y-axis testing likelihood compared global mle. finally apply methods several real word datasets including sensit vehicle dataset mixture ppca tested covertype epsilon datasets tested. figure kl-weight kl-control perform best. linear averaging performs poorly applicable mixture ppca. figure testing likelihood real world datasets. learning mixture ppca sensit vehicle. learning covertype epsilon. number local machines cases number mixture components taken number labels datasets. dimension latent variables epsilon ﬁrst applied principal components chosen. linear-matched kl-control applicable mixture ppca shown propose variance reduction techniques distributed learning complex probabilistic models including kl-weighted estimator statistically efﬁcient widely applicable even challenging practical scenarios. theoretical empirical analysis provided demonstrate methods. future directions include extending methods discriminant learning tasks well challenging deep generative networks exact computable tractable surrogate likelihood methods stochastic gradient descent need. note kl-averaging problem also appears knowledge distillation\" problem bayesian deep neural networks seems technique applied straightforwardly. acknowledgement work supported part crii references boyd parikh peleato eckstein. distributed optimization statistical learning alternating direction method multipliers. foundations trends machine learning merugu ghosh. privacy-preserving distributed clustering using generative models. data mining icdm third ieee international conference pages ieee", "year": 2016}