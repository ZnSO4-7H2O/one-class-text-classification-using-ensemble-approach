{"title": "Parameter Reference Loss for Unsupervised Domain Adaptation", "tag": ["cs.CV", "cs.AI"], "abstract": "The success of deep learning in computer vision is mainly attributed to an abundance of data. However, collecting large-scale data is not always possible, especially for the supervised labels. Unsupervised domain adaptation (UDA) aims to utilize labeled data from a source domain to learn a model that generalizes to a target domain of unlabeled data. A large amount of existing work uses Siamese network-based models, where two streams of neural networks process the source and the target domain data respectively. Nevertheless, most of these approaches focus on minimizing the domain discrepancy, overlooking the importance of preserving the discriminative ability for target domain features. Another important problem in UDA research is how to evaluate the methods properly. Common evaluation procedures require target domain labels for hyper-parameter tuning and model selection, contradicting the definition of the UDA task. Hence we propose a more reasonable evaluation principle that avoids this contradiction by simply adopting the latest snapshot of a model for evaluation. This adds an extra requirement for UDA methods besides the main performance criteria: the stability during training. We design a novel method that connects the target domain stream to the source domain stream with a Parameter Reference Loss (PRL) to solve these problems simultaneously. Experiments on various datasets show that the proposed PRL not only improves the performance on the target domain, but also stabilizes the training procedure. As a result, PRL based models do not need the contradictory model selection, and thus are more suitable for practical applications.", "text": "figure general architecture siamese network based unsupervised domain adaptation models. classiﬁcation loss applied source domain encoder since labels available target domain. domain loss loss function able minimize discrepancy domains including common discrepancy loss recent popular adversarial loss. question mark indicates missing component existing work lacking constraints preserve discriminative ability target domain features. nevertheless collecting large data sets many speciﬁc real world applications still difﬁcult expensive labor-intensive workload. particularly labeled data difﬁcult collect data like ground-truth categories image classiﬁcation. domain adaptation tries take advantage available labeled data source domain learn model target domain labels available. domain adaptation needed assumption identically independently distributed data usually satisﬁed real world applications i.e. data target/deploying phase drawn distribution different source training data. case dataset bias usually caused data collection procedure sometimes might want intentionally train model different domain help improve generalization target domain e.g. training model synthetic data improve performance real world data success deep learning computer vision mainly attributed abundance data. however collecting large-scale data always possible especially supervised labels. unsupervised domain adaptation aims utilize labeled data source domain learn model generalizes target domain unlabeled data. large amount existing work uses siamese network-based models streams neural networks process source target domain data respectively. nevertheless approaches focus minimizing domain discrepancy overlooking importance preserving discriminative ability target domain features. another important problem research evaluate methods properly. common evaluation procedures require target domain labels hyper-parameter tuning model selection contradicting deﬁnition task. hence propose reasonable evaluation principle avoids contradiction simply adopting latest snapshot model evaluation. adds extra requirement methods besides main performance criteria stability training. design novel method connects target domain stream source domain stream parameter reference loss solve problems simultaneously. experiments various datasets show proposed improves performance target domain also stabilizes training procedure. result based models need contradictory model selection thus suitable practical applications. availability large-scale data known critical success factors deep learning shown increasing amount training data almost always improves performance deep model. ∗part work done jiren intern preferred referred unsupervised domain adaptation already much progress deep models. recent trends involve combining traditional algorithms deep features well designing novel architectures deep domain adaptation siamese networks commonly used basic architecture methods encoders used source target domain data respectively shown figure existing methods focus minimizing domain discrepancy overlook importance preserving discriminative ability target domain features. note differences constraints source domain encoder target domain encoder figure constraints source domain encoder label classiﬁcation loss domain loss; constraint target domain encoder domain loss. missing classiﬁcation loss constraint cause learned target domain features lack discriminative ability since features optimized match source domain distribution. insufﬁciency comes lack labels target domain prevents direct connection target domain features label classiﬁer. natural solution would additional constraints target domain encoder encourage preserve important information discriminating different classes. unsupervised nature target domain reconstruction loss ﬁrst comes mind. however unlike common unsupervised learning recent style transfer tasks preserving pixel-to-pixel information contradicts objective learning domain invariant features. hand directly aligning features domains also applicable since domain inputs paired i.e. explicit matching features domains exists. learn meaningful representation target domain data propose novel parameter reference loss build ﬂexible connection source domain encoder target domain encoder. furthermore show improve training stability solves another important problem contradiction using target domain labels model selection uda. detailed discussion problem matters helps found section another motivation think current learned parameters wastes resources parameters often used initializing another model domain/task. hence make model able beneﬁt previous learned parameters even adaptation training phase. fact efﬁciently using resources plays important role summary contributions work include point problem poor discriminative ability caused lack constraint target domain encoder existing work. clarify contradiction evaluation procedures methods propose direction solve problem stabilize training. propose solution solve problems simultaneously using easily combined existing methods based siamese networks. show previously learned parameters useful training phase simply using model initialization. main objective domain adaptation methods learn representation invariant domain change categorize existing methods groups according loss function used minimizing domain discrepancy. ﬁrst group methods uses discrepancy loss like maximum mean discrepancy learn domain invariant features. tzeng proposed deep domain confusion ﬁrst domain adaptation methods based deep neural networks. apply alexnet model source target domain inputs explicitly minimizing discrepancy loss extracted features using mmd. deep adaptation networks extends work using multi-kernel three different layers arguing minimizing discrepancy last layer sufﬁcient remove domain difference caused early layers. besides loss deep coral minimizes domain discrepancy aligning correlations activation layers deep model. zellinger propose central moment discrepancy explicit orderwise matching higher order moments avoid computationally expensive distance kernel matrix computations. csurka done comparative study discrepancy based models using various deep features. figure typical workﬂow unsupervised domain adaptation image classiﬁcation task. first model trained source domain images labels using supervised learning approach common crossvalidation techniques hyper-parameter tuning model selection. second step adapt trained model target domain images using domain adaptation methods. note though target labels available deﬁnition task common practice target domain labels used evaluation model selection. however argue comparison different methods based model selection using target domain labels accurately reﬂect performance evaluated methods. adversarial loss recently popularized generative adversarial networks bousmalis propose gans generate target domain data conditioned source domain inputs. russo adopt cyclegan domain adaptation tackle problem caused unpaired inputs domains. adversarial loss also combined discriminative models. reversegrad applies adversarial loss features extracted discriminative model. implementation reversegrad uses gradient reverse layer compute gradients encoder indeed different compute adversarial loss generator part. tzeng summarize methods using siamese networks adversarial loss. categorize methods according three design options whether parameters shared siamese architecture whether models discriminative generative choice adversarial loss. adding discrepancy loss third option leads summarization general architecture tasks. methods using single encoder domains parameter sharing mechanism helps preserve discriminative ability learned source domain data however also limits ﬂexibility target domain model. methods using independent encoders preserving discriminative ability target domain features often overlooked. speciﬁc model designed adda target domain encoder initialized using parameters source domain encoder. alleviates problem caused single domain related conﬁrst give formal deﬁnition unsupervised domain adaptation image classiﬁcation task facilitate explanation proposed method. deﬁnition borrowed domain consists components feature space m-dimensionality marginal distribution speciﬁcally feature spaces task thus differences domains caused different marginal probability distributions. given speciﬁc domain task consists components label space k-cardinality objective predictive function probabilistic viewpoint also written consider different domains source domain target domain domain adaptation label space generally assumed domains i.e. image classiﬁcation task means possible classes domain same hence denote label space domains. source domain dataset denoted image instance corresponding class label image. target domain dataset denoted similar difference labels objective unsupervised domain adaptation learn model predict labels target domain data utilizing source domain data labels target domain data. particular image ﬁrst describe baseline model realize typical unsupervised domain adaptation method using deep neural networks. explain proposed parameter reference loss variants detail. baseline model baseline method used similar architecture adda neural network model ﬁrst trained source domain classiﬁcation using cross-entropy loss parameters model used initializing target domain encoder having architecture. adaptation process source domain encoder classiﬁer remain ﬁxed target domain encoder trained produce features similar source domain features using adversarial loss proposed reason different discrepancy loss instead directly using exactly model adda adversarial loss related more reasonable evaluation setting. compared adversarial loss discrepancy loss less sensitive used metric unsupervised hyper-parameter tuning hyperparameters same. reason adversarial loss sufﬁcient measure current domain discrepancy loss terms generator discriminator respectively inﬂuence other. moreover training adversarial loss issues instability complex mini-max optimization currently still needs much effort tune model work well usually loss terms source domain model label classiﬁcation loss discrepancy loss however since source domain encoder baseline model ﬁxed adaptation phase actually optimized classiﬁcation loss pretraining source domain data. models described section loss terms source domain encoder adaptation. classiﬁcation loss deﬁned cross-entropy loss figure illustrates typical workﬂow unsupervised domain adaptation image classiﬁcation task. ﬁrst step usually involves training model source domain dataset only. second step involves adapting trained model target domain. methods combine steps simultaneously learn classiﬁcation adaptation. note difﬁculties unsupervised domain adaptation tasks include unavailability directly training model supervised information target domain also contradiction hyper-parameter tuning model selection procedure. deﬁnition unsupervised domain adaptation impossible target domain labels validation purposes selecting hyper-parameters. simplest solution avoid contradiction hyperparameter tuning model selection. however methods generally sensitive hyper-parameter changes compared supervised learning approaches. result besides using complicated reverse cross validation feasible option obtain reliable performance would labeled supervision target domain hyper-parameter optimization know. nevertheless using target domain labels hyperparameter tuning biases reported accuracy accurately reﬂect performance real world tasks stability models might important possible performance gain. completely avoiding hyper-parameter tuning model selection difﬁcult consider another direction make hyper-parameter tuning model selection procedure easier. show avoid contradiction tune hyper-parameters without looking target domain labels stabilize adaptation training procedure large performance drop expected. hence modify existing evaluation procedure avoid contradictions using target domain labels adaptation training phase select hyperparameters without access target domain labels. given ﬁxed number training epochs always select latest epoch/snapshot trained model ﬁnal evaluation deployment. figure shows diagram proposed method. extra loss term parameter reference loss target domain model regularizer. deﬁned loss parameters source domain encoder denoted target domain model denoted call reference loss treat parameters source domain reference utilized training instead used initialization. intuition designing loss term three fold want build connection label classiﬁer target domain encoder direct connection components available; want selectively transfer knowledge learned source domain parameters instead reusing learned parameters like weight sharing. result constraint reference loss training expected stable independent source target domain encoders. reason choose loss instead loss property loss makes connection domain models sparse seen selection keeping parameters. connections allow discriminative ability learned label classiﬁer transfer target domain features. hand loss also allows relatively large variations parameters. sense still enough ﬂexibility model learn domain invariant features. naive setting loss term target domain encoder. since reference parameters ﬁxed adaptation training continues loss decreasing relative weight parameter reference loss increased. result later phase adaptation training plays leading role inﬂuence loss becomes smaller. figure diagram adaptation phase proposed method. parameter reference loss computed parameters source domain encoder target domain encoder. dotted line means forward pass needed. naive version learning disabled source domain encoder complete adaptation phase variants source domain encoder also trained using classiﬁcation loss discrepancy loss parameter reference loss. simple fair comparison classiﬁer model always ﬁxed adaptation. hand property makes training quite stable; hand also tends prevent loss decreasing later training phase. solve problem propose several variants remaining part section. simultaneous variant enables learning source domain encoder well adaptation phase. hence parameter changes ﬂexible naive prl. objective function source domain encoder composed classiﬁcation loss loss warm-up variant disables learning source domain encoder beginning adaptation. loss decreased small value stops descreasing learning source domain encoder enabled again adaptation phase. modiﬁcation simultaneous intended prevent unstable training behavior early phase adaptation. in-turn variant repeatedly disables learning source domain encoder epochs enables learning another epochs. intuition strategy disabling learning updates source encoder means target domain encoder reference constraint avoid unstable behavior unintentional dramatic change enabling learning updates source encoder means source domain model also current target domain model reference allowing progressive gradual change parameters domains. sufﬁciently evaluate proposed method state-of-the-art methods adopt widely used dataset ofﬁce evaluate method challenging domains also method relatively dataset landmarkda details datasets described below. ofﬁce- classic domain adaptation dataset three different domains amazon dslr webcam classes domain. among three domains dslr webcam similar data distribution thus adaptation domains easier combinations. evaluate overall accuracy available conﬁgurations landmarkda dataset visual domain adaptation. also includes three different domains photos paintings drawings classes total. differences domains much larger datasets hence useful evaluate adaptation method domains diversity. datasets construct domain adaptation task source domain target domain every possible combination. performance model evaluated overall classiﬁcation accuracy target domain. ofﬁce- landmarkda datasets share experimental setting including base model architecture. many design options unsupervised domain adaptation model. hand provides probability improve model; hand makes comparison different methods complicated. make fair comparison different methods insist using design options baselines proposed methods except feature methods. following settings used methods compared experiments. base architecture study csurka clearly shows different deep neural networks large performance variations even using methods. since focus work improve architecture dnns select basic widely used alexnet base model experiments. similar many existing works also adopt alexnet model pre-trained imagenet accelerate initial supervised learning phase source domain. aware freezing certain layers ﬁne-tuning pre-trained model tasks/domains might help improve performance datasets however many options selecting layers freeze choose avoid extra variance simply ﬁne-tuning parameters base model. fact idea freezing layers contradict since still easily apply layers frozen. implementation details pre-trained alexnet model domain adaptation datasets replace ﬁnal fully-connected layer original model randomly initialized layer suitable number classes datasets e.g. layer dimensional outputs ofﬁce- dataset. ﬁrst baseline model simply ﬁne-tuning pre-trained model source domain name encoder part model msource. baselines variants based model classiﬁer part model ﬁxed adaptation procedures. baselines msingle uses single encoder domains mdouble uses independent encoders source target domains respectively. encoders initialized parameters msource. baselines actually uniﬁed versions existing methods like adda using settings simple comparison. hyper-parameters gaussian kernel width loss methods domains. value obtained grid search without accessing target domain labels. note need value loss continuously decreases used ﬁxed optimizer learning rate weight decay particular values chosen based performance source domain labels available. used mini-batch size training msource size adaptation. variants main difference variants baseline models parameter reference loss term. hyper-parameter related loss namely reference weight. however hyper-parameter also easily selected observing change loss prl. using larger value beginning usually harm performance since prevents parameters change signiﬁcantly original model. observe loss decreasing choose smaller reference weight allow discrepancy loss minimized. experiments reference weight ofﬁce dataset landmarkda dataset respectively. table shows evaluation results ofﬁce- dataset using proposed evaluation procedure i.e. withusing target domain labels hyper-parameter tuning model selection. baseline models msingle almost performs better msource adaptation directions mdouble performs worse msource adaptation conﬁgurations. results indicate much ﬂexibility double stream encoders even harm performance. versus results show loss performs much better loss. though evidence needed results suggest selectively ﬁxing parameters allowing parameters vary relatively largely might better allowing many arbitrary changes parameters. variants simultaneous work well general large ﬂexibility early training phase though still generally better msingle. domains already highly similar adaptation performance simultaneous good small differences enough prove superiority methods. warmin-turn perform better candidates in-turn stable slightly better performance ofﬁce dataset. figure demonstrates superior stability adaptation training phase. comparing variants msingle observed source domain relatively large scale data performance signiﬁcantly better msingle baselines evidence utilize knowledge learned source domain effectively. discriminative ability shown figure msingle in-turn similar small loss large margin performance between models adaptation tasks. results support claim existing methods lack adequate preserve discriminative ability target domain features. contrast able generate better discriminative features making learned parameters efﬁciently. results also show performance msingle depends similarity source domain target domain however fully sharing parameters good idea knowledge learned source domain applicable target domain. advantage using loss achieve selective knowledge transfer results superior performance domains diversity. inﬂuence evaluation procedures seen results obtained using proposed evaluation procedure interesting results compare obtained traditional evaluation procedures. table lists evaluation results obtained using target domain labels model selection. baseline methods signiﬁcantly inﬂuenced evaluation table evaluation landmarkda using different evaluation principles. results reported model except msource. values left obtained using latest epoch model. values brackets obtained using target domain labels i.e. best result training. photo painting drawing. results landmarkda variants slightly better performance stability compared msingle signiﬁcantly better mdouble landmarkda dataset shown table results generally consistent ofﬁce dataset. besides even averaged accuracy latest epoch achieves superior performance state-of-the-art methods reported also uses alexnet extract deep features minimize domain discrepancy. conclusion observe existing siamese network-based domain adaptation methods limited ability produce target domain features able retain discriminative ability. propose novel parameter reference loss encourages parameters target domain encoder partially remain close source domain encoder direct connection classiﬁer. allows ﬂexible selective parameters learned source domain domain adaptation phase compared full parameter sharing. experiments show even naive approach using pre-trained encoder parameters ﬁxed reference domain adaptation improve adaptation performance well training stability. results agreement hypothesis using learned encoder parameters initializing ﬁne-tuning target encoder domain/task inefﬁcient resources. addition argue existing evaluation procedures contradictory requirements models expected meet real-world usage. therefore propose simple reasonable evaluation procedure hyper-parameter tuning model selection without access target domain labels. argue requirement make training stable actually important real applications. experimental results indicate even strict situation proposed method still manages achieve stable superior performance existing baselines.", "year": 2017}