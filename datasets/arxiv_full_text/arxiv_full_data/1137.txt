{"title": "Junction Tree Variational Autoencoder for Molecular Graph Generation", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "We seek to automate the design of molecules based on specific chemical properties. In computational terms, this task involves continuous embedding and generation of molecular graphs. Our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear SMILES strings instead of graphs. Our junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network. This approach allows us to incrementally expand molecules while maintaining chemical validity at every step. We evaluate our model on multiple tasks ranging from molecular generation to optimization. Across these tasks, our model outperforms previous state-of-the-art baselines by a significant margin.", "text": "direct generation graphs. speciﬁcally models start generating smiles linear string notation used chemistry describe molecular structures. smiles strings translated graphs deterministic mappings however design critical limitations. first smiles representation designed capture molecular similarity. instance molecules similar chemical structures encoded markedly different smiles strings prevents generative models like variational autoencoders learning smooth molecular embeddings. second essential chemical properties molecule validity easier express graphs rather linear smiles representations. hypothesize operating directly graphs improves generative modeling valid chemical structures. primary contribution generative model molecular graphs. could imagine solving problem standard manner generating graphs node node approach ideal molecules. creating molecules atom atom would force model generate chemically invalid intermediaries delaying validation complete graph generated. instead propose generate molecular graphs phases exploiting valid subgraphs components. overall generative approach cast junction tree variational autoencoder ﬁrst generates tree structured object whose role represent scaffold subgraph components coarse relative arrangements. components valid chemical substructures automatically extracted training using tree decomposition used building blocks. second phase subgraphs assembled together coherent molecular graph. seek automate design molecules based speciﬁc chemical properties. computational terms task involves continuous embedding generation molecular graphs. primary contribution direct realization molecular graphs task previously approached generating linear smiles strings instead graphs. junction tree variational autoencoder generates molecular graphs phases ﬁrst generating tree-structured scaffold chemical substructures combining molecule graph message passing network. approach allows incrementally expand molecules maintaining chemical validity every step. evaluate model multiple tasks ranging molecular generation optimization. across tasks model outperforms previous state-of-the-art baselines signiﬁcant margin. challenge drug discovery target molecules desired chemical properties. currently task takes years development exploration expert chemists pharmacologists. ultimate goal automate process. computational perspective decompose challenge complementary subtasks learning represent molecules continuous manner facilitates prediction optimization properties learning optimized continuous representation back molecular graph improved properties deep learning extensively investigated molecular graph encoding harder combinatorial task molecular graph generation latent representation remains under-explored. figure comparison graph generation schemes structure structure approach preferred avoids invalid intermediate states encountered node node approach. evaluate model multiple tasks ranging molecular generation optimization given molecule according desired properties. baselines utilize state-of-the-art smiles-based generation approaches demonstrate model produces valid molecules sampled prior distribution outperforming performing baseline signiﬁcant margin. addition show model excels discovering molecules desired properties yielding relative gain baselines. approach extends variational autoencoder molecular graphs introducing suitable encoder matching decoder. deviating previous work interpret molecule built subgraphs chosen vocabulary valid components. components used building blocks encoding molecule vector representation well decoding latent vectors back valid molecular graphs. advantage view decoder realize valid molecule piece piece utilizing collection valid components interact rather trying build molecule atom atom chemically invalid intermediaries aromatic bond example chemically invalid unless entire aromatic ring present. would therefore challenging learn build rings atom atom rather introducing rings part basic vocabulary. vocabulary components rings bonds individual atoms chosen large enough given molecule covered overlapping components clusters atoms. clusters serve role analogous cliques graphical models expressive enough molecule covered overlapping clusters withforming cluster cycles. sense clusters serve cliques triangulation molecular graph. form junction tree clusters tree representation molecule. since choice cliques constrained priori cannot guarantee junction tree exists clusters arbitrary molecule. however clusters built basis molecules training ensure corresponding figure overview molecule generation paradigm molecular graph ﬁrst decomposed junction tree colored node tree represents substructure molecule. encode tree graph latent embeddings decode molecule ﬁrst reconstruct junction tree assemble nodes tree back original molecule guided original molecular graph associated junction tree offer complementary representations molecule. therefore encode molecule two-part latent representation encodes tree structure clusters tree without fully capturing exactly clusters mutually connected. encodes graph capture ﬁne-grained connectivity. parts created tree graph encoders latent representation decoded back molecular graph stages. illustrated figure ﬁrst reproduce junction tree using tree decoder based information second predict grain connectivity clusters junction tree using graph decoder realize full molecular graph. junction tree approach allows maintain chemical feasibility generation. notation molecular graph deﬁned atoms bonds neighbor denote sigmoid function relu function nodes tree nodes graph. tree decomposition maps graph junction tree contracting certain vertices single node becomes cycle-free. formally given graph junction tree connected labeled tree whose node {c··· edge node cluster induced subgraph satisfying following constraints viewing induced subgraphs cluster labels junction trees labeled trees label vocabulary molecule tree decomposition contains cycles single edges. thus vocabulary size limited tree decomposition molecules present tree decomposition algorithm tailored molecules ﬁnds root chemistry cluster vocabulary includes chemical structures bonds rings given graph ﬁrst simple cycles edges belonging cycles. simple rings merged together overlapping atoms constitute speciﬁc structure called bridged compounds cycles edges considered cluster. next cluster graph constructed adding edges intersecting clusters. finally select spanning trees junction tree result ring merging clusters junction tree atoms common facilitating efﬁcient inference graph decoding phase. detailed procedure described supplementary. ﬁrst encode latent representation graph message passing network vertex feature vector indicating atom type valence properties. similarly edge feature vector indicating bond type hidden vectors denoting message vice versa. loopy structure graph messages exchanged loopy belief propagation fashion tree encoder similarly encode tree message passing network. cluster represented one-hot encoding representing label type. edge associated message vectors mji. pick arbitrary leaf node root propagate messages phases. ﬁrst bottom-up phase messages initiated leaf nodes propagated iteratively towards root. top-down phase messages propagated root leaf nodes. message updated message passing follows schedule computed precursors {mki computed. architectural design motivated belief propagation algorithm trees thus different graph encoder. ﬁnal tree representation hroot encodes rooted tree unlike graph encoder apply node average pooling confuses tree decoder node generate ﬁrst. sampled similar graph encoder. simplicity abbreviate tree encoder plays roles framework. first used compute requires bottom-up figure illustration tree decoding process. nodes labeled order generated. node expands child node predicts label message node leaf node decoder backtracks computes message decoder continues backtrack node children. node expands node predicts label. tree decoder decode junction tree encoding tree structured decoder. tree constructed top-down fashion generating node time. illustrated figure tree decoder traverses entire tree root generates nodes depth-ﬁrst order. every visited node decoder ﬁrst makes topological prediction whether node children generated. child node created predict label recurse process. recall cluster labels represent subgraphs molecule. decoder backtracks node children generate. time step node receives information nodes current tree making predictions. information propagated message vectors trees incrementally constructed. formally {··· edges traversed depth ﬁrst traversal edge traversed directions. model visnode time ﬁrst edges message hitjt updated previous messages recurrent unit tree encoder. topological prediction model visits node makes binary prediction whether still children generated. compute probability combining distribution label vocabulary root node parent virtual node learning tree decoder aims maximize likelihood ground truth topological label values decoder minimizes following cross entropy loss similar sequence generation training perform teacher forcing topological label prediction step replace ground truth model makes predictions given correct histories. decoding feasibility check algorithm shows tree sampled tree constructed recursively guided topological predictions without external guidance used training. ensure sampled tree could realized valid molecule deﬁne cluster labels chemically compatible node current neighbors. child node generated node sample label renormalized distribution masking invalid labels. bors score candidate subgraph ﬁrst deriving vector representation using subgraph score. specify atoms candidate subgraph indices used mark position atoms junction tree along edge obtained running tree encoding algorithm. neural messages pertaining atoms bonds subgraph obtained aggregated similarly encoding step different parameters tree dependent positional context bond learning graph decoder parameters learned maximize log-likelihood predicting correct subgraphs ground true graph tree node possible candidate subgraphs tree node training apply teacher forcing i.e. feed graph decoder ground truth trees input. complexity tree decomposition clusters share atoms need merge atoms bond. pruning chemically invalid subgraphs merging isomorphic graphs |gi| average tested standard zinc drug dataset. computational complexity jt-vae therefore linear number clusters scaling nicely large graphs. evaluation efforts measure various aspects molecular generation. ﬁrst evaluations follow previously proposed tasks also introduce third task constrained molecule optimization. combinations cluster neighbors. crossed arrows indicate combinations lead chemically infeasible molecules. note discard tree structure enumeration last candidates collapse molecule. rank subgraphs node. ﬁnal graph decoded putting together predicted subgraphs note step deterministic since potentially many molecules correspond junction tree. underlying degree freedom pertains neighboring clusters attached subgraphs. goal assemble subgraphs together correct molecular graph. graphs whose junction tree scoring function candidate graphs. consider scoring functions decompose across clusters neighbors. words term scoring function depends cluster assembly task could cast graphical model inference task model induced junction tree. however efﬁciency reasons assemble molecular graph neighborhood time following order tree decoded. words start sampling figure left random molecules sampled prior distribution right visualization local neighborhood molecule center. three molecules highlighted dashed tree structure center molecule different graph structure clusters combined differently. phenomenon emerges another group molecules molecule reconstruction validity test models task reconstructing input molecules latent representations decoding valid molecules sampling prior distribution. bayesian optimization moving beyond generating valid molecules test model produce novel molecules desired properties. perform bayesian optimization latent space search molecules speciﬁed properties. constrained molecule optimization task modify given molecules improve speciﬁed properties constraining degree deviation original molecule. realistic scenario drug discovery development drugs usually starts known molecules existing drugs since task cannot compare existing baselines. describe data baselines model conﬁguration shared across tasks. additional setup details provided task-speciﬁc sections. data zinc molecule dataset kusner experiments. contains drug molecules extracted zinc database follow training/testing split previous work. baselines compare approach smiles-based baselines character generates smiles strings character character; grammar generates smiles following syntactic constraints given context-free grammar; syntax-directed incorporates syntactic semantic constraints smiles attribute grammar. molecule generation task also compare graphvae directly generates atom labels adjacency matrices graphs. model conﬁguration comparable baselines latent space dimension i.e. tree graph representation dimensions each. full training details model conﬁgurations provided appendix. setup ﬁrst task reconstruct sample molecules latent space. since encoding decoding process stochastic estimate reconstruction accuracy monte carlo method used molecule encoded times encoding decoded times. report portion decoded molecules identical input molecule. fair comparison deﬁne molecules identical smiles string. testing time convert generated graphs smiles using rdkit. results table shows jt-vae outperforms previous models molecule reconstruction always produces valid molecules sampled prior distribution. sampled molecules non-trivial structures simple chains sampled molecules prior found distinct training set. thus model simple memorization. analysis qualitatively examine latent space jtvae visualizing neighborhood molecules. given molecule follow method kusner construct grid visualization neighborhood. comparison select molecule visualized figure shows local neighborhood molecule. compared ﬁgure neighborhood contain molecules huge rings rarely occur dataset. also highlight groups closely resembling molecules identical tree structures vary clusters attached together. demonstrates smoothness learned molecular embeddings. setup second task produce novel molecules desired properties. following target chemical property octanol-water partition coefﬁcients penalized synthetic accessibility score number long cycles. perform bayesian optimization ﬁrst train associate molecule latent vector given mean variational encoding distribution. learned train sparse gaussian process predict given latent representation. perform iterations batched using expected improvement heuristic. comparison report predictive performance trained latent encodings learned different vaes measured log-likelihood root mean square error -fold cross validation. top- molecules found different models. results shown table jt-vae ﬁnds molecules signiﬁcantly better scores previous methods. figure lists top- best molecules found jt-vae. fact jt-vae ﬁnds molecules scores moreover yields better predictive performance trained jt-vae embeddings setup third task perform molecule optimization constrained scenario. given molecule task different molecule highest property value molecular similarity sim) threshold tanimoto similarity morgan ﬁngerprint similarity metric penalized logp coefﬁcient target chemical property. task jointly train property predictor jt-vae predict latent embedding optimize molecule start latent representation apply gradient ascent latent space improve predicted score similar applying gradient steps molecules decoded resulting latent trajectories report molecule highest satisﬁes similarity constraint. modiﬁcation succeeds decoded molecules satisﬁes constraint distinct original. provide greatest challenge selected molecules lowest property score test set. report success rate among success cases average improvement molecular similarity sim) original modiﬁed molecules table constrained optimization result jt-vae mean standard deviation property improvement molecular similarity success rate constraints sim) varied results results summarized table unconstrained scenario best average improvement often proposes dissimilar molecules. tighten constraint time model ﬁnds similar molecules average improvement also demonstrates smoothness learned latent space. figure illustrates effective modiﬁcation resulting similar molecule great improvement. related work molecule generation previous work molecule generation mostly operates smiles strings. g´omezbombarelli segler built generative models smiles strings recurrent decoders. unfortunately models could generate invalid smiles result molecules. remedy issue kusner complemented decoder syntactic semantic constraints smiles context free attribute grammars grammars fully capture chemical validity. techniques active learning reinforcement learning encourage model generate valid smiles additional training signal. recently simonovsky komodakis proposed generate molecular graphs predicting adjacency matrices generated molecules node node. comparison method enforces chemical validity efﬁcient coarse-to-ﬁne generation. graph-structured encoders neural network formulation graphs ﬁrst proposed gori scarselli later enhanced gated recurrent units. recurrent architectures graphs designed weisfeiler-lehman kernel network inspired graph kernels. considered different architecture graphs viewed latent variable graphical models derived model message passing algorithms. tree graph encoder closely related graphical model perspective neural message passing networks convolutional architectures duvenaud introduced convolution-like propagation molecular graphs generalized domains niepert bruna henaff developed graph convolution spectral domain graph laplacian. applications graph neural networks used semisupervised classiﬁcation computer vision chemical domains tree-structured models tree encoder related recursive neural networks tree-lstm models encode tree structures nodes tree bottom-up transformed vector representations. contrast model propagates information bottom-up top-down. decoding side tree generation naturally arises natural language parsing different approach natural language parsers access input words predict topology tree. general purpose tree generation vinyals aharoni goldberg applied recurrent networks generate linearized version trees architectures entirely sequence-based. dong lapata alvarez-melis jaakkola proposed tree-based architectures construct trees top-down root. model closely related alvarez-melis jaakkola disentangles topological prediction label prediction generate nodes depth-ﬁrst order additional steps propagate information bottom-up. forward-backward propagation also appears parisotto model node based whereas based message passing. paper present junction tree variational autoencoder generating molecular graphs. method signiﬁcantly outperforms previous work molecule generation optimization. future work attempt generalize method general low-treewidth graphs. besnard j´er´emy ruda gian filippo setola vincent abecassis keren rodriguiz ramona huang xiping norval suzanne sassano maria shin antony webster lauren automated design ligands polypharmacological proﬁles. nature chung junyoung gulcehre caglar kyunghyun bengio yoshua. empirical evaluation gated recurrent neural networks sequence modeling. arxiv preprint arxiv. hanjun tian yingtao skiena steven song syntax-directed variational autoencoder structured data. international conference learning representations https//openreview. net/forum?id=syqshmzrb. duvenaud david maclaurin dougal iparraguirre jorge bombarell rafael hirzel timothy aspuru-guzik al´an adams ryan convolutional networks graphs learning molecular ﬁngerprints. advances neural information processing systems jennifer duvenaud david hern´andez-lobato jos´e miguel s´anchezlengeling benjam´ın sheberla dennis aguileraiparraguirre jorge hirzel timothy adams ryan aspuru-guzik al´an. automatic chemical design using data-driven continuous representation molecules. central science ./acscentsci. gori marco monfardini gabriele scarselli franco. model learning graph domains. neural networks ijcnn’. proceedings. ieee international joint conference volume ieee guimaraes gabriel lima sanchez-lengeling benjamin farias pedro luis cunha aspuru-guzik al´an. objective-reinforced generative adversarial networks sequence generation models. arxiv preprint arxiv. wengong coley connor barzilay regina jaakkola tommi. predicting organic reaction outcomes weisfeiler-lehman network. advances neural information processing systems kearnes steven mccloskey kevin berndl marc pande vijay riley patrick. molecular graph convolutions moving beyond ﬁngerprints. journal computer-aided molecular design segler marwin kogej thierry tyrchan christian waller mark generating focussed molecule libraries drug discovery recurrent neural networks. arxiv preprint arxiv. socher richard perelygin alex jean chuang jason manning christopher andrew potts christopher. recursive deep models semantic compositionality sentiment treebank. proceedings conference empirical methods natural language processing sheng socher richard manning christopher improved semantic representations treestructured long short-term memory networks. arxiv preprint arxiv. vinyals oriol kaiser łukasz terry petrov slav sutskever ilya hinton geoffrey. grammar foreign language. advances neural information processing systems weininger david. smiles chemical language information system. introduction methodology encoding rules. journal chemical information computer sciences yujia vinyals oriol dyer chris pascanu razvan battaglia peter. learning deep generative models graphs. https//openreview.net/ forum?id=hyd-ebab. monti federico boscaini davide masci jonathan rodol`a emanuele svoboda bronstein michael geometric deep learning graphs manifolds using mixture model cnns. arxiv preprint arxiv. mueller jonas gifford david jaakkola tommi. sequence better sequence continuous revision cominternational conference binatorial structures. machine learning parisotto emilio mohamed abdel-rahman singh rishabh lihong zhou dengyong kohli pushmeet. neuro-symbolic program synthesis. arxiv preprint arxiv. scarselli franco gori marco tsoi chung hagenbuchner markus monfardini gabriele. graph ieee transactions neural neural network model. networks sch¨utt kristof kindermans pieter-jan felix huziel enoc sauceda chmiela stefan tkatchenko alexandre m¨uller klaus-robert. schnet continuous-ﬁlter algorithm presents tree decomposition molecules. contain non-ring bonds simple rings respectively. simple rings extracted rdkit’s getsymmsssr function. merge rings share three atoms form bridged compounds. note junction tree molecule unique cluster graph contains cycles. introduces additional uncertainty probabilistic modeling. reduce variation three intersecting bonds intersecting atom cluster remove cycle connecting cluster graph. finally construct junction tree maximum spanning tree cluster graph note assign large weight edges involving clusters ensure edges cycles selected junction tree. though usually presented two-dimensional graphs molecules three-dimensional objects i.e. molecules deﬁned atom types bond connections also spatial conﬁguration atoms stereoisomers molecules structure differ orientations atoms space. note stereochemical feasibility could simply encoded context free attribute grammars. empirically found efﬁcient predict stereochemical conﬁguration separately molecule generation. speciﬁcally jt-vae ﬁrst generates structure molecule following procedure described section generate stereoisomers using rdkit’s enumeratestereoisomers function identiﬁes atoms could chiral. isomer encode graph representation graph encoder compute cosine similarity reconstruct training details applying tree decomposition molecules zinc dataset collected vocabulary size hidden state dimension modules jt-vae latent bottleneck dimension graph encoder initial atom features include atom type degree formal charge chiral conﬁguration. bond feature concatenation bond type whether bond ring cis-trans conﬁguration. tree encoder represent cluster neural embedding vector similar word embedding words. tree graph decoder feature setting encoders. graph encoder decoder runs three iterations neural message passing. fair comparison smiles based method minimized feature engineering. pytorch implement neural components rdkit process molecules. experimental results sampled molecules note degenerate model could also achieve prior validity keep generating simple structures like chains. prove model converge trivial solutions randomly sample plot molecules prior distribution shown figure sampled molecules present rich variety structural complexity. demonstrates soundness prior validity improvement model. neighborhood visualization given molecule follow kusner construct grid visualization neighborhood. speciﬁcally encode molecule latent space generate random orthogonal unit vectors axis grid. moving combinations directions yields latent vectors decode corresponding molecules. figure visualize local neighborhood molecules presented figure visualizes molecule figure wider neighborhood ranges. bayesian optimization directly used open sourced implementation kusner bayesian optimization speciﬁcally train sparse gaussian process inducing points predict properties molecules. five iterations batch expected improvement heuristic used propose latent vectors. iteration latent vectors proposed molecules decoded added training next iteration. perform independent runs aggregate results. figure present molecules found among runs using jt-vae. following kusner al.’s implementation scores reported normalized zero mean unit variance mean variance computed training set. constrained optimization task property predictor trained jointly predict logp latent embedding feed-forward network hidden layer dimension followed apply gradient ascent tanh activation. optimize molecule start mean encoding property calculated. steps molecular similarity sim) calculated morgan ﬁngerprint radius tanimoto similarity. molecule report best modiﬁed molecule sim) threshold figure present three groups modiﬁcation examples group present three pairs leads best improvement well pair decreased property caused inaccurate property prediction. figure tighter similarity constraint forces model preserve original structure. figure molecule modiﬁcation results similarity constraint sim) group plot three pairs leads actual property improvement pair decreased property. tighter similarity constraint forces model preserve original structure.", "year": 2018}