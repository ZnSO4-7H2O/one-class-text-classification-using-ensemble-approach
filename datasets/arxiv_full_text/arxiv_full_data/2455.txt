{"title": "Generative Moment Matching Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer perceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.", "text": "yujia kevin swersky richard zemel department computer science university toronto toronto canada canadian institute advanced research toronto canada consider problem learning deep generative models data. formulate method generates independent sample single feedforward pass multilayer preceptron recently proposed generative adversarial networks training generative adversarial network however requires careful optimization difﬁcult minimax program. instead utilize technique statistical hypothesis testing known maximum mean discrepancy leads simple objective interpreted matching orders statistics dataset samples model trained backpropagation. boost performance approach combining generative network auto-encoder network using learn generate codes decoded produce samples. show combination techniques yields excellent generative models compared baseline approaches measured mnist toronto face database. visible successes area deep learning come application deep models supervised learning tasks. models convolutional neural networks long short term memory networks achieving impressive results number tasks object recognition speech recognition image caption generation machine translation more. despite successes main bottlenecks supervised approach difﬁculty obtaining enough data learn abstract features capture rich structure data. well recognized promising avenue unsupervised learning unlabelled data plentiful cheaper obtain. long-standing inherent problem unsupervised learning deﬁning good method evaluation. generative models offer ability evaluate generalization data space also qualitatively assessed. work propose generative model unsupervised learning call generative moment matching networks gmmns generative neural networks begin simple prior easy draw samples. propagated deterministically hidden layers network output sample model. thus gmmns easy quickly draw independent random samples opposed expensive mcmc procedures necessary models boltzmann machines structure gmmn analogous recently proposed generative adversarial networks however unlike gans whose training involves difﬁcult minimax optimization problem gmmns comparatively simple; trained minimize straightforward loss function using backpropagation. idea behind gmmns statistical hypothesis testing framework called maximum mean discrepancy training gmmn minimize discrepancy interpreted matching moments model distribution empirical data distribution. using kernel trick represented simple loss function core training objective gmmns. using minibatch stochastic gradient descent training kept efﬁcient even large datasets. kernel trick implicitly lifts sample vectors inﬁnite dimensional feature space. feature space corresponds universal reproducing kernel hilbert space shown asymptotically universal kernels like gaussian kernel deﬁned exp. also capable disentangling factors variation means latent variable become responsible modelling single complex transformation original space would otherwise involve many variables even restrict ﬁeld deep learning vast array approaches generative modelling. below outline methods. popular class generative models used deep learning undirected graphical models boltzmann machines restricted boltzmann machines deep boltzmann machines models normalized typically intractable partition function making training evaluation sampling extremely difﬁcult usually requiring expensive markov-chain monte carlo procedures. next class fully visible directed models fully visible sigmoid belief networks neural autoregressive distribution estimator admit efﬁcient log-likelihood calculation gradient-based learning efﬁcient sampling require ordering imposed observable variables unnatural domains images cannot take advantage parallel computing methods sequential nature. second contribution show gmmns used bootstrap auto-encoder networks order further improve generative process. idea behind approach train auto-encoder network apply gmmn code space auto-encoder. allows leverage rich representations learned auto-encoder models basis comparing data model distributions. generate samples original data space simply sample code gmmn decoder auto-encoder network. experiments show relatively simple ﬂexible framework effective producing good generative models efﬁcient manner. mnist toronto face dataset demonstrate improved results comparable baselines including gans. source code training gmmns made available https//github.com/yujiali/gmmn. maximum mean discrepancy suppose given sets samples {xi}n {yj}m asked whether generating distributions maximum mean discrepancy frequentist estimator answering question also known sample test idea simple compare statistics datasets similar samples likely come distribution. neural network mapping function contain multiple layers nonlinearities represents parameters neural network. example architecture illustrated figure intermediate relu nonlinear layers logistic sigmoid output layer. goodfellow proposed train network using extra discriminative network tries distinguish model samples data samples. generative network trained counteract order make samples indistinguishable discriminative network. gradient objective backpropagated generative network. however minimax nature formulation easy stuck local optima. training generative network discriminative network must interleaved carefully scheduled. contrast learning algorithm simply involves minimizing objective. also related work class deep variational networks also deep directed generative models however make additional neural network designed approximate posterior latent variables. training carried variational lower bound log-likelihood model distribution. models trained using stochastic gradient descent however either require latent representation continuous require many secondary networks sufﬁciently reduce variance gradient estimates order produce sufﬁciently good learning signal finally early work proposed idea using feed-forward neural networks learn generative models. mackay proposed model closely related ours also used feed-forward network prior samples data space. however instead directly outputing samples extra distribution associated output. sampling used extensively learning inference model. magdon-ismail atiya proposed neural network learn transformation data space another space transformed data points uniformly distributed. transformation network learns cumulative density function. high-level idea gmmn neural network learn deterministic mapping samples simple easy sample distribution samples data distribution. architecture generative network exactly generative adversarial network however propose train network simply minimizing criterion avoiding hard minimax objective function used generative adversarial network training. found adding dropout encoding layers beneﬁcial terms creating smooth manifold code space. analogous motivation behind contractive denoising auto-encoders outline design choices found improve peformance gmmns. bandwidth parameter. bandwidth parameter kernel plays crucial role determining statistical efﬁciency optimally setting open problem. good heuristic perform line search obtain bandwidth produces maximal distance advanced heuristics also available simpler approximation experiments mixture kernels spanning multiple ranges. choose kernel gaussian kernel bandwidth parameter found choosing simple values etc. using mixture sufﬁcient obtain good results. weighting different kernels tuned achieve better results kept equally weighted simplicity. square root loss. practice found better results obtained optimizing lmmd loss important driving difference between distributions close possible. compared lmmd ﬂattens value gets close lmmd behaves much better small lmmd values. alternatively understood writing gradient lmmd respect ∂lmmd real-world data complicated high-dimensional reason generative modelling difﬁcult task. auto-encoders hand designed solve arguably simpler task reconstruction. trained properly auto-encoder models good representing data code space captures enough statistical information data reliably reconstructed. code space auto-encoder several advantages creating generative model. ﬁrst dimensionality explicitly controlled. visual data example represented high dimension often exists low-dimensional manifold. beneﬁcial statistical estimator like amount data required produce reliable estimator grows dimensionality data second advantage dimension code space representing complex variations original data space. concept referred literature disentangling factors variation reasons propose bootstrap auto-encoder models gmmn create refer gmmn+ae model. operate ﬁrst learning auto-encoder producing code representations data freezing auto-encoder weights learning gmmn minimize generated codes data codes. visualization model given figure drawn gmmn. within minibatch applied usual. using exact samples model data distribution minibatch still good estimator population mmd. found approach fast effective. minibatch training algorithm gmmn shown algorithm trained gmmns benchmark datasets mnist toronto face dataset mnist used standard test images split standard training images validation. remaining used training. used training test sets fold splits used split small training data used validation set. datasets rescaling images pixel intensities preprocessing step did. datasets trained gmmn network input data space code space auto-encoder. networks used section uniform distribution used prior h-dimensional stochastic hidden layer gmmn followed relu layers output layer logistic sigmoid units. autoencoder used mnist layers encoder decoder. auto-encoder layers total encoder decoder. auto-encoders encoder decoder mirrored architectures. layers auto-encoder network used sigmoid nonlinearities also guaranteed code space dimensions could match gmmn outputs. network architectures mnist shown figure auto-encoders trained separately gmmn. cross entropy used reconstruction loss. ﬁrst standard layer-wise pretraining ﬁne-tuned layers jointly. dropout used table log-likelihood test sets different models. baselines deep belief stacked contractive auto-encoder deep generative stochastic network adversarial nets encoder layers. training auto-encoder ﬁxed passed input data encoder corresponding codes. gmmn network trained code space match statistics generated codes statistics codes data examples. generating samples generated codes passed decoder samples input data space. experiments section gmmn networks trained minibatches size minibatch generated samples network. loss gradient computed points. used square root loss function lmmd throughout. evaluation model straight-forward explicit form probability density function easy compute log-likelihood data. however sampling model easy. therefore followed evaluation protocol used related models gaussian parzen window samples generated model. likelihood test data computed distribution. scale parameter gaussians selected using grid search ﬁxed range using validation set. hyperparameters networks including learning rate momentum auto-encoder gmmn training dropout rate auto-encoder number hidden units layer auto-encoder gmmn tuned using bayesian optimization optimize validation likelihood gaussian parzen window density estimation. figure independent samples nearest neighbors training gmmn+ae model trained mnist datasets. samples model bottom corresponding nearest neighbors training measured euclidean distance. samples generated gmmn models shown figure gmmn+ae produces visually appealing samples reﬂected parzen window log-likelihood estimates. likely explanation perturbations code space correspond smooth transformations along manifold data space. sense decoder able correct noise code space. determine whether models learned merely copy data follow example visualize nearest neighbour several samples terms euclidean pixel-wise distance figure metric appears though samples merely data examples. interesting aspects deep generative model gmmn possible directly explore data manifold. using gmmn+ae model randomly sampled points uniform space show corresponding data space projections figure points highlighted boxes. left right bottom linearly interpolate points uniform space show corresponding projections data space. manifold smooth part almost projections correspond realistic looking data. particular transformations involve complex attributes changing pose expression lighting gender facial hair. paper provide simple effective framework training deep generative models called generative moment matching networks. approach based optimizing maximum mean discrepancy samples generated model indistinguishable data examples terms moment statistics. standard kernel trick allows gmmn avoid explicitly computing moments resulting simple training objective minibatch stochastic gradient descent allows training scale large datasets. second contribution combines autoencoders learning generative model code layer. code samples model decoder order generate samples original space. auto-encoders makes generative model learning much simpler problem. combined pretrained auto-encoders readanother possibility utilize random features randomized feature expansions whose inner product converges kernel function increasing number features. idea recently explored advantage approach would cost would longer grow quadratically minibatch size could original objective given equation another advantage approach data statistics could pre-computed entire dataset would reduce variance objective gradients. another direction would like explore joint training auto-encoder model gmmn. currently treated separately joint training encourage learning codes suitable reconstruction well generation. gmmn provides easy sample data posterior distribution latent variables readily available. would interesting explore ways infer posterior distribution latent space. straightforward learn neural network predict latent vector given sample. reminiscent recognition models used wake-sleep algorithm variational auto-encoders interesting application directly related generative modelling comes recent work learning fair representations there objective train prediction method invariant particular sensitive attribute data. solution learn intermediate clustering-based representation. could instead applied learn powerful distributed representation statistics representation change conditioned sensitive variable. idea generalized learn representations invariant known biases. finally notion utilizing auto-encoder gmmn+ae model provides avenues creating generative models even complex datasets. example possible gmmn+ae convolutional auto-encoders order create generative models high resolution color images. thank david warde-farley helpful clariﬁcations regarding charlie tang providing relevant references. thank cifar nserc google research funding. figure linear interpolation uniform random points gmmn+ae prior projected network data space mnist tfd. random points highlighted boxes interpolation goes left right bottom. ﬁnal rows represent interpolation last highlighted image ﬁrst highlighted image. bootstrapped good generative model data. mnist toronto face database gmmn+ae model achieves superior performance compared approaches. datasets demonstrate gmmn+ae able discover implicit manifold data. many interesting directions research using mmd. extension consider alternatives standard criterion order speed training. possibility class linear-time estimators developed recently literature merrienboer gulcehre bougares schwenk bengio learning phrase representations using encoder-decoder statistical machine translation. conference empirical methods natural language processing goodfellow pouget-abadie mirza warde-farley ozair courville bengio generative adversarial nets. advances neural information processing systems hinton deng dahl mohamed jaitly senior vanhoucke nguyen sainath kingsbury deep neural networks acoustic modeling speech recognition shared ieee signal process. views four research groups. mag. hinton srivastava krizhevsky sutskever salakhutdinov improving neural networks preventing co-adaptation feature detectors. arxiv preprint arxiv. mackay bayesian neural networks density networks. nuclear instruments methods physics research section accelerators spectrometers detectors associated equipment sriperumbudur fukumizu gretton lanckriet sch¨olkopf kernel choice classiﬁability rkhs embeddings probability distributions. advances neural information processing systems vincent larochelle bengio manzagol p.a. extracting composing robust features deproceedings innoising autoencoders. ternational conference machine learning masci meier cires¸an schmidhuber stacked convolutional auto-encoders hierarchical feature extraction. artiﬁcial neural networks machine learning–icann springer ramdas reddi poczos singh wasserman decreasing power kernel distance based nonparametric hypothesis tests high dimensions. twenty-ninth aaai conference artiﬁcial intelligence rifai vincent muller glorot bengio contractive auto-encoders explicit invariance feature extraction. proceedings international conference machine learning sermanet eigen zhang mathieu fergus lecun overfeat integrated recognition localization detection using convolutional networks. international conference learning representations", "year": 2015}