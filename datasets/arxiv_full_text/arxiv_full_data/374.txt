{"title": "SENNS: Sparse Extraction Neural NetworkS for Feature Extraction", "tag": ["cs.CV", "cs.AI", "cs.NE", "math.OC", "stat.ML", "90-08"], "abstract": "By drawing on ideas from optimisation theory, artificial neural networks (ANN), graph embeddings and sparse representations, I develop a novel technique, termed SENNS (Sparse Extraction Neural NetworkS), aimed at addressing the feature extraction problem. The proposed method uses (preferably deep) ANNs for projecting input attribute vectors to an output space wherein pairwise distances are maximized for vectors belonging to different classes, but minimized for those belonging to the same class, while simultaneously enforcing sparsity on the ANN outputs. The vectors that result from the projection can then be used as features in any classifier of choice. Mathematically, I formulate the proposed method as the minimisation of an objective function which can be interpreted, in the ANN output space, as a negative factor of the sum of the squares of the pair-wise distances between output vectors belonging to different classes, added to a positive factor of the sum of squares of the pair-wise distances between output vectors belonging to the same classes, plus sparsity and weight decay terms. To derive an algorithm for minimizing the objective function via gradient descent, I use the multi-variate version of the chain rule to obtain the partial derivatives of the function with respect to ANN weights and biases, and find that each of the required partial derivatives can be expressed as a sum of six terms. As it turns out, four of those six terms can be computed using the standard back propagation algorithm; the fifth can be computed via a slight modification of the standard backpropagation algorithm; while the sixth one can be computed via simple arithmetic. Finally, I propose experiments on the ARABASE Arabic corpora of digits and letters, the CMU PIE database of faces, the MNIST digits database, and other standard machine learning databases.", "text": "feature extraction problem occupies central position pattern recognition machine learning. concept paper drawing ideas optimisation theory artiﬁcial neural networks graph embeddings sparse representations develop novel technique termed senns aimed addressing feature extraction problem. proposed method uses anns projecting input attribute vectors output space wherein pairwise distances maximized vectors belonging diﬀerent classes minimized belonging class simultaneously enforcing sparsity outputs. vectors result projection used features classiﬁer choice. mathematically formulate proposed method minimisation objective function interpreted output space negative factor squares pair-wise distances output vectors belonging diﬀerent classes added positive factor squares pair-wise distances output vectors belonging classes plus sparsity weight decay terms. derive algorithm minimizing objective function gradient descent multi-variate version chain rule obtain partial derivatives function respect weights biases required partial derivatives expressed terms. turns four terms computed using standard back propagation algorithm; ﬁfth computed slight modiﬁcation standard backpropagation algorithm; sixth computed simple arithmetic. finally propose experiments arabase arabic corpora digits letters database faces mnist digits database standard machine learning databases. pattern recognition systems comprise three stages pre-processing feature extraction classiﬁcation stages. three stages researchers believe feature extraction stage critical. example review paper authors unequivocally wrote ‘selection feature extraction method probably single important factor achieving high recognition performance...’ indeed agree authors since view feature extraction commitment made might irreversible classiﬁer however sophisticated. hence becomes highly paramount carefully rigorously study ‘commitments’ made features extracted optimal accuracies classiﬁcation stage? researchers proposed plethora methods aimed answering question appears philosophy followed good feature extraction method articulated dejiver kittler said feature extraction problem ‘extracting data information relevant classiﬁcation purposes sense minimizing within-class pattern variability enhancing between-class pattern variability.’ popular feature extraction techniques follow philosophy linear discriminant analysis marginal fisher analysis viewed speciﬁc examples uniﬁying concept called graph embediings inspired philosophy herein propose technique termed senns pronounced ‘sense’ addreesing feature extraction problem. like senns also viewed least partially lens graph embeddings concept. unlike however senns incorporates mechanism seeking sparse features employs apparatus non-linear artiﬁcial neural networks rather linear projections eﬀecting transformations result sought features. mathematically formulate method minimisation objective function. rigorous mathematical analysis derive gradient descent algorithm minimizing objective function. fortunately turns algorithm expressed terms standard backpropagation procedure except shall little tweaking accomodate norms. finally plan test senns standard machine learning datasets arabase arabic corpora digits letters database faces mnist digits database standard machine learning databases. describing neural networks almost entirely follow notation prof. andrew ng’s notation supervised training mode neural network learns training data denoted neural network proper consists layers number neurons l-th layer denoted weight denoted connects j-th neuron layer i-th neuron layer bias denoted emanates layer enters i-th neuron layer overall function i-th neuron layer compute activation denoted transfer function sigmoid tanh function. matter notational expedience deﬁnition employed means input viewed ‘activation’for ﬁrst layer. furthermore used denote column vector given similar notation applies well. similarly denotes column vector obtained ordered concatenation weights linking layer layer network. finally represents concatenation weights represents concatenation biases. herein follow notation except following modiﬁcations. firstly avoid confusion index placed within square brackets label training data based this write denote activation i-th neuron l-th layer applied input denote column vector activations associated l-th layer t-th input vector however layer question clear context simply write instead order achieve less clumsy notation. finally dealing partial derivatives further denotes input vectors pairs form drawn shall denote number times function outputs pairs passed similar deﬁnition applies well. training clear context simply write instead respectively. next non-negative regularisation constants deﬁne function follows connection function graph embedding framework clear. speciﬁcally clear plays role weights edges graphs underlying graph embeddings. that similar marginal fisher analysis described connects data points belonging class positive weight connects belonging diﬀerent classes negative weight. however shall soon proposed mathod herein diﬀers three ways. first herein eﬀect projections anns unlike employed either linearisation kernelisation tensorisation. second herein impose sparsity requirement sought features thereby seeking take advantage well known beneﬁts sparse features; instance. indeed would like eﬀect sparsity term ability gradient descent algorithm locate global minimum non-convex objective function. thirdly overall structure objective function herein diﬀerent since regularised terms whereas quotient terms. three things also distinguish method proposed herein linear discriminant analysis spell objective function wish minimize follows achieve slightly less clumsy notation equation written activations associated ann’s output layer instead shall carry practice henceforth unless otherwise stated. further ||.|| denotes norm. also notice objective function implicitly incorporates regularizers inclusion addition also regularizers simply substituting equation equation have form equation highlights ‘graph embedding’ aspect formulation. ﬁrst term gives measure output space widely separated output vectors belonging class are. clearly wish minimize non-negative quantity. contrary second term excluding negative sign gives measure output space widely separated output vectors belonging diﬀerent classes are. wish maximize non-negative measure minimizing negative quantity results negative sign pre-ﬁxed third term sparsity term wish make extracted features sparse. fourth term weight decay term prevents weights becoming large helps prevent overﬁtting. finally parameters allow control relative amount signiﬁcance objective function attaches four objectives trying achieve. question naturally arises pertaining computational feasibility sums appearing equation particular shall sums involving index variables carry directly algorithm minimizing objective function equation means term involving objective function would require time becomes undesirable grows. ameliorate this going propose heuristics sums follows. ﬁrst consider case term involving upfront point heuristic leads maximisation output space distances input vector given class nearest neighbour classes thereby given rise maximisation minimum distance formulation reminiscent support vector machines proceed classes denoted classiﬁcation problem extracting features. deﬁne belongs class nearest member heuristic denotes output-layer vector activations associated input vector clear compared original since problems usually expect heuristic lead signiﬁcant gains computational feasibility cases. however case term involving propose heuristic leads situation wherein distances input vector k-farthest neighbours class input vector minimised. formally simply deﬁne ˜nk) usual ˜nk) farthest elements equation a||. again case that cases heuristic lead improved computational feasibility since chosen lesser since heuristics constitute example host possible heuristics applied alleviate computational feasibilty issue equation therefore think would better develop technique general case formulated equation especially considering fact clear adapt developed technique particular heuristic interest. back equation expedient denote ﬁrst term equation second term third term hence equation re-written form consider minimize gradient descent. step computation next deﬁne vector column vector formed stacking column vector atop column vector denotes transpose notice function speak computing also observe that since function weights biases follows also function weights biases well speak path argue terms equation computed using standard backpropagation algorithm. consider function similar form except small diﬀerence. particular consider function herein term back-propagatable function deﬁne according where constant independent ann’s weights biases. reader compare function deﬁned equation particular reader note obtained simply replacing function latter constant however important back-propagatable function equation plays central role expression squares error aimed classiﬁcation must minimize supervised learning mode. given training data recall squares error written clearly equation constant independent weights biases. moreover also clear expression inside summation right hand side equation perfectly into supervised learning mode objective minimize total error b.one frequently employed techniques minimizing gradient descent algorithm requires partial derivatives respect weights biases. partial derivative respect arbitrary weight target output value plays supervised learning mode anns. proceed here shall denote speciﬁc constant value function function weights biases) evaluates given value given weights biases. consider function playing role target output value plays function foregoing mind write expression ‘proof’ ˆj|ˆa begin denotes activation i-th neuron output layer training example applied input number neurons output layer. compute ˆj|ˆa required. moving quickly ‘save’a implication result future reference along pair arguments standard backpropagation procedure. time computed plays role current vector activations produced plays role target output vector. based foregoing naturally denotes relevant transfer function typically sigmoid tan-sigmoid linear transfer function depending layer question. also deﬁne ‘sign function ’according observation indicates that problem hand obtain backpropagation algorithm computing partial derivatives simply letting play role normally plays standard backpropagation algorithm. speciﬁc implication observation could deﬁne analogy usually done standard backpropagation algorithm setting write section describe gradient descent algorithm minimisation objective function gave equation algorithm labeled algorithm below. algorithm relies inputs regularisation parameters learning rate training parameter number training pairs belonging class parameter number training pairs belonging diﬀerent classes initial randomized small weights concept paper proposed technique called senns feature extraction problem problem heart pattern recognition machine learning. philosophically proposed method draws idea extracting features maximise inter-class variances minimising intra-class variances. result method immediately framework graph embeddings. however unlike representative members class methods within graph embedding school thought proposed senns enforces sparsity extracted features utilises powerful non-linear projections rather linear kernel tensor transformations eﬀect feature extraction process. formulated senns minimisation regularised four terms derived eﬀective gradient descent algorithm resulting minimisation problem. rigorous mathematical analysis showed algorithm speciﬁed tasks involving standard back-propagation procedure modiﬁcation norms. finally natural next line action test senns standard machine learning datasets arabase database arabic characters database faces well mnist database digits.", "year": 2014}