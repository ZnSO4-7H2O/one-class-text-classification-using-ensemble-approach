{"title": "Adaptive learning rates and parallelization for stochastic, sparse,  non-smooth gradients", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Recent work has established an empirically successful framework for adapting learning rates for stochastic gradient descent (SGD). This effectively removes all needs for tuning, while automatically reducing learning rates over time on stationary problems, and permitting learning rates to grow appropriately in non-stationary tasks. Here, we extend the idea in three directions, addressing proper minibatch parallelization, including reweighted updates for sparse or orthogonal gradients, improving robustness on non-smooth loss functions, in the process replacing the diagonal Hessian estimation procedure that may not always be available by a robust finite-difference approximation. The final algorithm integrates all these components, has linear complexity and is hyper-parameter free.", "text": "recent work established empirically successful framework adapting learning rates stochastic gradient descent effectively removes needs tuning automatically reducing learning rates time stationary problems permitting learning rates grow appropriately nonstationary tasks. here extend idea three directions addressing proper minibatch parallelization including reweighted updates sparse orthogonal gradients improving robustness non-smooth loss functions process replacing diagonal hessian estimation procedure always available robust ﬁnite-difference approximation. ﬁnal algorithm integrates components linear complexity hyper-parameter free. many machine learning problems framed minimizing loss function large number samples. representation learning loss functions generally built multiple layers non-linearities precluding direct closed-form optimization admitting gradients guide iterative optimization loss. stochastic gradient descent among broadly applicable widely-used algorithms learning tasks simplicity robustness scalability arbitrarily large datasets. many small noisy updates instead fewer large ones gives speed-up makes learning process less likely stuck sensitive local optima. addition eminently well-suited learning non-stationary environments e.g. data stream generated changing environment; non-stationary adaptivity useful even stationary problems initial search phase learning process likened non-stationary environment. given increasingly wide adoption machine learning tools undoubted beneﬁt making learning algorithms particular easy hyper-parameter free. recent work made hyper-parameter free introducing optimal adaptive learning rates based gradient variance estimates broadly successful approach limited smooth loss functions minibatch sizes one. paper therefore complement work addressing resolving issues retaining optimal adaptive learning rates. issues practical importance minibatch parallelization strong diminishing returns combination sparse gradients adaptive learning rates show effect drastically mitigated. importance robustly dealing non-smooth loss functions also practical concern growing number learning architectures employ non-smooth nonlinearities like absolute value normalization rectiﬁed-linear units. ﬁnal algorithm addresses these remaining simple implement linear complexity. number adaptive settings learning rates equivalently diagonal preconditioning schemes found literature e.g. generally increase performance stochastic optimization tasks concern complementary focus producing algorithm works robustly without hyper-parameter tuning. often adaptive schemes produce monotonically decreasing rates however makes longer applicable non-stationary tasks. remainder paper build upon adaptive learning rate scheme monotonically decreasing recapitulate main results here. using idealized quadratic separable loss function possible derive optimal learning rate schedule preserves convergence guarantees sgd. problem approximately separable analysis simpliﬁed quantities one-dimensional. analysis also holds local approximation non-quadratic smooth case. idealized case dimension optimal learning rate derived analytically takes following form compared pure online computation time reduced minibatch-parallelization sample-gradients computed single update resulting averaged minibatch gradient performed. seen hyperparameter algorithm often constrained large extent computational hardware memory requirements communication bandwidth. derivation like equation used determine optimal learning figure diminishing returns minibatch parallelization. plotted relative log-loss gain given minibatch size compared gain case ﬁgure corresponds different sparsity level. example ratio means takes times samples obtain gain loss pure sgd. strongly diminishing returns less drastic noise level high sample gradients somewhat sparse however fact increase learning rates appropriately diminishing returns kick much larger minibatch sizes; left ﬁgures. expresses intuition using minibatches reduces sample noise turn permitting larger step sizes noise small gains minimal large substantial varying minibatch sizes tend impractical implement however common practice simply minibatch size re-tune learning rates adaptive minibatch-aware scheme longer necessary fact automatic transition initially small effective minibatches large minibatches toward noise level higher. many common learning architectures lead sample gradients increasingly sparse non-zero small fraction problem dimensions. possible exploit speed learning averaging many sparse gradients minibatch asynchronous updates here investigate learning rates presence sparsity result simply based observation update using sparse gradients equivalent update smaller effective minibatch size ignoring zero entries. element-by-element basis deﬁne number non-zero elements dimension within current minibatch. dimension rescale minibatch gradient accordingly factor time reduce learning rate reﬂect smaller effective minibatch size. compounding effects gives optimal learning rate sparse minibatches figure shows using minibatches adaptive learning rates reduces impact diminishing returns sample gradients sparse. words right learning rates higher sparsity directly translated higher parallelizability. figure difference global instance-based computation effective minibatch sizes presence sparse gradients. proposed method computes number non-zero entries current mini-batch learning rate involves additional computation compared using long-term average sparsity obtains substantially higher relative gain especially regime sparsity level produces minibatches non-zero entries noise level effect much pronounced noise higher. comparison performance different ﬁxed learning-rate settings plotted yellow dots. figure illustrating effect reweighting minibatch gradients. assume samples drawn different noisy clusters clusters higher probability occurrence. regular minibatch gradient simply arithmetic average dominated common cluster. reweighted minibatch gradient full step toward clusters closely resembling gradient would obtain performing hard clustering samples dotted green. alternative computing minibatch anew would instead. figure shows suboptimal long-term average sparsity especially noise level small regime minibatch expected contain non-zero entries. ﬁgure also shows equation produces higher relative gain compared outer envelope performance ﬁxed learning rates. figure illustrating expectation non-smooth sample losses. dotted blue loss functions individual samples shown non-smooth function. however expectation distribution functions smooth shown thick magenta curve. left absolute value right rectiﬁed linear function; samples identical offset value drawn reason boost parallelizability gradients sparse comes fact sparse gradients mostly orthogonal allowing independent progress direction. sparse gradients fact special case orthogonal gradients obtain similar speedups reweighting minibatch gradients words sample weighted number times gradient interfering another sample’s gradient. limit scheme simpliﬁes sparse-gradient cases discussed above sample gradients aligned averaged sample gradients orthogonal summed figure illustration. practice reweighting comes certain cost increasing computational expense single iteration problem dimension. words likely viable forward-backward passes gradient computation non-trivial minibatch size small. many commonly used non-linearities produce non-smooth sample loss functions. however optimizing distribution samples variability samples lead smooth expected loss function even though sample non-smooth contribution. figure illustrates point samples absolute value rectiﬁed linear contribution loss. clear observation possible reliably estimate curvature true expected loss function curvature individual sample losses sample losses non-smooth. means previous approach estimating term optimal learning rate expression moving average sample curvatures estimated bbprop procedure limited smooth sample loss functions need different approach general case. good estimate relevant curvature purposes compute true hessian current point take expectation noisy ﬁnite-difference steps steps scale actually performed update steps regime care about. practice obtain ﬁnite-difference estimates computing gradients sample loss points differing typical update distance approach related diagonal hessian preconditioning sgd-qn step-difference used different moving average scheme decaying time thus loses suitability non-stationary problems. increase robustness reuse intuition originally motivated vsgd take account variance curvature estimates reduce likelihood becoming overconﬁdent using variance-normalization based signal-to-noise ratio curvature estimates. outlier sample encountered time constants close potential disrupt optimization process. here statistics keep adaptive learning rates additional unforeseen beneﬁt make trivial detect outliers. outlier’s effect mitigated relatively simply increasing time-constant incorporating sample statistics perceived variance shooting learning rate automatically reduced. outlier genuine change data distribution algorithm quickly adapt increase learning rates again. practice detection threshold standard deviations increase corresponding algorithm gives explicit pseudocode ﬁnite-difference estimation combination minibatch size-adjusted rates equation termed vsgd-fd. initialization akin vsgd moving averages bootstrapped samples updates done. also wise tiny term necessary avoid divisions zero. algorithm ambition work out-of-the-box without tuning hyper-parameters must able pass number elementary tests sufﬁcient necessary. purpose collection elementary stochastic optimization test cases varying shape loss function curvature noise level. sample loss functions curvature setting drawn vary curvature noise levels orders magnitude i.e. giving test cases. visualize large number results summarize test case algorithm combination concise heatmap square figure show results test cases range algorithms minibatch sizes square shows gain loss independent runs updates each. group columns corresponds four functions inner columns using different curvature noise level settings. color scales identical heatmaps within column across columns. group rows corresponds algorithm using different hyperparameter setting namely initial learning rates natural gradient decay rate sgd. rows come pairs upper using pure lower using minibatches figure explanation read concise heatmap performance plots based common representation learning curves learning curve representation plot curve algorithm trial unique color/line-type algorithm mean performance algorithm contrast. performance measured every power iterations. gives good idea progress becomes quickly hard read. right side plot identical data heatmap format. square corresponds algorithm horizontal axis still iterations vertical axis arrange performance different trials given iteration. color scale follows white initial loss value stronger blue lower loss color reddish algorithm overjumped loss values bigger initial one. good algorithm performance visible square becomes blue right side instability marked variability algorithm across trials visible color range vertical axis. ﬁndings clear contrast algorithms tested vsgd-fd require hyper-parameter tuning give reliably good performance broad range tests learning rates adapt automatically different curvatures noise levels. contrast predecessor vsgd also deals non-smooth loss functions appropriately. learning rates adjusted automatically according minibatch size improves convergence speed noisier test cases larger potential gain minibatches. earlier variant shown work robustly broad range real-world benchmarks non-convex deep neural network-based loss functions. expect results smooth losses transfer directly vsgd-fd. bodes well future work determine performance real-world non-smooth problems. presented novel variant adaptive learning rates expands previous work three directions. adaptive rates properly take account minibatch size combination sparse gradients drastically alleviates diminishing returns parallelization. also curvature estimation procedure based ﬁnite-difference approach deal non-smooth sample loss functions. ﬁnal algorithm integrates components linear complexity hyper-parameter free. unlike adaptive schemes works broad range elementary test cases necessary condition out-of-the-box method. future work investigate adjust presented element-wise approach highly nonseparable problems potentially relying low-rank block-decomposed estimate gradient covariance matrix tonga authors want thank sixin zhang durk kingma daan wierstra camille couprie cl´ement farabet arthur szlam helpful discussions. also thank reviewers helpful suggestions ‘open reviewing network’ perfectly managing novel open transparent reviewing process. work funded part postdoc grant number national research fund luxembourg.", "year": 2013}