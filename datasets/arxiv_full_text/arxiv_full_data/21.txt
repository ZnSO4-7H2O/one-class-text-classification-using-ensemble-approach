{"title": "The Mythos of Model Interpretability", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "stat.ML"], "abstract": "Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.", "text": "supervised machine learning models boast remarkable predictive capabilities. trust model? work deployment? else tell world? want models good interpretable. task interpretation appears underspeciﬁed. papers provide diverse sometimes non-overlapping motivations interpretability offer myriad notions attributes render models interpretable. despite ambiguity many papers proclaim interpretability axiomatically absent explanation. paper seek reﬁne discourse interpretability. first examine motivations underlying interest interpretability ﬁnding diverse occasionally discordant. then address model properties techniques thought confer interpretability identifying transparency humans post-hoc explanations competing notions. throughout discuss feasibility desirability different notions question oft-made assertions linear models interpretable deep neural networks not. machine learning models penetrate critical areas like medicine criminal justice system ﬁnancial markets inability humans understand models seems problematic suggest model interpretability remedy articulate precisely interpretability means important. despite absence deﬁnition papers frequently make claims interpretability various models. this might conclude either deﬁnition interpretability universally agreed upon managed writing term interpretability ill-deﬁned thus claims regarding interpretability various models exhibit quasi-scientiﬁc character. investigation literature suggests latter case. motives interpretability technical descriptions interpretable models diverse occasionally discordant suggesting interpretability refers concept. paper seek clarify both suggesting interpretability monolithic concept fact reﬂects several distinct ideas. hope critical analysis bring focus dialogue. here mainly consider supervised learning machine learning paradigms reinforcement learning interactive learning. scope derives original interest oft-made claim linear models preferable deep neural networks account interpretability gain conceptual clarity reﬁning questions interpretability important? broadening scope discussion seems counterproductive respect aims. research investigating interpretability context reinforcement learning point studies human interpretability robot actions. reasoning delve much papers might bayesian methods however draw connections appropriate. ground discussion might constitute interpretability ﬁrst consider various desiderata forth work addressing topic many papers propose interpretability means engender trust trust? refer faith model’s performance robustness property decisions makes? interpretability simply mean low-level mechanistic understanding models? apply features parameters models training algorithms? papers suggest connection interpretable model uncovers causal structure data legal notion right explanation offers another lens interpretability. perfect matches real-life tasks meant solve. happen simpliﬁed optimization objectives fail capture complex real-life goals. consider medical research longitudinal data. real goal discover potentially causal associations smoking cancer optimization objective supervised learning models simply minimize error feat might achieved purely correlative fashion. another divergence real-life machine learning problem formulations emerges off-line training data supervised learner perfectly representative likely deployment environment. example environment typically stationary. case product recommendation products introduced preferences items shift daily. extreme cases actions inﬂuenced model alter environment invalidating future predictions. discussions interpretability sometimes suggest human decision-makers interpretable explain actions precisely notion interpretability explanations satisfy? seem unlikely clarify mechanisms precise algorithms brains work. nevertheless information conferred interpretation useful. thus purpose interpretations convey useful information kind. addressing desiderata interpretability consider properties models might render interpretable papers equate interpretability understandability intelligibility work. papers understandable models sometimes called transparent incomprehensible models called black boxes. constitutes transparency? might look algorithm itself. converge? produce unique solution? might look parameters understand represents? alternatively could consider model’s complexity. simple enough examined human? papers investigate so-called post-hoc interpretations. interpretations might explain predictions without elucidating mechanisms models work. examples post-hoc interpretations include verbal explanations produced people saliency maps used analyze deep neural networks. thus humans decisions might admit post-hoc interpretability despite black nature human brains revealing contradiction popular notions interpretability. present interpretability formal technical meaning. paper propose speciﬁc definitions. determine meanings might appropriate must real-world objectives interpretability research are. section spell various desiderata interpretability research lens literature. desiderata diverse might instructive ﬁrst consider common thread persists throughout literature demand interpretability arises mismatch formal objectives supervised learning real world costs deployment setting. figure typically evaluation metrics require predictions ground truth labels. stakeholders additionally demand interpretability might infer existence desiderata cannot captured fashion. consider common evaluation metrics supervised learning require predictions together ground truth produce score. metrics assessed every supervised learning model. desire interpretation suggests scenarios predictions alone metrics calculated predictions sufﬁce characterize model desiderata circumstances sought? however inconveniently turns many situations arise real world objectives difﬁcult encode simple real-valued functions. example algorithm making hiring decisions simultaneously optimize productivity ethics legality. typically ethics legality cannot directly optimized. problem also arise dynamics deployment environment differ training environment. cases interpretations serve objectives deem important struggle model formally. papers motivate interpretability suggesting prerequisite trust trust? simply conﬁdence model perform well? sufﬁciently accurate model demonstrably trustworthy interpretability would serve purpose. trust might also deﬁned subjectively. example person might feel ease wellunderstood model even understanding served obvious purpose. alternatively training deployment objectives diverge trust might denote conﬁdence model perform well respect real objectives scenarios. example consider growing machine learning models forecast crime rates purposes allocating police ofﬁcers. trust model make accurate predictions account racial biases training data model’s effect perpetuating cycle incarceration over-policing neighborhoods. another sense might trust machine learning model might feel comfortable relinquishing control sense might care often model right also examples right. model tends make mistakes regions input space humans also make mistakes typically accurate humans accurate considered trustworthy sense expected cost relinquishing control. model tends make mistakes inputs humans classify accurately always advantage maintaining human supervision algorithms. although supervised learning models optimized directly make associations researchers often hope inferring properties generating hypotheses natural world. example simple regression model might reveal strong association thalidomide birth defects smoking lung cancer associations learned supervised learning algorithms guaranteed reﬂect causal relationships. could always exist unobserved causes responsible associated variables. might hope however interpreting supervised learning models could generate hypotheses scientists could test experimentally. example emphasizes regression trees bayesian neural networks suggesting models interpretable thus better able provide clues causal relationships physiologic signals affective states. task inferring causal relationships observational data extensively studied typically choose training test data randomly partitioning examples distribution. judge model’s generalization error performance training test data. however humans exhibit richer capacity generalize transferring learned skills unfamiliar situations. already machine learning algorithms situations abilities required environment nonstationary. also deploy models settings might alter environment invalidating future predictions. along lines caruana describe model trained predict probability death pneumonia assigned less risk patients also asthma. fact asthma predictive lower risk death. owed aggressive treatment patients received. model deployed triage patients would receive less aggressive treatment invalidating model. even worse could imagine situations like machine learning security environment might actively adversarial. consider recently discovered susceptibility convolutional neural networks adversarial examples. cnns made misclassify images imperceptibly perturbed course isn’t overﬁtting classical sense. results achieved training data generalize well i.i.d. test data. mistakes human wouldn’t make would prefer models make mistakes either. already supervised learning models regularly subject adversarial manipulation. consider models used generate credit ratings scores higher signify higher probability individual repays loan. according technical report fico trains credit models using logistic regression speciﬁcally citing interpretability motivation choice model. features include dummy variables representing binned values average accounts debt ratio number late payments number accounts good standing. several factors manipulated credit-seekers. example one’s debt ratio improved simply requesting periodic increases credit lines keeping spending patterns constant. similarly total number accounts increased simply applying accounts probability acceptance reasonably high. indeed fico experian acknowledge credit ratings manipulated even suggesting guides improving one’s credit rating. rating improvement strategies fundamentally change one’s underlying ability debt. fact individuals actively successfully game rating system invalidate predictive power. sometimes apply decision theory outputs supervised models take actions real world. however another common paradigm supervised model used instead provide information human decision makers setting considered huysmans machine learning objective might reduce error real-world purpose provide useful information. obvious model conveys information outputs. however possible procedure convey additional information human decision-maker. analogy might consider student seeking advice advisor. suppose student asks venue would best suit paper. advisor could simply name conference especially useful. even advisor reasonably intelligent terse reply doesn’t enable student meaningfully combine advisor’s knowledge own. interpretation prove informative even without shedding light model’s inner workings. example diagnosis model might provide intuition human decision-maker pointing similar cases support diagnostic decision. cases train supervised learning model real task closely resembles unsupervised learning. here real goal explore data objective serves weak supervision. present politicians journalists researchers expressed concern must produce interpretations purpose assessing whether decisions produced automatically algorithms conform ethical standards concerns timely algorithmic decision-making mediates interactions inﬂuencing social experiences news ﬁnances career opportunities. task computer programs approving lines credit curating news ﬁltering applicants. courts even deploy computerized algorithms predict risk recidivism probability individual relapses criminal behavior seems likely trend accelerate breakthroughs artiﬁcial intelligence rapidly broaden capabilities software. recidivism predictions already used determine release detain raising ethical concerns. sure predictions discriminate basis race? conventional evaluation metrics accuracy offer little assurance model decision theory actions behave acceptably. thus demands fairness often lead demands interpretable models. regulations european union propose individuals affected algorithmic decisions right explanation precisely form explanation might take explanation could proven correct merely appeasing remain open questions. moreover regulations suggest algorithmic decisions contestable. order explanations useful seems must present clear reasoning based falsiﬁable propositions offer natural contesting propositions modifying decisions appropriately falsiﬁed. turn consider techniques model properties proposed either enable comprise interpretations. broadly fall categories. ﬁrst relates transparency i.e. model work? second consists post-hoc explanations i.e. else model tell division useful organizationally note absolute. example post-hoc analysis techniques attempt uncover significance various parameters group heading transparency. transparency opposite opacity informally blackbox-ness. connotes sense understanding mechanism model works. consider transparency level entire model level individual components level training algorithm strictest sense might call model transparent person contemplate entire model once. deﬁnition suggests interpretable model simple model. might think example model fully understood human able take input data together parameters model reasonable time step every calculation required produce prediction. accords common claim sparse linear models produced lasso regression interpretable dense linear models learned inputs. ribeiro also adopt notion interpretability suggesting interpretable model readily presented user visual textual artifacts. models decision trees size model grow much faster time perform inference suggests simulatability admit subtypes based total size model another based computation required perform inference. fixing notion simulatability quantity denoted reasonable subjective. clearly given limited capacity human cognition ambiguity might span several orders magnitude. light suggest neither linear models rule-based systems decision trees intrinsically interpretable. sufﬁciently highdimensional models unwieldy rule lists deep decision trees could considered less transparent comparatively compact neural networks. second notion transparency might part model input parameter calculation admits intuitive explanation. accords property intelligibility described example node decision tree might correspond plain text description similarly parameters linear model could described representing strengths association feature label. note notion interpretability requires inputs individually interpretable disqualifying models highly engineered anonymous features. notion popular shouldn’t accept blindly. weights linear model might seem intuitive fragile respect feature selection pre-processing. example associations risk vaccination might positive negative depending whether feature includes indicators infancy immunodeﬁciency. ﬁnal notion transparency might apply level learning algorithm itself. example case linear models understand shape error surface. prove training converge unique solution even previously unseen datasets. give conﬁdence model might behave online setting requiring programmatic retraining previously unseen data. hand modern deep learning methods lack sort algorithmic transparency. heuristic optimization procedures neural networks demonstrably powerful don’t understand work present cannot guarantee priori work problems. note however humans exhibit none forms transparency. post-hoc interpretability presents distinct approach extracting information learned models. posthoc interpretations often elucidate precisely model works nonetheless confer useful information practitioners users machine learning. common approaches post-hoc interpretations include natural language explanations visualizations learned representations models explanations example extent might consider humans interpretable sort interpretability applies. know processes humans make decisions explain distinct. advantage concept interpretability interpret opaque models after-the-fact without sacriﬁcing predictive performance. humans often justify decisions verbally. similarly might train model generate predictions separate model recurrent neural network language model generate explanation. approach taken line work krening propose system model chooses actions optimize cumulative discounted return. train another model model’s state representation onto verbal explanations strategy. explanations trained maximize likelihood previously observed ground truth explanations human players faithfully describe agent’s decisions however plausible appear. note connection approach recent work neural image captioning representations learned discriminative convolutional neural network co-opted second model generate captions. captions might regarded interpretations accompany classiﬁcations. model. method consists simultaneously training latent factor model rating prediction topic model product reviews. training alternate decreasing squared error rating prediction increasing likelihood review text. models connected normalized latent factors topic distributions. words latent factors regularized also good explaining topic distributions review text. authors explain user-item compatibility examining words topics corresponding matching components latent factors. note practice interpreting topic models presenting words post-hoc interpretation technique invited scrutiny another common approach generating post-hoc interpretations render visualizations hope determining qualitatively model learned. popular approach visualize high-dimensional distributed representations t-sne technique renders visualizations nearby data points likely appear close together. mordvintsev attempt explain image classiﬁcation network learned altering input gradient descent enhance activations certain nodes selected hidden layers. inspection perturbed inputs give clues model learned. likely model trained large corpus animal images observed enhancing nodes caused faces appear throughout input image. computer vision community similar approaches explored investigate information retained various layers neural network. mahendran vedaldi pass image discriminative convolutional neural network generate representation. demonstrate original image recovered high ﬁdelity even reasonably high-level representations performing gradient descent randomly initialized pixels. difﬁcult succinctly describe full mapping learned neural network papers focus instead explaining neural network depends locally. popular approach deep neural nets compute saliency map. typically take gradient output corresponding correct class respect given input vector. images gradient applied mask highlighting regions input that changed would inﬂuence output work authors explain decisions model local region near particular point learning separate sparse linear model explain decisions ﬁrst. post-hoc mechanism explaining decisions model might report examples model considers similar method suggested caruana training deep neural network latent variable model discriminative task access predictions also learned representations. then example addition generating prediction activations hidden layers identify k-nearest neighbors based proximity space learned model. sort explanation example precedent humans sometimes justify actions analogy. example doctors often refer case studies support planned treatment protocol. neural network literature mikolov approach examine learned representations words wordvec training. model trained discriminative skip-gram prediction examine relationships model learned enumerate nearest neighbors words based distances calculated latent space. also point related work bayesian methods doshi-velez investigate cased-base reasoning approaches interpreting generative models. concept interpretability appears simultaneously important slippery. earlier analyzed motivations interpretability attempts research community confer discussion consider implications analysis offer several takeaways reader. despite claim’s enduring popularity truth content varies depending notion interpretability employ. respect algorithmic transparency claim seems uncontroversial given high dimensional heavily engineered features linear models lose simulatability decomposability respectively. choosing linear deep models must often make trade-off algorithic transparency decomposability. deep neural networks tend operate lightly processed features. nothing else features intuitively meaningful post-hoc reasoning sensible. however order comparable performance linear models often must operate heavily hand-engineered features. lipton demonstrates case linear models approach performance rnns cost decomposability. kinds post-hoc interpretation deep neural networks exhibit clear advantage. learn rich representations visualized verbalized used clustering. considering desiderata interpretability linear models appear better track record studying natural world know theoretical reason must conceivably post-hoc interpretations could prove useful similar scenarios. demonstrated paper term reference monolithic concept. meaningful assertion regarding interpretability speciﬁc deﬁnition. model satisﬁes form transparency shown directly. post-hoc interpretability papers ought clear objective demonstrate evidence offered form interpretation achieves arguments black-box algorithms appear preclude model could match surpass abilities complex tasks. concrete example shortterm goal building trust doctors developing transparent models might clash longer-term goal improving health care. careful giving predictive power desire transparency justiﬁed isn’t simply concession institutional biases methods. caution blindly embracing post-hoc notions interpretability especially optimized placate subjective demands. cases might deliberately optimize algorithm present misleading plausible explanations. humans known engage behavior evidenced hiring practices college admissions. several journalists social scientists demonstrated acceptance decisions attributed virtues like leadership originality often disguise racial gender discrimination rush gain acceptance machine learning emulate human intelligence careful reproduce pathological behavior scale. several promising directions future work. first problems discrepancy real-life machine learning objectives could mitigated developing richer loss functions performance metrics. exemplars direction include research sparsity-inducing regularizers cost-sensitive learning. second expand analysis paradigms reinforcement learning. reinforcement learners address objectives interpretability research directly modeling interaction models environments. however capability come cost allowing models experiment world incurring real consequences. notably reinforcement learners able learn causal relationships actions real world impacts. however like supervised learning reinforcement learning relies well-deﬁned scalar objective. problems like fairness struggle verbalize precise deﬁnitions success shift paradigm unlikely eliminate problems face. meaning machine learning conferences frequently publish papers wield term quasimathematical way. papers meaningful ﬁeld progress must critically engage issue problem formulation. moreover identify incompatibility presently investigated interpretability techniques pressing problems facing machine learning wild. example little published work model intepretability addresses idea contestability. paper makes ﬁrst step towards providing comprehensive taxonomy desiderata methods interpretability research. argue paucity critical writing machine learning community problematic. solid problem formulations ﬂaws methodology addressed articulating methods. problem formulation ﬂawed neither algorithms experiments sufﬁcient address underlying problem. moreover machine learning continues exert inﬂuence upon society must sure solving right problems. lawmakers policymakers must increasingly consider impact machine learning responsibility account impact machine learning ensure alignment societal desiderata must ultimately shared practitioners researchers ﬁeld. thus believe critical writing ought voice machine learning conferences. caruana rich kangarloo hooshang dionisio sinha usha johnson david. case-based explanation non-case-based learning methods. proceedings amia symposium caruana rich gehrke johannes koch paul sturm marc elhadad no´emie. intelligible models healthcare predicting pneumonia risk hospital -day readmission. dragan anca kenton srinivasa siddhartha legibility predictability robot motion. human-robot interaction acm/ieee international conference ieee huysmans johan dejaeger karel mues christophe vanthienen baesens bart. empirical evaluation comprehensibility decision table tree rule based predictive models. decision support systems krening samantha harrison brent feigh karen isbell charles riedl mark thomaz andrea. learning explanations using sentiment advice ieee transactions cognitive developmental systems changchun rani pramila sarkar nilanjan. empirical study machine learning techniques afinterfect recognition human-robot interaction. national conference intelligent robots systems. ieee simonyan karen vedaldi andrea zisserman andrew. deep inside convolutional networks visualising image classiﬁcation models saliency maps. arxiv. szegedy christian zaremba wojciech sutskever ilya bruna joan erhan dumitru goodfellow fergus rob. intriguing properties neural networks. arxiv. wang hui-xin fratiglioni laura frisoni giovanni viitanen matti winblad bengt. smoking occurence alzheimer’s disease cross-sectional longitudinal data population-based study. american journal epidemiology", "year": 2016}