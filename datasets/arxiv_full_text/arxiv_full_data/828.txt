{"title": "Accelerating Deep Learning with Memcomputing", "tag": ["cs.LG", "cs.AI", "cs.ET", "cs.NE"], "abstract": "Restricted Boltzmann machines (RBMs) and their extensions, called 'deep-belief networks', are powerful neural networks that have found applications in the fields of machine learning and big data. The standard way to training these models resorts to an iterative unsupervised procedure based on Gibbs sampling, called 'contrastive divergence' (CD), and additional supervised tuning via back-propagation. However, this procedure has been shown not to follow any gradient and can lead to suboptimal solutions. In this paper, we show an efficient alternative to CD by means of simulations of digital memcomputing machines (DMMs). We test our approach on pattern recognition using a modified version of the MNIST data set. DMMs sample effectively the vast phase space given by the model distribution of the RBM, and provide a very good approximation close to the optimum. This efficient search significantly reduces the number of pretraining iterations necessary to achieve a given level of accuracy, as well as a total performance gain over CD. In fact, the acceleration of pretraining achieved by simulating DMMs is comparable to, in number of iterations, the recently reported hardware application of the quantum annealing method on the same network and data set. Notably, however, DMMs perform far better than the reported quantum annealing results in terms of quality of the training. We also compare our method to advances in supervised training, like batch-normalization and rectifiers, that work to reduce the advantage of pretraining. We find that the memcomputing method still maintains a quality advantage ($>1\\%$ in accuracy, and a $20\\%$ reduction in error rate) over these approaches. Furthermore, our method is agnostic about the connectivity of the network. Therefore, it can be extended to train full Boltzmann machines, and even deep networks at once.", "text": "restricted boltzmann machines extensions called deep-belief networks powerful neural networks found applications ﬁelds machine learning data. standard train models resorts iterative unsupervised procedure based gibbs sampling called contrastive divergence additional supervised tuning back-propagation. however procedure shown follow gradient lead suboptimal solutions. paper show eﬃcient alternative means simulations digital memcomputing machines test approach pattern recognition using modiﬁed version mnist data set. dmms sample eﬀectively vast phase space given model distribution provide good approximation close optimum. eﬃcient search signiﬁcantly reduces number pretraining iterations necessary achieve given level accuracy well total performance gain fact acceleration pretraining achieved simulating dmms comparable number iterations recently reported hardware application quantum annealing method network data set. notably however dmms perform better reported quantum annealing results terms quality training. also compare method advances supervised training like batch-normalization rectiﬁers work reduce advantage pretraining. memcomputing method still maintains quality advantage approaches. furthermore method agnostic connectivity network. therefore extended train full boltzmann machines even deep networks once. progress machine learning data driven successes deep learning diﬃcult overstate. deep learning models artiﬁcial neural networks certain amount layers proven useful variety applications computer vision speech recognition super-human performance complex games name few. models existed time dramatic increases computational power combined advances eﬀective training methods pushed forward ﬁelds considerably successful training deep-belief models relies heavily variant iterative gradient-descent procedure called back-propagation layers network since optimization method uses gradient information error landscapes deep networks highly non-convex would best hope appropriate local minimum. stuck local minima rather saddle points gradient also vanishes hence making gradient-descent procedure limited use. takeaway good initialization procedure assigning weights network known pretraining highly advantageous. deep-learning framework utilize pretraining procedure restricted boltzmann machine extension deep belief network machines class neural network models capable unsupervised learning parametrized probability distribution inputs. also easily extended supervised learning case training output layer using back-propagation standard methods training rbms usually distinguishes unsupervised pretraining whose purpose initialize good weights supervised procedure. current eﬀective technique pretraining rbms utilizes iterative sampling technique called contrastive divergence computing exact gradient log-likelihood exponentially hard size approximates computationally friendly sampling procedure. procedure brought rbms success suﬀers slow mixing gibbs sampling known follow partly shortcomings pretraining much research gone making backpropagation procedure robust less sensitive initialization weights biases network. includes research diﬀerent non-linear activation functions combat vanishing gradient problem normalization techniques make back-propagation deep networks stable less dependent initial conditions. techniques make training deep networks easier optimization problem gradient-based approach like back-propagation. this turn relegates standard pretraining procedure’s usefulness cases training sparse becoming increasingly rare occurrence. parallel research back-propagation sizable eﬀort expended toward improving power pretraining procedure including extensions done memristive hardware recently approaches based quantum annealing recover exact gradient involved pretraining. methods classical algorithms simulating quantum sampling still others attempt hardware quantum device contact environment take independent samples boltzmann distribution accurate gradient computation. instance recent work state mapped onto commercial quantum annealing processor latter used sampler model distribution results reported reduced version well-known mnist data look promising compared however approaches require expensive hardware cannot scaled larger problems yet. present paper inspired theoretical underpinnings recent empirical demonstrations advantages computing paradigm –memcomputing variety combinatorial/optimization problems seek test power toward computationally demanding problems deep learning. memcomputing novel computing paradigm solves complex computational problems using processing embedded memory. formalized introducing concept universal memcomputing machines. short perform computation task hand mapped continuous dynamical system employs highly-correlated states machine navigate phase space eﬃciently solution given problem mapped equilibrium states dynamical system. paper employ subset machines called digital memcomputing machines speciﬁcally self-organizing circuit realizations distinctive feature dmms ability read practical point view dmms built standard circuit elements without memory elements however non-quantum. therefore ordinary diﬀerential equations corresponding circuits eﬃciently simulated present computers. here indeed employ simulations dmms single xeon processor train rbms. simulations show already substantial advantages respect even quantum annealing despite latter executed hardware. course hardware implementation dmms applied problems would oﬀer even advantages since simulation times replaced actual physical time circuits reach equilibrium. would oﬀer realistic path real-time pretraining deep-belief networks. order compare directly quantum annealing results recently reported demonstrate advantage memcomputing approach ﬁrst training reduced mnist data used show method requires less pretraining iterations achieve accuracy well overall accuracy gain quantum annealing. also train rbms reduced mnist data without mini-batching quantum annealing results available. also case substantial reduction pretraining iterations needed well higher level accuracy memcomputing approach traditional approach seems oﬀer many advantages quantum approaches. however since based completely classical system eﬃciently deployed software well easily implemented hardware scaled full-size problems. finally investigate role recent advances supervised training comparing accuracy obtained using back-propagation batch-normalization rectiﬁers starting random initial condition versus back-propagation procedure initiated network pretrained memcomputing sigmoidal activations without batch-normalization. even without advantages namely operating non-convex landscape network pretrained memcomputing maintains accuracy gain state-of-the-art backpropagation reduction error rate. gives evidence fact memcomputing pretraining navigates advantageous initial point non-convex loss surface deep network. form weight updates referred stochastic gradient optimization momentum. momentum parameter learning rate. ﬁrst expectation value eqs. taken respect conditional probability distribution data ﬁxed visible layer. relatively easy compute. evaluation second expectation eqs. exponentially hard size network since obtaining independent samples high-dimensional model distribution easily becomes prohibitive increasing size term attempts approximate. approach attempts reconstruct diﬃcult expectation term iterative gibbs sampling. works sequentially sampling layer given sigmoidal conditional probabilities namely calculated resulting samples. limit inﬁnite sampling iterations expectation value recovered. however convergence slow practice usually iteration referred used replace reconstruction approach utilizes memcomputing compute much better approximation model expectation speciﬁcally pretraining problem onto dmms realized self-organizing logic circuits electrical circuits deﬁne coupled diﬀerential equations random initial conditions integrated forward toward global solution given problem. ordinary diﬀerential equations solve found ref. appropriately adapted deal particular problem discussed paper. shown). note lack connections nodes layer distinguishes boltzmann machine. weights trained separately output layer generative pretraining tuned together back-propogation weight i-th hidden neuron j-th visible neuron real numbers indicating biases neurons. value normalization constant known statistical mechanics partition function. training amounts ﬁnding weights biases maximizes likelihood observed data. common approach training rbms supervised task ﬁrst perform generative unsupervised learning initialize weights biases back-propagation input-label pairs tune parameters network. pretraining framed gradient ascent log-likelihood observed data gives particularly tidy form weight figure plot total weight max-sat clause function simulation time dmm. lower weight variable assignment corresponds directly higher probability assignment nodes rbm. simulation changed assignments time restart another random initial condition. inset shows full simulation restarts. main ﬁgure focuses last three restarts signiﬁed black inset. within memcomputing context construct reinterpretation pretraining explicitly shows corresponds np-hard optimization problem tackle using dmms. ﬁrst observe obtain sample near probability employ mapping general qubo problem weighted maximum satisﬁability problem similar directly solved dmm. weighted max-sat problem assignment boolean variables minimizes total weight given boolean expression written conjunctive normal form problem well-known problem np-hard complexity class however recently shown ref. simulations dmms show dramatic speed-up state-of-the-art solvers attempting better approximations hard max-sat instances beyond inapproximability figure memcomputing accuracy test reduced mnist problem versus contrastive divergence iterations back-propagation minin error bars calculated across dbns trained diﬀerent partitions training set. dramatic acceleration memcomputing approach needing less iterations achieve accuracy well overall performance back-propagation cannot seem overcome. note error bars mem-qubo small reported scale number pretraining iterations larger assignment obtained integration ordinary differential equations deﬁne solcs dynamics. collect entire trajectory begins random initial condition phase space problem ends lowest energy conﬁguration variables since problem tackling optimization guarantee ﬁnding global optimum. therefore ambiguity exactly constitutes stopping time simulation since priori cannot know simulation reached global minimum. perform restarts simulation stop simulation machine found better conﬁguration within number restarts. restarts clearly seen fig. spikes total weight boolean expression. work employed restarts over-kill since much smaller number would given similar results. testbed memcomputing advantage deep learning direct comparison quantum annealing hardware approaches ﬁrst looked reduced mnist data reported quantum annealing using d-wave machine. therefore ﬁrst applied reduction full mnist problem given work consists removing figure mem-qubo accuracy reduced mnist test iterations back-propagation mini-batching. resulting pretraining acceleration shown memcomputing approach denoted horizontal arrow. performance also appears emphasized vertical arrow mem-qubo obtaining higher level accuracy even highest number back-propagation iterations. error bars appear since trained full test set. work back-propagation iterations applied approaches using mini-batches samples generate fig. pretraining backpropagation learning rate momentum parameters ﬁrst iterations rest accuracy test versus function number pretraining iterations seen fig. memcomputing method reaches better solution faster maintains advantage even hundreds back-propagation iterations. interestingly software approach even competitive quantum annealing method done hardware quite remarkable result since integrate diﬀerential equations classical system scalable comparable sampling power physically-realized system takes advantage quantum eﬀects improve finally also trained reduced mnist data without mini-batches. aware quantum-annealing results full data still compare approach. follow similar procedure discussed above. case however mini-batching used direct comparison gibbs sampling memcomputing approach. results shown fig. diﬀerent numbers back-propagation iterations. even full modiﬁed mnist memcomputing approach requires substantially lower number pretraining iterations achieve high accuracy additionally shows higher level accuracy traditional even back-propagation iterations. computational diﬃculty computing exact gradient update pretraining combined inaccuracies inspired research methods reduce role pretraining deep models. techniques include changes numerical gradient procedure itself like adaptive gradient estimation changes activation functions reduce gradient decay enforce sparsity techniques like batch normalization make back-propagation less sensitive initial conditions updates many contexts deep networks initialized random initial condition found compete networks pretrained complete analysis compared network pretrained memcomputing approach back-propagation methods pretraining. networks trained stochastic gradient descent momentum learning rates momentum parameter used section figure accuracy reduced mnist test obtained network pretrained mem-qubo sigmoidal activation functions versus size network pretraining batch normalization rectiﬁed linear units networks trained stochastic gradient descent momentum mini-batches inset clearly shows accuracy advantage mem-qubo greater error rate reduction throughout training. employ batch-normalization procedure coupled rectiﬁed linear units anticipated batch-normalization smooths role initial conditions rectiﬁers render energy landscape deﬁned convex. therefore combined indeed seem provide advantage compared network trained using sigmoidal functions however enough overcome advantages memcomputing approach. fact obvious fig. network pretrained memcomputing maintains accuracy advantage test thousand back-propagation iterations. note network pretrained memcomputing contains sigmoidal activations compared rectiﬁers network pretraining. also pretrained network trained without batch normalization procedure. therefore considering this pretrained network pose diﬃcult optimization problem stochastic gradient descent. instead found accuracy advantage memcomputing throughout course training. points fact memcomputing pretraining procedure able operate close true gradient training initializes weights biases network advantageous way. paper demonstrated memcomputing paradigm applied toward chief bottlenecks deep learning today. paper directly assisted popular algorithm pretrain rbms dbns fact simulations digital memcomputing machines achieve accelerations pretraining comparable number iterations hardware application quantum annealing method better quality. addition unlike quantum computers approach easily scaled classical hardware full size problems. addition memcomputing method retains advantage also respect advances supervised training like batch-norming rectiﬁers introduced eliminate need pretraining. indeed despite pretraining done sigmoidal functions hence non-convex landscape provided rectiﬁers maintain accuracy advantage greater throughout training. finally form energy quite general encompasses full dbns. method also applied pretraining entire deep-learning models once potentially exploring parameter spaces inaccessible classical quantum methods. leave interesting line research future studies. acknowledgments thank forrest sheldon useful discussions yoshua bengio pointing role supervised training techniques. h.m. acknowledges support dod-smart fellowship. m.d. acknowledges partial support center memory recording research ucsd.", "year": 2018}