{"title": "A Shallow High-Order Parametric Approach to Data Visualization and  Compression", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Explicit high-order feature interactions efficiently capture essential structural knowledge about the data of interest and have been used for constructing generative models. We present a supervised discriminative High-Order Parametric Embedding (HOPE) approach to data visualization and compression. Compared to deep embedding models with complicated deep architectures, HOPE generates more effective high-order feature mapping through an embarrassingly simple shallow model. Furthermore, two approaches to generating a small number of exemplars conveying high-order interactions to represent large-scale data sets are proposed. These exemplars in combination with the feature mapping learned by HOPE effectively capture essential data variations. Moreover, through HOPE, these exemplars are employed to increase the computational efficiency of kNN classification for fast information retrieval by thousands of times. For classification in two-dimensional embedding space on MNIST and USPS datasets, our shallow method HOPE with simple Sigmoid transformations significantly outperforms state-of-the-art supervised deep embedding models based on deep neural networks, and even achieved historically low test error rate of 0.65% in two-dimensional space on MNIST, which demonstrates the representational efficiency and power of supervised shallow models with high-order feature interactions.", "text": "explicit high-order feature interactions efﬁciently capture essential structural knowledge data interest used constructing generative models. present supervised discriminative high-order parametric embedding approach data visualization compression. compared deep embedding models complicated deep architectures hope generates effective high-order feature mapping embarrassingly simple shallow model. furthermore approaches generating small number exemplars conveying high-order interactions represent large-scale data sets proposed. exemplars combination feature mapping learned hope effectively capture essential data variations. moreover hope exemplars employed increase computational efﬁciency classiﬁcation fast information retrieval thousands times. classiﬁcation two-dimensional embedding space mnist usps datasets shallow method hope simple sigmoid transformations signiﬁcantly outperforms state-of-the-art supervised deep embedding models based deep neural networks even achieved historically test error rate twodimensional space mnist demonstrates representational efﬁciency power supervised shallow models high-order feature interactions. high-order feature interactions naturally exist many real-world data including images documents ﬁnancial time series biological sequences medical records amongst many others. interplays often convey essential information latent structures datasets interest. data embedding visualization therefore crucial utilize high-order characteristic features generate dimensionality reduction function. recently supervised deep learning models made promising progresses sensory data regularities images speeches terms generating powerful complex parametric embedding functions capture high-order feature interactions deep architectures. current state-of-the-art deep strategies however fail deploy explicit high-order parametric form high-dimensional data low-dimensional space. explicit parametric mapping effectively avoids need develop out-of-sample extension cases non-parametric methods t-sne also reveals structural information intuitively understandable human enables people make good sense data visualization acquire interpretative knowledge visualization. furthermore current embedding methods often ignore fail perform data compression summarization generating embedding. functionality desirable dealing large-scale datasets fast information retrieval often perform classiﬁcation computational efﬁciency important. address mentioned challenges paper present high-order parametric embedding approach. aims hope two-fold learning explicit high-order parametric embedding function data visualization constructing small synthetic exemplars high-order feature interactions borne represent whole input dataset. speciﬁc approach targets supervised data visualization procedures. firstly linearly explicit high-order k-order interaction features products possible features two-dimensional space visualization pairwise data points class stay together pairwise data points different classes stay farther apart. avoid directly enumerating possible k-feature interactions computationally prohibitive propose using tensor factorization learn feature interaction ﬁlters. result high-order interactions preserved low-dimensional embedding space also explicitly represented feature interaction ﬁlters. consequently directly compute explicit high-order interactions hidden data. secondly develop exemplar learning techniques create small exemplars associated embedding represent entire data set. result exemplars perform fast information retrieval widely adopted classiﬁcation instead whole data speed computation gain insight characteristic features data. particularly important data massive. evaluated performance hope nonlinear extension using benchmarking mnist usps datasets. experimental results strongly support effectiveness efﬁciency methods data visualization data compression. dimensionality reduction data visualization methods mainly fall categories unsupervised approaches supervised approaches among supervised approaches mcml lmnn linear methods dt-mcml dt-nca dnn-knn deep nonlinear methods. method hope supervised embedding approach. unlike methods directly maps explicit high-order interaction features instead original input features either linear projection sigmoid transformations followed linear projection low-dimensional embedding space. simple feature mapping enables users identify important interaction features. linear projection hope viewed linear method applied high-order interaction features baseline counterparts linear embedding methods. sigmoid transformation followed linear projection viewed shallow nonlinear method. hope small number exemplars conveying high-order feature interactions synthesized. worth noting that hope proposed exemplar learning techniques similar intrinsically different stochastic neighbor compression speciﬁcally learning exemplars hope aims constructing embedding mapping optimizes objective function maximally collapsing classes instead neighbourhood component analysis snc. particular unlike joint exemplar learning technique hope coupled high-order embedding parameter learning powers exemplars created capture essential data variations bearing high-order interactions. addition results hope k-means based exemplar learning technique show that using powerful feature mapping generated hope sigmoid transformations optimization exemplars unnecessary actually motivations snc. high-order feature interactions studied building powerful generative models boltzmann machines autoencoders factorization machine fhim similar version hope linear projection used feature interactions classiﬁcation regression feature selection. none previous research conducted context data embedding visualization compression therefore different objective function parametric form. especially joint learning approach completely different previous methods. best knowledge work ﬁrst successful model input feature interactions order higher practical supervised embedding. given data points input feature vector last component absorbing bias terms class label labeled data points total number classes. hope intends high-order parametric embedding function maps high-dimensional data points low-dimensional space expect data points class stay tightly close data points different classes stay farther apart other. data visualization often unlike previous methods directly embed original input features hope assumes high-order feature interactions essential capturing structural knowledge learns similarity metric directly based feature interactions. suppose hope directly embeds o-order feature interactions i.e. products possible features straightforward approach explicitly calculate o-order feature interactions input feature vectors data points learn linear projection matrix h-dimensional space follows -way tensor however it’s expensive enumerate possible o-order feature interactions. example must deal -dimensional vector high-order features. speed computation factorize tensor follows constrained tensor factorization easily calculate linear embedding high-order interaction features high-dimensional data embarrassingly simple operation linear projection followed power operation. worth noting that factorization form reduces computational complexity signiﬁcantly also amenable explicitly model different order feature interactions data user-speciﬁed parameter hope method explicit high-order parametric form mapping essentially equivalent linear model explicit high-order feature interactions expanded shown above. compared supervised deep embedding methods complicated deep architectures linear projection method limited modeling power. fortunately simple signiﬁcantly enhance model’s expressive power. simply adding sigmoid transformations factorized model performing linear projection. call resulting model sigmoid hope s-hope s-th coordinate low-dimensional embedding vector computed bias term +e−x s-hope dramatically improves modeling power hope trivial modiﬁcation. shown experimental result section resulting shallow high-order parametric method even signiﬁcantly outperforms state-of-the-art deep learning models many layers supervised embedding clearly demonstrates representational power shallow models high-order feature interactions. given high-order feature mapping i-th data point perform supervised metric learning maximally collapsing classes following line research deploy stochastic neighbourhood criterion compute pairwise similarity data points transformed space. setting similarity data points measured probability qi|j. qj|i indicates chance data point assigns nearest neighbor low-dimensional embedding space. following work heavy-tailed t-distribution compute qj|i supervised embedding capabilities reducing overﬁtting creating tight clusters increasing class separation easing gradient optimization. formally stochastic neighborhood metric ﬁrst centers t-distribution computes density distribution follows. maximally collapsing classes parameters hope learned minimizing kullback-leibler divergence conditional probabilities qj|i computed embedding space ground-truth probabilities pj|i calculated based class labels training data. speciﬁcally pj|i pj|i formally objective function hope method follows indicator function. objective function essentially maximizes product pairwise probabilities data points class creates favorable tight clusters suitable supervised two-dimensional embedding limited accommodable space. conjugate gradient descent optimize objective function. although hope shares objective mcml dt-mcml learns shallow explicit high-order embedding function. contrary mcml aims linear mapping original input features dt-mcml targets complicated deep nonlinear function parametrized deep neural network. addition learning explicit high-order feature interactions data embedding also synthesize small exemplars exist training data compression fast information retrieval classiﬁcation efﬁciently performed embedding space dataset huge. given dataset formal descriptions introduced section learn exemplars class designated class labels ﬁxed user-speciﬁed free parameter denote exemplars propose approaches exemplar learning. ﬁrst straightforward relies supervised k-means. speciﬁc perform k-means training data identify exemplars class. powerful feature mapping shope learned data points class mapped compact point cloud two-dimensional space therefore simple exemplar learning approach achieve excellent performance; otherwise optimization exemplars needed. second approach based joint optimization. jointly learn high-order embedding parameters exemplars optimizing following objective function indexes training data points indexes exemplars denotes high-order embedding parameters pj|i calculated section qj|i calculated respect exemplars denotes high-order embedding function described equation please note that unlike symmetric probability distribution equation asymmetric qj|i computed using pairwise distances training data points exemplars. saves computations compared using original distribution equation derivative objective function respect exemplar follows derivatives model parameters easily calculated similarly. update synthetic exemplars embedding parameters hope deterministic expectationmaximization fashion using conjugate gradient descent. speciﬁc exemplars belonging class initialized ﬁrst exemplar learning approach. early phase joint optimization exemplars high-order embedding parameters learning process alternatively ﬁxes updating other. algorithm updates parameters simultaneously reaching convergence speciﬁed maximum number epochs. table error rates obtained two-dimensional representations produced different dimensionality reduction methods mnist dataset; result mcml unavailable unscalability. experiments section evaluate effectiveness hope comparing nine different baseline methods based upon handwritten digit datasets i.e. mnist usps. mnist dataset contains training test gray-level -dimensional images. usps data contains -pixel gray-level images training test. compare shallow linear hope linear counterparts including lmnn mcml; non-linear shallow s-hope uses four deep learning baselines including deep unsupervised models deep autoencoder pt-sne well deep supervised models i.e. dt-nca dt-mcml size exemplars experiments. used training data validation tune hyper-parameters order feature interactions number factors number high-order units batch size number iterations conjugate gradient descent mini-batch. hope mnist usps. s-hope mnist usps datasets hope s-hope. parameters baseline methods carefully tuned achieve best results. table presents test error rates -nearest neighbor classiﬁer -dimensional embedding generated hope baseline methods. results indicate linear hope error rate signiﬁcantly outperforms linear counterparts namely lmnn methods promisingly results table also suggest shallow method hope simple sigmoid transformations namely s-hope signiﬁcantly outperforms deep embedding models based deep neural networks terms accuracy obtained -dimensional embedding visualization. example error rate s-hope lower ones deep-ae pt-sne dt-nca dt-mcml methods. results clearly demonstrate representational efﬁciency power supervised shallow models high-order feature interactions. conﬁrm representation power hope extracted -dimensional features softmax layer learned well-known deep convolutional architecture currently holds the-state-of-the-art classiﬁcation performance softmax layers mnist. next s-hope based features generate embedding. promisingly achieve error contrast lmnn respectively produces test error rate error rate s-hope represents historically test error rate two-dimensional space mnist. observation implies even powerful deep learning networks modeling explicit high-order feature interactions achieve predictive accuracy outperform models without feature interactions. section evaluate approaches propose generating small number exemplars conveying high-order interactions represent large-scale data sets. table presents classiﬁcation errors -dimensional embeddings generated hope proposed exemplar learning. results suggest following first using k-means exemplar learning works well coupled s-hope demonstrates power feature mapping s-hope; hand coupled optimized exemplar learning hope s-hope work well. observations suggest that sophisticated exemplar learning method unnecessary powerful feature mapping function s-hope. figures present optimized exemplars created accurate models hope s-hope. ﬁgures indicate s-hope construct better representative exemplars hope. exemplars generated s-hope clearly captured global shape information. contrast exemplars created hope barely recognized human. part reason former achieved much lower error latter anreason hope s-hope different focus optimizing cost function depicted equation promisingly bottom subﬁgure clearly show exemplars capture important variations data skew style information different digits. intuitively exemplars based entire data thus summarize global essential information data set. contrast local knowledge contained individual digits small sample exploring massive data. figure two-dimensional embeddings mnist test data points constructed linear mcml dt-mcml hope-exemplar shope; empty circles optimized exemplars generated hope. figures shows test data embeddings minst. embeddings constructed respectively linear mcml dt-mcml hope optimized exemplars s-hope. s-hope produced best visualization collapsed data points class close other generated large separations class clusters. furthermore embeddings optimized exemplars created training located almost centers clusters suggest synthetic exemplars bear high-order feature interactions capturing essential data variations. also conducted experiments usps data set. table presents performances classiﬁcation two-dimensional embeddings constructed various dimensionality reduction techniques. results presented tables draw similar conclusions ones mnist data. visualization exemplars embeddings learned also show consistent behaviors hope models mnist data. included plotted images high-resolution supplementary material paper. s-hope exemplar learning speeds computational efﬁciency fast information retrieval classiﬁcation used experiments thousands times. mnist usps feature space test data prediction against respectively training data points -dimensional space training data points -dimensional space s-hope synthesized exemplars test data prediction exemplars -dimensional space even gets comparable much better performance original feature space computational speedup pronounced massive datasets. paper present supervised high-order parametric embedding approach data visualization compression. experimental results indicate modeling high-order feature interactions signiﬁcantly improve data visualization low-dimensional embedding space compared linear counterparts. surprisingly shallow method hope simple sigmoid transformations signiﬁcantly outperforms state-of-the-art supervised deep embedding models based deep neural networks even achieved historically test error rate two-dimensional space mnist. addition learned synthetic exemplars combination shallow high-order feature mapping speed classiﬁcation thousands times comparable much better performance original feature space. results clearly demonstrate high representational efﬁciency power supervised shallow models highorder feature interactions suggest performance representational efﬁciency supervised deep learning models might signiﬁcantly improved incorporating explicit high-order feature interactions. methods readily extended setting unsupervised learning need compute pairwise probabilities pj|i’s using high-dimensional feature", "year": 2016}