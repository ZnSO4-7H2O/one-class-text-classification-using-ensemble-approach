{"title": "Linear-Nonlinear-Poisson Neuron Networks Perform Bayesian Inference On  Boltzmann Machines", "tag": ["cs.AI", "cs.NE", "q-bio.NC", "stat.ML"], "abstract": "One conjecture in both deep learning and classical connectionist viewpoint is that the biological brain implements certain kinds of deep networks as its back-end. However, to our knowledge, a detailed correspondence has not yet been set up, which is important if we want to bridge between neuroscience and machine learning. Recent researches emphasized the biological plausibility of Linear-Nonlinear-Poisson (LNP) neuron model. We show that with neurally plausible settings, the whole network is capable of representing any Boltzmann machine and performing a semi-stochastic Bayesian inference algorithm lying between Gibbs sampling and variational inference.", "text": "conjecture deep learning classical connectionist viewpoint biological brain implements certain kinds deep networks backend. however knowledge detailed correspondence important want bridge neuroscience machine learning. recent researches emphasized biological plausibility linearnonlinear-poisson neuron model. show neurally plausible settings whole network capable representing boltzmann machine performing semi-stochastic bayesian inference algorithm lying gibbs sampling variational inference. classical connectionist viewpoint long inspired brain works invention perceptron parallel distributed processing approach modern deep learning unsupervised feature learning also assume connections biological brain certain kinds deep networks either probabilistic not. example properties visual area found comparable sparse autoencoder networks sparse coding learning algorithm originated directly neuroscience observations; also psychological phenomenon end-stopping observed sparse coding experiments addition architectural similarity neuroscience machine learning. example neuronal spike propagation correspond prediction inference tasks learned model; synaptic plasticity correspond parameter estimation given network structure; neuroplasticity correspond learning network structure together inventing hidden units network; axonal path ﬁnding help guidance signals regarded structural priors making learning network structure easier. reason helpful refer brain works dealing problems currently puzzling machine learning researchers. example training deep networks made practical mostly breakthrough deep learning starting etc.). success safe adapting network structure parameter estimation part effective enough reach non-trivial local optima. however since brain exactly layer-wise wonder choosing network structure along parameter estimation? want transfer neuroscience knowledge answering questions build bridge areas want ﬁrst ﬁgure exactly deep network brain representing. good starting point answering look single neuron computation. transfer information among them reverse engineer representation prediction inference algorithm main purpose paper. theoretical neuroscience single neuron models divided different levels detail level survey). detailed models quite realistic simpliﬁcation assumptions hold. effective simpliﬁcation holds different extent. example integrate-and-ﬁre models capture essential properties hodgkin-huxley model variant exponential integrateand-ﬁre neuron models found capable reproducing spike timing several types cortical neurons. community also investigating linear-nonlinear-poisson models long recent study analyzed models effectively reproduce ﬁring rates realistic neurons. based results start formally presenting model turn presenting semi-stochastic inference algorithm boltzmann machines derived combining gibbs sampling variational inference. make detailed matching model inference algorithm. since semi-stochastic inference algorithm explored learning community also show experiments illustrating computational property including stochastic convergence similarity variational inference. neuron model formalizes spike train generated neuron nonhomogeneous poisson point process whose rate function time depends input spike trains presynaptic neurons certain form short-term memory dependence. equations continuous time index whose unit millisecond. subscript index neuron question index presynaptic neurons neuron represents spike train spikes dirac functions located time steps rate function poisson point process. represents postsynaptic current particular synapse efﬁcacy wij. certain non-negative function certain time course. evidence poisson-like statistics cortical neurons found implies spike counts certain interval poisson distributed. dendritic summation. different viewpoints whether linearity holds true. phenomenon like mutual inhibition different dendritic shafts commonly known nonlinear summation could happen example) studies show dendritic spines presented linear summation true happens frequently excitatory mammalian cortex. addition survey neuron models summarized different computations performed dendritic processing notable examples multiplication summation. include generalizes multi-linear function i.e. linear respect arguments individually. notation {·}j∈m denote arguments. special multi-linear function that taking expectation respect certain argument expectation goes resulted formula simply replaces argument mean. useful mean-ﬁeld algorithms. dendritic summation relationship summed current ﬁring rate well researched in-vitro neuroscience experiments input current externally controlled. various frequency-current function also derived popular spike generation models leaky integrate-and-fire model overview) exponential integrate-andfire model spike response model realistic models hodgkin-huxley model particular formulated simpliﬁcation nonlinear mapping followed poisson point process random sampling. analytical study simulation show simpliﬁcation effective. since neural spikes refractory period least ﬁring rate exceed won’t spike happening practice ﬁring rate often even lower. addition discrete time steps resolution step seems quite ﬁne-grained perceptual tasks reasons discretizing time step reasonable. lecam’s theorem discrete counterpart nonhomogeneous poisson point process nonhomogeneous bernoulli process bernoulli probability discrete time step approximated integration rate function time period. denote discrete time steps whose unit millisecond. denote bernoulli probability denote spike indicator time corresponding discrete form substituting substituting functions discrete form similar deﬁnition ways proceed formulation. case function dirac function located close that convolution ignored discrete counterpart non-trivial multi-linear function reduces linear summation associativity convolution discrete counterpart reduces following assuming ﬁnite time course function functions resulted spiking neuron models dirac functions close. latter part paper focus case linear summation true holds) although multi-linearity generalize formulation high-order boltzmann machines. another observation exponential function exponential function assuming functions scale parameter becomes classic α-function used postsynaptic modeling. another issue whether linear summation multi-linear integration contains constant bias term bias term spiking response model. make latter part easier derivation later focus case bias. trivial turn case bias exists. function data realistic neuron models relationship frequency-current curve. according nonlinearity different spiking neuron models non-decreasing close zero towards left increase almost linearly certain interval. also since ﬁring rate upper bounds won’t increase unboundedly. reason considered modiﬁcation sigmoid function meet certain demands inference. example frequency-current curve shown fig. also another type nonlinearity. biological brain neurons computing parallel. thus want investigate case executed parallel vectors time forms markov chain order latter part focuses showing stochastic inference nature. section ﬁrst compare gibbs sampling variational inference boltzmann machines emphasizing architectural similarity present semi-stochastic inference algorithm combining them. notations section overlap last section. intended since want link quantities sections. collection random variables. possible values partition index denote visible variables hidden variables. lower case denote values e.g. paper boltzmann machines softmax units denote binary indicator. statement true otherwise. iujv four-dimensional tensor size matrix size normalization constant depends note family capacity original boltzmann machine ising model variational inference factorized approximation family often takes form mean-ﬁeld adopt family variational distributions version gibbs sampling accumulating empirical expectation sufﬁcient statistics online take examples gibbs sampling online data instances estimate distribution within variational approximation family getting marginal probability distribution random variable however also change sequence generated online estimated variational distribution biases towards whatever want. particular take calculated distribution proposal distribution used sample data example incrementally update variational distribution using that. updated variational distribution serve next input decaying used accumulating sufﬁcient statistics variational distribution estimated bernoulli case simply weighted recent examples simply means starting time accumulation. function non-negative effective time span. case constant decaying ratio used last accumulated sufﬁcient statistics function decays exponentially. choices function online updating scheme produces practice either online updating ﬁnite time course sequential inference every random variable carry updating steps pass recent copies next time step. step additional normalization needed details ignored clarity presentation. another also look algorithm random slowing-down stochastic approximation momentum version variational inference. updated variational distribution computed don’t immediately turn updated distribution. instead sample example incrementally update original variational distribution. expectation example updated variational distribution. reason always modify algorithm make bias want. experiments want remove bias subtract biu/|m wiujv wiuj¯v resolve issue propose event-network. ﬁrst split every sampling step namely allocate neurons take care parts three steps units resulted network longer corresponds random variables probabilistic events. this guarantee approximately true since weights wiujv’s compatible other. whether results meaningful inference algorithms needs investigation. show veriﬁcation experiment part. resolve issue network containing neurons excitatory inhibitory outgoing synapses duplicate neuron incoming synapses original neural efﬁcacy. take positive outgoing synapses splitting take negative ones. last modiﬁcation network exactly invariant variational inference approximately true semi-stochastic inference. show experimental veriﬁcation modiﬁcation well. resolving issues denote re-index everything resulted algorithm exactly executed parallel. parallelism issue semi-stochastic inference. noticed adjacent random variables boltzmann machine updated parallel gibbs sampling sample sequence converge right distribution. issue also raised often recently variational inference ﬁxed point iteration. converge answer updated parallel observed experimentally variational parameters converges ﬁxed point severe oscillation. whether slowing version alleviates problem requires investigation. section focus illustrating semi-stochastic inference well modiﬁcations section valid bayesian inference algorithms behave similar variational inference. experiments normalized summation domain inferences carried synchronized full parallelism. avoid complication learning deep boltzmann machines take learned boltzmann machine code since focus inference boltzmann machine take learned model back-propagation. boltzmann machine consists three layers ﬁrst effect modiﬁcations section show results semi-stochastic inference them. note variational inference exactly invariant modiﬁcations hence need verify them. following abbreviations part experiment similar reconstruction experiment following plug image input layer apply inference algorithm frozen activations topmost layer round topmost layer observed layers hidden infer back read image activations input layer. randomly initialized iterations. semi-stochastic inference steps activations taken averages last iterations variational inference simply last activation neuron. semi-stochastic inference random convergence activations approach ﬁxed point random perturbation reconstruction examples shown fig. algorithm good cases also note randomness even identical input image identical initialization semi-stochastic inference different results different trials. ﬁgure showed sample trajectory activations step varo semio variables. trajectory neuron iterations. activations variational inference converge stable solution oscillation convergence semi-stochastic inference moving average converges similar values random perturbation. figure scatter plot mean trajectories corresponding converged activation variational inference extracted trajectories randomly chosen trials. also observed mean close extreme values standard variance smaller found ﬁgure clearly ﬁgure means semi-stochastic inference converges randomly variational inference solution conﬁdent variables less random perturbation. finally investigated well splitting section preserves identity relationships expected semiu. particular neurons resulted splitting sampling resolving issue summed activation close neurons resulted splitting neuron resolving issue activation equal. figure illustrated three samples histogram random trajectories given ﬁgure figure sample trajectory semiu. plot shows four neurons resulted splits single neuron. thick dash lines neurons positive outgoing weights. thin solid lines corresponding neurons negative outgoing weights. blue lines neurons splitting sampling two. green line average blue red. ideally lines color equal green lines means trajectories semio converged figure statistics algorithms. activation values varo hidden variables. standard deviation mean trajectories semio. histogram standard deviation. histogram standard deviation. pointed stochastic inference nature neuron models serve interpretation neural coding. stochastic convergence seems reasonable e.g. ambiguous images perception switch back forth plausible explanations. many behavioral experiments showing statistical optimality perception learning many calls neuronal modeling achieves optimality arguably mode-seeking nature variational inference make natural interpreting perception. since boltzmann machines hidden variables universal approximators modeling knowledge representation safe world approximately binarized. show particular choices weights neurons capable representing boltzmann machine carry inference. question network recurrent neural network general. real biological neural networks learned taking boltzmann machine representation undergoing discriminative training reinforcement learning optimize performance inference directly yields model necessarily convey consistent probabilistic semantics limit. particular form nonlinearity biological neurons also result optimization. another issue average activity biological neurons. certain form sparse coding inference optimizing inference procedure yield degenerate solution. case likelihood-based learning contrastive divergence still applicable reﬁnement. want thank geoffrey hinton mikhail belkin deliang wang brian kulis helpful discussion ohio supercomputer center providing computation resource.", "year": 2012}