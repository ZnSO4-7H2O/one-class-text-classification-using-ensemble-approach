{"title": "Deep clustering: Discriminative embeddings for segmentation and  separation", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "We address the problem of acoustic source separation in a deep learning framework we call \"deep clustering.\" Rather than directly estimating signals or masking functions, we train a deep network to produce spectrogram embeddings that are discriminative for partition labels given in training data. Previous deep network approaches provide great advantages in terms of learning power and speed, but previously it has been unclear how to use them to separate signals in a class-independent way. In contrast, spectral clustering approaches are flexible with respect to the classes and number of items to be segmented, but it has been unclear how to leverage the learning power and speed of deep networks. To obtain the best of both worlds, we use an objective function that to train embeddings that yield a low-rank approximation to an ideal pairwise affinity matrix, in a class-independent way. This avoids the high cost of spectral factorization and instead produces compact clusters that are amenable to simple clustering methods. The segmentations are therefore implicitly encoded in the embeddings, and can be \"decoded\" by clustering. Preliminary experiments show that the proposed method can separate speech: when trained on spectrogram features containing mixtures of two speakers, and tested on mixtures of a held-out set of speakers, it can infer masking functions that improve signal quality by around 6dB. We show that the model can generalize to three-speaker mixtures despite training only on two-speaker mixtures. The framework can be used without class labels, and therefore has the potential to be trained on a diverse set of sound types, and to generalize to novel sources. We hope that future work will lead to segmentation of arbitrary sounds, with extensions to microphone array methods as well as image segmentation and other domains.", "text": "address problem acoustic source separation deep learning framework call deep clustering. rather directly estimating signals masking functions train deep network produce spectrogram embeddings discriminative partition labels given training data. previous deep network approaches provide great advantages terms learning power speed previously unclear separate signals classindependent way. contrast spectral clustering approaches ﬂexible respect classes number items segmented unclear leverage learning power speed deep networks. obtain best worlds objective function train embeddings yield low-rank approximation ideal pairwise afﬁnity matrix classindependent way. avoids high cost spectral factorization instead produces compact clusters amenable simple clustering methods. segmentations therefore implicitly encoded embeddings decoded clustering. preliminary experiments show proposed method separate speech trained spectrogram features containing mixtures speakers tested mixtures held-out speakers infer masking functions improve signal quality around show model generalize three-speaker mixtures despite training twospeaker mixtures. framework used without class labels therefore potential trained diverse sound types generalize novel sources. hope future work lead segmentation arbitrary sounds extensions microphone array methods well image segmentation domains. real world perception often confronted problem selectively attending objects whose features intermingled another incoming sensory signal. computer vision problem scene analysis partition image video regions attributed visible objects present scene. audio corresponding problem known auditory scene analysis seeks identify components audio signals corresponding individual sound sources mixture signal. problems approached segmentation problems formulate elements signal indexed features carries information part signal. images elements typically deﬁned spatially terms pixels whereas audio signals deﬁned terms time-frequency coordinates. segmentation problem solved segmenting elements groups partitions example assigning group label element. note although clustering methods applied segmentation problems segmentation problem technically different clustering classically formulated domain-independent problem based simple objective functions deﬁned pairwise point relations whereas partitioning depend complex processing whole input task objective arbitrarily deﬁned training examples given segment labels. segmentation problems broadly categorized class-based segmentation problems goal learn training class labels label known object classes versus general partition-based segmentation problems task learn labels partitions without requiring object class labels segment input. solving partition-based problem advantage unknown objects could segmented. paper propose partitionbased approach learns embeddings input elements correct labeling determined simple clustering methods. focus single-channel audio domain although methods applicable domains images multi-channel audio. motivation segmenting domain shall describe later using segmentation mask extract parts target signals corrupted signals. since class-based approaches relatively straightforward tremendously successful task ﬁrst brieﬂy mention general approach. class based vision models hierarchical classiﬁcation scheme trained estimate class label pixel superpixel region. audio domain single-channel speech separation methods example segment time-frequency elements spectrogram regions dominated target speaker either based classiﬁers generative models recent years success deep neural networks classiﬁcation problems naturally inspired class-based segmentation problems proven successful. however class-based approaches important limitations. first course assumed task labeling known classes fundamentally address general problem real world signals large number possible classes many objects well-deﬁned class. also clear directly apply current class-based approaches general problem. class-based deep network models separating sources require explicitly representing output classes object instances output nodes leads complexities general case. although generative model-based methods theory ﬂexible respect number model types instances training inference typically cannot scale computationally potentially larger problem posed general segmentation tasks. contrast humans seem solve partition-based problem since apparently segment well even novel objects sounds. observation basis gestalt theories perception attempt explain perceptual grouping terms features proximity similarity partition-based segmentation task closely related follows tradition work image segmentation audio separation. application perceptual grouping theory audio segmentation generally known computational auditory scene analysis spectral clustering active area machine learning research application image audio segmentation. uses local afﬁnity measures features elements signal optimizes various objective functions using spectral decomposition normalized afﬁnity matrix contrast conventional central clustering algorithms k-means spectral clustering advantage require points tightly clustered around central prototype clusters arbitrary topology provided form connected sub-graph. local form pairwise kernel functions used difﬁcult spectral clustering problems afﬁnity matrix sparse block-diagonal structure directly amenable central clustering works well block diagonal afﬁnity structure dense. powerful computationally expensive eigenspace transformation step spectral clustering addresses this effect fattening block structure connected components become dense blocks prior central clustering although afﬁnity-based methods originally unsupervised inference methods multiple-kernel learning methods later introduced train weights used combine separate afﬁnity measures. allows consider using partition-based segmentation tasks partition labels available without requiring speciﬁc class labels. applied speech separation including variety complex features developed implement various auditory scene analysis grouping principles similarity onset/offset pitch spectral envelope afﬁnities time-frequency regions spectrogram. input features included dual pitch-tracking model order improve upon relative simplicity kernel-based features expense generality. rather using specially designed features relying strength spectral clustering framework clusters propose deep learning derive embedding features make segmentation problem amenable simple computationally efﬁcient clustering algorithms k-means using partition-based training approach. learned feature transformations known embeddings recently gaining signiﬁcant interest many ﬁelds. unsupervised embeddings obtained auto-associative deep networks used relatively simple clustering algorithms recently shown outperform spectral clustering methods cases. embeddings trained using pairwise metric learning wordvec using neighborhood-based partition labels also shown interesting invariance properties. present objective function minimizes distances embeddings elements within partition maximizing distances embeddings elements different partitions. appears appropriate criterion central clustering methods. proposed embedding approach attractive property partitions permutations represented implicitly using ﬁxed-dimensional output network. experiments described show proposed method separate speech using speaker-independent model open speakers test time. derive partition labels mixing signals together observing spectral dominance patterns. training database mixtures speakers trained show without modiﬁcation model shows promising ability separate three-speaker mixtures despite training two-speaker mixtures. although results preliminary hope work leads methods achieve class-independent segmentation arbitrary sounds additional application image segmentation domains. deﬁne input signal image time-domain waveform feature vector indexed element case images typically superpixel index vector-valued features superpixel; case audio signals time-frequency index indexes frames signal frequencies value complex spectrogram corresponding timefrequency bin. assume exists reasonable partition elements regions would like example process features separately region. case audio source separation example regions could deﬁned sets time-frequency bins source dominates estimating partition would enable build time-frequency masks applied leading time-frequency representations inverted obtain isolated sources. estimate partition seek k-dimensional embedding rn×k parameterized performing simple clustering embedding space likely lead partition close target one. work based deep neural network global function entire input signal thus transformation take account global properties input embedding considered permutationcardinality-independent encoding network’s estimate {vnk} value k-th dimension embedding element omit dependency simplify notations. partition-based training requires reference label indicator {ync} mapping element arbitrary partition classes element partition training objective seek embeddings enable accurate clustering according partition labels. this need convenient expression invariant number permutations partition labels training example next. objective minimization vector partition sizes yj}|. fact |vn| intuitively objective pushes inner product partition different partitions. alternately pulls squared distance elements within partition preventing embeddings trivially collapsing point. note ﬁrst term objective function minimized k-means function cluster assignments context second term constant. objective reasonably tries lower k-means score labeled cluster assignments training time. formulation related spectral clustering follows. deﬁne ideal afﬁnity matrix block diagonal permutation inner-product kernel afﬁnity matrix. objective becomes measures deviation model’s afﬁnity matrix ideal afﬁnity. note although function ostensibly sums pairs data points low-rank nature objective leads efﬁcient implementation deﬁning diag avoids explicitly constructing afﬁnity matrix. practice orders magnitude greater leading signiﬁcant speedup. optimize deep network typically need ﬁrst-order methods. fortunately derivatives objective function respect also efﬁciently obtained low-rank structure low-rank formulation also relates spectral clustering latter typically requires nystr¨om low-rank approximation afﬁnity matrix efﬁciency singular value decomposition matrix substituted much expensive eigenvalue decomposition normalized afﬁnity matrix. rather following spectral clustering making low-rank approximation full-rank model method thought directly optimizing low-rank afﬁnity matrix processing efﬁcient parameters tuned low-rank structure. test time compute embeddings test signal cluster rows example using k-means. also alternately perform spectral-clustering style dimensionality reduction clustering starting singular value decomposition normalized sorted decreasing eigenvalue clustering normalized rows matrix principal left singular vectors i’th given general considered difﬁcult problem separating speech speech signals particularly challenging sources belong class share similar characteristics. mixtures involving speech gender speakers difﬁcult since pitch voice range. consider mixtures speakers three speakers however method limited number sources handle vocabulary discourse style speakers. investigate effectiveness proposed model built dataset speech mixtures based wall street journal corpus leading challenging task existing datasets. existing datasets limited evaluation model because example speech separation challenge contains mixture speakers limited vocabulary insufﬁcient training data. sisec challenge limited size designed evaluation multi-channel separation easier single-channel separation general. training consisting hours two-speaker mixtures generated randomly selecting utterances different speakers training si_tr_s mixing various signal-to-noise ratios also designed training subsets whole training considered balance mixture genders used mixture female speakers hours cross validation generated similarly training used optimize tuning parameters evaluate source separation performance closed speaker experiments hours evaluation data generated similarly using utterances sixteen speakers development si_dt_ evaluation si_et_ based different speakers training closed speaker sets note many existing speech separation methods cannot handle open speaker problem without special adaptation procedures generally require knowledge speakers evaluation. evaluation data also created utterances three-speaker mixtures closed open speaker advanced setup. data downsampled processing reduce computational memory costs. input features short-time fourier spectral magnitudes mixture speech computed window length window shift square root hann window. ensure local coherency mixture speech segmented length frames roughly length word speech processed separately output embedding based proposed model. ideal binary mask used build target training network. ideal binary mask gives ownership time-frequency source whose magnitude maximum among sources bin. mask values assigned active otherwise making ideal afﬁnity matrix mixture. avoid problems silence regions separation binary weight timefrequency used training process retaining bins source’s magnitude greater ratio source’s maximum magnitude. intuitively binary weight guides neural network ignore bins important sources. networks proposed model trained given input ideal afﬁnity matrix network structure used experiments bi-directional long short-term memory layers followed feedforward layer. blstm layer hidden cells feedforward layer corresponds embedding dimension stochastic gradient descent momentum ﬁxed learning rate used training. updating step gaussian noise zero mean variance added weight. prepared several networks used speech separation experiments using different embedding dimensions addition different activation functions explored form embedding different ranges vnk. embedding dimension weights corresponding network initialized randomly scratch according normal distribution zero mean variance tanh activation whole training set. experiments different activation different training subsets network initialized tanh activation whole training set. implementation test stage speech separation performed constructing time-domain speech signal based time-frequency masks speaker. time-frequency masks source speaker obtained clustering vectors embedding outputted proposed model segment similarly training stage. number clusters corresponds number speakers mixture. evaluated various types clustering methods k-means whole utterance concatenating embeddings segments; k-means clustering within segment; spectral clustering within segment. withinsegment clusterings needs solve permutation problem clusters guaranteed consistent across segments. cases report oracle permutation results upper bound performance. interesting property proposed model potentially generalize case three-speaker mixtures without changing training procedure section verify this speech separation experiments three-speaker mixtures conducted using network trained speaker mixtures simply changing clustering step clusters. course training network including mixtures involving speakers improve performance further shall method surprisingly well even without retraining. standard speech separation method supervised sparse non-negative matrix factorization used baseline snmf stand chance separating speakers male-female mixtures using concatenation bases trained separately speech speakers gender would make sense case same-gender mixtures. give snmf best possible advantage oracle test time give basis functions trained actual speaker mixture. speaker bases learned clean training utterances speaker. magnitude spectra consecutive frames left context used input features. test time basis functions speakers test mixture concatenated corresponding activations computed mixture. estimated models speaker used build wiener-ﬁlter like mask applied mixture corresponding signals reconstructed inverse stft. experiment performance evaluated terms averaged signal-to-distortion ratio using bss_eval toolbox initial averaged mixtures speaker mixtures three speaker mixtures. shown table oracle non-oracle clustering methods proposed system signiﬁcantly outperform oracle baseline even though oracle strong model important advantage knowing speaker identity speaker-dependent models. proposed system open speaker performance similar closed speaker results indicating system generalize well unknown speakers without explicit adaptation methods. different clustering methods oracle k-means outperforms oracle spectral clustering showing embedding represents centralized clusters. fair call spectral clustering using outer product kernel instead local kernel function gaussian commonly used spectral clustering. however gaussian kernel could used computational complexity. also note oracle clustering method experiment resolves permutation speakers segment. dataset utterance usually contains segments permutation search space relatively small utterance. hence problem easy solution explored future work. non-oracle experiments whole utterance clustering also performs relatively well compared baseline. given fact system trained individual segments effectiveness whole utterance clustering suggests network learns features globally important pitch timbre etc. table system completely fails either optimization current network architecture fails embedding fundamentally requires dimensions. performance similar showing system operate wide range parameter values. arbitrarily used tanh networks experiments tanh network larger embedding space logistic network. however table show retrospect logistic network performs slightly better tanh one. table since female male mixture intrinsically easier segmentation problem performance mixture female male signiﬁcantly better gender mixtures situations. mentioned section random selection speaker would also factor large gap. balanced training data system better performance gender separation sacriﬁce performance different gender mixture. focus female mixtures performance still better. figure shows example embeddings different mixtures embedding dimensions plotted time-frequency order show sensitive different aspects signal. table proposed system also separate mixture three speakers even though trained two-speaker mixtures. discussed previous sections unlike many separation algorithms deep clustering naturally scale sources thus make suitable many real world tasks number sources available ﬁxed. figure shows figure example three-speaker separation. spectrogram input mixture. middle ideal binary mask three speakers. dark blue shows silence part mixture. bottom output mask proposed system trained two-speaker mixtures. example separation three speaker mixture open speaker case. note also experiments mixtures three ﬁxed speakers training testing data improvement proposed system deep clustering evaluated variety conditions parameter regimes challenging speech separation problem. since preliminary results expect reﬁnement model lead signiﬁcant improvement. example combining clustering step embedding blstm network using deep unfolding technique separation could jointly trained embedding lead potential better result. also work blstm network relatively uniform structure. alternative architectures different time frequency dependencies deep convolutional neural networks hierarchical recursive embedding networks could also helpful terms learning regularization. finally scaling training databases disparate audio types well applications domains image segmentation would prime candidates future work. references bregman auditory scene analysis perceptual organization sound press darwin carlyon auditory grouping hearing moore elsevier farabet couprie najman lecun learning hierarchical features scene labeling cooke modelling auditory processing organisation ph.d. thesis univ. shefﬁeld ellis prediction-driven computational auditory scene analysis ph.d. thesis malik normalized cuts image segmentation ieee trans. pami vol. vincent araki theis nolte boﬁll sawada ozerov gowreesunker lutter duong signal separation evaluation campaign achievements remaining challenges signal processing vol.", "year": 2015}