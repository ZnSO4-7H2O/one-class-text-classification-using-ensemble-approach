{"title": "Attention for Fine-Grained Categorization", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "This paper presents experiments extending the work of Ba et al. (2014) on recurrent neural models for attention into less constrained visual environments, specifically fine-grained categorization on the Stanford Dogs data set. In this work we use an RNN of the same structure but substitute a more powerful visual network and perform large-scale pre-training of the visual network outside of the attention RNN. Most work in attention models to date focuses on tasks with toy or more constrained visual environments, whereas we present results for fine-grained categorization better than the state-of-the-art GoogLeNet classification model. We show that our model learns to direct high resolution attention to the most discriminative regions without any spatial supervision such as bounding boxes, and it is able to discriminate fine-grained dog breeds moderately well even when given only an initial low-resolution context image and narrow, inexpensive glimpses at faces and fur patterns. This and similar attention models have the major advantage of being trained end-to-end, as opposed to other current detection and recognition pipelines with hand-engineered components where information is lost. While our model is state-of-the-art, further work is needed to fully leverage the sequential input.", "text": "paper presents experiments extending work recurrent neural models attention less constrained visual environments specifically ﬁne-grained categorization stanford dogs data set. work structure substitute powerful visual network perform large-scale pre-training visual network outside attention rnn. work attention models date focuses tasks constrained visual environments whereas present results ﬁne-grained categorization better state-of-the-art googlenet classiﬁcation model. show model learns direct high resolution attention discriminative regions without spatial supervision bounding boxes able discriminate ﬁne-grained breeds moderately well even given initial low-resolution context image narrow inexpensive glimpses faces patterns. similar attention models major advantage trained end-to-end opposed current detection recognition pipelines hand-engineered components information lost. model state-of-the-art work needed fully leverage sequential input. work presents experiments extending work recurrent neural models attention less constrained visual environments speciﬁcally ﬁne-grained categorization stanford dogs data set. tackles challenging problem sequence prediction simpliﬁed visual settings using recurrent model attention similar mnih complementary work addressing simpler task classiﬁcation visual environment signiﬁcant clutter occlusion variations lighting pose difﬁcult class discrimination task. previous work learned visual attention models tackled number computer vision problems demonstrated beneﬁts various attention mechanisms though work focused constrained environments detecting simple shapes tasks based mnist digits vision-control game catch expression classiﬁcation aligned faces detection frontal faces tracking hockey players gesture recognition recently papers explored attention mechanisms complex visual input gonzalez-garcia presented attention-like model detection chairs people cars doors tables images applied different neural network visual attention models task caption generation mscoco images recent work object detection ﬁne-grained categorization candidate object part proposals precursor expensive classiﬁcation stage. systems proposals generated bottom-up segmentation girshick separate neural network szegedy work pipelines able process candidate regions classiﬁcation higher resolution save processing focusing restricted regions. drawbacks systems consist independent processing steps engineered connections information lost. example either aggregate evidence independent candidate image regions technique. also lose rich information candidate proposal classiﬁcation parts pipeline. contrast models others similar structure trained end-to-end incorporate information across observations expected yield better ﬁnal result. figure three classes dogs data difﬁcult tell apart high intra-class variability high similarity across classes. lines show class boundaries; classes eskimo left malamute center siberian husky right. classes stanford dogs model performs worst siberian husky. apply visual attention model stanford dogs ﬁne-grained categorization task choosing perform task without using provided bounding boxes training testing. amounts learning simultaneously localize classify objects within scenes despite difﬁcult class boundaries large variations pose lighting varying cluttered backgrounds occlusion fine-grained categorization natural proving ground attention-based models. performing classiﬁcation sub-category level e.g. german shepherd versus poodle background often uncorrelated class acts distraction primary task. result several hand-crafted vision pipelines provided bounding boxes isolate object interest perform segmentation object background e.g. parkhi chai angelova attention models could address challenge learning focus processing discriminatory power parts image relevant task without requiring expensive hand-labeled bounding boxes. addition ignoring distractors image good attention model could learn focus processing power speciﬁc features objects help tell apart example face ears particular patterns dogs. future versions model could potentially also choose scale examine details. structure model nearly presented differences; give overview model describe ways model differs. refer reader work in-depth description network choices training procedure. figure shows structure model. system whole takes input image size outputs n-way classiﬁcation scores using softmax classiﬁer similar task ﬁnding digits digit addition tasks model recurrent neural network steps correlate glimpses input image. step model receives column coordinates describe point input image. network extracts multi-resolution patch input image coordinates passes pixels fullyfigure diagram model. grayed-out boxes denote resolutions use; experiments context always low-resolution patch glimpse combination low- medium- high-resolution patches. connected layers combine activations previous glimpse step either outputs coordinates next glimpse ﬁnal classiﬁcation structure glimpse images shown figure glimpse multi-resolution image created extracting patches varying sizes resizing concatenating side-by-side. goal emulate foveal structure sharpest part image center lower resolution toward periphery. shows glimpses -resolution model bottom -resolution model. high-resolution patch extracted square ﬁxed size given image medium-resolution patch square twice length side high-resolution patch lowresolution patch twice length medium-resolution patch side. example high resolution patch mediumlow-resolution patches respectively. extraction extends edge image pixels random noise. figure shows composite images helpful visualization understand pixels network sees aggregate though network presented form; images generated glimpse pixels displaying pixel highest-resolution pixel available glimpse pixels captured glimpses ﬁlled noise. model begins processing context image square low-resolution patch input image size low-resolution glimpse patch also resized location context image chosen randomly training centered inference captures central square image. context image used layer produce ﬁrst glimpse location inﬂuence selection subsequent glimpse locations recurrent connections layers along deck model. however double-decker structure prevents context image pathway classiﬁer except coordinates. discussion design choice. training layers produce coordinates trained backpropagation connection layer four major differences system classiﬁcation-type models first wide variation image size across data however size objects scales image size time. robust input image size multi-resolution patches sized relative input image. experiments side highfigure visualizations -resolution -resolution glimpses image validation learned ﬁxation points. glimpse images order bottom diagram corresponds second glimpse. composite image created three glimpses. context image shown always resolution size low-resolution glimpse patches shown second vanilla instead lstm glimpse consist nodes fully-connected third instead element-wise multiplying outputs glimpse visual core gimage gloc model linearly combines concatenating outputs passing fully-connected layer. future experiments incorporate variations. ﬁnal largest difference replace visual glimpse network gimage described powerful visual core based googlenet model ilsvrc classiﬁcation challenge. start googlenet model pre-trained imagenet -way task subset ilsvrc training data ﬁne-tune visual network outside model ilsvrc images using random multi-scale patches input targeting imagenet -way classiﬁcation labels. stage training replicate visual model input scale yielding towers share parameters join outputs different combinations depth-concatenating layers towers jointly trained back-propagating loss multiple -way softmax classiﬁers shown ﬁgure. multi-headed training model ensures tower remains independently relevant even another tower informative. found taken independently lowest-resolution patch typically yields best results learning might rely solely otherwise. initially used truncated version googlenet visual core full model designed inputs applied inputs subsampling pooling layers cause ﬁnal output small purposes. remedy this initially chopped last inception layers skipping convolutional layers average-pooling layer. later discovered large gain performance changing stride ﬁrst convolution network restoring visual network full depth. stride- convolutions historically used early deep learning works however krizhevsky later popularized strides early layers efﬁciency reasons. experiments changed stride ﬁne-tuning phase starting pre-trained model stride two. although obvious changing stride pre-trained model work incorporated training attention model remove training heads take output depth concatenation multiple towers glimpse input shown figure work hold visual core’s parameters ﬁxed attention model training. pre-train three resolutions used vary across experiments subsets resolutions used training regime experiments slightly different testing. stages training attention experimental baselines subset ilsvrc training removed stanford dogs test images well dogs images validation. refer experimental section de-duped ilsvrc data set. pre-training de-duped data instead original make small difference performance drop accuracy full googlenet baseline model trained de-duped data relative trained full ilsvrc training set. finally worth noting ﬁne-tuning visual core stanford dogs training since parameters visual core held ﬁxed training dogs means powerful visual processing component trained ﬁnal task. performed experiment visual core ﬁne-tuned stanford dogs training data increase performance demonstrating ﬁnal model fairly robust pre-training ﬁne-tuning procedure. trained evaluated model stanford dogs ﬁne-grained categorization data task categorize test images containing breeds. training consists images class test images unevenly distributed across classes averaging test images class. training test sets include bounding boxes provide tight crop around target best results know literature bounding boxes training testing neither training testing boxes. follow practice common literature augmenting training reﬂecting images along vertical axis. model starts full images without cropping reshaping scaling. performed experiments chose hyperparameters using training/validation split stanford dogs training set. selected hyperparameters using validation trained full training performed ﬁnal evaluation dogs test selected hyperparameters. background images highly correlated class label method using bounding boxes needs localize object interest order classify nice task explore attention model couple ways model context image order focus glimpses object interest intuit parts image model table results stanford dogs model googlenet baselines previous state-of-the-art results measured mean accuracy percentage described chai googlenet baseline models pre-trained de-duped ilsvrc training ﬁne-tuned stanford dogs training set. results marked star indicate tight ground truth bounding boxes around dogs training testing. observe make prediction. many natural image object classiﬁcation data sets imagenet signal surrounding context mixed object classiﬁcation size data also suitable deep learning method available ﬁne-grained data sets though caltech-ucsd birds similar size training images categories. lastly remains difﬁcult data large amount intra-class variation similarity across classes large variation pose lighting background table shows mean accuracy different combinations resolutions number glimpses. experimented high medium resolutions individually medium high combined three resolutions three glimpses. table also shows previously published results data set. versions model medium resolution patches outperform state-of-the-art results. using small high-resolution patches image model matches best published result. previously published results shown ground truth bounding boxes training testing model not. high-resolution single-glimpse model lowest performance visualizations selected glimpse locations show learning take good ﬁrst action fairly representative behavior model frequently chooses patch near dog’s face. make informative ﬁrst glimpse often able correctly classify single sample. important note model automatically learned focus discriminative features faces without ever receiving spatial clues bounding boxes. pretty remarkable bounding boxes usually required good performance task obtaining bounding boxes scale difﬁcult expensive. also raises possibility attention models providing signal detection without labeled bounding boxes. ﬁgure shows images classiﬁed correctly green assigned incorrect label. pathological pattern noticed dogs image often chooses patch halfway likely regression-style output glimpse coordinates encourage model output average predicted targets. randomly chosen examples shown appendix figure comparisons results using deep learning give good sense strength model however. last couple years deep nets winning ilsvrc classiﬁcation challenge signiﬁcant margin expected deep neural would outperform existing results. address also evaluated googlenet full image without attention experimented caltech-ucsd birds approach apply nicely. missing results entries fgcomp ﬁne-grained competition. highperforming entries deep learning models dogs category though knowledge models published. cognitivevision cafenet scored challenge respectively using bounding boxes training testing. challenge training stanford dogs test independent class labels made public evaluation server longer running. such cannot compare directly numbers told anecdotally scores fgcomp challenge tend absolute lower stanford dogs test set. figure selected examples high-resolution one-glimpse model validation set. system takes original image subsamples create context image uses context select single high resolution glimpse. outline high-resolution glimpse also shown full image context. rows surrounded green correctly classiﬁed indicates classiﬁcation error. note bottom example misclassiﬁed model still learned look face despite clutter occlusion uncommon pose. figure random selection results. rnn. experimented baseline versions googlenet full low-resolution. full googlenet uses architecture szegedy trained tested padded versions full images. strongest baselines. low-resolution googlenet architecture visual core also takes inputs uses stride ﬁrst convolution. three input scales like visual core instead takes full image padding centers lowresolution googlenet input close resolution low-resolution foveal input attention model. versions pre-trained using de-duped ilsvrc data fullyconnected layer softmax layers resized reinitialized full model trained convergence dogs training set. addition mirroring applied training brightness color transformations also applied training images baselines. unlike szegedy comparison average across patches different googlenet models. three-resolution one-glimpse attention model reached compared full googlenet model version attention model gets three inputs classiﬁcation context image choosing position glimpse. compared input googlenet perform better pixels. doesn’t reduce amount computation however attention model uses stride ﬁrst convolutional layer compared full googlenet. three glimpses performance increases slightly though almost triples number pixels input. medium high resolution inputs attention model nearing performance full googlenet pixels. comparison low-resolution googlenet resolution attention model shows increase convolution stride account strong performance models share visual network similar resolution inputs indicates large difference performance network choosing informative part image input classiﬁcation. lastly interesting compare one- two- threeglimpse results different resolution inputs. using three resolutions performance increases slightly glimpse three. likely cause three-resolution glimpse contains enough information full image information gained additional glimpses minimal. additional piece evidence performance order lowmedium-only resolution models swap going glimpses. high resolution-only glimpse experiment test this; results three glimpses respectively demonstrating amount information glimpse restricted model beneﬁts several glimpses. however improvement increased number glimpses ﬂattens quickly indicating model limited capacity make three glimpses. hypothesis passing enough information early glimpses along classiﬁcation layer. future work explore using lstm cells increasing recurrent capacity network. references angelova anelia shenghuo. efﬁcient object detection segmentation ﬁne-grained recognition. cvpr’ ieee conference computer vision pattern recognition june ./cvpr... bazzani loris freitas nando larochelle hugo murino vittorio ting jo-anne. learning attentional policies object tracking recognition video deep networks. icml’ proceedings international conference machine learning chai yuning lempitsky victor zisserman andrew. symbiotic segmentation part localization fine-grained categorization. iccv’ ieee international conference computer vision girshick ross donahue jeff darrell trevor malik jitendra. rich feature hierarchies accurate object detection semantic segmentation. proceedings ieee conference computer vision pattern recognition gonzalez-garcia abel vezhnevets alexander ferrari vittorio. active search strategy efﬁcient object detection. corr abs/. http//arxiv.org/abs/ khosla aditya jayadevaprakash nityananda bangpeng fei-fei novel dataset fine-grained image categorization. first workshop fine-grained visual categorization cvpr’ ieee conference computer vision pattern recognition june krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems larochelle hugo hinton geoffrey learning combine foveal glimpses third-order boltzmann machine. nips’ advances neural information processing systems curran associates inc. tsung-yi maire michael belongie serge hays james perona pietro ramanan deva doll´ar piotr zitnick lawrence. microsoft coco common objects context. corr abs/. http//arxiv.org/abs/.. russakovsky olga deng krause jonathan satheesh sanjeev sean huang zhiheng karpathy andrej khosla aditya bernstein michael berg alexander fei-fei imagenet large scale visual recognition challenge szegedy christian yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich andrew. going deeper convolutions. corr abs/. http//arxiv.org/abs/.. catherine branson steve welinder peter perona pietro belongie serge. caltechucsd birds-- dataset. technical report cns-tr-- california institute technology xiao jianxiong hays james ehinger krista oliva aude torralba antonio. computer vision patdatabase large-scale scene recognition abbey zoo. tern recognition http//dblp.uni-trier.de/db/conf/cvpr/ cvpr.htmlxiaoheot. kelvin jimmy kiros ryan kyunghyun courville aaron salakhutdinov ruslan zemel richard bengio yoshua. show attend tell neural image caption generation visual attention. corr arxiv. http//arxiv.org/abs/ yang shulin liefeng wang shapiro linda unsupervised template learning fine-grained object recognition. nips’ advances neural information processing systems december zheng zemel richard zhang yu-jin larochelle hugo. neural autoregressive approach attention-based recognition. ijcv’ international journal computer vision figure shows randomly-selected validation examples four different versions attention model three-resolution three glimpses three-resolution glimpse high-resolution three glimpses high-resolution glimpse. model input left shows original image dots centers glimpses right shows composite image. green borders indicate correct classiﬁcation borders indicate error. leftmost column accurate system however interesting rightmost column least accurate model correctly directs attention informative areas lacks enough information classify correctly. also interesting note last sample rightmost model correctly classiﬁes breed given non-face feature showing system learned identify variety useful parts instead relying solely facial features.", "year": 2014}