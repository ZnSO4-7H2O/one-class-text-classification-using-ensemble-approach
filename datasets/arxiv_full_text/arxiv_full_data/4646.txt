{"title": "First Experiments with PowerPlay", "tag": ["cs.AI", "cs.LG"], "abstract": "Like a scientist or a playing child, PowerPlay not only learns new skills to solve given problems, but also invents new interesting problems by itself. By design, it continually comes up with the fastest to find, initially novel, but eventually solvable tasks. It also continually simplifies or compresses or speeds up solutions to previous tasks. Here we describe first experiments with PowerPlay. A self-delimiting recurrent neural network SLIM RNN is used as a general computational problem solving architecture. Its connection weights can encode arbitrary, self-delimiting, halting or non-halting programs affecting both environment (through effectors) and internal states encoding abstractions of event sequences. Our PowerPlay-driven SLIM RNN learns to become an increasingly general solver of self-invented problems, continually adding new problem solving procedures to its growing skill repertoire. Extending a recent conference paper, we identify interesting, emerging, developmental stages of our open-ended system. We also show how it automatically self-modularizes, frequently re-using code for previously invented skills, always trying to invent novel tasks that can be quickly validated because they do not require too many weight changes affecting too many previous tasks.", "text": "like scientist playing child powerplay learns skills solve given problems also invents interesting problems itself. design continually comes fastest initially novel eventually solvable tasks. also continually simpliﬁes compresses speeds solutions previous tasks. describe ﬁrst experiments powerplay. selfdelimiting recurrent neural network slim used general computational problem solving architecture. connection weights encode arbitrary self-delimiting halting non-halting programs affecting environment internal states encoding abstractions event sequences. powerplay-driven slim learns become increasingly general solver selfinvented problems continually adding problem solving procedures growing skill repertoire. extending recent conference paper identify interesting emerging developmental stages open-ended system. also show automatically self-modularizes frequently re-using code previously invented skills always trying invent novel tasks quickly validated require many weight changes affecting many previous tasks. automatically construct increasingly general problem solver recent powerplay framework incrementally efﬁciently searches space possible pairs task descriptions modiﬁcations current problem solver. search continues ﬁrst pair discovered current solver cannot solve task modiﬁed solver provably solves previously learned tasks plus one. task actually simplify compress speed previous solutions turn invoke partially re-use solutions tasks. process discovering solving novel task repeated forever open-ended fashion. concrete implementation solver special neural network architecture called self-delimiting slim given slim already solve ﬁnite known previously learned tasks asymptotically optimal program search algorithm used pair provably properties pair found cycle repeats itself. results continually growing tasks solvable increasingly powerful solver. resulting repertoire self-invented problem-solving procedures skills exploited time solve externally posed tasks. slim modiﬁable components namely connection weights. keeping track tasks depend connection powerplay reduce time required testing previously solved tasks certain newly modiﬁed connection weights tasks depend changed connections need retested. solution recently invented task require changes many weights changed connections affect many previous tasks validation efﬁcient. since powerplay’s efﬁcient search process built-in bias towards tasks whose validity check requires little computational effort implicit incentive generate weight modiﬁcations impact many previous tasks. leads natural decomposition space tasks note active learning methods adaboost totally different set-up purpose user provides samples learned classiﬁer series classiﬁers focuses samples badly classiﬁed previous classiﬁers. open-ended powerplay however computational tasks self-invented; need predeﬁned global tasks solver tries solve better instead task continually grows based task easy invent validate given already known. unlike ﬁrst implementations curious creative playful agents powerplay provably problems online learning—it cannot forget previously learned skills automatically segmenting life sequence clearly identiﬁed tasks explicitly recorded solutions. unlike task search theoretically optimal creative agents powerplay’s task search greedy practically feasible. present ﬁrst experiments extending recent work notation algorithmic framework powerplay notation original paper brieﬂy review basics relevant here. denotes ﬁnite strings binary alphabet natural numbers real numbers. computational architecture powerplay’s problem solver deterministic universal computer limited device feedforward problem solvers uniquely encoded implemented universal computers universal turing machines therefore without loss generality assume ﬁxed universal reference computer whose inputs outputs elements user-deﬁned subsets deﬁne sets possible problem solvers task descriptions. example inﬁnite computable tasks small subset thereof. deﬁnes possible programs used generate modify members solver feedforward could highly restricted subset programs encoding nn’s possible topologies weights could encodings input-output pairs supervised learning task could algorithm modiﬁes weights network. problem solver’s initial program called particular sequence unique task descriptions chosen invented search method solutions computed i-th instance program cannot solved si−. consists unique problem identiﬁer read built-in mechanism unique description deterministic procedure deciding whether problem solved. example simple task require solver answer particular input pattern particular output pattern. require solver steer robot towards goal sequence actions. denote ti}; ti−}. valid task require solving least previously solved task efﬁciently using less resources storage space computation time energy etc. quantiﬁed function cost. algorithmic framework incrementally trains problem solver ﬁnding increase solvable tasks. details reader encouraged refer original report search algorithm candidate program. give limited time task invention unless user speciﬁes solver modification computing modiﬁcation si−. correctness demonstration compute cost start pattern classiﬁcation tasks. setup encodes arbitrary weights ﬁxedtopology multi-layer perceptron maps two-dimensional real-valued input vectors unit square binary labels; i.e. output label depending whether real-valued activation mlp’s single output neuron exceeds binary programs length length compute tasks modify follows. specify current task simplify weight decay assumption smaller weights simpler. programs implement compression tasks. target label current task candidate given next two-dimensional input vector uniquely encoded remainder string follows. string taken binary representation integer gaussian pseudo-random number generator used generate numbers used coordinates unit square. task label coordinates random number generator re-seeded seed every time task search begins thus ensuring deterministic search order. since labels experiment need choose target label different label currently assigned encoded input. steps means execute epochs gradient descent training check whether patterns correctly classiﬁed. step always refers processing single pattern regardless task. assume powerplay already learned version called able classify previously invented training patterns next task deﬁned simple enumerative search style universal search combines task simpliﬁcation systematic run-time growth figure experiment right initialization ﬁrst compressions decision boundary arbitrary possibly non-linear. drive compress simplify however ﬁrst encourages linear separability associations invented becomes harder harder learn ones break previous solver’s generalization ability maintaining linear boundary. eventually causes decision boundary become non-linear decision boundary becomes increasingly non-linear associations invented learned. since compression task code single roughly half total search time spent simpliﬁcation rest spent invention training patterns break mlp’s current generalization ability. monitor evolution solver’s generalization successful search task labels grid points plotted rather dense grid unit square maps expected experiments show beginning powerplay prefers invent learn simple linear functions. however phase transition complex non-linear functions tasks indicating developmental stage natural by-product search simple tasks—they easier invent verify complex non-linear tasks. learning proceeds observe decision boundary becomes increasingly non-linear system come tasks solver cannot solve solver becomes increasingly powerful system invent increasingly harder tasks. hand search time solutions harder harder tasks need grow time since solutions learnt scratch re-use previous solutions encoded parts previous solver. describe experiments powerplay-based continually invents novel sequences actions affecting external environment time becoming general solver selfinvented problems. figure slim activation scheme. various time steps active/winning neurons outgoing connections highlighted. step neuron witas become active propagate activations outgoing connections. rnns general computers allow sequential parallel computations. given enough neurons appropriate weight matrix compute function computable standard particular named slim deﬁne experiment. brieﬂy review basics. k-th computational unit neuron slim denoted real-valued weight directed connection discrete time step tend ﬁnite interaction sequence environment denotes real-valued activation designated neurons serving online inputs read real-valued observations environment outputs whose activations encode actions environment e.g. movement commands wlkul) form otherwise. program slim means weight matrix wlk. special feature slim single halt neuron ﬁxed halt-threshold. time activation exceeds halt-threshold network’s computation stops. thus network topology exists path online task inputs halt neuron self-delimiting programs studied theory kolmogorov complexity algorithmic probability inspired previous architecture neurons inputs outputs arranged winner-take-all subsets nwitas neurons time step winning neuron witas otherwise. feature gives slim potential modularize itself since neurons gates various self-determined regions network. regulating information network fraction weights given task. apart online input output halt neurons ﬁxed number neurons task inputs. inputs remain constant tend serve self-generated task speciﬁcations. finally subset internal state neurons whose activations considered ﬁnal outcome program halts. thus non-compression task given particular task input interact environment network halts produces particular internal state—the abstract goal—which read internal state neurons. since slim general computer represent essentially arbitrary computable tasks way. fig. illustrates network’s activation spreading particular task. detailed discussion slim rnns efﬁcient implementation found original report slim trained fovea environment described sec. using powerplay framework according algorithm below. difference algorithm lies task set-speciﬁc details encoding task inputs deﬁnition ‘inventing learning’ task. string encodes real numbers denote constant task inputs program. given task inputs task considered learned network halts reaches particular internal state remains able properly reproduce saved internal states previously learned tasks. implemented ﬁrst checking network halt produce internal state newly generated task inputs. network cannot halt within chosen fraction time budget dictated length length program remaining budget used trying learn task using simple mutation rule modifying weights network. single task interpreted compression task. compression either means reduction squared weights without increasing total number connection usages previously learned tasks reduction total number connection usages previously learned tasks without increasing squared weights. since powerplay variant methodically increases search time half used compression automatically encourages network invent novel tasks require many changes weights used many previous tasks. figure fovea design. pixel intensities square averaged produce real valued input. smallest squares center size controls fovea movement static image experiments photo city lugano. slim implementation efﬁciently resets activations computed numerous unsuccessful tested candidate programs. keep track used connections active neurons time step reset activations tracking/undoing effects programs essentially cost execution. environment experiment consists static image observed sequentially fovea whose movement control time step. size fovea pixels; produces real valued online inputs averaging pixel intensities regions varying sizes higher resolution center lower resolution periphery fovea controlled using real-valued outputs network parameter win-threshold. ﬁrst four outputs highest value greater win-threshold interpreted movement command down left right. none ﬁrst four outputs exceeds threshold fovea move. similarily next four outputs interpreted fovea step size image network’s internal states viewed abstract summaries trajectories fovea environment parallel internal thoughts. system invents novel skills breaking generalization ability previous slim weight matrix without forgetting previously learned skills. within hours standard slim consisting witas neurons witas invented novel action sequences guiding fovea halting. varied length consuming steps. time slim invented skills solve novel tasks also learned speed solutions previously learned tasks shown fig. clarity ﬁgures presented depict aspects though results consistent many different runs. slim also learns reduce interactions environment. fig. shows number interactions required solve certain previously learned fovea control tasks. interaction slim computation step produces least non-zero output neuron activation. general trend different tasks runs interactions decrease time. slim essentially learns figure ﬁrst self-invented non-compression tasks plot number connection usages task. solutions self-generated tasks learned. non-compression tasks rest resulted successful compressions slim rnn’s weight matrix. time previously learned skills tend require less less computational resources i.e. slim rnn-based solver learns speed solutions previous self-invented tasks. although plot lines occasionally compensated decrease connection usages dozens tasks slim often uses partially overlapping subsets connection weights generating different self-invented trajectories. fig. shows connections used tasks connections used solve individual tasks become progressively separated. general variation degree separation depends network parameters environment. expected powerplay-based slim prefer modify connections novel task. randomly choosing ﬁfteen weight modiﬁcations task average weights changed invent skill—see fig. why? powerplay always going novel task fastest validate fewer weight changes tend affect fewer previously learned tasks; less time needed re-validate performance previous tasks. powerplay avoids naively expected slowdown linear number tasks. although number skills must forgotten grows time search time skills grow proportion number previously solved tasks. consequence bias towards fast-to-validate solutions powerplay-based slim automatically self-modularizes. slim tested connections. typically used solve particular task average less three changed. means newly invented task system re-uses previously acquired knowledge without modiﬁcation. truly novel aspects task solution often encoded within handful bits. type self-modularization general found traditional modular reinforcement learning systems whose action sequences chunked macros reused higher-level macros like options framework hierarchical since figure selected tasks plot number interactions environment novel non-compression tasks learned besides numerous additional compression tasks ignored here. interaction slim computation step produces least non-zero output neuron activation. total number interactions cannot exceed number steps halt neuron activated. slim general computer weights program subsets weights viewed sub-programs sub-programs formed ones essentially arbitrary computable ways like general incremental program search powerplay slim represents greedy implementation central aspects formal theory creativity setup permits practically feasible curious/creative agents learn hierarchically modularly using general computational problem solving architectures. task invention either breaks solver’s present generalization ability compresses solver speeds know precisely learned powerplaying slim self-invented tasks clearly deﬁned inputs abstract internal outcomes results. human interpretation nn’s weight changes however difﬁcult like baby generates internal representations skills skill fragments play. meaning eyes parents baby’s internal state black box? example case fovea tasks learner invents certain inputdependent movements well abstractions trajectories environment weights stage encode agent’s present understanding environment done powerplay problems noisy inputs environment. however noisy version previously solved task must considered task general know noise not. time powerplay automatically learn generalize away noise eventually ﬁnding compact solver solves noisy instances seen far. ﬁrst experiments focused developmental stages purely creative systems involve externally posed tasks yet. future work test hypothesis systems running powerplay faster solving many user-provided tasks systems without purely explorative components. hypothesis inspired babies creatively seem invent learn many skills autonomously helps learn additional teacher-deﬁned external tasks. intend identify conditions knowledge transfer expected. figure connection usage ratios slim connections learning total self-invented tasks non-compression tasks forming so-called task repertoire rest compression tasks. usage ratio y-axis number repertoire tasks using connection divided number repertoire tasks. ratio ﬁrst connections frequently used outgoing connections task online inputs. network learns better utilize architecture using different connections different tasks thus reducing number connections high usage ratio. modularization help speed task search later stages. number input output state neurons network input output state respectively. comp number computation blocks block size neurons. thus comp×block size computation neurons network. network wired follows. task input neuron connected comp computation neurons random. online input neuron connected comp/ neurons random. internal state neuron receives connections comp/ random computation neurons. halt neuron recieves connections comp/ random computation neurons. comp×n output random computation neurons connected random output neurons. neuron computation block randomly connected block size computation neurons. used comp block size state input output fovea control task. halt-threshold witas fovea control win-thresholds connection weights initialized random values cost using connection connections. mutation rule follows. non-compression tasks network ﬁrst using task inputs check task already solved generalization. randomly generate integer number connections used unsuccessful randomly change weights adding uniformly random number compression tasks randomly generate number connections used tasks current repertoire randomly modify connections. time budget fraction available check whether candidate task solvable chosen randomly time budget/. compression tasks squared weights decrease least factor acceptable. figure self-invented non-compression task plot number modiﬁed slim weights needed learn without forgetting solutions tasks. task search number connections modify chosen randomly. growing repertoire reached signiﬁcant size however successfully learned additional tasks tend require weight changes affecting previous tasks powerplay’s bias towards tasks fast validate entire repertoire. text details. powerplay self-delimiting recurrent neural networks developed schmidhuber implemented r.k. srivastava b.r. steunebrink. thank stollenga n.e. toklu help implementations. research funded following projects im-clever", "year": 2012}