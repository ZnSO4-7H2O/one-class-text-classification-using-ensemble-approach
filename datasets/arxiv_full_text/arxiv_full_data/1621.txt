{"title": "Weakly Supervised PLDA Training", "tag": ["cs.LG", "cs.AI", "cs.CL", "cs.SD"], "abstract": "PLDA is a popular normalization approach for the i-vector model, and it has delivered state-of-the-art performance in speaker verification. However, PLDA training requires a large amount of labelled development data, which is highly expensive in most cases. We present a cheap PLDA training approach, which assumes that speakers in the same session can be easily separated, and speakers in different sessions are simply different. This results in `weak labels' which are not fully accurate but cheap, leading to a weak PLDA training.  Our experimental results on real-life large-scale telephony customer service achieves demonstrated that the weak training can offer good performance when human-labelled data are limited. More interestingly, the weak training can be employed as a discriminative adaptation approach, which is more efficient than the prevailing unsupervised method when human-labelled data are insufficient.", "text": "pervised adaptation techniques. example wang proposed domain-adaptation approach based maximum likelihood linear transformation rahman proposed dataset-invariant covariance normalization approach normalized i-vectors global covariance matrix computed in-domain out-domain data. equal projecting i-vectors in-domain out-domain speakers onto dataset-invariant space plda model trained projected i-vectors robust data mismatch. another approach utilizing unlabelled data produce labels data automatically. labels accurate human labels still convey speaker-related information therefore used supplemental materials plda training. importantly labels cheap allowing vast unlabelled data used. call cheap labels ‘weak labels’ plda training based labels ‘weak training’. correspondingly plda training human labels called ‘strong training’. research conducted weak plda training. garcia-romero proposed semi-supervised learning approach used out-of-domain plda cluster indomain data based plda projection matrix adapted. villalba colleagues proposed variational bayesian method unknown label unlabelled utterance treated latent variable. seen extension semi-supervised method. proposed approach treated unlabelled data special universal speaker plda trained universal speaker involved. paper proposes knowledge-based weak plda training approach produces cheap labels based prior knowledge. example telephony customer service domain prior knowledge speakers different sessions almost different therefore session used label speakers. labels certainly noisy since knowledge absolutely correct convey valuable information used enhance plda. experiments real-life large-scale customer service archive demonstrated knowledgebased weak training rather effective domains knowledge ‘sufﬁciently correct’ provide performance improvement even outperform unsupervised adaptation approach scenarios human-labelled data limited. structure paper follows section presents details weak training section presents experiments. finally section concludes paper discusses future work. plda popular normalization approach i-vector model delivered state-of-the-art performance speaker veriﬁcation. however plda training requires large amount labelled development data highly expensive cases. present cheap plda training approach assumes speakers session easily separated speakers different sessions simply different. results ‘weak labels’ fully accurate cheap leading weak plda training. experimental results real-life large-scale telephony customer service demonstrated weak plda training offer good performance human-labelled data limited. interestingly weak training employed adaptation approach efﬁcient prevailing unsupervised method human-labelled data insufﬁcient. index terms plda i-vector weak training speaker veriﬁcation i-vector model plus various normalization approaches offers standard framework modern speaker veriﬁcation basically i-vector model uses gaussian mixture model deep neural network collect baum-welch statistics based afﬁne transform learned speech segments projected onto low-dimensional continuous vectors although possible discriminate speaker i-vectors using simple cosine distance normalization discriminative techniques often preferred since promote speaker-related information thus bring signiﬁcant performance improvement. probabilistic linear discriminant analysis popular normalization methods. assumes i-vectors particular speaker subject gaussian distribution mean vector following normal distribution combined length normalization plda delivered state-of-the-art performance various test benchmarks plda training generally requires large amount humanlabelled data usually thousands speakers multiple sessions. example popular development databases fisher switchboard speakers respectively. practice labelling large amount data human challenging discriminating voice-similar speakers difﬁcult also identifying speaker utterance among thousands people nearly impossible. therefore quite appealing data utilized directly without human labeling. knowledge-based weak plda training section conventional plda model brieﬂy reviewed proposed knowledge-based weak training approach presented. also discuss relation proposal methods others. plda extension linear discriminative analysis introducing gaussian prior mean i-vector speaker. combined length normalization plda delivered state-of-the-art performance speaker veriﬁcation. letting denote i-vector utterance speaker plda model formulated follows speaker-independent global factor represent speaker-level utterance-level factors respectively. matrix consists basis speaker subspace. note assumed follow diagonal full-rank gaussian prior. model trained algorithm similarity i-vectors computed ratio evidence hypothesises whether i-vectors belong speaker propose weak training approach relies prior knowledge cheap labels unlabelled data. example customer service domain paper focuses utilize pieces prior knowledge participants single session easily separated; speakers different sessions probably different especially customers. knowledge utterance simply assigned label involves session local speaker i.e. valid within session. labels fully correct cases are. figure illustrates difference human labels weak labels derived prior knowledge speaker represented particular color. human labels segments speaker different sessions correctly labelled. weak labels speakers different sessions labelled different even actually same. weak labels generated plda training conducted usual human labels. knowledge-based weak training proposed related semi-supervised plda training rely weak cheap labels labels produced different ways knowledge-based weak training relies domainspeciﬁc prior knowledge performance determined correctness knowledge; semi-supervised training relies existing plda model performance determined quality existing model. perspective semi-supervised training regarded model-based weak training. argue knowledge-based weak training superior scenarios human-labelled data insufﬁcient strong primary plda available. knowledge-based weak training also related unsupervised plda adaptation methods make distribution information unlabelled data thus employed perform model adaptation. difference weak training also utilizes speaker-discriminant information which although noisy still beneﬁcial knowledge mostly correct. therefore conjecture knowledge-based weak training effective unsupervised adaptation scenarios discriminative information desirable. proposed weak training approach tested practical speaker veriﬁcation system trained large-scale telephony customer service archive. system implemented based gmm-ivector framework. ﬁrst present data proﬁle report results. training data used train gmm-ivector system composed hours conversational speech signals sampled large-scale telephony customer service archive. data used train ubms matrix i-vector model. development data used train plda model divided data sets strong weak labelled human prior knowledge described previous section respectively. note that acoustic condition weak close evaluation data means weak regarded in-domain therefore improvement could partially model adaptation. strong involves speech signals speakers weak consists double-channel sessions speakers. session consists customer channel service channel channels separated physically. weak customer channel forms weak-customer subset weak service channel forms weak-service subset. distinguish customer data service data hold different properties particularly probability ‘different session different speaker’ assumption holds. finally sample sessions weak-customer sessions weak-service composing weak-mix subset. details development data shown table evaluation involves speakers enrollment speech speaker seconds length. length test utterances seconds speaker contains test utterances. pair-wised composition trails constructed including target trials imposter trails. second order derivatives. frame size frame shift involves gaussian components dimensionality i-vector space performance evaluated terms equal error rate ﬁrst experiment studies performance knowledgebased weak plda training compare strong training uses human-labelled data. results shown table results strong three weak subsets reported. comparison results cosine scoring also presented. ﬁrst observe plda models outperform cosine scoring. particular interesting weak training approach inaccurate labels used. conﬁrms conjecture possible weak labels derived prior knowledge train plda least scenarios prior knowledge correct. comparing results three weak subsets observed weak-customer delivers best performance weak-service shows worst also understandable since number service people limited speaker labels sessions weak-service subset probably incorrect contrast probability customers appear sessions weak-customer fairly means perfect match prior knowledge real data leads good performance. second experiment investigate performance different plda training methods various amount training data. results shown figure data volume controlled number speakers. diamond starting point curve represents performance cosine scoring. seen limited data plda models despite strong training weak training applied provide better performance simple cosine scoring. data plda models offer better performance cosine baseline. strong training superior weak training weak training model trained weak-customer shows better performance weak-service reason discussed already. experiment assume limited human-labelled data weakly labelled data enhance plda model. precisely weakly-labelled data augmented human-labelled data train plda call ‘pooled training’. according experience last experiment data weak-customer used data augmentation. figure shows contour performance pooled training various amount data strong weak-customer. seen human-labelled data limited augmenting weakly-labelled data offers clear performance improvement data augmented performance improved. however effectiveness augmentation unlimited additional contribution becomes marginal amount weakly-labelled data speakers. surprising considering noise involved data. figure also suggests interesting concept ‘substitution amount’ i.e. many weakly-labelled data substitute certain amount human-labelled data. contour figure indicates human-labelled data provided difﬁcult substituted weaklylabelled data. words value human-labelled finally compare pooled training unsupervised learning. note weak dataset close evaluation acoustic condition methods play role model adaptation. figure shows results four plots present conﬁgurations different amount human-labelled data train initial plda model. again weak-customer subset used adaptation data. figure performance pooled training unsupervised adaptation. green diamonds represent performance strong training blue circles represent best performance pooled training crosses represent best performance unsupervised adaptation. figure observed human-labelled data limited pooled training unsupervised adaptation offer clear performance improvement though pooled training effective. attribute superiority pooled training fact adapts acoustic condition also utilizes speaker-related discriminant information associated weak labels. since human-labelled data limited initial plda strong therefore additional discriminant information essentially valuable leading clear advantage pooled training. human-labelled data sufﬁcient initial plda model covers acoustic conditions holds sufﬁcient discriminative capability diminishing contribution pooled training unsupervised adaptation. paper proposed knowledge-based weak training approach plda veriﬁed potential speaker veriﬁcation. based assumption speakers different sessions different weak labels easily produced used supplemental data train plda. experiments large-scale customer service archive demonstrated weak training approach works well ‘different session different speaker’ assumption held. approach effective human-labelled data limited even outperforming unsupervised adaptation method. future work investigate possibility utilize knowledge-based weak labels model-based weak labels investigate active learning select valuable data human labeling. garcia-romero espy-wilson analysis i-vector length normalization speaker recognition systems proceedings annual conference international speech communication association scheffer ferrer mclaren novel scheme speaker recognition using phonetically-aware deep neural network proceedings ieee international conference acoustics speech signal processing ieee wang yamamoto koshinaka domain adaptation using maximum likelihood linear transformation plda-based speaker veriﬁcation proceedings ieee international conference acoustics speech signal processing ieee rahman kanagasundaram dean sridharan dataset-invariant covariance normalization out-domain plda speaker veriﬁcation proceedings annual conference international speech communication association garcia-romero zhang mccree povey improving speaker recognition performance domain adaptation challenge using deep neural networks spoken language technology workshop villalba lleida unsupervised adaptation plda using variational bayes methods proceedings ieee international conference acoustics speech signal processing sparse probabilistic linear discriminant analysis speaker veriﬁcation proceedings annual conference international speech communication association greenberg stanford martin yadagiri doddington godfrey hernandez-cordero nist speaker recognition evaluation proceedings annual conference international speech communication association", "year": 2016}