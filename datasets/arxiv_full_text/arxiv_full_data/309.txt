{"title": "Where is my forearm? Clustering of body parts from simultaneous tactile  and linguistic input using sequential mapping", "tag": ["cs.NE", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "abstract": "Humans and animals are constantly exposed to a continuous stream of sensory information from different modalities. At the same time, they form more compressed representations like concepts or symbols. In species that use language, this process is further structured by this interaction, where a mapping between the sensorimotor concepts and linguistic elements needs to be established. There is evidence that children might be learning language by simply disambiguating potential meanings based on multiple exposures to utterances in different contexts (cross-situational learning). In existing models, the mapping between modalities is usually found in a single step by directly using frequencies of referent and meaning co-occurrences. In this paper, we present an extension of this one-step mapping and introduce a newly proposed sequential mapping algorithm together with a publicly available Matlab implementation. For demonstration, we have chosen a less typical scenario: instead of learning to associate objects with their names, we focus on body representations. A humanoid robot is receiving tactile stimulations on its body, while at the same time listening to utterances of the body part names (e.g., hand, forearm and torso). With the goal at arriving at the correct \"body categories\", we demonstrate how a sequential mapping algorithm outperforms one-step mapping. In addition, the effect of data set size and noise in the linguistic input are studied.", "text": "umbrella term uniting higher level representations perception action accessible consciousness. schwoebel coslett amassed evidence distinguishing three types body representations body schema body structural description body semantics—constituting kind hierarchy. body structural description topological locations derived primarily visual input deﬁnes body part boundaries proximity relationships. finally body semantics lexical–semantic representation body including body part names functions relations artifacts details every particular taxonomy hierarchy discussed clearly trend continuous modality-speciﬁc representations multimodal aggregated representations. ﬁrst instantiated increasing receptive ﬁeld size combining sensory modalities apparent somatosensory processing e.g. areas relatively specialized proprioception touch small receptive ﬁelds touch proprioception getting increasingly combined areas then going anterior posterior parietal cortex receptive ﬁelds grow somatosensory information combined visual. whether process bottom-up integration aggregation give rise discrete entities categories similar individual body parts. vignemont focused body segmentation hand could appear based combined tactile visual perception. explored category boundary eﬀect appeared tactile stimuli presented stimuli felt farther away applied across wrist applied within single body part conclusion suggest representation body structured categorical body parts delineated joints categorical representation modulates tactile spatial perception. humans animals constantly exposed continuous stream sensory information diﬀerent modalities. time form compressed representations like concepts symbols. species language process structured interaction mapping sensorimotor concepts linguistic elements needs established. evidence children might learning language simply disambiguating potential meanings based multiple exposures utterances diﬀerent contexts existing models mapping modalities usually found single step directly using frequencies referent meaning co-occurrences. paper present extension one-step mapping introduce newly proposed sequential mapping algorithm together publicly available matlab implementation. demonstration chosen less typical scenario instead learning associate objects names focus body representations. humanoid robot receiving tactile stimulations body time listening utterances body part names goal arriving correct body categories demonstrate sequential mapping algorithm outperforms one-step mapping. addition eﬀect data size noise linguistic input studied. body representation topic psychological neuroanatomical neurophysiological studies many decades. spurred account head holmes proposal superﬁcial postural schema number diﬀerent concepts proposed since body schema body image corporeal schema them. body schema usually thought low-level sensorimotor representation body used action. body image multimodal body-related information additional categorization body parts imposed language infant hears parents naming body parts. interestingly recent research showed cross-linguistic variabilities naming body parts turn override inﬂuence bottom-up multimodal body part categorization. ﬁeld relatively rich experimental observations mechanisms behind development operation representations still well understood. here computational particular robotic modeling ties in—see surveys body schema robots. petit demiris developed algorithm icub humanoid robot associate labels body parts later proto-actions embodied counterparts. could recombined hierarchical fashion mimura used dirichlet process gaussian mixture model latent joint provide bayesian body schema estimation based tactile information. results suggest kinematic structure could estimated directly tactile information provided moving fetus without additional visual information—albeit lower accuracy. work icub humanoid robot thus focused learning primary representations—tactile proprioceptive work former input processing—interaction linguistic input. work strive segmentation body parts based simultaneous tactile linguistic information. however body part categorization mapping body part names instance general problem segmenting objects environment learning compressed representations stand associating words infant often exposed simultaneously. borghi example studied interaction object names situated action objects. made newly proposed sequential mapping algorithm extends idea one-step mapping compared overall accuracy one-step mapping well accuracies segmenting individual body parts. explore accuracy learned mapping inﬂuenced level noise linguistic domain data size. sequential mapping strategy shown robust mapping circumstances noisy input clearly outperformed one-step mapping. section ﬁrst present inputs preprocessing pipelines tactile input linguistic input total body parts right half robot’s upper body stimulated torso/chest upper forearm palm ﬁngertips. tactile stimulation coincided utterance body part’s name. then one-step sequential mapping algorithms presented description evaluation generate tactile stimulation pertaining diﬀerent body parts built previous work icub humanoid robot. particular tactile homunculus primary representation artiﬁcial sensitive skin robot covered current work skin physically stimulated anymore activations emulated relayed homunculus detailed below. created yarp software module generate virtual skin contacts. skin part randomly selected stimulated. number pressure-sensitive elements different skin parts torso upper forearm hand taxels total. skin part randomly selected small region also randomly picked within part tactile stimulation— taxels time corresponding triangular modules skin composed hand situation slightly diﬀerent entire hand treated skin part. then within hand random choice made subregions palm skin ﬁngertips data collected minutes corresponding approximately individual second stimulations. skin parts stimulation lasted seconds sampled label–body part name–was saved along tactile data. labels used generate linguistic input performance evaluation later directly take tactile stimulation body part accompanied corresponding utterance. case separate body parts ’torso’ ’upper arm’ ’forearm’ ’palm’ ’little ﬁnger’ ’ring ﬁnger’ ’middle ﬁnger’ ’index ﬁnger’ ’thumb’. linguistic tactile inputs processed simultaneously. conducted experiments spoken language input—one-word utterances pronounced nonnative english speaker. process data made sphinx achieved accuracy word recognition. word-forms extracted audio input compared prelearned language models means log-scale scores directly. allowed fully explore eﬀect misclassiﬁcation linguistic subdomain mapping accuracy. noise language data added subsequently evenly classes possible establish mapping sensorimotor concepts linguistic elements frequencies referent meaning co-occurrences ones highest co-occurrence mapped together method usually called crosssituational learning supposes availability ideal associative learner keep track store co-occurrences trials internally memorizing representing word–object co-occurrence matrix input. allows learner subsequently choose strongly associated referent part clustering tactile information. please note separate labels palm individual ﬁngers treated skin part virtual touch generation hence number samples ﬁnger example lower non-hand body parts. input layer tactile homunculus consists vector activations taxels time t—the output previous section—that binary values output layer forms grid figure layer compressed representation skin surface—the receptive ﬁelds neurons schematically color-coded. however code available part tactile input. output layer represented single vector activations output neurons calculated products weight vector corresponding i-th output neuron tactile activation vector follows output ﬁrst layer vector serves input second tactile processing layer. layer aims cluster individual body parts represent abstract models. resulting models subsequently mapped multimodal layer clusters found language layer. process outputs ﬁrst layer used gaussian mixture model convex mixture d-dimensional gaussian densities case tactile model described parameters posterior probabilities computed follows fig. icub skin tactile homunculus. photograph icub robot artiﬁcial skin exposed right half upper body representation tactile inputs learned using self-organizing neuronal sheet. schematics skin patches unfolded colored mark correspondence arrows illustrate relationship orientation skin parts learned capture dynamic competition among models extend basic one-step mapping algorithm crosssituational learning sequential addition inhibitory connections. inhibitory mechanisms situationtime dynamics already partially included model cross-situational learning proposed mcmurray even though model shares similarities model proposed mcmurray stems diﬀerent computational mechanisms. reliable assignment language tactile model found inhibitory connections among tactile model language models added. thanks mechanism mutual exclusivity principle guaranteed. word object strengthened pair co-occurs trial. extend basic idea enable also forgetting introducing parameter capture memory decay supposing trial observe object hear corresponding word describe update strength association word model object—in case tactile model t—as follows number trials kronecker delta function indicate word–object association model attends attempts learn trial parameter controlling gain strength association. let’s assume word modeled model language domain object modeled model tactile domain. goal corresponding model tactile subdomain model language domain assign together. indices found follows accuracy learned mapping calculated following manner cluster output activations tactile homunculus assign data point probable cluster. then indices clusters deﬁned equation one-step mapping equation sequential mapping. based mapping assign data point language label. language labels subsequently compared ground truth accuracy computed studied performance one-step sequential mapping algorithms ability cluster individual body parts simultaneous tactile linguistic input. skin regions body part learn belong together thanks co-occurrences body part labels. addition eﬀect data size levels noise linguistic domain investigated detailed analysis mapping accuracy individual body parts backward projection onto tactile homunculus shown sections respectively. performance one-step sequential mapping algorithms shown fig. comparison provided diﬀerent data sizes noise levels. seen accuracy sequential mapping remains stable outperforms one-step mapping values noise comparing bottom panel fig. demonstrates poorer performance higher variance especially ﬁngers. tactile data homunculus clustered clusters mapped appropriate language clusters project labels back onto original tactile homunculus. considering activations neuron homunculus whole data consisting vector homunculus activations data point langlabel language label assigned data point based sequential mapping procedure described section project results sequential mapping onto homunculus following manner. first compute strength activation neuron given language label follows afterwards visualize neuron much activated individual body parts. results data sets diﬀering size level noise linguistic domain seen fig. clearly large enough data sets limited noise mapping language tactile modality successful delineating body part categories seen comparing panels fig. accuracy one-step sequential mapping diﬀerent levels noise language. number denotes size data sequential mapping one-step mapping. mean standard deviation repetitions visualized. fig. accuracy sequential mapping individual body parts visualization sequential mapping accuracy based noise linguistic data data sizes data points data points noise language data mean standard deviation repetitions visualized. categories language symbols chosen speciﬁc less studied instance problem segmentation labeling body parts. perhaps developmental perspective could plausible body part categories quite accurate. given nature tactile input—the skin continuous receptor surface—and random-uniform tactile input generator used linguistic input facilitate cluster formation. however realistic non-uniform touch particular addition additional modalities enable bottom-up non-linguistic body part category formation described example. constitute possible directions future work modal cluster formation interact labels imposed language. furthermore thus half body considered— corresponding lateralized representations tactile homunculus— imagine stimulating left right example hearing always utterance ‘upper arm’. study brain areas involved processing needed order develop models closely inspired functional cortical networks like model experimental ﬁndings experiments used artiﬁcially generated linguistic input added noise future planning actual auditory input real noise. also additional dimension similarity auditory domain ‘arm’ ‘forearm’ phonetically closer ‘torso’. thus linguistic modality constitute crisp discrete labels anymore extracted ﬁrst—opening possibilities bidirectional interaction modalities. k.s. m.h. supported czech science foundation project ga-y. m.h. additionally supported marie curie intra european fellowship within european community framework programme. z.s. supported grant agency prague project sgs//ohk/t/. m.v. supported european research project tradr funded programme cognitive systems interaction robotics", "year": 2017}