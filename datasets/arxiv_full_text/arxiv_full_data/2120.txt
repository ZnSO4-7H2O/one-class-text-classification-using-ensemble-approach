{"title": "Probabilistic Programming with Gaussian Process Memoization", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Gaussian Processes (GPs) are widely used tools in statistics, machine learning, robotics, computer vision, and scientific computation. However, despite their popularity, they can be difficult to apply; all but the simplest classification or regression applications require specification and inference over complex covariance functions that do not admit simple analytical posteriors. This paper shows how to embed Gaussian processes in any higher-order probabilistic programming language, using an idiom based on memoization, and demonstrates its utility by implementing and extending classic and state-of-the-art GP applications. The interface to Gaussian processes, called gpmem, takes an arbitrary real-valued computational process as input and returns a statistical emulator that automatically improve as the original process is invoked and its input-output behavior is recorded. The flexibility of gpmem is illustrated via three applications: (i) robust GP regression with hierarchical hyper-parameter learning, (ii) discovering symbolic expressions from time-series data by fully Bayesian structure learning over kernels generated by a stochastic grammar, and (iii) a bandit formulation of Bayesian optimization with automatic inference and action selection. All applications share a single 50-line Python library and require fewer than 20 lines of probabilistic code each.", "text": "gaussian processes widely used tools statistics machine learning robotics computer vision scientiﬁc computation. however despite popularity diﬃcult apply; simplest classiﬁcation regression applications require speciﬁcation inference complex covariance functions admit simple analytical posteriors. paper shows embed gaussian processes higherorder probabilistic programming language using idiom based memoization demonstrates utility implementing extending classic state-of-the-art applications. interface gaussian processes called gpmem takes arbitrary real-valued computational process input returns statistical emulator automatically improve original process invoked input-output behavior recorded. ﬂexibility gpmem illustrated three applications robust regression hierarchical hyper-parameter learning discovering symbolic expressions time-series data fully bayesian structure learning kernels generated stochastic grammar bandit formulation bayesian optimization automatic inference action selection. applications share single -line python library require fewer lines probabilistic code each. gaussian processes widely used tools statistics machine learning robotics computer vision scientiﬁc computation also central probabilistic numerics emerging eﬀort develop computationally eﬃcient numerical procedures bayesian optimization family meta-optimization techniques widely used tune parameters deep learning algorithms even seen artiﬁcial intelligence. example searching structured kernels generated stochastic grammar automated statistician system produce symbolic descriptions time series translated natural language paper shows integrate higher-order probabilistic programming languages illustrates utility integration implementing venture platform. idea implement kind statistical generalizing memoization. resulting higher-order procedure called gpmem takes kernel function source function returns gp-based statistical emulator source function queried locations source function evaluated. source function invoked datapoints incorporated emulator. principle covariance function also allowed arbitrary probabilistic program. simple packaging covers full range uses described above including statistical applications applications scientiﬁc computation uncertainty quantiﬁcation. paper illustrates gpmem embedding venture general-purpose higherorder probabilistic programming platform venture several distinctive capabilities needed applications paper. first supports ﬂexible foreign interface modeling components supports eﬃcient rank- updates required standard implementations. second provides inference programming constructs used describe custom inference strategies combine elements gradient-based monte carlo variational inference techniques. level control inference state-of-the-art applications gps. third supports models stochastic recursion priori unbounded support sets higher-order procedures; together enable combination stochastic grammars fast implementation needed structure learning. fourth venture permits nesting modeling inference needed bayesian optimization general objective functions general derived modeling inference. best knowledge ﬁrst general-purpose integration probabilistic programming language. unlike software libraries embedding allows uses beyond classiﬁcation regression include state-of-the applications structure learning meta-optimization. paper presents three applications gpmem replication results neal outlier rejection hyper-parameter inference; fully bayesian extension automated statistician project; implementation bayesian optimization thompson sampling. ﬁrst application principle replicated several probabilistic languages embedding proposal described paper. remaining applications rely distinctive capabilities venture support fully bayesian structure learning language constructs inference programming. applications share single -line python library require fewer lines probabilistic code each. gaussian processes bayesian method regression. consider regression input real-valued scalars regression output value function complete training data denoted column vectors unseen test input denoted present non-parametric express prior knowledge collection random variables represents values function location write mean function covariance function kernel. prior mean random variable prior covariance random variables output mean covariance function conditioned free hyper-parameters parameterizing refer hyper-parameters θmean respectively. simplify calculation below assume prior mean identically zero; derivation done assumption easily relaxed translation. upper case italic function returns matrix dimension entries indicate length column vectors throughout write prior covariance matrix results computing following sometimes drop subscript writing clarity. note vectors conditioning observed value predictive posterior computed ﬁrst forming joint density training test data treated randomly chosen prior ﬁxing value constant. start often assumes observed regression output noisily measured sees values ynoisy gaussian white noise variance noise. noise term absorbed covariance function log-likelihood covariance function governs high-level properties observed data smoothness linearity. high-level properties indicated superscript functions. linear covariance written figure depict kernel composition. shows data generated sine function linearly growing amplitude data used plots shows linear periodic base kernel functional form well composition both. multiplication kernels indicates local interaction. local interaction account case growing amplitude column diﬀerent. show samples prior predictive random parameters used sample data points observed. show samples predictive posterior data observed. consider local interaction actual interaction depends similarity data points. addition covariance functions models global interaction interaction high-level components qualitatively dependent input space. example periodic function linear trend. structure implies unsuitable behaviour example additional recurring spikes introduced account changing amplitude true function generated data. memoization practice storing previously computed values function future calls inputs evaluated lookup rather re-computation. transfer idea probabilistic programming introduce language construct called statistical memoizer. suppose function evaluated wish learn behavior using evaluations possible. statistical memoizer give name gpmem motivated purpose. produces outputs function fcompute calls stores output memo table traditional memoization does. function femu online statistical emulator uses memo table training data. fully bayesian emulator modelling true function random function would satisfy diﬀerent implementations statistical memoizer diﬀerent prior distributions paper deploy prior note require ability sample femu jointly multiple inputs values general dependent. figure gpmem tutorial. shows schematic gpmem. compute probes outside resource. expensive every probe memoized improves emulator. schematic evolution gpmem’s state believe world given certain venture directives. right depict true function samples emulator incorporated observations gpmem. external process memoize. evaluated using resources potentially come outside venture. feed function gpmem alongside parameterised kernel example make qualitative assumption smooth deﬁne squared-exponential covariance function traditional perspective. value pairs stored memoization table incorporated observations simply feed regression input emulator output predictive posterior gaussian distribution determined memoization table. either deﬁne function serves input gpmem natively venture interleave venture foreign code. useful computed help outside resources. deﬁne parameterize squared-exponential kernel supply gpmem making observations calls sample prior inputs using emulator information femu presumable value pairs without calling fcompute friend tells value call observe store information incorporated observations femu only value pair available computation sampling emulator eﬀect calling predict fcompute. however imagine least scenario distinction treatment observations beneﬁcial. real function available also domain expert knowledge function. expert could tell value given input. potentially value provided expert could disagree value computed example diﬀerent levels observation noise. finally update posterior inferring posterior hyper-parameter values deﬁned scopes collection related random choices hyper-parameters tags supplied inference program specify random variables inference done case perform metropolis-hastings transition scope hyperparameters choose random member scope choose hyperparameter random. also deﬁne custom inference actions. let’s deﬁne gaussian drift proposals. note inference figure. important part code snippet drift kernel step markov chain would like propose transition sampling state unit normal distribution whose mean current state. paper illustrates ﬂexibility gpmem showing concisely encode three diﬀerent applications gps. ﬁrst standard example hierarchical bayesian statistics bayesian inference hierarchical hyper-prior used provide curveﬁtting methodology robust outliers. second structure learning application probabilistic artiﬁcial intelligence used discover qualitative structure time series data. third reinforcement learning application used part thompson sampling formulation bayesian optimization general real-valued objective functions real inputs. functions. since apply gpmem process procedure used situations data available look-up function look fact demonstrate gpmem’s application regression using example data generated function available provide synthetic function gpmem data function ftrue taken paper treatment outliers hierarchical bayesian hyper-priors synthetically generate outliers setting σnoise cases σnoise remaining cases. instead accessing ftrue directly accessing data form dimensional array look kse+wn parameterize hyper-parameters fig. panel kse+wn deﬁned composite covariance function. squared exponential kernel white noise kernel implemented make whitenoise. initialize incorporate observations using observe predict posterior incorporates knowledge data. hyper-parameters sigma still random emulator capture true underlying dynamics data correctly. nested steps hyper-parameters good approximation posterior nested ﬁrst take sweeps scope hyperhyper characterizes sweep scope hyper characterizes finally change inference strategy altogether. decide instead following bayesian sampling approach would like perform empirical optimization changing line code deploying gradient-ascent instead inductive learning symbolic expressions continuous-valued time series data hard task recently tackled using greedy search approximate posterior possible kernel compositions gpmem provide fully bayesian treatment this previously unavaible using stochastic grammar allows read unstructured time series automatically output high-level qualitative description stochastic grammar assumed ﬁxed general margin hypothesis space. following drop notation. building block missing combine sampled base kernels compositional covariance function interaction infer whether data supports local interaction figure bayesian structure learning. base kernels priors hyperparameters serves hypothesis space supplied input stochastic grammar. stochastic grammar parts sampler selects random primitive kernels kernel composer combines individual base kernels generates composite kernel function serves input gpmem. observe value pairs unstructured time series data bottom schematic. parse parse structure. kernel functions simpliﬁed simplify-operator. simplifed used input struct interprets symbolically. base kernels compositional kernels shown alongside interpretation struct. many equivalent covariance structures sampled covariance function algebra equivalent representations diﬀerent parameterization inspect posterior equivalent structures convert kernel expression products subsequently simplify. introduce three diﬀerent operators work kernel functions deﬁned simple space covariance structures allows produce results coherent work presented automatic statistician results illustrated data sets. mauna data illustrate results depict data. mean centered measurements mauna observatory atmospheric baseline station mauna island hawaii. description data found rasmussen williams chapter data compute posterior structure parameters samples. latter shown zoom show posterior captures error bars adequately. posterior generated random sample parameters peak distribution structure diﬀerentiate posterior distribution kernel functions posterior distribution symbolic expressions describing diﬀerent kernel structures. allows compute posterior symbollically equivalent structures struct struct. structures yield addition linear kernel periodic kernel per. therefore parse parse simplify expression simplify compute struct. mauna data distribution peaks write kernel equation kernel structure natural language interpretation spell explaining posterior peaks kernel structure four additive components. holds globally higher level qualitative aspects data vary input space. additive components result follows illustrate results data depict data. again data mean centered compute posterior structure parameters samples. latter shown posterior generated posterior peaks kernel structure four additive components. additive components hold globally higher level qualitative aspects data vary input space. additive components follows linearly increasing function trend; periodic function; smooth function; white noise. figure structure learning. starting data compute posterior distribution structures take sample peak distribution including parameters write functional form depict human readable interpretation used plot posterior peaks kernel structure three additive components. additive components hold globally higher level qualitative aspects data vary input space. additive components follows linearly increasing function trend; approximate periodic function; white noise. write kernel equation kernel structure natural language interpretation spell explaining posterior peaks kernel structure three additive components. additive components hold globally higher level qualitative aspects data vary input space. additive components follows querying time series bayesian approach structure learning gain valuable insights time series data previously unavailable. ability estimate posterior marginal probabilities kernel structure. marginal deﬁne boolean search operations allow query data probability certain structures hold true globally. figure querying structural motifs time series using posterior inference kernel structure. kernel structure serves formulate natural language questions data initial question interest fairly general what probability trend recurring pattern noise data? natural language version question question formulated inference problem written. right query asks whether noise data computing disjunction marginal global white noise kernel multiplication linear white noise kernel samples predictive prior kernels give indication qualitative aspects kernel structure implies probability noise data high makes sense drill even deeper asking detailed questions. regards noise translates querying whether data supports hypothesis heteroskedastic noise white noise. queries motifs repeating structure shown middle tree queries related trends left. querying data statistical implications stark contrast previous research automatic kernel construction able provide. could view approach time series search engine allows test whether certain structures found available time series. another view approach language interact world. real-world observations often come timestamps form continuous valued sensor measurements. provide toolbox query observations similar manner would query knowledge base logic programming language. ﬁnal application demonstrating power gpmem illustrates bayesian optimization. introduce thompson sampling basic solution strategy underlying bayesian optimization gpmem. thompson sampling thompson widely used bayesian framework addressing trade-oﬀ exploration exploitation multi-armed bandit problems. cast multi-armed bandit problem one-state markov decision process describe thompson sampling used choose actions mdp. described follows agent take sequence actions possible actions action reward received according unknown conditional distribution ptrue agent’s goal contexts context believed model conditional distributions }x∈x least believed statistic conditional distributions suﬃcient statistic believed conditional mean viewed believed value function. consistency follows assume context takes form vector past actions vector rewards contains information included context. exploration less likely occur chosen actions tend receive high rewards. amounts exploitation. trade-oﬀ exploration exploitation illustrated figure roughly speaking exploration happen context reasonably sure unexplored actions probably optimal time thompson sampler exploit choosing actions regions knows high value. limited choice representation. traditional programming environments often consists numerical parameters family distributions ﬁxed functional form. work mixture functional forms possible; without probabilistic programming machinery implementing rich context space would unworkably large technical burden. probabilistic programing language however representation figure possible actions iteration thompson sampling. believed distribution value function depicted red. example true reward function deterministic drawn blue. action right receives high reward action left receives reward greatly improves accuracy believed distribution transition operators τsearch τupdate described section note context space ﬁnite-dimensional parametric family since vectors grow samples taken. however representable computational procedure together parameters past samples representation proposal acceptance/rejection process described deﬁne transition operator τupdate iterated speciﬁed number times; resulting state markov chain taken sampled semicontext step algorithm step thompson sampling explore action space using mhlike transition operator τsearch. iteration τsearch produces proposal either accepted rejected state markov chain speciﬁed number steps action markov chain’s initial state recent action proposal distribution gaussian drift higher current action always accepted proposals estimated value lower current action accepted probability decays exponentially respect diﬀerence value. rate decay determined temperature parameter high temperature corresponds generous acceptance indeed taking action estimated value high uncertainty serves useful function improving accuracy estimated value function points near complete probabilistic program gpmem implementing bayesian optimization thompson sampling both uniform proposals drift proposals fig. show results implementation bayesian optimization thompson sampling. compare diﬀerent proposal distributions namely uniform proposals gaussian drift proposals. experiment gaussian drift starting near global optimum drifts quickly towards uniform proposals take longer global optimum surpass local optima curve. bottom panel fig. depicts sequence actions using uniform proposals. sequence illustrates exploitation exploration trade-oﬀ implementation overcomes. start complete uncertainty bayesian agent performs exploration gets idea optimum could shows change tactic. bayesian agent exploited local optima previous steps figure estimated optimum time. blue represent optimization uniform gaussian drift proposals. black lines indicate local optima true functions. bottom sequence actions. depicted iterations uniform proposals. paper shown feasible useful embed gaussian processes higherorder probabilistic programming languages treating kind statistical memoizer. described classic regression fully bayesian inference hierarchical hyperprior well state-of-the-art applications discovering symbolic structure time series bayesian optimization. applications share common -line python library require fewer lines probabilistic code each. results suggest several research directions. first important develop versions gpmem optimized larger-scale applications. possible approaches include standard low-rank approximations kernel matrix popular machine learning well sophisticated sampling algorithms approximate conditioning second seems fruitful abstract notion generalizing memoizer speciﬁc choice gaussian process model mechanism generalization. generalizing statistical memoizers custom regression techniques could broadly useful performance engineering scheduling systems. timing data performance benchmarks could generalizing memoizer default. memoizer could queried inform best strategy performing computation predict likely runtime long-running jobs. third structure learning application suggests follow-on research information retrieval structured data. possible build time series search engine handle search predicates rising trend starting around perodic variation automated statistician presented paper provide ranked result sets sorts queries tracks posterior uncertainty structure also space structural patterns handle easy modify making small changes short venturescript program. ﬁeld bayesian nonparametrics oﬀers principled fully bayesian response empirical modeling philosophy machine learning bayesian inference used encode state broad ignorance rather bias stemming strong prior knowledge. perhaps surprising objects bayesian nonparametrics dirichlet processes gaussian processes naturally probabilistic programming variants memoization clear true processes e.g. wishart processes hierarchical beta processes. hope results paper encourage development nonparametric libraries higher-order probabilistic programming languages. acknowledgements research supported darpa iarpa oﬃce naval research army research oﬃce bill melinda gates foundation gifts analog devices google. bottom squared-exponential covariance function also know smoothing kernel; linear kernel constant kernel white noise kernel rational quadratic kernel periodic kernel gaussian gaussian process expectation scalar possibly indexed column vector training data regression input column vector training data regression output possible actions column vector unseen test input regression input column vector sample predictive posterior sample data matrix mean function hyper-parameters mean function covariance function kernel function takes scalars input hyper-parameters kernel/covariacne function kernel conditioned hyper-parameters covariance matrix computed posterior mean vector posterior covariance matrix lower triangular matrix given cholesky factorization chol squared exponential covariance function linear covariance function constant covariance function white noise covariance function periodic covariance function symbolic expression squared exponential covariance function symbolic expression linear covariance function symbolic expression periodic covariance function symbolic expression rational quadratic covariance function symbolic expression constant covariance function symbolic expression white noise covariance function parse structure kernel simplify functional expression kernel symbolic interpretation kernel kernel contains global kernel structure operator check struct operator check struct operator check struct operator check struct gamma distribution shape parameter rate length-scale parameter scale factor parameter context thompson sampling context space value function proposal distribution energy function temperature parameter", "year": 2015}