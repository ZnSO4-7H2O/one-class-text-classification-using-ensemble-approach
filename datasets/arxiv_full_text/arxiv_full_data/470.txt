{"title": "A Constrained Sequence-to-Sequence Neural Model for Sentence  Simplification", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "Sentence simplification reduces semantic complexity to benefit people with language impairments. Previous simplification studies on the sentence level and word level have achieved promising results but also meet great challenges. For sentence-level studies, sentences after simplification are fluent but sometimes are not really simplified. For word-level studies, words are simplified but also have potential grammar errors due to different usages of words before and after simplification. In this paper, we propose a two-step simplification framework by combining both the word-level and the sentence-level simplifications, making use of their corresponding advantages. Based on the two-step framework, we implement a novel constrained neural generation model to simplify sentences given simplified words. The final results on Wikipedia and Simple Wikipedia aligned datasets indicate that our method yields better performance than various baselines.", "text": "sentence simpliﬁcation reduces semantic complexity beneﬁt people language impairments. previous simpliﬁcation studies sentence level word level achieved promising results also meet great challenges. sentencelevel studies sentences simpliﬁcation ﬂuent sometimes really simpliﬁed. word-level studies words simpliﬁed also potential grammar errors different usages words simpliﬁcation. paper propose two-step simpliﬁcation framework combining word-level sentence-level simpliﬁcations making corresponding advantages. based twostep framework implement novel constrained neural generation model simplify sentences given simpliﬁed words. ﬁnal results wikipedia simple wikipedia aligned datasets indicate method yields better performance various baselines. sentence simpliﬁcation standard task reducing reading complexity people limited linguistic skills read. particular children non-native speakers individuals language impairments dyslexia aphasic autistic would beneﬁt task makes sentences easier understand. categories task lexical simpliﬁcation sentence simpliﬁcation. categories enable paraphrasing proinspired great achievements machine translation several studies treate sentence simpliﬁcation problem monolingual translation task achieve promising results studies apply phrase-based statistical machine translation model syntacticbased translation model pbsmt sb-smt require high-level features even rules empirically chosen humans. recently neural machine translation based sequence-to-sequence model shows powerful capabilities traditional systems. applies deep learning regimes extracts features automatically withhuman supervision. observe sentence simpliﬁcation usually means simple output sentence derived normal input sentence parts terms changed shown table intrinsic character applying machine translation methods directly likely generate completely identical sentence input sentence matter standard nmt. although methods indeed advanced research sentence simpliﬁcation tasks still plenty room improvements. best knowledge competitive simpliﬁcation models using translation models even neural network architectures far. besides sentence-level simpliﬁcation lexical simpliﬁcation substitutes long infrequent words shorter frequent synonyms complex word identiﬁcation substitution generation substitution selection processes. recent lexical simpliﬁcation models horn glavaˇs paetzold pavlick accumulated substantial numbers synonymous word pairs. makes possible simplify complex words simplifying whole sentence. however even though synonyms similar semantic meaning might different usages. replacing complex words simpler synonyms intuitive simplify sentences always works potential grammatical errors switchings moreover lexical substitution simplify sentences. also simplify sentences splitting deletion reordering sentence-level simpliﬁcation generally obtain output results grammar errors although guarantee simpliﬁed; lexical-level simpliﬁcation simplify complicated parts sentences always guarantee grammatically ﬂuent. intuitive exciting idea combine methods together make corresponding advantages obtain simpliﬁed sentences good readability ﬂuency. speciﬁc simpliﬁcation process input sentence conducted steps. ﬁrst identify complex word replace simpler synonyms according pre-constructed knowledge base. second step generate legitimate sentence given simpliﬁed word appropriate syntactic structures grammar. another issue second step need maintain same/similar semantic meaning input sentence. still stick translation paradigm translating complex sentence simple sentence. paper contributions follows propose two-step simpliﬁcation framework combine advantages wordlevel simpliﬁcation sentence-level simpliﬁcation make generated sentences ﬂuent readable simpliﬁed. last decades life dukas became well known teacher composition many famous students. later life dukas became well known music teacher many famous students. table last decades replaced simpliﬁed word later words later need reordered guarantee correct grammaticality output sentence. word pair composition music situation last decades later. evaluate proposed method neural model english wikipedia simple english wikipedia datasets. experimental results indicate model achieves better results series baseline algorithms terms ibleu scores flesch readability human judgments. paper organized follows. section review related works describe proposed method model section section describe experimental setups show results. section conclude paper discuss future work. previous studies researchers sentence-level simpliﬁcation mostly address simpliﬁcation task monolingual machine translation problem. standard pb-smt implemented moses toolkit translate original sentences simpliﬁed ones. similarly coster kauchak extend pb-smt model adding phrase deletion. wubben make effort reranking moses’ nbest output based dissimilarity input. recently proposed sb-smt model achieving better performance wubben’s system. general sentence-level simpliﬁcation maintains semantic meaning ﬂuency always guarantee literal simpliﬁcation. pressive results well. horn extract paraphrase rules lexical simpliﬁcation identifying aligned words english wikipedia simple english wikipedia. glavaˇs employ glove generate synonyms complex words. instead using parallel datasets approach requires single corpus. paetzold propose word embeddings model deal limitation traditional models accommodate ambiguous lexical semantics. pavlick release simple paraphrase rules extracting normal paraphrases rules bilingual corpus reranking simplicity scores rules supervised model. thanks efforts large number effective methods identifying complex words ﬁnding corresponding simple synonyms selecting qualiﬁed substitutions. however sometimes simplifying complicated words directly simple synonyms violates grammar rules usages. recent progress deep learning neural networks brings great opportunities development stronger systems neural machine translation deep learning heavily driven data requiring human efforts create high-level features. speciﬁcally sequence-to-sequence model remarkable ability characterize word sequences generate natural language sentences. however seqseq model still shares problem mt-based methods lack literal simpliﬁcation time time. overall sentence-level word-level sentence simpliﬁcation strengths weaknesses. paper propose two-step method fusing corresponding advantages. generate simpliﬁed sentences using sequence-to-sequence model trained parallel corpus namely english wikipedia simple english wikipedia. simpliﬁed word identiﬁed ﬁrst step standard sequence-to-sequence model cannot guarantee existence word. therefore propose constrained sequence-to-sequence model given simpliﬁed word methodology since many efforts working establishment word simpliﬁcation pairs focus identiﬁcation words require simpliﬁcation methods selecting simpler words switch. instead change words according knowpledge base proceed neural sentence generation model assuming word substitutions correct based previous studies synonym alignments. speciﬁc given input sentence simpliﬁcation process conducted steps step according previous studies lexical substitution ﬁrst identify complex words input sentence substitute simpler synonyms; step given simpliﬁed words ﬁrst step constraints propose constrained seqseq model encodes input sentence vector decodes vector simpliﬁed sentence generation process conditioned simpliﬁed word consists backward forward generation. constrained seqseq model given input sequence switching word pair complex word simpler synonym generate simpliﬁed sentence output contained i.e. could multiple constraint words start simplest situation constraint word. denote lengths source target sentences respectively vocabulary size source target sentences. embedding vector word denotes word embedding dimen∈ sionality; rdim×dim rdim×dim weight matrices denotes number hidden states. initial hidden state computed tanh hmean mean value hidden states encoder rdim×dim. according turn obtain backward sentence forward sentence maximal estimated probability beam search. finally concatenate reverse backward sequence simpler word forward sequence output entire sentence notice exist position shown figure generate sequence constraint ﬁrst generate sequence then generate forward sequence conditioned generated sequence output sequence reverse paper apply bi-directional recurrent neural network gated recurrent units backward forward generation process. encode input sequence follows embedding vector word denotes word embedding dimensionality; rdim×e rdim×dim weight matrices denotes rdim number hidden states; hidden state birnn’s forward direction time likewise hidden state birnn’s backward direction denoted illustrated single constraint word sequence-to-sequence generation process actually constraint words simpliﬁed sentence generation shown table extend single constraint multiple constraints multi-constrained seqseq model. withloss generosity deﬁne multiple keywords illustrate multiconstrained seqseq figure ﬁrst illustrate situation simpliﬁed words i.e. namely generally take complex word least term frequency ﬁrst constrained word method section generate ﬁrst output sentence second round generation take ﬁrst output sentence input generate second output sentence constraint. compared singlepass generation single constraint word output sentence constraint words two-pass generation. relative position depends input sentence dataset setups evaluate proposed approach parallel corpus wikipedia simple wikipedia english. randomly split corpus sentence pairs training sentences validation sentences testing. noises dataset. ﬁlter test samples output identical input without simpliﬁcation. also applied lowercasing preprocess samples. vocabulary size out-ofvocabulary words mapped token unk. encoder decoder model hidden units; word embedding dimensionality adadelta comparison methods paper conduct experiments english wikipedia simple english wikipedia datasets compare proposed method several representative algorithms. moses. translation model sbmt. sbmt syntactic-based machine translation model implemented open-source joshua toolkit simpliﬁcation model optimized sari metric leverages ppdb dataset rich source simpliﬁcation operations. lexical substitution. method substitutes complex words simpliﬁed word constraint word model leave words input sentence unchanged. model shares hypothesis model. seqseq. sequence-to-sequence model state-of-the-art neural machine translation model attention mechanism applied constrained seqseq. propose novel neural sentence generation model based sequence-tosequence paradigm constraint word. multi-constrained seqseq denote scenario constraint words. evaluation metrics automatic evaluation evaluate performance different methods simpliﬁcation task leverage four automatic evaluation metrics flesch-kincaid grade level sari bleu ibleu widely used readability. sari evaluates simplicity explicitly measuring goodness words added deleted kept. bleu originally designed evaluates output using n-gram matching between output reference. several studies indicate bleu alone really suitable simpliﬁcation task many cases sentence simpliﬁcation output sequence looks similar input sequence part simpliﬁed. situation prominent insufﬁciency standard bleu metric even though output sequence perform simpliﬁcation operations input sequence still likely obtain high bleu score. necessary penalize output sentence similar input sentence. therefore ibleu metric suitable simpliﬁcation balances similarity simplicity. given output sentence reference sentence input sentence ibleu deﬁned human evaluation human judgment ultimate evaluation metric natural language processing tasks. randomly select source sentences test dataset invite graduate students evaluate simpliﬁed sentences systems according source sentence. fairness conduct blind review evaluators aware methods produce simpliﬁcation results. following earlier studies asked participants rate grammaticality meaning simplicity three human evaluation metrics -points scale lowest points highest points. note generated sentence identical source sentence rate grammaticality points meaning points simplicity points target sentence. overall performance automatic evaluation results listed table moses worst performance. obtains fair bleu score bleu score indicating moses fails simplify sentences. failure ibleu sari scores quite low. sbmt similar performance like moses neither simplify output sentences promote readability. overall results seqseq system better moses sbmt. though bleu score little lower moses sbmt output sentences mostly identical input sentences bleu score also achieves better ibleu sari scores moses sbmt. lexical substitution substitutes complex words obtains highest bleu sari scores. gets worst readability. general constrained seqseq multi-constrained seqseq proposed framework outperform baselines. higher similarities reference lower similarities input systems. ibleu scores systems higher baselines respectively. sari score systems also pretty high. readability systems achieve best result. grammaticality identical sentences meaning rated points points simplicity points moses gets highest score grammaticality meaning obtains lowest score simplicity. similar moses sbmt generates sentences really simpliﬁed sbmt obtains similar results like moses. seqseq outperforms moses sbmt systems judged overall performance obtains grammaticality meaning simplicity. results lexical substitution meaning simplicity rather high. shown score grammaticality sentences generated lexical substitution contain many grammar errors surprising. constrained seqseq multi-constrained seqseq outperform simplicity baselines. meaning scores systems simple english wikipedia quite similar score indicates extent systems simple english wikipedia semantic loss simplifying sentences. grammatiparkes became country location completion railway serving great deal passenger freight transport parkes important transport center railway built many passenger freight trains stopped parkes parkes became country location completion railway serving great deal passenger freight transport parkes became country location completion railway serving great deal passenger freight transport parkes became country location completion railway serving great deal passenger freight transport parkes became important country location completion railway serving center many passenger transport parkes became country location completion railway serving center great deal passenger freight transport parkes became important country location completion railway became center great deal passenger freight transport cality constrained seqseq better lexical substitution. multi-constrained seqseq performs worse constrained seqseq grammaticality better simplicity. judged overall performances constrained seqseq multi-constrained seqseq outperform off-the-shelf sentence-level simpliﬁcation methods generated sentences literally simpliﬁed legitimate. table show typical examples systems. among them moses sbmt seqseq model generate completely identical sentence input sentence cases. lexical substitution paraphrases complex words great deal simple words important center many. seen article word important changed lexical substitution fails deal kind errors. proposed model generates output sentences conditioned simpliﬁed word center deletes complex phrase great deal taking generated sentence constrained seqseq model input multiconstrained seqseq model substitutes less frequent words word important. also changes adverbial clause serving simple sentence structure became shows models ﬂexible effective baseline systems. paper propose two-step method sentence simpliﬁcation combining wordlevel simpliﬁcation sentence-level simpliﬁcation. experiments parallel datasets wikipedia simple wikipedia results show methods outperform various baselines better readability ﬂexibility simplicity achieved. future plan take factors account formulate constraints proposed model. john carroll guido minnen darrenz pearce canning yvonne devlin siobhan john tait. simplifying text language-impaired readers. proceedings eacl. pages kyunghyun bart merrienboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation william coster david kauchak. learning simplify sentences using wikipedia. proceedings workshop monolingual text-to-text generation. association computational linguistics pages richard evans constantin orasan iustin dornescu. evaluation syntactic simpliﬁcation rules people autism. proceedings workshop predicting improving text readability target reader populations pages goran glavaˇs sanja ˇstajner. simplifying lexical simpliﬁcation need simpliﬁed corpora. proceedings annual meeting association computational linguistics international joint conference natural language processing. pages peterand robert fishburne kincaid richard rogers brad derivation readability formulas navy enlisted naval technical training command personnel. millington research branch. philipp koehn hieu hoang alexandra birch chris c.burch marcello federico nicola bertoldi brooke cowan wade shen christine moran richard zens chris dyer ondrej bojar alexandra constantin evan herbst. moses open source toolkit statistical machine translation. acl. association computer linguistics. lili yiping song zhang jin. sequence backward forward sequences content-introducing approach arxiv preprint generative short-text conversation. arxiv.. gustavo paetzold lucia specia. unsupervised lexical simpliﬁcation non-native speakers. proceedings thirtieth aaai conference artiﬁcial intelligence. aaai press pages kishore papineni salim roukos todd ward weijing zhu. bleu method automatic evalproceedings uation machine translation. annual meeting association computational linguistics. pages ellie pavlick chris callison-burch. simple ppdb paraphrase database simpliﬁcation. annual meeting association computational linguistics. page matt post juri ganitkevitch luke orland jonathan weese yuan chris callison-burch. joshua sparser better faster server. proceedings eighth workshop statistical machine translation. pages rello ricardo baeza-yates horacio saggion. impact lexical simpliﬁcation verbal paraphrases people without dyslexia. international conference intelligent text processing computational linguistics. springer berlin heidelberg pages lucia specia. translating complex simpliﬁed sentences. international conference computational processing portuguese language. springer berlin heidelberg pages hong ming zhou. joint learning dual system paraphrase generation. proceedings annual meeting association computational linguistics short papers. association computational linguistics volume sanja ˇstajner hannah b´echara horacio saggion. deeper exploration standard pb-smt approach text simpliﬁcation evaluation. proceedings annual meeting association computational linguistics sander wubben antal bosch emiel krahmer. sentence simpliﬁcation monolinproceedings gual machine translation. annual meeting association computational linguistics long papers. volume pages courtney napoles ellie pavlick quanze chen chris callison-burch. optimizing statistical machine translation text simpliﬁcation. transactions association computational linguistics pages zhemin delphine bernhard iryna gurevych. monolingual tree-based translation model sentence simpliﬁcation. proceedings international conference computational linguistics. association computational linguistics pages", "year": 2017}