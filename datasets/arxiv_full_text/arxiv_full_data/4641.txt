{"title": "Optimistic Agents are Asymptotically Optimal", "tag": ["cs.AI", "cs.LG"], "abstract": "We use optimism to introduce generic asymptotically optimal reinforcement learning agents. They achieve, with an arbitrary finite or compact class of environments, asymptotically optimal behavior. Furthermore, in the finite deterministic case we provide finite error bounds.", "text": "optimism introduce generic asymptotically optimal reinforcement learning agents. achieve arbitrary ﬁnite compact class environments asymptotically optimal behavior. furthermore ﬁnite deterministic case provide ﬁnite error bounds. article studies fundamental question artiﬁcial intelligence; given environments deﬁne agent eventually acts optimally regardless environments question relates even fundamental question intelligence deﬁnes intelligent agent well large range environments. studies arbitrary classes environments particular attention universal classes environments like computable environments lower semi-computable environments. deﬁnes aixi agent bayesian reinforcement learning agent universal hypothesis class solomonoﬀ prior. agent interesting optimality properties. besides maximizing expected utility respect priori distribution design also pareto optimal self-optimizing possible considered class. however shown guaranteed asymptotically optimal computable environments. shows surprising since least geometric discounting agent also shows weaker sense optimality achieved class computable environments using algorithm includes long exploration phases. furthermore simple realize bayesian agents always achieve optimality ﬁnite class deterministic environments even prior weights strictly positive. principle optimism deﬁne agent ﬁnite class deterministic environments eventually acts optimally. extend results case ﬁnite compact classes stochastic environments. deterministic case also prove ﬁnite error bounds. optimism previously used design exploration strategies discounted undiscounted mdps though deﬁne optimistic algorithms ﬁnite class environments. related work. besides aixi discussed above introduces agent achieves asymptotic optimality average sense class deterministic computable environments. however time step optimal every time step. inﬁnite number long exploration phases. introduce agent ﬁnite classes environments eventually achieve optimality every time step. stochastic case agent achieves given probability optimality within simple agent relying elegantly principle optimism used previously restrictive case discounting without instead indeﬁnite number explicitly enforced bursts exploration. also introduces agent relies bursts exploration achieving asymptotic optimality. asymptotic optimality guarantees restricted setting environments satisfy certain restrictive value-preservation property. studied learning general partially observable markov decision processes though pomdps constitute general reinforcement learning setting interested agents given class environments successfully utilize knowledge true environment lies class. obvious extension inﬁnite sequences. function called deterministic environment function value equals zero ﬁrst formulation sent case stochastic environments study section instead function furthermore deﬁne probability measure strings sequences discussed next section deﬁne conditioning ht−. deﬁne ν-expected return policy special case environment markov decision process classical setting reinforcement learning. case environment depend full history latest observation action therefore function situation often refers observations states since latest observation tells everything need know. situation optimal policy represented function state need base decision latest observation. several algorithms devised solving discounted mdps prove bounds. ﬁnite time bounds hold high probability depend polynomially number states actions discount factor. methods relying optimism method making agent suﬃciently explorative. optimism roughly means high expectations know. optimism also used prove regret bounds undiscounted mdps extended feature mdps note methods restricted mdps make assumptions environments size class. outline. article deﬁne optimistic agents general setting mdps prove asymptotic optimality results. question mere existence already non-trivial hence asymptotic results deserve attention. section consider ﬁnite classes deterministic environments introduce simple optimistic agent guaranteed eventually optimally. also provide ﬁnite error bounds. section generalize ﬁnite classes stochastic environments section compact classes. given ﬁnite class deterministic environments deﬁne algorithm unknown environment eventually achieves optimal behavior sense exists maximum reward achieved time onwards. algorithm chooses optimistic hypothesis sense picks environment achieve highest reward policy optimal environment followed. hypothesis contradicted feedback environment optimistic hypothesis picked environments still consistent technique important consequence hypothesis contradicted still acting optimally optimizing incorrect hypothesis. history time generated policy environment particular hπ◦µ history generated algorithm interacting actual true environment cycle know environments consistent algorithm needs check whether oπ◦ν since previous cycles ensure hπ◦ν maximization algorithm deﬁnes optimism time performed consistent hypotheses time πall class deterministic policies. proving theorem time-consistency geometric discounting. following lemma tells optimally respect chosen optimistic hypothesis remains optimistic contradicted. ﬁrst equality follows equals onwards. second equality follows consistency third equality follows optimism constancy time-consistency geometric discounting last inequality follows reverse inequality follows therefore acting optimally times besides eventual optimality guarantee above also provide bound number time steps value following algorithm certain less optimal. reason bound true suboptimality certain number time steps point current hypothesis becomes inconsistent number inconsistency points bounded number environments. refer algorithm conservative agent since sticks model long can. corresponding liberal agent reevaluates optimistic hypothesis every time step switch diﬀerent optimistic policies time. algorithm actually special case shown lemma liberal agent really class algorithms larger class algorithms consists exactly algorithms optimistic every time step without restrictions. conservative agent subclass algorithms switch hypothesis previous contradicted. results conservative agent extended liberal omit space reasons. stochastic hypothesis never become completely inconsistent sense assigning zero probability observed sequence still assigning diﬀerent probabilities true environment. therefore exclude based threshold probability assigned generated history. unlike deterministic case hypothesis cease optimistic without excluded. therefore consider algorithm reevaluates optimistic hypothesis every time step. algorithm speciﬁes procedure theorem states asymptotically optimal. borrow techniques introduced merging opinions result generalized classical theorem classical result says suﬃcient true measure absolutely continuous respect chosen priori distribution guarantee almost surely merge sense total variation distance. generalized version given lemma combine policy environment letting actions taken policy deﬁned measure denoted space inﬁnite sequences ﬁnite alphabet. denote sample sequence elements ωab. σ-algebra generated cylinder sets {ω|ωt measure determined values sets. simplify notation next lemmas write meaning ojrj furthermore martingale sequence true measure therefore converges probability crucial question limit strictly positive not. following lemma shows probability either case lemma suppose class stochastic environments containing true environment also suppose none environments excluded time algorithm inﬁnite history generated running given converge almost surely argued using martingale convergence theorem. lemma tells given environment eventually excluded permanently included merge true remaining environments does according lemma merge true environment. lemma tells diﬀerence value functions merging environments converges zero. since ﬁnitely many environments ones remain indeﬁnitely merge true environment every following holds section discuss inﬁnite compact classes stochastic environments. first note without assumptions asymptotic optimality impossible achieve even countably inﬁnite deterministic environments consider classes compact respect total variation distance precisely respect total variation distance section example class markov decision processes certain number states. algorithm need modiﬁcation achieve asymptotic optimality compact case. alternative modifying algorithm satisﬁed reaching optimality within pre-chosen achieved ﬁrst choosing ﬁnite covering balls total variation radius less algorithm centers balls. algorithm eventually achieves optimality within demanding task. need able true environment remain indeﬁnitely considered class given conﬁdence. purpose introduce conﬁdence radius inspired solving algorithms like mbie ucrl still notation algorithm deﬁne algorithm based replacing larger ˜mt. true environment likely excluded. deﬁnition suppose class true environment policy holds probability converges random variables call class radon-nikodym diﬀerentiable. property holds respect speciﬁc policy class rn-diﬀerentiable respect remark every countable class rn-diﬀerentiable class mdps certain number states. mbie ucrl algorithms based fact deﬁne conﬁdence radiuses mdps though bounds need separate intervals state-action pair depending number visits. ergodic state-action pairs almost surely seen inﬁnitely often length intervals tend zero. therefore deﬁne radius based maximum length alternatively easily allow algorithm rectangular sets instead. theorem suppose algorithm threshold compact rn-diﬀerentiable class stochastic environments denote resulting policy conﬁdence radius sequence true environment probability every proof. since compact subset question closed follows also compact. using arzel`a-ascoli theorem conclude subsequence converges uniformly means proof. strategy environment excluded within certain distance environment merges true excluded certain ﬁnite time. remaining environments’ value functions diﬀer certain amount apply lemma following consider total variation ball radius ˜ε/. note whenever collection balls induces open cover compact follows ﬁnite subcover. consider balls ﬁnite cover intersect union ﬁnitely many open balls. closed subset want ﬁnite time environments excluded ˜mt. happens deﬁned union closed balls radius every point excluded large enough also closed subset lemma tells environments excluded ﬁnite amount time therefore environments excluded ˜mt. thus particular optimistic hypothesis optimistic hypothesis time π∗(= introduced optimistic agents ﬁnite compact classes arbitrary environments proved asymptotic optimality. deterministic case also bound number time steps value following algorithm certain amount lower optimal. future work includes investigating ﬁnite-error bounds classes stochastic environments.", "year": 2012}