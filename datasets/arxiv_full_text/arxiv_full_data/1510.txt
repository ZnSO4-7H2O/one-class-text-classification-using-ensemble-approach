{"title": "Visual Storytelling", "tag": ["cs.CL", "cs.AI", "cs.CV"], "abstract": "We introduce the first dataset for sequential vision-to-language, and explore how this data may be used for the task of visual storytelling. The first release of this dataset, SIND v.1, includes 81,743 unique photos in 20,211 sequences, aligned to both descriptive (caption) and story language. We establish several strong baselines for the storytelling task, and motivate an automatic metric to benchmark progress. Modelling concrete description as well as figurative and social language, as provided in this dataset and the storytelling task, has the potential to move artificial intelligence from basic understandings of typical visual scenes towards more and more human-like understanding of grounded event structure and subjective expression.", "text": "language. difference between example sitting next other versus having good time setting versus illuminated brilliance... ﬁrst descriptions capture image content literal concrete; second requires inference good time look like special worth sharing particular sunset. introduce ﬁrst dataset sequential images corresponding descriptions captures subtle important differences advance task visual storytelling. release data three tiers language descriptions imagesimages in-isolation descriptions images-insequence stories images-insequence tiered approach reveals effect temporal context effect narrative language. tiers aligned images dataset facilitates directly modeling relationship literal abstract visual concepts including relationship visual imagery typical event patterns. additionally propose automatic evaluation metric best introduce ﬁrst dataset sequential vision-to-language explore data used task visual storytelling. ﬁrst release dataset sind includes unique photos sequences aligned descriptive story language. establish several strong baselines storytelling task motivate automatic metric benchmark progress. modelling concrete description well ﬁgurative social language provided dataset storytelling task potential move artiﬁcial intelligence basic understandings typical visual scenes towards human-like understanding grounded event structure subjective expression. beyond understanding simple objects concrete scenes lies interpreting causal structure; making sense visual input disparate moments together give rise cohesive narrative events time. requires moving reasoning single images static moments devoid context sequences images depict events occur change. vision side progressing single images images context allows begin create artiﬁcial intelligence reason visual moment given already seen. language side progressing literal description narrative helps learn evaluative conversational abstract work focuses direct literal description image content. encouraging ﬁrst step connecting vision language capabilities needed intelligent agents naturalistic interactions. signiﬁcant difference unexplored remarking visual scene shows sitting room typical image captioning work visual scene shows bonding. latter description grounded visual signal brings bear information social relations emotions additionally inferred context visually-grounded stories facilitate evaluative ﬁgurative language previously seen vision-to-language research system recognize colleagues look bored remark information directly. storytelling oldest known human activities providing educate preserve culture instill morals share advice; focusing research towards task therefore potential bring humanlike intelligence understanding. storyable events tend involve form possession e.g. john’s birthday party shabnam’s visit. using flickr data release aggregate -grams photo titles descriptions using stanford corenlp extract possessive dependency patterns. keep heads possessive phrases classiﬁed event wordnet. relying manual winnowing target collection efforts. terms used collect albums using flickr api. include albums photos album photos taken within -hour span cc-licensed. table query terms albums returned. photos returned stage presented crowd workers using amazon’s mechanical turk collect corresponding stories descriptions. crowdsourcing workﬂow developing complete dataset shown figure crowdsourcing stories sequence develop -stage crowdsourcing workﬂow collect naturalistic stories text aligned images. ﬁrst stage storytelling crowd worker selects subset photos given album form photo sequence writes story second stage re-telling worker writes story based photo sequence stages album photos displayed order time photos taken storyboard underneath. storytelling clicking photo album story card photo appears storyboard. worker instructed pick least photos arrange order selected photos write sentence phrase card form story; appears full story underneath text aligned image. additionally interface captures alignments text photos. workers skip album seem storyable albums skipped workers discarded. interface re-telling similar displays photo sequences already created ﬁrst stage worker chooses write story. album workers perform storytelling workers perform re-telling yielding total workers. hits quality controls ensure varied text least words long. crowdsourcing descriptions images isolation images sequence also crowdsourcing collect descriptions imagesin-isolation descriptions images-insequence photo sequences stories majority workers ﬁrst task tasks workers asked follow instructions image captioning proposed coco describe important parts. table summary dataset following proposed analyses ferraro including frazier yngve measures syntactic complexity. balanced brown corpus provided comparison contains text. perplexity calculated -gram language model learned generic english words dataset scraped web. coco image captioning interface. storyboard story cards storytelling interface display photo sequence coco instructions adapted sequences. recruit workers workers data post-processing storylets descriptions corenlp tokenizer replace people names generic male/female tokens identiﬁed named entities entity type data released training validation test following %/%/% split stories-insequence albums. example language tier shown figure in-isolation reﬂect impoverished disambiguating context references people often lack social speciﬁcity people referred simply woman. single images often convey much information underlying events actions leads abundant posture verbs turn descriptions-in-sequence relatively uninformative words much less represented. finally story-in-sequence words include storytelling elements names temporal references words dynamic abstract given nature complex storytelling task best reliable evaluation assessing quality generated stories human judgment. however automatic evaluation metrics useful quickly benchmark progress. better understand metric could serve proxy human evaluation compute pairwise correlation coefﬁcients automatic metrics human judgments stories sampled training set. human judgements crowdsourcing mturk asking judges story rate strongly agreed statement photos would like using story like share experience friends. take average judgments ﬁnal score story. automatic metrics meteor smoothed-bleu skip-thoughts compute similarity story given sequence. skip-thoughts provide sentencevec embedding models semantic space novels. efﬁcients. signals metric meteor incorporates paraphrasing correlates best human judgement task. detailed study automatic evaluation stories area interest future work. report baseline experiments storytelling task table training tier testing half validation example output system presented table highlight differences story caption generation also train tier isolation produce captions per-image rather sequence. results shown table train story generation model sequence-to-sequence recurrent neural approach naturally extends single-image captioning technique devlin vinyals multiple images. here encode image sequence running vectors image reverse order. used initial hidden state story decoder model learns produce story word time using softmax loss training data vocabulary. gated recurrent units image encoder story decoder. baseline system generate story using simple beam search successful image captioning previously however story generation results model subjectively appear poor system produces generic repetitive highlevel descriptions predictable result given label bias problem inherent maximum likelihood training; recent work looked ways address issue directly establish stronger baseline explore several decode-time heuristics improve quality generated story. ﬁrst heuristic lower decoder beam size substantially. using beam size signiﬁcantly increases story quality resulting gain meteor score. however effect seen caption generation greedy caption model obtaining worse quality beam search model. highlights difference generating stories versus generating captions. although stories produced using greedy search result signiﬁcant gains include many repeated words phrases e.g. kids great time. kids great time. introduce simple heuristic avoid this content word cannot produced within given story. improves meteor another points. advantage comparing captioning storytelling side-by-side captioning output used help inform storytelling output. include additional baseline visually grounded words produced licensed caption model. deﬁne visually grounded words occurred higher frequency caption training story training least sentences -best list marked ‘licensed’ caption model. greedy decoding without duplication proceeds additional constraint word visually grounded generated story model licensed caption model photo set. results meteor improvement. interesting note strong effect relatively simple heuristics generated stories. intend suggest heuristics right approach story generation. instead main purpose provide clear baselines demonstrate story generation fundamentally different challenges caption generation; space wide open explore training decoding methods generate ﬂuent stories. introduced ﬁrst dataset sequential vision-to-language incrementally moves images-in-isolation stories-in-sequence. argue modelling ﬁgurative social language captured dataset essential evolving towards human-like understanding. established several strong baselines task visual storytelling motivated meteor automatic metric evaluate progress task moving forward.", "year": 2016}