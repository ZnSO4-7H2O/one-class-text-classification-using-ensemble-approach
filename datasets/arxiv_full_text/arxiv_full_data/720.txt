{"title": "Evolutionary Algorithms for Reinforcement Learning", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "There are two distinct approaches to solving reinforcement learning problems, namely, searching in value function space and searching in policy space. Temporal difference methods and evolutionary algorithms are well-known examples of these approaches. Kaelbling, Littman and Moore recently provided an informative survey of temporal difference methods. This article focuses on the application of evolutionary algorithms to the reinforcement learning problem, emphasizing alternative policy representations, credit assignment methods, and problem-specific genetic operators. Strengths and weaknesses of the evolutionary approach to reinforcement learning are presented, along with a survey of representative applications.", "text": "journalofarticialintelligenceresearch\u0001\u0001\u0002\u0004\u0001-\u0002\u0007\u0006 submitted\u0001/ ;published evolutionaryalgorithmsforreinforcementlearning moriartyisi.edu davide.moriarty universityofsoutherncaliforniainformationsciencesinstitute \u0004\u0006\u0007\u0006admiraltywaymarinadelreyca schultzaic.nrl.navy.mil alanc.schultz navycenterforappliedresearchinarticialintelligence navalresearchlaboratorywashingtondc\u0002\u0000\u0003\u0007\u0005-\u0005\u0003\u0003\u0007 johnj.grefenstette grefib\u0003.gmu.edu instituteforbiosciencesbioinformaticsandbiotechnology georgemasonuniversitymanassasva\u0002\u0000\u0001\u0001\u0000abstract therearetwodistinctapproachestosolvingreinforcementlearningproblemsnamely searchinginvaluefunctionspaceandsearchinginpolicyspace.temporaldierencemethodsandevolutionaryalgorithmsarewell-knownexamplesoftheseapproaches.kaelbling littmanandmoorerecentlyprovidedaninformativesurveyoftemporaldierencemethods.thisarticlefocusesontheapplicationofevolutionaryalgorithmstothereinforcement learningproblememphasizingalternativepolicyrepresentationscreditassignmentmethodsandproblem-specicgeneticoperators.strengthsandweaknessesoftheevolutionary approachtoreinforcementlearningarepresentedalongwithasurveyofrepresentative applications. \u0001.introduction kaelblinglittmanandmooreandmorerecentlysuttonandbartoprovideinformativesurveysoftheeldofreinforcementlearning.theycharacterizetwo classesofmethodsforreinforcementlearningmethodsthatsearchthespaceofvaluefunctionsandmethodsthatsearchthespaceofpolicies.theformerclassisexempliedby thetemporaldierencemethodandthelatterbytheevolutionaryalgorithm approach.kaelblingetal.focusentirelyontherstsetofmethodsandtheyprovidean excellentaccountofthestateoftheartintdlearning.thisarticleisintendedtoround outthepicturebyaddressingevolutionarymethodsforsolvingthereinforcementlearning problem.askaelblingetal.clearlyillustratereinforcementlearningpresentsachallengingarray ofdicultiesintheprocessofscalinguptorealistictasksincludingproblemsassociated withverylargestatespacespartiallyobservablestatesrarelyoccurringstatesandnonstationaryenvironments.atthispointwhichapproachisbestremainsanopenquestionso itissensibletopursueparallellinesofresearchonalternativemethods.whileitisbeyond thescopeofthisarticletoaddresswhetheritisbetteringeneraltosearchvaluefunction spaceorpolicyspacewedohopetohighlightsomeofthestrengthsoftheevolutionary approachtothereinforcementlearningproblem.thereaderisadvisednottoviewthis aiaccessfoundationandmorgankaufmannpublishers.allrightsreserved. articleasaneavs.tddiscussion.insomecasesthetwomethodsprovidecomplementary strengthssohybridapproachesareadvisable;infactoursurveyofimplementedsystems illustratesthatmanyea-basedreinforcementlearningsystemsincludeelementsoftdlearningaswell. thenextsectionspellsoutthereinforcementlearningproblem.inordertoprovidea specicanchorforthelaterdiscussionsection\u0003presentsaparticulartdmethod.section\u0004outlinestheapproachwecallevolutionaryalgorithmsforreinforcementlearning andprovidesasimpleexampleofaparticularearlsystem.thefollowingthree sectionsfocusonfeaturesthatdistinguisheasforrlfromeasforgeneralfunctionoptimizationincludingalternativepolicyrepresentationscreditassignmentmethodsand rl-specicgeneticoperators.sections\band highlightsomestrengthsandweaknesses oftheeaapproach.section\u0001\u0000brieysurveyssomesuccessfulapplicationsofeasystems onchallengingrltasks.thenalsectionsummarizesourpresentationandpointsout directionsforfurtherresearch. \u0002.reinforcementlearning allreinforcementlearningmethodssharethesamegoaltosolvesequentialdecisiontasks throughtrialanderrorinteractionswiththeenvironment.inasequentialdecisiontaskanagentinteracts withadynamicsystembyselectingactionsthataectstatetransitionstooptimizesome rewardfunction.moreformallyatanygiventimesteptanagentperceivesitsstate standselectsanactionat.thesystemrespondsbygivingtheagentsome numericalrewardrandchangingintostatest+\u0001=.thestatetransitionmaybe determinedsolelybythecurrentstateandtheagent'sactionormayalsoinvolvestochastic processes. theagent'sgoalistolearnapolicysawhichmapsstatestoactions.the optimalpolicycanbedenedinmanywaysbutistypicallydenedasthepolicythat producesthegreatestcumulativerewardoverallstatess =argmax wherevisthecumulativerewardreceivedfromstatesusingpolicy.therearealso manywaystocomputev.oneapproachusesadiscountratetodiscountrewards overtime.thesumisthencomputedoveraninnitehorizon v=\u0001xi=\u0000irt+i wherertistherewardreceivedattimestept.alternativelyvcouldbecomputedby summingtherewardsoveranitehorizonh v=hxi=\u0000rt+i theagent'sstatedescriptionsareusuallyidentiedwiththevaluesreturnedbyits sensorswhichprovideadescriptionofboththeagent'scurrentstateandthestateofthe evolutionaryalgorithmsforreinforcementlearning world.oftenthesensorsdonotgivetheagentcompletestateinformationandthusthe stateisonlypartiallyobservable. besidesreinforcementlearningintelligentagentscanbedesignedbyotherparadigms notablyplanningandsupervisedlearning.webrieynotesomeofthemajordierences amongtheseapproaches.ingeneralplanningmethodsrequireanexplicitmodelofthe statetransitionfunction.givensuchamodelaplanningalgorithmcansearch throughpossibleactionchoicestondanactionsequencethatwillguidetheagentfrom aninitialstatetoagoalstate.sinceplanningalgorithmsoperateusingamodelofthe environmenttheycanbacktrackor\\undo\"statetransitionsthatenterundesirablestates. incontrastrlisintendedtoapplytosituationsinwhichasucientlytractableaction modeldoesnotexist.consequentlyanagentintherlparadigmmustactivelyexplore itsenvironmentinordertoobservetheeectsofitsactions.unlikeplanningrlagents cannotnormallyundostatetransitions.ofcourseinsomecasesitmaybepossibleto buildupanactionmodelthroughexperienceenablingmoreplanningas experienceaccumulates.howeverrlresearchfocusesonthebehaviorofanagentwhenit hasinsucientknowledgetoperformplanning. agentscanalsobetrainedthroughsupervisedlearning.insupervisedlearningtheagent ispresentedwithexamplesofstate-actionpairsalongwithanindicationthattheaction waseithercorrectorincorrect.thegoalinsupervisedlearningistoinduceageneralpolicy fromthetrainingexamples.thussupervisedlearningrequiresanoraclethatcansupply correctlylabeledexamples. incontrastrldoesnotrequirepriorknowledgeofcorrect andincorrectdecisions.rlcanbeappliedtosituationsinwhichrewardsaresparse;for examplerewardsmaybeassociatedonlywithcertainstates. insuchcasesitmaybe impossibletoassociatealabelof\\correct\"or\\incorrect\"onparticulardecisionswithout referencetotheagent'ssubsequentdecisionsmakingsupervisedlearninginfeasible. insummaryrlprovidesaexibleapproachtothedesignofintelligentagentsinsituationsforwhichbothplanningandsupervisedlearningareimpractical.rlcanbeapplied toproblemsforwhichsignicantdomainknowledgeiseitherunavailableorcostlytoobtain. forexampleacommonrltaskisrobotcontrol.designersofautonomousrobotsoften lacksucientknowledgeoftheintendedoperationalenvironmenttouseeithertheplanning orthesupervisedlearningregimetodesignacontrolpolicyfortherobot.inthiscasethe goalofrlwouldbetoenabletherobottogenerateeectivedecisionpoliciesasitexplores itsenvironment. figure\u0001showsasimplesequentialdecisiontaskthatwillbeusedasanexamplelater inthispaper.thetaskoftheagentinthisgridworldistomovefromstatetostateby selectingamongtwoactionsrightordown.thesensoroftheagentreturnsthe identityofthecurrentstate.theagentalwaysstartsinstatea\u0001andreceivesthereward indicateduponvisitingeachstate.thetaskcontinuesuntiltheagentmovesothegrid world.thegoalistolearnapolicythatreturns thehighestcumulativerewards.forexampleapolicywhichresultsinthesequencesof actionsr;d;r;d;d;r;r;dstartingfromfromstatea\u0001givestheoptimalscoreof\u0001\u0007. moriartyschultz&grefenstette figure\u0001asimplegrid-worldsequentialdecisiontask.theagentstartsinstatea\u0001and receivestherowandcolumnofthecurrentboxassensoryinput.theagentmoves fromoneboxtoanotherbyselectingbetweentwomovesandthe agent'sscoreisincreasedbythepayoindicatedineachbox.thegoalistond apolicythatmaximizesthecumulativescore. \u0002.\u0001policyspacevs.value-functionspace giventhereinforcementlearningproblemasdescribedintheprevioussectionwenow addressthemaintopichowtondanoptimalpolicy.weconsidertwomainapproaches oneinvolvessearchinpolicyspaceandtheotherinvolvessearchinvaluefunctionspace. policy-spacesearchmethodsmaintainexplicitrepresentationsofpoliciesandmodify themthroughavarietyofsearchoperators.manysearchmethodshavebeenconsidered includingdynamicprogrammingvalueiterationsimulatedannealingandevolutionary algorithms.thispaperfocusesonevolutionaryalgorithmsthathavebeenspecializedfor thereinforcementlearningtask. incontrastvaluefunctionmethodsdonotmaintainanexplicitrepresentationofa insteadtheyattemptlearnthevaluefunctionvwhichreturnstheexpected policy. cumulativerewardfortheoptimalpolicyfromanystate.thefocusofresearchonvalue functionapproachestorlistodesignalgorithmsthatlearnthesevaluefunctionsthrough experience.themostcommonapproachtolearningvaluefunctionsisthetemporaldierencemethodwhichisdescribedinthenextsection. \u0003.temporaldierencealgorithmsforreinforcementlearning asstatedintheintroductionacomprehensivecomparisonofvaluefunctionsearchand directpolicy-spacesearchisbeyondthescopeofthispaper.neverthelessitwillbeuseful topointoutkeyconceptualdierencesbetweentypicalvaluefunctionmethodsandtypical evolutionaryalgorithmsforsearchingpolicyspace.themostcommonapproachforlearning avaluefunctionvforrlproblemsisthetemporaldierencemethod. evolutionaryalgorithmsforreinforcementlearning thetdlearningalgorithmusesobservationsofpredictiondierencesfromconsecutive statestoupdatevaluepredictions.forexampleiftwoconsecutivestatesiandjreturn payopredictionvaluesof\u0005and\u0002respectivelythenthedierencesuggeststhatthepayo fromstateimaybeoverestimatedandshouldbereducedtoagreewithpredictionsfrom statej.updatestothevaluefunctionvareachievedusingthefollowingupdaterule v=v+v+rt) whererepresentsthelearningrateandrtanyimmediatereward.thusthedierencein predictionsv)fromconsecutivestatesisusedasameasureofpredictionerror. considerachainofvaluepredictionsvvfromconsecutivestatetransitionswith thelastpredictionvcontainingtheonlynon-zerorewardfromtheenvironment.over manyiterationsofthissequencetheupdaterulewilladjustthevaluesofeachstatesothat theyagreewiththeirsuccessorsandeventuallywiththerewardreceivedinv.inother wordsthesinglerewardispropagatedbackwardsthroughthechainofvaluepredictions. thenetresultisanaccuratevaluefunctionthatcanbeusedtopredicttheexpectedreward fromanystateofthesystem. asmentionedearlierthegoaloftdmethodsistolearnthevaluefunctionforthe optimalpolicyv.givenvtheoptimalactioncanbecomputedusingthe followingequation =argmaxa ofcoursewehavealreadystatedthatinrlthestatetransitionfunctionisunknown totheagent.withoutthisknowledgewehavenowayofevaluating.analternative valuefunctionthatcanbeusedtocomputeiscalledaq-functionq.theq-functionisavaluefunctionthatrepresentsthe expectedvalueoftakingactionainstatesandactingoptimallythereafter q=r+v) whererrepresentsanyimmediaterewardreceivedinstates.giventheq-function actionsfromtheoptimalpolicycanbedirectlycomputedusingthefollowingequation =argmaxa table\u0001showstheq-functionforthegridworldproblemoffigure\u0001.thistable-based representationoftheq-functionassociatescumulativefuturepayosforeachstate-action pairinthesystem. thetdmethodadjuststheq-valuesaftereachdecision.whenselectingthenextaction theagentconsiderstheeectofthatactionbyexaminingtheexpectedvalueofthestate transitioncausedbytheaction. theq-functionislearnedthroughthefollowingtdupdateequation q=q+q+r) moriartyschultz&grefenstette a\u0001a\u0002a\u0003a\u0004a\u0005b\u0001b\u0002b\u0003b\u0004b\u0005c\u0001c\u0002c\u0003c\u0004c\u0005d\u0001d\u0002d\u0003d\u0004d\u0005e\u0001e\u0002e\u0003e\u0004e\u0005 \u0001\u0007\u0001\u0005 r\u0001\u0007\u0001\u0006\u0001\u0000\u0007 \u0001\u0005\u0001\u0004\u0001\u0002\b d\u0001\u0006\u0001\u0001\u0001\u0000 table\u0001aq-functionforthesimplegridworld.avalueisassociatedwitheachstate-action pair. essentiallythisequationupdatesqbasedonthecurrentrewardandthepredicted rewardifallfutureactionsareselectedoptimally.watkinsanddayanprovedthat ifupdatesareperformedinthisfashionandifeveryq-valueisexplicitlyrepresented theestimateswillasymptoticallyconvergetothecorrectvalues.areinforcementlearning systemcanthususetheqvaluestoselecttheoptimalactioninanystate.becauseqlearningisthemostwidelyknownimplementationoftemporaldierencelearningwewill useitinourqualitativecomparisonswithevolutionaryapproachesinlatersections. \u0004.evolutionaryalgorithmsforreinforcementlearning thepolicy-spaceapproachtorlsearchesforpoliciesthatoptimizeanappropriateobjective function.whilemanysearchalgorithmsmightbeusedthissurveyfocusesonevolutionary algorithms.webeginwithabriefoverviewofasimpleeaforrlfollowedbyadetailed discussionoffeaturesthatcharacterizethegeneralclassofeasforrl. \u0004.\u0001designconsiderationsforevolutionaryalgorithms evolutionaryalgorithmsareglobalsearchtechniquesderivedfromdarwin'stheory ofevolutionbynaturalselection.aneaiterativelyupdatesapopulationofpotential solutionswhichareoftenencodedinstructurescalledchromosomes.duringeachiteration calledagenerationtheeaevaluatessolutionsandgeneratesospringbasedonthetness ofeachsolutioninthetaskenvironment.substructuresorgenesofthesolutionsarethen modiedthroughgeneticoperatorssuchasmutationandrecombination.theideaisthat structuresthatareassociatedwithgoodsolutionscanbemutatedorcombinedtoform evenbettersolutionsinsubsequentgenerations.thecanonicalevolutionaryalgorithmis showninfigure\u0002.therehavebeenawidevarietyofeasdevelopedincludinggenetic algorithmsevolutionaryprogramminggeneticprogrammingandevolutionarystrategies.easaregeneralpurposesearchmethodsandhavebeenappliedinavarietyofdomains includingnumericalfunctionoptimizationcombinatorialoptimizationadaptivecontrol adaptivetestingandmachinelearning.onereasonforthewidespreadsuccessofeasis thattherearerelativelyfewrequirementsfortheirapplicationnamely \u0001.anappropriatemappingbetweenthesearchspaceandthespaceofchromosomesand \u0002.anappropriatetnessfunction. evolutionaryalgorithmsforreinforcementlearning procedureea begint=\u0000; initializep; evaluatestructuresinp; whileterminationconditionnotsatiseddo begint=t+\u0001; selectpfromp; alterstructuresinp; evaluatestructuresinp; end. figure\u0002pseudo-codeevolutionaryalgorithm. forexampleinthecaseofparameteroptimizationitiscommontorepresentthelistof parametersaseitheravectorofrealnumbersorabitstringthatencodestheparameters. witheitheroftheserepresentationsthe\\standard\"geneticoperatorsofmutationand cut-and-splicecrossovercanbeappliedinastraightforwardmannertoproducethegenetic variationsrequired.theusermuststilldecideonanumber ofcontrolparametersfortheeaincludingpopulationsizemutationratesrecombination ratesparentselectionrulesbutthereisanextensiveliteratureofstudieswhichsuggest thateasarerelativelyrobustoverawiderangeofcontrolparametersettingsercaruanaeshelman&das\u0001 ).thusformanyproblemseascanbe appliedinarelativelystraightforwardmanner. howeverformanyotherapplicationseasneedtobespecializedfortheproblemdomain.themostcriticaldesignchoicefacingtheuseristherepresentationthatisthemappingbetweenthesearchspaceofknowledgestructuresandthespaceofchromosomes.manystudieshave shownthattheeectivenessofeasissensitivetothechoiceofrepresentations.itisnot sucientforexampletochooseanarbitrarymappingfromthesearchspaceintothespace ofchromosomesapplythestandardgeneticoperatorsandhopeforthebest.whatmakesa goodmappingisasubjectforcontinuingresearchbutthegeneralconsensusisthatcandidatesolutionsthatshareimportantphenotypicsimilaritiesmustalsoexhibitsimilarforms of\\buildingblocks\"whenrepresentedaschromosomes.itfollowsthatthe userofaneamustcarefullyconsiderthemostnaturalwaytorepresenttheelementsof thesearchspaceaschromosomes.moreoveritisoftennecessarytodesignappropriate mutationandrecombinationoperatorsthatarespecictothechosenrepresentation.the endresultofthisdesignprocessisthattherepresentationandgeneticoperatorsselected fortheeacompriseaformofsearchbiassimilartobiasesinothermachinelearningmeth\u0002\u0004\u0007 moriartyschultz&grefenstette figure\u0003geneticoperatorsonxed-positionrepresentation.thetwoospringaregeneratedbycrossingovertheselectedparents.theoperationshowniscalledone-point crossover.therstospringinheritstheinitialsegmentofoneparentandthe nalsegmentoftheotherparent.thesecondospringinheritsthesamepattern ofgenesfromtheoppositeparents.thecrossoverpointisposition\u0003chosenat random.thesecondospringhasalsoincurredamutationintheshadedgene. ods.giventheproperbiastheeacanquicklyidentifyuseful\\buildingblocks\"withinthe populationandconvergeonthemostpromisingareasofthesearchspace.\u0001 inthecaseofrltheuserneedstomaketwomajordesigndecisions.firsthowwillthe spaceofpoliciesberepresentedbychromosomesintheea?secondhowwillthetnessof populationelementsbeassessed?theanswerstothesequestionsdependonhowtheuser choosestobiastheea.thenextsectionpresentsasimpleearlthatadoptsthemost straightforwardsetofdesigndecisions.thisexampleismeantonlytoprovideabaseline forcomparisonwithmoreelaboratedesigns. \u0004.\u0002asimpleearl astheremainderofthispapershowstherearemanywaystouseeastosearchthespace ofrlpolicies.thissectionprovidesaconcreteexampleofasimpleearlwhichwecall earl\u0001.thepseudo-codeisshowninfigure\u0004.thissystemprovidestheeacounterpart tothesimpletable-basedtdsystemdescribedinsection\u0003. themoststraightforwardwaytorepresentapolicyinaneaistouseasinglechromosomeperpolicywithasinglegeneassociatedwitheachobservedstate.inearl\u0001each gene'svaluerepresentstheactionvalueassociatedwith thecorrespondingstateasshowninfigure\u0005.table\u0002showspartofanearl\u0001population ofpoliciesforthesamplegridworldproblem.thenumberofpoliciesinapopulationis usuallyontheorderof\u0001\u0000\u0000to\u0001\u0000\u0000\u0000. thetnessofeachpolicyinthepopulationmustreecttheexpectedaccumulatedtness foranagentthatusesthegivenpolicy.therearenoxedconstraintsonhowthetnessof anindividualpolicyisevaluated.iftheworldisdeterministiclikethesamplegrid-world \u0001.otherwaystoexploitproblemspecicknowledgeineasincludetheuseofheuristicstoinitializethe populationandthehybridizationwithproblemspecicsearchalgorithms.seefor furtherdiscussionsofthesemethods. evolutionaryalgorithmsforreinforcementlearning procedureearl-\u0001 begint=\u0000; initializeapopulationofpoliciesp; evaluatepoliciesinp; whileterminationconditionnotsatiseddo begint=t+\u0001; selecthigh-payopoliciespfrompoliciesinp; updatepoliciesinp; evaluatepoliciesinp; end. figure\u0004pseudo-codeforevolutionaryalgorithmreinforcementlearningsystem. figure\u0005table-basedpolicyrepresentation.eachobservedstatehasagenewhichindicates thepreferredactionforthatstate.withthisrepresentationstandardgenetic operatorssuchasmutationandcrossovercanbeapplied. thetnessofapolicycanbeevaluatedduringasingletrialthatstartswiththeagentinthe initialstateandterminateswhentheagentreachesaterminalstatethegrid inthegrid-world).innon-deterministicworldsthetnessofapolicyisusuallyaveraged overasampleoftrials.otheroptionsincludemeasuringthetotalpayoachievedbythe agentafteraxednumberofstepsormeasuringthenumberofstepsrequiredtoachieve axedlevelofpayo. oncethetnessofallpoliciesinthepopulationhasbeendeterminedanewpopulation isgeneratedaccordingtothestepsintheusualea.firstparentsareselected forreproduction.atypicalselectionmethodistoprobabilisticallyselectindividualsbased onrelativetness fitness pnj=\u0001fitness wherepirepresentsindividualiandnisthetotalnumberofindividuals.usingthisselection ruletheexpectednumberofospringforagivenpolicyisproportionaltothatpolicy's tness.forexampleapolicywithaveragetnessmighthaveasingleospringwhereas moriartyschultz&grefenstette policya\u0001a\u0002a\u0003a\u0004a\u0005b\u0001b\u0002b\u0003b\u0004b\u0005c\u0001c\u0002c\u0003c\u0004c\u0005d\u0001d\u0002d\u0003d\u0004d\u0005e\u0001e\u0002e\u0003e\u0004e\u0005fitness drddrrrrrrdrddrrdrrrdrrdr ddddrrrrrrddrrdrdrrrdrddr rddrrdrdrrdddrdrdrrrdrddd ddddrdrrrrrdrrrdrrdrdrddr rdddrdrrdrrdrrdrdrrddrddd table\u0002aneapopulationofvedecisionpoliciesforthesamplegridworld.thissimple policyrepresentationspeciesanactionforeachstateoftheworld.thetness correspondstothepayosthatareaccumulatedusingeachpolicyinthegrid world. apolicywithtwicetheaveragetnesswouldhavetwoospring.\u0002ospringareformed bycloningtheselectedparents.thennewpoliciesaregeneratedbyapplyingthestandard geneticoperatorsofcrossoverandmutationtotheclonesasshowninfigure\u0003.theprocess ofgeneratingnewpopulationsofstrategiescancontinueindenitelyorcanbeterminated afteraxednumberofgenerationsoronceanacceptablelevelofperformanceisachieved. forsimplerlproblemssuchasthegrid-worldearl\u0001mayprovideanadequateapproach. inlatersectionswewillpointoutsomewaysinwhichevenearl\u0001exhibits strengthsthatarecomplementarytotdmethodsforrl.howeverasinthecaseoftd methodsearlmethodshavebeenextendedtohandlethemanychallengesinherentin morerealisticrlproblems.thefollowingsectionssurveysomeoftheseextensionsorganizedaroundthreespecicbiasesthatdistinguisheasforreinforcementlearning frommoregenericeaspolicyrepresentationstness/credit-assignmentmodelsandrlspecicgeneticoperators. \u0005.policyrepresentationsinearl perhapsthemostcriticalfeaturethatdistinguishesclassesofeasfromoneanotheristhe representationused.forexampleeasforfunctionoptimizationuseasimplestringor vectorrepresentationwhereaseasforcombinatorialoptimizationusedistinctiverepresentationsforpermutationstreesorothergraphstructures.likewiseeasforrlusea distinctivesetofrepresentationsforpolicies.whiletherangeofpotentialpolicyrepresentationsisunlimitedtherepresentationsusedinmostearlsystemstodatecanbe largelycategorizedalongtwodiscretedimensions.firstpoliciesmayberepresentedeitherbycondition-actionrulesorbyneuralnetworks.secondpoliciesmayberepresented byasinglechromosomeortherepresentationmaybedistributedthroughoneormore populations. \u0005.\u0001single-chromosomerepresentationofpolicies \u0005.\u0001.\u0001rule-basedpolicies formostrlproblemsofpracticalinterestthenumberofobservablestatesisverylarge andthesimpletable-basedrepresentationinearl\u0001isimpractical.forlargescalestate \u0002.manyotherparentselectionruleshavebeenexplored. evolutionaryalgorithmsforreinforcementlearning figure\u0006rule-basedpolicyrepresentation.eachgenerepresentsacondition-actionrule thatmapsasetofstatestoanaction.ingeneralsuchrulesareindependent ofthepositionalongthechromosome.conictresolutionmechanismsmaybe needediftheconditionsofrulesareallowedtointersect. figure\u0007asimpleparameterrepresentationofweightsforaneuralnetwork.thetness ofthepolicyisthepayowhentheagentusesthecorrespondingneuralnetas itsdecisionpolicy. spacesitismorereasonabletorepresentapolicyasasetofcondition-actionrulesinwhich theconditionexpressesapredicatethatmatchesasetofstatesasshowninfigure\u0006.early examplesofthisrepresentationincludethesystemsls-\u0001andls-\u0002er &grefenstette\u0001 \b\u0005)followedlaterbysamuel. \u0005.\u0001.\u0002neuralnetrepresentationofpolicies asintd-basedrlsystemsearlsystemsoftenemployneuralnetrepresentationsas functionapproximators. inthesimplestcaseaneuralnetworkforthe agent'sdecisionpolicyisrepresentedasasequenceofreal-valuedconnectionweights.a straightforwardeaforparameteroptimizationcanbeusedtooptimizetheweightsof theneuralnetwork.thisrepresentationthusrequirestheleast modicationofthestandardea.wenowturntodistributedrepresentationsofpoliciesin earlsystems. \u0005.\u0002distributedrepresentationofpolicies intheprevioussectionweoutlinedearlapproachesthattreattheagent'sdecisionpolicy asasinglegeneticstructurethatevolvesovertime.thissectionaddressesearlapproaches thatdecomposeadecisionpolicyintosmallercomponents.suchapproacheshavetwo potentialadvantages.firsttheyallowevolutiontoworkatamoredetailedlevelofthetask e.g.onspecicsubtasks.presumablyevolvingasolutiontoarestrictedsubtaskshouldbe moriartyschultz&grefenstette figure\bholland'slearningclassiersystem. easierthanevolvingamonolithicpolicyforacomplextask.seconddecompositionpermits theusertoexploitbackgroundknowledge.theusermightbasethedecompositioninto subtasksonaprioranalysisoftheoverallperformancetask;forexampleitmightbeknown thatcertainsubtasksaremutuallyexclusiveandcanthereforebelearnedindependently. theusermightalsodecomposeacomplextaskintosubtaskssuchthatcertaincomponents canbeexplicitlyprogrammedwhileothercomponentsarelearned. intermsofknowledgerepresentationinearlthealternativetothesinglechromosome representationistodistributethepolicyoverseveralpopulationelements.byassigninga tnesstotheseindividualelementsofthepolicyevolutionaryselectionpressurecanbe broughttobearonmoredetailedaspectsofthelearningtask.thatistnessisnowa functionofindividualsubpoliciesorindividualrulesorevenindividualneurons.thisgeneral approachisanalogoustotheclassictdmethodsthattakethisapproachtotheextremeof learningstatisticsconcerningeachstate-actionpair.asinthecaseofsingle-chromosome representationswecanpartitiondistributedearlrepresentationsintorule-basedand neural-net-basedclasses. \u0005.\u0002.\u0001distributedrule-basedpolicies themostwell-knownexampleofadistributedrule-basedapproachtoearlisthelearningclassiersystemsmodel.anlcsusesanevolutionaryalgorithmtoevolveif-thenrulescalledclassiersthat mapsensoryinputtoanappropriateaction.figure\boutlinesholland'slcsframework .whensensoryinputisreceiveditispostedonthemessagelist.iftheleft handsideofaclassiermatchesamessageonthemessagelistitsrighthandsideisposted onthemessagelist.thesenewmessagesmaysubsequentlytriggerotherclassierstopost messagesorinvokeadecisionfromthelcsasinthetraditionalforward-chainingmodel ofrule-basedsystems. inanlcseachchromosomerepresentsasingledecisionruleandtheentirepopulation representstheagent'spolicy.ingeneralclassiersmapasetofobservedstatestoasetof messageswhichmaybeinterpretedaseitherinternalstatechangesoractions.forexample evolutionaryalgorithmsforreinforcementlearning condition actionstrength \u0000.\u0007\u0005 \u0000.\u0002\u0005 \u0000.\u0005\u0000 table\u0003lcspopulationforgridworld.theisadon'tcaresymbolwhichallowsfor generalityinconditions.forexampletherstrulesays\\turnrightincolumn a.\"thestrengthofaruleisusedforconictresolutionandforparentselectionin thegeneticalgorithm. figure atwo-levelhierarchicalalecsyssystem.eachlcslearnsaspecicbehavior. theinteractionsamongtherulesetsarepre-programmed. ifthelearningagentforthegridworldinfigure\u0001hastwosensorsoneforthecolumnand onefortherowthenthepopulationinanlcsmightappearasshownintable\u0003.the rstclassiermatchesanystateinthecolumnaandrecommendsactionr.eachclassier hasastatisticcalledstrengththatestimatestheutilityoftherule.thestrengthstatistics areusedinbothconictresolutionandas tnessforthegeneticalgorithm.geneticoperatorsareappliedtohighlytclassiersto generatenewrules.generallythepopulationsize iskeptconstant.thusclassierscompeteforspaceinthepolicy. anotherwaythatearlsystemsdistributetherepresentationofpoliciesistopartition thepolicyintoseparatemoduleswitheachmoduleupdatedbyitsownea.dorigoand colombettidescribeanarchitecturecalledalecsysinwhichacomplexreinforcementlearningtaskisdecomposedintosubtaskseachofwhichislearnedviaaseparate lcsasshowninfigure .theyprovideamethodcalledbehavioranalysisandtraining tomanagetheincrementaltrainingofagentsusingthedistributedlcsarchitecture. thesingle-chromosomerepresentationcanalsobeextendedbypartitioningthepolicyacrossmultipleco-evolvingpopulations.forexampleinthecooperativeco-evolution modeltheagent'spolicyisformedbycombiningchromosomesfromseveralindependentlyevolvingpopulations.eachchromosomerepresentsasetofrulesas infigure\u0006buttheserulesaddressonlyasubsetoftheperformancetask.forexample separatepopulationsmightevolvepoliciesfordierentcomponentsofacomplextaskor moriartyschultz&grefenstette figure\u0001\u0000cooperativecoevolutionaryarchitecturefromtheperspectiveoftheitheainstance.eacheacontributesarepresentativewhichismergedwiththeothers' representativestoformacollaborationorpolicyfortheagent.thetnessof eachrepresentativereectstheaveragetnessofitscollaborations. mightaddressmutuallyexclusivesetsofobservedstates.thetnessofeachchromosomeis computedbasedontheoveralltnessoftheagentsthatemploythatchromosomeaspartof itscombinedchromosomes.thecombinedchromosomesrepresentthedecisionpolicyand arecalledacollaboration. \u0005.\u0002.\u0002distributednetwork-basedpolicies distributedearlsystemsusingneuralnetrepresentationshavealsobeendesigned.in separatepopulationsofneuronsevolvewiththeevaluationof eachneuronbasedonthetnessofacollaborationofneuronsselectedfromeachpopulation. insanetwoseparatepopulationsaremaintained andevolvedapopulationofneuronsandapopulationofnetworkblueprints.themotivationforsanecomesfromouraprioriknowledgethatindividualneuronsarefundamental buildingblocksinneuralnetworks.saneexplicitlydecomposestheneuralnetworksearch problemintoseveralparallelsearchesforeectivesingleneurons.theneuron-levelevolutionprovidesevaluationandrecombinationoftheneuralnetworkbuildingblockswhilethe populationofblueprintssearchforeectivecombinationsofthesebuildingblocks.figure\u0001\u0001 givesanoverviewoftheinteractionofthetwopopulations. eachindividualintheblueprintpopulationconsistsofasetofpointerstoindividuals intheneuronpopulation.duringeachgenerationneuralnetworksareconstructedby combiningthehiddenneuronsspeciedineachblueprint.eachblueprintreceivesatness accordingtohowwellthecorrespondingnetworkperformsinthetask.eachneuronreceives atnessaccordingtohowwellthetopnetworksinwhichitparticipatesperforminthe task.anaggressivegeneticselectionandrecombinationstrategyisusedtoquicklybuild andpropagatehighlytstructuresinboththeneuronandblueprintpopulations. evolutionaryalgorithmsforreinforcementlearning figure\u0001\u0001anoverviewofthetwopopulationsinsane.eachmemberoftheneuronpopulationspeciesaseriesofconnectionstobe madewithinaneuralnetwork.eachmemberofthenetworkblueprintpopulationspeciesaseriesofpointerstospecicneuronswhichareusedtobuilda neuralnetwork. \u0006.fitnessandcreditassignmentinearl evolutionaryalgorithmsarealldrivenbytheconceptofnaturalselectionpopulation elementsthathavehighertnessleavemoreospringtolatergenerationsthusinuencing thedirectionofsearchinfavorofhighperformanceregionsofthesearchspace.theconcept oftnessiscentraltoanyea.inthissectionwediscussfeaturesofthetnessmodelthat arecommonacrossmostearlsystems.wespecicallyfocusonwaysinwhichthetness functionreectsthedistinctivestructureoftherlproblem. \u0006.\u0001theagentmodel therstcommonfeaturesofallearltnessmodelsisthattnessiscomputedwith respecttoanrlagent.thatishoweverthepolicyisrepresentedintheeaitmustbe convertedtoadecisionpolicyforanagentoperatinginarlenvironment.theagentis assumedtoobserveadescriptionofthecurrentstateselectitsnextactionbyconsulting itscurrentpolicyandcollectwhateverrewardisprovidedbytheenvironment.inearl systemsasintdsystemstheagentisgenerallyassumedtoperformverylittleadditional computationwhenselectingitsnextaction.whileneitherapproachlimitstheagentto strictstimulus-responsebehavioritisusuallyassumedthattheagentdoesnotperform extensiveplanningorotherreasoningbeforeacting.thisassumptionreectsthefactthat rltasksinvolvesomesortofcontrolactivityinwhichtheagentmustrespondtoadynamic environmentwithinalimitedtimeframe. moriartyschultz&grefenstette \u0006.\u0002policylevelcreditassignment asshownintheprevioussectionthemeaningoftnessinearlsystemsmayvarydependingonwhatthepopulationelementsrepresent.inasingle-chromosomerepresentation tnessisassociatedwithentirepolicies;inadistributedrepresentationtnessmaybeasinanycasetnessalwaysreectsaccumulated sociatedwithindividualdecisionrules. rewardsreceivedbytheagentduringthecourseofinteractionwiththeenvironmentas speciedintherlmodel.fitnessmayalsoreecteortexpendedoramountofdelay. itisworthwhileconsideringthedierentapproachestocreditassignmentinthetd andeamethods. inareinforcementlearningproblempayosmaybesparsethatis associatedonlywithcertainstates.consequentlyapayomayreectthequalityofan extendedsequenceofdecisionsratherthananyindividualdecision.forexamplearobot mayreceivearewardafteramovementthatplacesitina\\goal\"positionwithinaroom. therobot'srewardhoweverdependsonmanyofitspreviousmovementsleadingitto thatpoint.adicultcreditassignmentproblemthereforeexistsinhowtoapportionthe rewardsofasequenceofdecisionstoindividualdecisions. ingeneraleaandtdmethodsaddressthecreditassignmentprobleminverydifferentways.intdapproachescreditfromtherewardsignalisexplicitlypropagatedto eachdecisionmadebytheagent.overmanyiterationspayosaredistributedacrossa sequenceofdecisionssothatanappropriatelydiscountedrewardvalueisassociatedwith eachindividualstateanddecisionpair. insimpleearlsystemssuchasearl\u0001rewardsareassociatedonlywithsequences ofdecisionsandarenotdistributedtotheindividualdecisions.creditassignmentforan individualdecisionismadeimplicitlysincepoliciesthatprescribepoorindividualdecisions willhavefewerospringinfuturegenerations.byselectingagainstpoorpoliciesevolution automaticallyselectsagainstpoorindividualdecisions.thatisbuildingblocksconsisting ofparticularstate-actionpairsthatarehighlycorrelatedwithgoodpoliciesarepropagated throughthepopulationreplacingstate-actionpairsassociatedwithpoorerpolicies. figure\u0001\u0002illustratesthedierencesincreditassignmentbetweentdandearl\u0001inthe gridworldoffigure\u0001.theq-learningtdmethodexplicitlyassignscreditorblametoeach individualstate-actionpairbypassingbacktheimmediaterewardandtheestimatedpayo fromthenewstate.thusanerrortermbecomesassociatedwitheachactionperformedby theagent.theeaapproachdoesnotexplicitlypropagatecredittoeachactionbutrather associatesanoveralltnesswiththeentirepolicy.creditisassignedimplicitlybasedonthe tnessevaluationsofentiresequencesofdecisions.consequentlytheeawilltendtoselect againstpoliciesthatgeneratetherstandthirdsequencesbecausetheyachievelowertness scores.theeathusimplicitlyselectsagainstactiondinstateb\u0002forexamplewhichis presentinthebadsequencesbutnotpresentinthegoodsequences. \u0006.\u0003subpolicycreditassignment besidestheimplicitcreditassignmentperformedonbuildingblocksearlsystemshave alsoaddressedthecreditassignmentproblemmoredirectly.asshowninsection\u0004the individualsinanearlsystemmightrepresenteitherentirepoliciesorcomponentsof apolicy.for distributed-representationearlstnessisexplicitlyassignedtoindividualcomponents. evolutionaryalgorithmsforreinforcementlearning figure\u0001\u0002explicitvs.implicitcreditassignment.theq-learningtdmethodassignscredit toeachstate-actionpairbasedontheimmediaterewardandthepredictedfuture rewards.theeamethodassignscreditimplicitlybyassociatingtnessvalues withentiresequencesofdecisions. incasesinwhichapolicyisrepresentedbyexplicitcomponentsdierenttnessfunctions canbeassociatedwithdierentevolvingpopulationsallowingtheimplementerto\\shape\" theoverallpolicybyevolvingsubpoliciesforspecicsubtasks.themostambitiousgoalistoallowthesystemto managethenumberofco-evolvingspeciesaswellastheformofinteractions. thisexcitingresearchisstillatanearlystage. forexampleinthelcsmodeleachclassierhasastrengthwhichis updatedusingatd-likemethodcalledthebucketbrigadealgorithm.inthe bucketbrigadealgorithmthestrengthofaclassierisusedtobidagainstotherclassiers fortherighttopostmessages.bidsaresubtractedfromwinningclassiersandpassedback totheclassiersthatpostedtheenablingmessageonthepreviousstep.classierstrengths arethusreinforcediftheclassierpostsamessagethattriggersanotherclassier.the classierthatinvokesadecisionfromthelcsreceivesastrengthreinforcementdirectly fromtheenvironment.thebucketbrigadebidpassingmechanismclearlybearsastrong relationtothemethodoftemporaldierences.thebucketbrigadeupdates agivenclassier'sstrengthbasedonthestrengthoftheclassiersthatreasadirectresult ofitsactivation.thetdmethodsdierslightlyinthisrespectbecausetheyassigncredit basedstrictlyontemporalsuccessionanddonottakeintoaccountcausalrelationsofsteps. itremainsunclearwhichismoreappropriatefordistributingcredit. evenforsinglechromosomerepresentationstd-likemethodshavebeenadoptedin someearlsystems.insamueleachgenealsomaintainsaquantitycalled strengththatisusedtoresolveconictwhenmorethanonerulematchestheagent'scurrent sensorreadings.whenpayoisobtainedthestrengthsof allrulesthatredduringthetrialareupdated.inadditiontoresolving conictsarule'sstrengthalsoplaysaroleintriggeringmutationoperationsasdescribed inthenextsection. \u0007.rl-specicgeneticoperators thecreationofspecialgeneticoperatorsprovidesanotheravenueforimposinganrlspecicbiasoneas.specializedoperatorsinearlsystemsrstappearedininwhichso-calledtriggeredoperatorswereresponsibleforcreatingnewclassiers whenthelearningagentfoundthatnoclassierinitsexistingpopulationmatchedthe agent'scurrentsensorreadings.inthiscaseahigh-strengthrulewasexplicitlygeneralized tocoverthenewsetofsensorreadings.asimilarrule-creationoperatorwasincludedin earlyversionsofsamuel.laterversionsofsamuelincluded anumberofmutationoperatorswhichcreatedalteredrulesbasedonanagent'searly experiences.forexamplesamuel'sspecializationmutationoperatoristriggeredwhen alow-strengthgeneralruleresduringanepisodethatresultsinhighpayo.insucha casetherule'sconditionsarereducedingeneralitytomorecloselymatchtheagent'ssensor readings.forexampleiftheagenthasasensorreadings andtheoriginalruleis ifrange=andbearing=thensetturn=\u0002\u0004 thenthenewrulewouldbe ifrange=andbearing=thensetturn=\u0002\u0004sincetheepisodetriggeringtheoperatorresultedinhighpayoonemightsuspectthat theoriginalrulewasover-generalizedandthatthenewmorespecicversionmightlead tobetterresults.receivedduring thetriggeringepisode.)thisisconsideredalamarckianoperatorbecausetheagent's experienceiscausingageneticchangewhichispassedontolaterospring.\u0003 samuelalsousesanrl-speciccrossoveroperatortorecombinepolicies.inparticular crossoverinsamuelattemptstoclusterdecisionrulesbeforeassigningthemtoospring. forexamplesupposethatthetracesofthemostpreviousevaluationsoftheparentstrategiesareasfollows traceforparent\u0001 episode...\b.r\u0001;\u0003r\u0001;\u0001r\u0001;\u0007r\u0001;\u0005 highpayo .r\u0001;\u0002r\u0001;\br\u0001;\u0004 lowpayo \u0003.jeanbaptistelamarckdevelopedanevolutionarytheorythatstressedtheinheritanceofacquiredcharacteristicsinparticularacquiredcharacteristicsthatarewelladaptedtothesurroundingenvironment. ofcourselamarck'stheorywassupersededbydarwin'semphasisontwo-stageadaptationundirected variationfollowedbyselection.researchhasgenerallyfailedtosubstantiateanylamarckianmechanisms inbiologicalsystems. evolutionaryalgorithmsforreinforcementlearning traceforparent\u0002 \u0004.r\u0002;\u0007r\u0002;\u0005 lowpayo \u0005.r\u0002;\u0006r\u0002;\u0002r\u0002;\u0004 highpayo thenonepossibleospringwouldbe fr\u0001;\b;;r\u0001;\u0003;r\u0001;\u0001;r\u0001;\u0007;r\u0001;\u0005;;r\u0002;\u0006;r\u0002;\u0002;r\u0002;\u0004;;r\u0002;\u0007g themotivationhereisthatrulesthatreinsequencetoachieveahighpayoshouldbe treatedasagroupduringrecombinationinordertoincreasethelikelihoodthattheospring policywillinheritsomeofthebetterbehaviorpatternsofitsparents.rulesthatdonot reinsuccessfulepisodesarerandomlyassignedtooneofthetwoospring. thisformofcrossoverisnotonlylamarckianbutisdirectlyrelatedtothestructureoftherlproblemsinceitgroups componentsofpoliciesaccordingtothetemporalassociationamongthedecisionrules. \b.strengthsofearl theeaapproachrepresentsaninterestingalternativeforsolvingrlproblemsoering severalpotentialadvantagesforscalinguptorealisticapplications.inparticularearl systemshavebeendevelopedthataddressdicultchallengesinrlproblemsincluding largestatespaces; incompletestateinformation;and non-stationaryenvironments. thissectionfocusesonwaysthatearladdressthesechallenges. \b.\u0001scalinguptolargestatespaces manyearlypapersintherlliteratureanalyzetheeciencyofalternativelearningmethods ontoyproblemssimilartothegridworldshowninfigure\u0001.whilesuchstudiesareuseful asacademicexercisesthenumberofobservedstatesinrealisticapplicationsofrlislikely toprecludeanyapproachthatrequirestheexplicitstorageandmanipulationofstatistics associatedwitheachobservablestate-actionpair.therearetwowaysthatearlpolicy representationshelpaddresstheproblemoflargestatespacesgeneralizationandselectivity. \b.\u0001.\u0001policygeneralization mostearlpolicyrepresentationsspecifythepolicyatalevelofabstractionhigherthanan explicitmappingfromobservedstatestoactions.inthecaseofrule-basedrepresentations therulelanguageallowsconditionstomatchsetsofstatesthusgreatlyreducingthestorage moriartyschultz&grefenstette a\u0001a\u0002a\u0003a\u0004a\u0005b\u0001b\u0002b\u0003b\u0004b\u0005c\u0001c\u0002c\u0003c\u0004c\u0005d\u0001d\u0002d\u0003d\u0004d\u0005e\u0001e\u0002e\u0003e\u0004e\u0005 \u0001\u0002\u0001\u0001\u0001\u0001\u0001\u0002\u0001\u0004\u0007\u0001\u0002\u0001\u0003 \u0001\u0002\u0001\u0001\u0001\u0002\u0001\u0002\u0001\u0001 r\u0001\u0006\u0007 ?\u0001\u0007\u0001\u0002\b ?\u0001\u0002\u0007 \u0001\u0007\u0001\u0006 \u0001\u0001\u0001\u0003\u0001\u0002\u0007\u0001\u0004\u0001\u0001\u0001\u0002 \u0001\u0003\u0001\u0002\u0001\u0001? \u0001\u0001\u0001\u0006\u0001\u0002?\u0001\u0003\u0001\u0002\u0001\u0006 table\u0004anapproximatedvaluefunctionfromthepopulationintable\u0002.thetabledisplaystheaveragetnessforpoliciesthatselecteachstate-actionpairandreects theestimatedimpacteachactionhasonoveralltness.giventhetinypopulation sizeinthisexampletheestimatesarenotparticularlyaccurate.notethequestion marksinstateswhereactionshaveconverged.sincenopoliciesselectthealternativeactionthepopulationhasnostatisticsontheimpactoftheseactionson tness.thisisdierentfromsimpletdmethodswherestatisticsonallactions aremaintained. requiredtospecifyapolicy.itshouldbenotedhoweverthatthegeneralityoftherules withinapolicymayvaryconsiderablyfromthelevelofrulesthatspecifyanactionfor asingleobservedstateallthewaytocompletelygeneralrulesthatrecommendanaction regardlessofthecurrentstate.likewiseinneuralnetrepresentationsthemappingfunction isstoredimplicitlyintheweightsontheconnectionsoftheneuralnet.ineithercasea generalizedpolicyrepresentationfacilitatesthesearchforgoodpoliciesbygroupingtogether statesforwhichthesameactionisrequired. \b.\u0001.\u0002policyselectivity mostearlsystemshaveselectiverepresentationsofpolicies.thatistheealearnsmappingsfromobservedstatestorecommendedactionsusuallyeliminatingexplicitinformation concerninglessdesirableactions.knowledgeaboutbaddecisionsisnotexplicitlypreserved sincepoliciesthatmakesuchdecisionsareselectedagainstbytheevolutionaryalgorithm andareeventuallyeliminatedfromthepopulation.theadvantageofselectiverepresentationsisthatattentionisfocusedonprotableactionsonlyreducingspacerequirementsfor policies.considerourexampleofthesimpleearloperatingonthegridworld.asthepopulationevolvespoliciesnormallyconvergetothebestactionsfromaspecicstatebecauseof theselectivepressuretoachievehightnesslevels.forexamplethepopulationshownin table\u0002hasconvergedallelesinstatesa\u0003;a\u0005;b\u0002;b\u0005;d\u0003;e\u0001;ande\u0002.eachofthese convergedstate-actionpairsishighlycorrelatedwithtness.forexampleallpolicieshave convergedtoactionrinstateb\u0002.takingactionrinstateb\u0002achievesamuchhigher expectedreturnthanactiond.policiesthatselectactiondfrom stateb\u0002achievelowertnessscoresandareselectedagainst.forthissimpleearlasnapshotofthepopulationprovidesanimplicitestimateofacorrespondingtdvalue functionbutthedistributionisbiasedtowardthemoreprotablestate-actions pairs. evolutionaryalgorithmsforreinforcementlearning figure\u0001\u0003anenvironmentwithincompletestateinformation.thecirclesrepresentthe statesoftheworldandthecolorsrepresenttheagent'ssensoryinput.theagent isequallylikelytostartintheredstateorthegreenstate \b.\u0002dealingwithincompletestateinformation clearlythemostfavorableconditionforreinforcementlearningoccurswhentheagentcan observethetruestateofthedynamicsystemwithwhichitinteracts.whencompletestate informationisavailabletdmethodsmakeecientuseofavailablefeedbackbyassociating rewarddirectlywithindividualdecisions. inrealworldsituationshowevertheagent's sensorsaremorelikelytoprovideonlyapartialviewthatmayfailtodisambiguatemany states.consequentlytheagentwilloftenbeunabletocompletelydistinguishitscurrent state.thisproblemhasbeentermedperceptualaliasingorthehiddenstateproblem.in thecaseoflimitedsensoryinformationitmaybemoreusefultoassociaterewardswith largerblocksofdecisions.considerthesituationinfigure\u0001\u0003inwhichtheagentmust actwithoutcompletestateinformation.circlesrepresentthespecicstatesoftheworld andthecolorsrepresentthesensorinformationtheagentreceiveswithinthestate.square nodesrepresentgoalstateswiththecorrespondingrewardshowninside.ineachstatethe agenthasachoiceoftwoactions.wefurtherassumethatthestatetransitions aredeterministicandthattheagentisequallylikelytostartineitherthestatewiththe redorgreensensorreadings. inthisexampletherearetwodierentstatesthatreturnasensorreadingofblue andtheagentisunabletodistinguishbetweenthem.moreovertheactionsforeachblue statereturnverydierentrewards.aqfunctionappliedtothisproblemtreatsthesensor readingofblueasoneobservablestateandtherewardsforeachactionareaveragedover bothbluestates.thusqandqwillconvergeto-\u0000.\u0005and\u0001respectively. sincetherewardfromqishigherthanthealternativesfromobservablestatesred andgreentheagent'spolicyunderq-learningwillchoosetoenterobservablestateblue eachtime.thenaldecisionpolicyunderq-learningisshownintable\u0005.thistablealso showstheoptimalpolicywithrespecttotheagent'slimitedviewofitsworld.inother moriartyschultz&grefenstette valuefunctionpolicyoptimalpolicy green blue \u0001.\b\u0007\u0005 expectedreward table\u0005thepolicyandexpectedrewardreturnedbyaconvergedqfunctioncomparedto theoptimalpolicygiventhesamesensoryinformation. wordsthepolicyreectstheoptimalchoicesiftheagentcannotdistinguishthetwoblue states.byassociatingvalueswithindividualobservablestatesthesimpletdmethodsare inthisexampletheambiguousstateinformation vulnerabletohiddenstateproblems. misleadsthetdmethodanditmistakenlycombinestherewardsfromtwodierentstates ofthesystem.byconfoundinginformationfrommultiplestatestdcannotrecognizethat advantagesmightbeassociatedwithspecicactionsfromspecicstatesforexamplethat actionlfromthetopbluestateachievesaveryhighreward. incontrastsinceeamethodsassociatecreditwithentirepoliciestheyrelymoreon thenetresultsofdecisionsequencesthanonsensorinformationthatmayafterallbe ambiguous.inthisexampletheevolutionaryalgorithmexploitsthedisparityinrewards fromthedierentbluestatesandevolvespoliciesthatenterthegoodbluestateandavoid thebadone.theagentitselfremainsunabletodistinguishthetwobluestatesbuttheevolutionaryalgorithmimplicitlydistinguishesamongambiguousstatesbyrewardingpolicies thatavoidthebadstates. forexampleaneamethodcanbeexpectedtoevolveanoptimalpolicyinthecurrent examplegiventheexistingambiguousstateinformation.policiesthatchoosetheaction sequencerlwhenstartingintheredstatewillachievethehighestlevelsoftnessand willthereforebeselectedforreproductionbytheea.ifagentsusingthesepoliciesare placedinthegreenstateandselectactionltheyreceivethelowesttnessscoresince theirsubsequentactionlfromthebluesensorsreturnsanegativereward.thusmanyof thepoliciesthatachievehightnesswhenstartedintheredstatewillbeselectedagainstif theychooselfromthegreenstate.overthecourseofmanygenerationsthepoliciesmust chooseactionrfromthegreenstatetomaximizetheirtnessandensuretheirsurvival. weconrmedthesehypothesesinempiricaltests.aq-learnerusingsingle-stepupdates andatable-basedrepresentationconvergedtothevaluesintable\u0005ineveryrun.an evolutionaryalgorithm\u0004consistentlyconverged\b\u0000%ofitspopulationontheoptimalpolicy. figure\u0001\u0004showstheaveragepercentageoftheoptimalpolicyinthepopulationasafunction oftimeaveragedover\u0001\u0000\u0000independentruns. thusevensimpleeamethodssuchasearl\u0001appeartobemorerobustinthepresence ofhiddenstatesthansimpletdmethods.howevermorerenedsensorinformationcould stillbehelpful.inthepreviousexamplealthoughtheeapoliciesachieveabetteraverage rewardthanthetdpolicytheevolvedpolicyremainsunabletoprocureboththe\u0003.\u0000 \u0004.weusedabinarytournamentselectiona\u0005\u0000policypopulation\u0000.\bcrossoverprobabilityand\u0000.\u0000\u0001 mutationrate. evolutionaryalgorithmsforreinforcementlearning figure\u0001\u0004theoptimalpolicydistributioninthehiddenstateproblemforanevolutionary algorithm.thegraphplotsthepercentageofoptimalpoliciesinthepopulation averagedover\u0001\u0000\u0000runs. and\u0001.\u0000rewardsfromthetwobluestates.theserewardscouldberealizedhoweverif theagentcouldseparatethetwobluestates.thusanymethodthatgeneratesadditional featurestodisambiguatestatespresentsanimportantassettoeamethods.kaelbling etal.describeseveralpromisingsolutionstothehiddenstateprobleminwhich additionalfeaturessuchastheagent'spreviousdecisionsandobservationsareautomatically generatedandincludedintheagent'ssensoryinformation.thesemethodshavebeeneectiveatdisambiguating statesfortdmethodsininitialstudiesbutfurtherresearchisrequiredtodeterminethe extenttowhichsimilarmethodscanresolvesignicanthiddenstateinformationinrealistic applications.itwouldbeusefultodevelopwaystousesuchmethodstoaugmentthesensory dataavailableineamethodsaswell. \b.\u0003non-stationaryenvironments iftheagent'senvironmentchangesovertimetherlproblembecomesevenmoredicult sincetheoptimalpolicybecomesamovingtarget.theclassictrade-obetweenexploration andexploitationbecomesevenmorepronounced.techniquesforencouragingexploration intd-basedrlincludeaddinganexplorationbonustotheestimatedvalueofstate-action pairsthatreectshowlongithasbeensincetheagenthastriedthataction andbuildingastatisticalmodeloftheagent'suncertainty. simplemodicationsofstandardevolutionaryalgorithmsoeranabilitytotracknonstationaryenvironmentsandthusprovideapromisingapproachtorlforthesedicult cases.thefactthatevolutionarysearchisbasedoncompetitionwithinapopulationofpolicies suggestsomeimmediatebenetsfortrackingnon-stationaryenvironments.totheextent thatthepopulationmaintainsadiversesetofpolicieschangesintheenvironmentwillbias moriartyschultz&grefenstette selectivepressureinfavorofthepoliciesthataremosttforthecurrentenvironment.as longastheenvironmentchangesslowlywithrespecttothetimerequiredtoevaluatea populationofpoliciesthepopulationshouldbeabletotrackachangingtnesslandscape withoutanyalterationofthealgorithm.empiricalstudiesshowthatmaintainingthe diversitywithinthepopulationmayrequireahighermutationratethanthoseusually adoptedforstationaryenvironments. inadditionspecialmechanismshavebeenexploredinordertomakeeasmoreresponsivetorapidlychangingenvironments.forexamplesuggests maintainingarandomsearchwithinarestrictedportionofthepopulation.therandom populationelementsareanalogoustoimmigrantsfromotherpopulationswithuncorrelated tnesslandscapes.maintainingthissourceofdiversitypermitstheeatorespondrapidly tolargesuddenchangesinthetnesslandscape.bykeepingtherandomizedportionof thepopulationtolessthanabout\u0003\u0000%ofthepopulationtheimpactonsearcheciencyin stationaryenvironmentsisminimized.thisisageneralapproachthatcaneasilybeapplied inearlsystems. otherusefulalgorithmsthathavebeendevelopedtoensurediversityinevolvingpopultionsincludetnesssharingcrowding andlocalmatingerson\u0001 \u0001).ingoldberg'stnesssharingmodelforexamplesimilarindividualsareforcedtosharealargeportionofasingletnessvaluefrom thesharedsolutionpoint.sharingdecreasesthetnessofsimilarindividualsandcauses evolutiontoselectagainstindividualsinoverpopulatedniches. earlmethodsthatemploydistributedpolicyrepresentationsachievediversityautomaticallyandarewell-suitedforadaptationindynamicenvironments. inadistributed representationeachindividualrepresentsonlyapartialsolution.completesolutionsare builtbycombiningindividuals.becausenoindividualcansolvethetaskonitsownthe evolutionaryalgorithmwillsearchforseveralcomplementaryindividualsthattogethercan solvethetask.evolutionarypressuresarethereforepresenttopreventconvergenceofthe population.moriartyandmiikkulainenshowedhowtheinherentdiversityandspecializationinsaneallowittoadaptmuchmorequicklytochangesintheenvironment thanstandardconvergentevolutionaryalgorithms. finallyifthelearningsystemcandetectchangesintheenvironmentevenmoredirect responseispossible. intheanytimelearningmodelan earlsystemmaintainsacase-baseofpoliciesindexedbythevaluesoftheenvironmental detectorscorrespondingtotheenvironmentinwhichagivenpolicywasevolved.when anenvironmentalchangeisdetectedthepopulationofpoliciesispartiallyreinitialized usingpreviouslylearnedpoliciesselectedonthebasisofsimilaritybetweenthepreviously encounteredenvironmentandthecurrentenvironment.asaresultiftheenvironment changesarecyclicthenthepopulationcanbeimmediatelyseededwiththosepoliciesin eectduringthelastoccurrenceofthecurrentenvironment.byhavingapopulationof policiesthisapproachisprotectedagainstsomekindsoferrorsindetectingenvironmental changes.forexampleevenifaspuriousenvironmentalchangeismistakenlydetected learningisnotundulyaectedsinceonlyapartofthecurrentpopulationofpoliciesis replacedbypreviouslylearnedpolicies.zhouexploredasimilarapproachbasedon lcs. evolutionaryalgorithmsforreinforcementlearning insummaryearlsystemscanrespondtonon-stationaryenvironmentsbothbytechniquesthataregenerictoevolutionaryalgorithmsandbytechniquesthathavebeenspecificallydesignedwithrlinmind. .limitationsofearl althoughtheeaapproachtorlispromisingandhasagrowinglistofsuccessfulapplicationsanumberofchallengesremain. .\u0001onlinelearning wecandistinguishtwobroadapproachestoreinforcementlearning|onlinelearningand oinelearning. inonlinelearninganagentlearnsdirectlyfromitsexperiencesinits operationalenvironment.forexamplearobotmightlearntonavigateinawarehouseby actuallymovingaboutitsphysicalenvironment.therearetwoproblemswithusingearl inthissituation.firstitislikelytorequirealargenumberofexperiencesinorderto evaluatealargepopulationofpolicies.dependingonhowquicklytheagentperformstasks thatresultinsomeenvironmentalfeedbackitmaytakeanunacceptableamountoftime torunhundredsofgenerationsofaneathatevaluateshundredsorthousandsofpolicies. seconditmaybedangerousorexpensivetopermitanagenttoperformsomeactionsin itsactualoperationalenvironmentthatmightcauseharmtoitselforitsenvironment.yet itisverylikelythatatleastsomepoliciesthattheeagenerateswillbeverybadpolicies. bothoftheseobjectionsapplytotdmethodsaswell.forexamplethetheoreticalresults thatprovetheoptimalityofq-learningrequirethateverystatebevisitedinnitelyoften whichisobviouslyimpossibleinpractice.likewisetdmethodsmayexploresomevery undesirablestatesbeforeanacceptablevalue-functionisfound. forbothtdandearlpracticalconsiderationspointtowardtheuseofoinelearning inwhichtherlsystemperformsitsexplorationonsimulationmodelsoftheenvironment. simulationmodelsprovideanumberofadvantagesforearlincludingtheabilityto performparallelevaluationsofallthepoliciesinapopulationsimultaneously. .\u0002rarestates thememoryorrecordofobservedstatesandrewardsdiersgreatlybetweeneaandtd methods.temporaldierencemethodsnormallymaintainstatisticsconcerningeverystateactionpair.asstatesarerevisitedthenewreinforcementiscombinedwiththeprevious value.newinformationthussupplementspreviousinformationandtheinformationcontentoftheagent'sreinforcementmodelincreasesduringexploration.inthismannertd methodssustainknowledgeofbothgoodandbadstate-actionpairs. aspointedoutpreviouslyeamethodsnormallymaintaininformationonlyaboutgood policiesorpolicycomponents.knowledgeofbaddecisionsisnotexplicitlypreservedsince policiesthatmakesuchdecisionsareselectedagainstbytheevolutionaryalgorithmand areeventuallyeliminatedfromthepopulation.forexamplereferonceagaintotable\u0004 whichshowstheimplicitstatisticsofthepopulationfromtable\u0002.notethequestion moriartyschultz&grefenstette marksinstateswhereactionshaveconverged.sincenopoliciesinthepopulationselectthe alternativeactiontheeahasnostatisticsontheimpactoftheseactionsontness. thisreductionininformationcontentwithintheevolvingpopulationcanbeadisadvantagewithrespecttostatesthatarerarelyvisited.inanyevolutionaryalgorithmthevalue ofgenesthathavenorealimpactonthetnessoftheindividualtendstodrifttorandom valuessincemutationstendtoaccumulateinthesegenes.ifastateisrarelyencountered mutationsmayfreelyaccumulateinthegenethatdescribesthebestactionforthatstate. asaresulteveniftheevolutionaryalgorithmlearnsthecorrectactionforararestatethat informationmayeventuallybelostduetomutations.incontrastsincetable-basedtd methodspermanentlyrecordinformationaboutallstate-actionpairstheymaybemore robustwhenthelearningagentdoesencounterararestate.ofcourseifatdmethod usesafunctionapproximatorsuchasaneuralnetworkasitsvaluefunctionthenittoo cansuerfrommemorylossconcerningrarestatessincemanyupdatesfromfrequently occurringstatescandominatethefewupdatesfromtherarestates. .\u0003proofsofoptimality oneoftheattractivefeaturesoftdmethodsisthattheq-learningalgorithmhasaproof ofoptimality.howeverthepracticalimportanceofthisresultis limitedsincetheassumptionsunderlyingtheproofnitelyoften)arenotsatisedinrealisticapplications.thecurrenttheoryofevolutionary algorithmsprovideasimilarlevelofoptimalityproofsforrestrictedclassesofsearchspaces .howevernogeneraltheoreticaltoolsareavailablethatcanbe appliedtorealisticrlproblems.inanycaseultimateconvergencetoanoptimalpolicy maybelessimportantinpracticethanecientlyndingareasonableapproximation. amorepragmaticapproachmaybetoaskhowecientalternativerlalgorithmsare intermsofthenumberofreinforcementsreceivedbeforedevelopingapolicythatiswithin sometolerancelevelofanoptimalpolicy.inthemodelofprobablyapproximatelycorrect learningtheperformanceofalearnerismeasuredbyhowmany learningexperiencesarerequiredbeforeconverging toacorrecthypothesiswithinspeciederrorbounds.althoughdevelopedinitiallyfor supervisedlearningthepacapproachhasbeenextendedrecentlytobothtdmethods andtogeneraleamethods.theseanalyticmethodsare stillinanearlystageofdevelopmentbutfurtherresearchalongtheselinesmayoneday provideusefultoolsforunderstandingthetheoreticalandpracticaladvantagesofalternative approachestorl.untilthattimeexperimentalstudieswillprovidevaluableevidencefor theutilityofanapproach. \u0001\u0000.examplesofearlmethods finallywetakealookatafewsignicantexamplesoftheearlapproachandresults onrlproblems.ratherthanattemptanexhaustivesurveywehaveselectedfourearl systemsthatarerepresentativeofthediversepoliciesrepresentationsoutlinedinsection\u0005. samuelrepresentstheclassofsingle-chromosomerule-basedearlsystems.alecsysis anexampleofadistributedrule-basedearlmethod.genitorisasinglechromosome neural-netsystemandsaneisadistributedneuralnetsystem.thisbriefsurveyshould evolutionaryalgorithmsforreinforcementlearning provideastartingpointforthoseinterestedininvestigatingtheevolutionaryapproachto reinforcementlearning. \u0001\u0000.\u0001samuel samuelisanearlsystemthatcombinesdarwinianandlamarckianevolutionwithaspectsoftemporaldierencereinforcementlearning.samuelhas beenusedtolearnbehaviorssuchasnavigationandcollisionavoidancetrackingandherdingforrobotsandotherautonomousvehicles. samuelusesasingle-chromosomerule-basedrepresentationforpoliciesthatiseach memberofthepopulationisapolicyrepresentedasarulesetandeachgeneisarulethat mapsthestateoftheworldtoactionstobeperformed.anexamplerulemightbe ifrange=andbearing=thensetturn=\u0001\u0006theuseofahigh-levellanguageforrulesoersseveraladvantagesoverlow-levelbinary patternlanguagestypicallyadoptedingeneticlearningsystems.firstitmakesiteasierto incorporateexistingknowledgewhetheracquiredfromexpertsorbysymboliclearningprograms.seconditiseasiertotransfertheknowledgelearnedtohumanoperators.samuel alsoincludesmechanismstoallowcoevolutionofmultiplebehaviorssimultaneously. additiontotheusualgeneticoperatorsofcrossoverandmutationsamuelusesmoretraditionalmachinelearningtechniquesintheformoflamarckianoperators.samuelkeepsa recordofrecentexperiencesandwillallowoperatorssuchasgeneralizationspecialization coveringanddeletiontomakeinformedchangestotheindividualgenesbasedon theseexperiences. samuelhasbeenusedsuccessfullyinmanyreinforcementlearningapplications.here wewillbrieydescribethreeexamplesoflearningcomplexbehaviorsforrealrobots.in theseapplicationsofsamuellearningisperformedundersimulationreectingthefact thatduringtheinitialphasesoflearningcontrollingarealsystemcanbeexpensiveor dangerous.learnedbehaviorsarethentestedontheon-linesystem. insamuel isusedtolearncollisionavoidanceandlocalnavigationbehaviorsforanomad\u0002\u0000\u0000mobile robot.thesensorsavailabletothelearningtaskwerevesonarsveinfraredsensors andtherangeandbearingtothegoalandthecurrentspeedofthevehicle.samuel learnedamappingfromthosesensorstothecontrollableactions{aturningrateanda translationrateforthewheels.samueltookahuman-writtenrulesetthatcouldreach thegoalwithinalimitedtimewithouthittinganobstacleonly\u0007\u0000percentofthetimeand after\u0005\u0000generationswasabletoobtaina \u0003.\u0005percentsuccessrate. intherobotlearnedtoherdasecondrobottoa\\pasture\".inthistaskthelearningsystemusedtherangeandbearingtothesecondrobotthe headingofthesecondrobotandtherangeandbearingtothegoalasitsinputsensors. thesystemlearnedamappingfromthesesensorstoaturningrateandsteeringrate.in theseexperimentssuccesswasmeasuredasthepercentageoftimesthattherobotcould maneuverthesecondrobottothegoalwithinalimitedamountoftime.thesecondrobot implementedarandomwalkplusabehaviorthatmadeitavoidanynearbyobstacles.the rstrobotlearnedtoexploitthistoachieveitsgoalofmovingthesecondrobottothegoal. samuelwasgivenaninitialhuman-designedrulesetwithaperformanceof\u0002\u0007percent andafter\u0002\u0005\u0000generationswasabletomovethesecondrobottothegoal\b\u0006percentofthe time.inthesamueleasystemiscombinedwithcase-basedlearningto addresstheadaptationproblem.inthisapproachcalledanytimelearningthelearningagentinteractsbothwiththeexternalenvironmentandwith aninternalsimulation.theanytimelearningapproachinvolvestwocontinuouslyrunning andinteractingmodulesanexecutionmoduleandalearningmodule.theexecution modulecontrolstheagent'sinteractionwiththeenvironmentandincludesamonitorthat dynamicallymodiestheinternalsimulationmodelbasedonobservationsoftheactualagent andtheenvironment.thelearningmodulecontinuouslytestsnewstrategiesfortheagent againstthesimulationmodelusingageneticalgorithmtoevolveimprovedstrategiesand updatestheknowledgebaseusedbytheexecutionmodulewiththebestavailableresults. wheneverthesimulationmodelismodiedduetosomeobservedchangeintheagentorthe environmentthegeneticalgorithmisrestartedonthemodiedmodel.thelearningsystem operatesindenitelyandtheexecutionsystemusestheresultsoflearningastheybecome available.theworkwithsamuelshowsthattheeamethodisparticularlywell-suited foranytimelearning.previouslylearnedstrategiescanbetreatedascasesindexedbythe setofconditionsunderwhichtheywerelearned.whenanewsituationisencountereda nearestneighboralgorithmisusedtondthemostsimilarpreviouslylearnedcases.these nearestneighborsareusedtore-initializethegeneticpopulationofpoliciesforthenewcase. grefenstettereportsonexperimentsinwhichamobilerobotlearnstotrackanother robotanddynamicallyadaptsitspoliciesusinganytimelearningasitsencountersaseries ofpartialsystemfailures.thisapproachblursthelinebetweenonlineandoinelearning sincetheonlinesystemisbeingupdatedwhenevertheoinelearningsystemdevelopsan improvedpolicy. infacttheoinelearningsystemcanevenbeexecutedon-boardthe operatingmobilerobot. \u0001\u0000.\u0002alecsys asdescribedpreviouslyalecsysisadistributedrule-based eathatsupportsanapproachtothedesignofautonomoussystemscalledbehavioralengineering.inthisapproachthetaskstobeperformedbyacomplexautonomoussystemsare decomposedintoindividualbehaviorseachofwhichislearnedviaalearningclassiersystemsmoduleasshowninfigure .thedecompositionisperformedbythehumandesigner sothetnessfunctionassociatedwitheachlcscanbecarefullydesignedtoreecttherole oftheassociatedcomponentbehaviorwithintheoverallautonomoussystem.furthermore theinteractionsamongthemodulesisalsopreprogrammed.forexamplethedesignermay decidethattherobotshouldlearntoapproachagoalexceptwhenathreateningpredator isnearinwhichcasetherobotshouldevadethepredator.theoverallarchitectureofthe setofbehaviorscanthenbesetsuchthattheevasionbehaviorhashigherprioritythan thegoal-seekingbehaviorbuttheindividuallcsmodulescanevolvedecisionrulesfor optimallyperformingthesubtasks. alecsyshasbeenusedtodevelopbehavioralrulesforanumberofbehaviorsfor autonomousrobotsincludingcomplexbehaviorgroupssuchaschase/feed/escape evolutionaryalgorithmsforreinforcementlearning .theapproachhasbeenimplementedandtestedonboth simulatedrobotsandonrealrobots.becauseitexploitsbothhumandesignandearl methodstooptimizesystemperformancethismethodshowsmuchpromiseforscalingup torealistictasks. \u0001\u0000.\u0003genitor genitorisanaggressivegeneralpurposegenetic algorithmthathasbeenshowneectivewhenspecializedforuseonreinforcement-learning problems.whitleyetal.demonstratedhowgenitorcanecientlyevolvedecision policiesrepresentedasneuralnetworksusingonlylimitedreinforcementfromthedomain. genitorreliessolelyonitsevolutionaryalgorithmtoadjusttheweightsinneural networks.insolvingrlproblemseachmemberofthepopulationingenitorrepresentsa neuralnetworkasasequenceofconnectionweights.theweightsareconcatenatedinarealvaluedchromosomealongwithagenethatrepresentsacrossoverprobability.thecrossover genedetermineswhetherthenetworkistobemutatedorwhethera crossoveroperationistobeperformed.thecrossover geneismodiedandpassedtotheospringbasedontheospring'sperformancecompared totheparent.iftheospringoutperformstheparentthecrossoverprobabilityisdecreased. otherwiseitisincreased.whitleyetal. refertothistechniqueasadaptivemutation whichtendstoincreasethemutationrateaspopulationsconverge.essentiallythismethod promotesdiversitywithinthepopulationtoencouragecontinualexplorationofthesolution space.genitoralsousesaso-called\\steady-state\"geneticalgorithminwhichnewparentsare selectedandgeneticoperatorsareappliedaftereachindividualisevaluated.thisapproach contrastswith\\generational\"gasinwhichtheentirepopulationisevaluatedandreplaced duringeachgeneration.inasteady-stategaeachpolicyisevaluatedjustonceandretains thissametnessvalueindenitely.sincepolicieswithlowertnessaremorelikelytobe replaceditispossiblethatatnessbasedonanoisyevaluationfunctionmayhavean undesirableinuenceonthedirectionofthesearch.inthecaseofthepole-balancingrl applicationthetnessvaluedependsonthelengthoftimethatthepolicycanmaintain agoodbalancegivenarandomlychoseninitialstate.thetnessisthereforearandom variablethatdependsontheinitialstate.theauthorsbelievethatnoiseinthetness functionhadlittlenegativeimpactonlearninggoodpoliciesperhapsbecauseitwasmore dicultforpoornetworkstoobtainagoodtnessthanforgoodnetworkstosurviveanoccasionalbadtnessevaluation.this isaninterestinggeneralissueinearlthatneedsfurtheranalysis. genitoradoptssomespecicmodicationforitsrlapplications.firsttherepresentationusesareal-valuedchromosomeratherthanabit-stringrepresentationfortheweights. consequentlygenitoralwaysrecombinespoliciesbetweenweightdenitionsthusreducingpotentiallyrandomdisruptionofneuralnetworkweightsthatmightresultifcrossover operationsoccurredinthemiddleofaweightdenition.thesecondmodicationisavery highmutationratewhichhelpstomaintaindiversityandpromoterapidexplorationofthe policyspace.finallygenitorusesunusuallysmallpopulationsinordertodiscourage dierentcompetingneuralnetwork\\species\"fromformingwithinthepopulation.whitmoriartyschultz&grefenstette leyetal.arguethatspeciationleadstocompetingconventionsandproducespoor ospringwhentwodissimilarnetworksarerecombined. whitleyetal.comparegenitortotheadaptiveheuristiccriticwhichusesthetdmethodofreinforcementlearning. inseveraldierent versionsofthecommonpole-balancingbenchmarktaskgenitorwasfoundtobecomparabletotheahcinbothlearningrateandgeneralization.oneinterestingdierence whitleyetal.foundwasthatgenitorwasmoreconsistentthantheahcinsolvingthe pole-balancingproblemwhenthefailuresignalsoccursatwiderpolebounds.forahcthepreponderanceoffailuresappearstocauseallstates tooverpredictfailure.incontrasttheeamethodappearsmoreeectiveinndingpolicies thatobtainbetteroverallperformanceevenifsuccessisuncommon.thedierenceseems tobethattheeatendstoignorethosecaseswherethepolecannotbebalancedandconcentrateonsuccessfulcases.thisservesasanotherexampleoftheadvantagesassociated withsearchinpolicyspacebasedonoverallpolicyperformanceratherthanpayingtoo muchattentiontothevalueassociatedwithindividualstates. \u0001\u0000.\u0004sane thesanesystemwasdesignedasaecientmethod forbuildingarticialneuralnetworksinrldomainswhereitisnotpossibletogenerate trainingdatafornormalsupervisedlearning.the sanesystemusesanevolutionaryalgorithmtoformthehiddenlayerconnectionsand weightsinaneuralnetwork.theneuralnetworkformsadirectmappingfromsensorsto actionsandprovideseectivegeneralizationoverthestatespace.sane'sonlymethodof creditassignmentisthroughtheeawhichallowsittoapplytomanyproblemswhere reinforcementissparseandcoversasequenceofdecisions.asdescribedpreviouslysane usesadistributedrepresentationforpolicies. saneoerstwoimportantadvantagesforreinforcementlearningthatarenormallynot presentinotherimplementationsofneuro-evolution.firstitmaintainsdiversepopulations. unlikethecanonicalfunctionoptimizationeathatconvergethepopulationonasinglesolutionsaneformssolutionsinanunconvergedpopulation.becauseseveraldierenttypes ofneuronsarenecessarytobuildaneectiveneuralnetworkthereisinherentevolutionary pressuretodevelopneuronsthatperformdierentfunctionsandthusmaintainseveraldifferenttypesofindividualswithinthepopulation.diversityallowsrecombinationoperators suchascrossovertocontinuetogeneratenewneuralstructureseveninprolongedevolution. thisfeaturehelpsensurethatthesolutionspacewillbeexploredecientlythroughoutthe learningprocess.saneisthereforemoreresilienttosuboptimalconvergenceandmore adaptivetochangesinthedomain. thesecondfeatureofsaneisthatitexplicitlydecomposesthesearchforcompletesolutionsintoasearchforpartialsolutions.insteadofsearchingforcompleteneuralnetworks allatoncesolutionstosmallerproblemsareevolvedwhichcanbecombinedtoformaneectivefullsolution.inotherwordssaneeectively performsaproblemreductionsearchonthespaceofneuralnetworks. sanehasbeenshowneectiveinseveraldierentlargescaleproblems.inoneproblem saneevolvedneuralnetworkstodirectorfocusaminimaxgame-treesearch.byselectingwhichmovesshouldbeevaluatedfromagivengame situationsaneguidesthesearchawayfrommisinformationinthesearchtreeandtowards themosteectivemoves.sanewastestedinagametreesearchinothellousingthe evaluationfunctionfromtheformerworldchampionprogrambill. testedagainstafull-widthminimaxsearchsanesignicantlyimprovedtheplayofbill whileexaminingonlyasubsetoftheboardpositions. inasecondapplicationsanewasusedtolearnobstacleavoidancebehaviorsina robotarm.mostapproachesforlearningrobotarm controllearnhand-eyecoordinationthroughsupervisedtrainingmethodswhereexamples ofcorrectbehaviorareexplicitlygiven.unfortunatelyindomainswithobstacleswherethe armmustmakeseveralintermediatejointrotationsbeforereachingthetargetgenerating trainingexamplesisextremelydicult.areinforcementlearningapproachhoweverdoes notrequireexamplesofcorrectbehaviorandcanlearntheintermediatemovementsfrom generalreinforcements.sanewasimplementedtoformneuro-controlnetworkscapableof maneuveringtheoscar-\u0006robotarmamongobstaclestoreachrandomtargetlocations. givenbothcamera-basedvisualandinfraredsensoryinputtheneuralnetworkslearnedto eectivelycombinebothtargetreachingandobstacleavoidancestrategies. forfurtherrelatedexamplesofevolutionarymethodsforlearningneural-netcontrol systemsforroboticsthereadershouldseeharvey&husbands\u0001 \u0003;husbands harvey&cli\u0001 \u0005;yamauchi&beer\u0001 \u0001\u0001.summary thisarticlebeganbysuggestingtwodistinctapproachestosolvingreinforcementlearning problems;onecansearchinvaluefunctionspaceoronecansearchinpolicyspace.td andearlareexamplesofthesetwocomplementaryapproaches.bothapproachesassume limitedknowledgeoftheunderlyingsystemandlearnbyexperimentingwithdierentpoliciesandusingreinforcementtoalterthosepolicies.neitherapproachrequiresaprecise mathematicalmodelofthedomainandbothmaylearnthroughdirectinteractionswith theoperationalenvironment. unliketdmethodsearlmethodsgenerallybasetnessontheoverallperformance ofapolicy.inthissenseeamethodspaylessattentiontoindividualdecisionsthantd methodsdo.whileatrstglancethisapproachappearstomakelessecientuseof informationitmayinfactprovidearobustpathtowardlearninggoodpoliciesespecially insituationswherethesensorsareinadequatetoobservethetruestateoftheworld. itisnotusefultoviewthepathtowardpracticalrlsystemsasachoicebetweenea andtdmethods.wehavetriedtohighlightsomeofthestrengthsoftheevolutionary approachbutwehavealsoshownthatearlandtdwhilecomplementaryapproaches arebynomeansmutuallyexclusive.wehavecitedexamplesofsuccessfulearlsystems suchassamuelandalecsysthatexplicitlyincorporatetdelementsintotheirmultilevelcreditassignmentmethods.itislikelythatmanypracticalapplicationswilldepend onthesekindsofmulti-strategyapproachestomachinelearning. wehavealsolistedanumberofareasthatneedfurtherworkparticularlyonthetheoreticalside.inrlitwouldbehighlydesirabletohaveabettertoolsforpredictingthe amountofexperienceneededbyalearningagentbeforereachingaspeciedlevelofper\u0002\u0007\u0001 moriartyschultz&grefenstette formance.theexistingproofsofoptimalityforbothq-learningandeaareofextremely limitedpracticaluseinpredictinghowwelleitherapproachwillperformonrealisticproblems.preliminaryresultshaveshownthatthetoolsofpacanalysiscanbeappliedtoboth eaantdmethodsbutmuchmoreeortisneededinthisdirection. manyseriouschallengesremaininscalingupreinforcementlearningmethodstorealisticapplications.bypointingoutthesharedgoalsandconcernsoftwocomplementary approacheswehopetomotivatefurthercollaborationandprogressinthiseld. references andersonc.w..learningtocontrolaninvertedpendulumusingneuralnetworks. ieeecontrolsystemsmagazine \u0003\u0001{\u0003\u0007. bartoa.g.suttonr.s.&watkinsc.j.c.h..learningandsequential decisionmaking.ingabrielm.&moorej.w.learningandcomputational neuroscience.mitpresscambridgema. belewr.k.mcinerneyj.&schraudolphn.n..evolvingnetworksusing thegeneticalgorithmwithconnectionistlearning. infarmerj.d.langtonc. rasmussens.&taylorc.articiallifeiireadingma.addison-wesley. chrismanl..reinforcementlearningwithperceptualaliasingtheperceptual distinctionsapproach.inproceedingsofthetenthnationalconferenceonarticial intelligencepp.\u0001\b\u0003{\u0001\b\bsanjoseca. clid.harveyi.&husbandsp..explorationsinevolutionaryrobotics.adaptive behavior\u0002\u0007\u0003{\u0001\u0001\u0000. cobbh.g.&grefenstettej.j..geneticalgorithmsfortrackingchangingenvironments.inproc.fifthinternationalconferenceongeneticalgorithmspp.\u0005\u0002\u0003{\u0005\u0003\u0000. collinsr.j.&jeersond.r..selectioninmassivelyparallelgeneticalgorithms. inproceedingsofthefourthinternationalconferenceongeneticalgorithmspp. {\u0002\u0005\u0006sanmateoca.morgankaufmann. dayanp.&sejnowskit.j..explorationbonusesanddualcontrol.machine learning\u0002\u0005\u0005{\u0002\u0002. dejongk.a..ananalysisofthebehaviorofaclassofgeneticadaptivesystems. ph.d.thesistheuniversityofmichiganannarbormi. dorigom.&colombettim..robotshapinganexperimentinbehavioralengineering.mitpresscambridgema. fiechterc.-n..ecientreinforcementlearning. inproceedingsoftheseventh annualacmconferenceoncomputationallearningtheorypp.\b\b{ \u0007.association forcomputingmachinery. fogell.j.owensa.j.&walshm.j..articialintelligencethroughsimulated evolution.wileypublishingnewyork.\u0002\u0007\u0002 evolutionaryalgorithmsforreinforcementlearning goldbergd.e..geneticalgorithmsinsearchoptimizationandmachinelearning.addison-wesleyreadingma. goldbergd.e.&richardsonj..geneticalgorithmswithsharingformultimodal functionoptimization.inproceedingsofthesecondinternationalconferenceongeneticalgorithmspp.\u0001\u0004\b{\u0001\u0005\u0004sanmateoca.morgankaufmann. grefenstettej.j..optimizationofcontrolparametersforgeneticalgorithms.ieee transactionsonsystemsman&cyberneticssmc-\u0001\u0006\u0001\u0002\u0002{\u0001\u0002\b. grefenstettej.j..incorporatingproblemspecicknowledgeintogeneticalgorithms. indavisl.geneticalgorithmsandsimulatedannealingpp.\u0004\u0002{\u0006\u0000sanmateo ca.morgankaufmann. grefenstettej.j..creditassignmentinrulediscoverysystembasedongenetic algorithms.machinelearning\u0003\u0002\u0002\u0005{\u0002\u0004\u0005. grefenstettej.j..geneticalgorithmsforchangingenvironments.inmannerr. &manderickb.parallelproblemsolvingfromnature\u0002pp.\u0001\u0003\u0007{\u0001\u0004\u0004. grefenstettej.j..robotlearningwithparallelgeneticalgorithmsonnetworked computers. inproceedingsofthe\u0001 \u0005summercomputersimulationconference pp.\u0003\u0005\u0002{\u0002\u0005\u0007. grefenstettej.j..geneticlearningforadaptationinautonomousrobots.inrobotics andmanufacturingrecenttrendsinresearchandapplicationsvolume\u0006pp.\u0002\u0006\u0005{ \u0002\u0007\u0000.asmepressnewyork. grefenstettej.j..proportionalselectionandsamplingalgorithms.inhandbookof evolutionarycomputationchap.c\u0002.\u0002.ioppublishingandoxforduniversitypress. grefenstettej.j..rank-basedselection.inhandbookofevolutionarycomputationchap.c\u0002.\u0004.ioppublishingandoxforduniversitypress. grefenstettej.j.&ramseyc.l..anapproachtoanytimelearning.inproc. ninthinternationalconferenceonmachinelearningpp.\u0001\b \u0005sanmateoca. morgankaufmann. grefenstettej.j.ramseyc.l.&schultza.c..learningsequentialdecision rulesusingsimulationmodelsandcompetition.machinelearning\u0005\u0003\u0005\u0005{\u0003\b\u0001. hollandj.h..adaptationinnaturalandarticialsystemsanintroductory analysiswithapplicationstobiologycontrolandarticialintelligence.university ofmichiganpressannarbormi. hollandj.h..escapingbrittlenessthepossibilitiesofgeneral-purposelearning algorithmsappliedtoparallelrule-basedsystems.inmachinelearninganarticial intelligenceapproachvol.\u0002.morgankaufmannlosaltosca. moriartyschultz&grefenstette hollandj.h..geneticalgorithmsandclassiersystemsfoundationsandfuture directions.inproceedingsofthesecondinternationalconferenceongeneticalgorithmspp.\b\u0002{\b hillsdalenewjersey. hollandj.h.&reitmanj.s..cognitivesystemsbasedonadaptivealgorithms. inpattern-directedinferencesystems.academicpressnewyork. husbandsp.harveyi.&clid..circleintheroundstatespaceattractorsfor evolvedsightedrobots.robot.autonomoussystems\u0001\u0005\b\u0003{\u0001\u0000\u0006. kaelblingl.p.littmanm.l.&moorea.w..reinforcementlearningasurvey. journalofarticialintelligenceresearch\u0004\u0002\u0003\u0007{\u0002\b\u0005. kozaj.r..geneticprogrammingontheprogrammingofcomputersbymeans ofnaturalselection.mitpresscambridgema. leek.-f.&mahajans..thedevelopmentofaworldclassothelloprogram. articialintelligence\u0004\u0003\u0002\u0001{\u0003\u0006. linl.-j.&mitchellt.m..memoryapproachestoreinforcementlearninginnonmarkoviandomains.tech.rep.cmu-cs\u0002-\u0001\u0003\bcarnegiemellonuniversityschool ofcomputerscience. mccalluma.k..reinforcementlearningwithselectiveperceptionandhidden state.ph.d.thesistheuniversityofrochester. moriartyd.e.&miikkulainenr..evolvingneuralnetworkstofocusminimax search.inproceedingsofthetwelfthnationalconferenceonarticialintelligence pp.\u0001\u0003\u0007\u0001{\u0001\u0003\u0007\u0007seattlewa.mitpress. moriartyd.e.&miikkulainenr..ecientreinforcementlearningthrough symbioticevolution.machinelearning\u0002\u0002\u0001\u0001{\u0003\u0002. moriartyd.e.&miikkulainenr..evolvingobstacleavoidancebehaviorina robotarm. infromanimalstoanimatsproceedingsofthefourthinternational conferenceonsimulationofadaptivebehaviorpp.\u0004\u0006\b{\u0004\u0007\u0005capecod moriartyd.e.&miikkulainenr..formingneuralnetworksthroughecientand adaptiveco-evolution.evolutionarycomputation\u0005\u0003\u0007\u0003{\u0003 potterm.a..thedesignandanalysisofacomputationalmodelofcooperative coevolution.ph.d.thesisgeorgemasonuniversity. potterm.a.&dejongk.a..evolvingneuralnetworkswithcollaborative species.inproceedingsofthe\u0001 \u0005summercomputersimulationconferenceottawa canada. potterm.a.dejongk.a.&grefenstettej..acoevolutionaryapproachto learningsequentialdecisionrules. ineshelmanl.proceedingsofthesixth internationalconferenceongeneticalgorithmspittsburghpa. evolutionaryalgorithmsforreinforcementlearning rechenbergi..cyberneticsolutionpathofanexperimentalproblem.inlibrary translation\u0001\u0001\u0002\u0002.royalaircraftestablishmentfarnboroughhantsaug.\u0001 ringm.b..continuallearninginreinforcementenvironments.ph.d.thesisthe universityoftexasataustin. rosj.p..probablyapproximatelycorrectlearninganalysis.inhandbookof evolutionarycomputationchap.b\u0002.\b.ioppublishingandoxforduniversitypress. schaerj.d.caruanar.a.eshelmanl.j.&dasr..astudyofcontrol parametersaectingonlineperformanceofgeneticalgorithmsforfunctionoptimization.inproceedingsofthethirdinternationalconferenceongeneticalgorithms pp.\u0005\u0001{\u0006\u0000.morgankaufmann. schaerj.d.&grefenstettej.j..multi-objectivelearningviageneticalgorithms. inproceedingsoftheninthinternationaljointconferenceonarticialintelligence pp.\u0005 \u0005.morgankaufmann. schultza.c..learningrobotbehaviorsusinggeneticalgorithms.inintelligent automationandsoftcomputingtrendsinresearchdevelopmentandapplications pp.\u0006\u0000\u0007{\u0006\u0001\u0002.tsipressalbuquerque. schultza.c.&grefenstettej.j..usingageneticalgorithmtolearnbehaviorsfor autonomousvehicles.inproceedingsoftheaiaaguidancenavigationandcontrol conferencehiltonheadsc. schultza.c.&grefenstettej.j..robo-shepherdlearningcomplexroboticbehaviors.inroboticsandmanufacturingrecenttrendsinresearchandapplications volume\u0006pp.\u0007\u0006\u0003{\u0007\u0006\b.asmepressnewyork. smiths.f..flexiblelearningofproblemsolvingheuristicsthroughadaptivesearch. inproceedingsoftheeighthinternationaljointconferenceonarticialintelligence pp.\u0004\u0002\u0002{\u0004\u0002\u0005.morgankaufmann. suttonr..integratedarchitecturesforlearningplanningandreactingbasedon approximatedynamicprogramming.inmachinelearningproceedingsoftheseventh internationalconferencepp.\u0002\u0001\u0006{\u0002\u0002\u0004. suttonr.s..learningtopredictbythemethodsoftemporaldierences.machine learning\u0003 {\u0004\u0004. suttonr.s.&bartoa..reinforcementlearninganintroduction.mitpress cambridgema. valiantl.g..atheoryofthelearnable.communicationsoftheacm\u0002\u0007\u0001\u0001\u0003\u0004{ \u0001\u0001\u0004\u0002. vosem.d.&wrighta.h..simplegeneticalgorithmswithlineartness.evolutionarycomputation\u0002\u0003\u0004\u0007{\u0003\u0006\b. moriartyschultz&grefenstette watkinsc.j.c.h..learningfromdelayedrewards.ph.d.thesisuniversityof cambridgeengland. watkinsc.j.c.h.&dayanp..q-learning.machinelearning\b\u0002\u0007 whitleyd..thegenitoralgorithmandselectivepressure.inproceedingsofthe thirdinternationalconferenceongeneticalgorithmspp.\u0001\u0001\u0006{\u0001\u0002\u0001sanmateoca. morgankaufman. whitleyd.&kauthj..genitoradierentgeneticalgorithm.inproceedings oftherockymountainconferenceonarticialintelligencepp.\u0001\u0001\b{\u0001\u0003\u0000denverco. whitleyd.dominics.dasr.&andersonc.w..geneticreinforcement learningforneurocontrolproblems.machinelearning\u0001\u0003\u0002\u0005 {\u0002\b\u0004. wilsons.w..zcsazerothlevelclassiersystem.evolutionarycomputation \u0002\u0001{\u0001\b. yamauchib.m.&beerr.d..sequentialbehaviorandlearninginevolved dynamicalneuralnetworks.adaptivebehavior\u0002\u0002\u0001 {\u0002\u0004\u0006. zhouh..csmacomputationalmodelofcumulativelearning.machinelearning \u0005\u0003\b\u0003{\u0004\u0000\u0006.", "year": 2011}