{"title": "Near Optimal Behavior via Approximate State Abstraction", "tag": ["cs.LG", "cs.AI"], "abstract": "The combinatorial explosion that plagues planning and reinforcement learning (RL) algorithms can be moderated using state abstraction. Prohibitively large task representations can be condensed such that essential information is preserved, and consequently, solutions are tractably computable. However, exact abstractions, which treat only fully-identical situations as equivalent, fail to present opportunities for abstraction in environments where no two situations are exactly alike. In this work, we investigate approximate state abstractions, which treat nearly-identical situations as equivalent. We present theoretical guarantees of the quality of behaviors derived from four types of approximate abstractions. Additionally, we empirically demonstrate that approximate abstractions lead to reduction in task complexity and bounded loss of optimality of behavior in a variety of environments.", "text": "combinatorial explosion plagues planning reinforcement learning algorithms moderated using state abstraction. prohibitively large task representations condensed essential information preserved consequently solutions tractably computable. however exact abstractions treat fully-identical situations equivalent fail present opportunities abstraction environments situations exactly alike. work investigate approximate state abstractions treat nearly-identical situations equivalent. present theoretical guarantees quality behaviors derived four types approximate abstractions. additionally empirically demonstrate approximate abstractions lead reduction task complexity bounded loss optimality behavior variety environments. abstraction plays fundamental role learning. abstraction intelligent agents reason salient features environment ignoring irrelevant. consequently agents able solve considerably complex problems would able without abstraction. however exact abstractions treat fully-identical situations equivalent require complete knowledge computationally intractable obtain. furthermore often situations identical exact abstractions often ineﬀective. overcome issues investigate approximate abstractions enable agents treat suﬃciently similar situations identical. work characterizes impact equating suﬃciently similar states context planning markov decision processes remainder introduction contextualizes intuitions mdps. solving optimal behavior mdps planning setting known p-complete size state space similarly many algorithms solving mdps known require number samples polynomial size state space although polynomial runtime sample complexity seem like reasonable constraint size state space grows super-polynomially number variables characterize domain result bellman’s curse dimensionality. thus solutions polynomial state space size often ineﬀective suﬃciently complex tasks. instance robot involved pick-and-place task might able employ planning algorithms solve manipulate objects desired conﬁguration time polynomial number states number states must consider grows exponentially number objects working thus research agenda planning leveraging abstraction reduce large state spaces agenda given rise methods reduce ground mdps large state spaces abstract mdps smaller state spaces aggregating states according notion equality similarity. context mdps understand exact abstractions aggregate states equal values particular quantities example optimal q-values. existing work characterized exact abstractions fully maintain optimality mdps thesis work performing approximate abstraction mdps relaxing state aggregation criteria equality similarity achieves polynomially bounded error resulting behavior oﬀering three beneﬁts. first approximate abstractions employ sort knowledge expect planning learning algorithm compute without fully solving mdp. contrast exact abstractions often require solving optimal behavior thereby defeating purpose abstraction. second relaxed criteria approximate abstractions achieve greater degrees compression exact abstractions. diﬀerence particularly important environments states identical. third state aggregation criteria relaxed near equality approximate abstractions able tune aggressiveness abstraction adjusting consider suﬃciently similar states. serve near-optimal behavior aggregating states diﬀerent criteria φq∗ε similar optimal q-values φmodelε similarity rewards transitions φboltε similarity boltzmann distribution optimal q-values φmultε similarity multinomial distribution optimal q-values. furthernext section introduce necessary terminology background mdps state abstraction. section surveys existing work state abstraction applied sequential decision making. section introduces primary result; bounds error guaranteed four classes approximate state abstraction. following sections introduce simulated domains used experiments discussion experiments apply class approximate abstraction variety diﬀerent tasks empirically illustrate relationship degree compression error incurred here ﬁnite state space; ﬁnite actions available agent; denotes probability agent transitioning state applying action state denotes reward received agent executing action state discount factor determines much agent prefers future rewards immediate rewards. assume without loss generality range reward functions normalized solution called policy denoted state denoted denote expected discounted reward following policy state value state policy similarly denote expected discounted reward taking dean leverage notion bisimulation investigate partitioning mdp’s state space clusters states whose transition model reward function within other. develop algorithm called interval value iteration converges correct bounds family abstract mdps called bounded mdps. several approaches build dean ferns investigated state similarity metrics mdps; bounded value diﬀerence ground states abstract states several bisimulation metrics induce abstract mdp. diﬀers work develops theory abstraction bounds suboptimality applying optimal policy abstract ground covering four types state abstraction closely parallels bisimulation. even-dar mansour analyzed diﬀerent distance metrics used identifying state space partitions subject ε-similarity also providing value bounds ε-homogeneity subject norm parallels claim ortner developed algorithm learning partitions online setting taking advantage conﬁdence bounds provided ucrl hutter investigates state aggregation beyond setting. hutter presents variety results aggregation functions reinforcement learning. relevant investigation hutter’s theorem illustrates properties aggregating states based similar values. hutter’s theorem part parallels claim bound value diﬀerence ground abstraction states part analogous lemma bound value diﬀerence applying optimal abstraction approximate state abstraction also applied planning problem agent given model environment must compute plan satisﬁes goal. hostetler apply state abstraction monte carlo tree search expectimax search giving value bounds applying optimal abstract action ground tree similarly setting. dearden boutilier also formalize stateabstraction planning focusing abstractions quickly computed oﬀer bounded value. primary analysis abstractions remove negligible literals planning domain description yielding value bounds abstractions means incrementally improving abstract solutions planning problems. jiang analyze similar setting applying abstractions upper conﬁdence bound applied trees algorithm adapted planning introduced kocsis szepesv´ari mandel advance bayesian aggregation deﬁne thompson clustering reinforcement learning extension achieves near-optimal bayesian regret bounds. jiang analyze problem choosing candidate abstractions. develop algorithm based statistical many previous works targeted creation algorithms enable state abstraction mdps. andre russell investigated method state abstraction hierarchical reinforcement learning leveraging programming language called alisp promotes notion safe state abstraction. agents programmed using alisp ignore irrelevant parts state achieving abstractions maintain optimality. dietterich developed maxq framework composing tasks abstracted hierarchy state aggregation applied. bakker schmidhuber also target hierarchical abstraction focusing subgoal discovery. jong stone introduced method called policy-irrelevance agents identify state variables safely abstracted away factored-state mdp. dayan hinton develop feudal reinforcement learning presents early form hierarchical restructures q-learning manage decomposition task subtasks. complete survey algorithms leverage state abstraction past reinforcement-learning papers survey early works hierarchical reinforcement learning barto mahadevan developed framework exact state abstraction mdps. particular authors deﬁned types state aggregation functions inspired existing methods state aggregation mdps. generalize types φmodel approximate abstraction case. generalizations equivalent exact criteria used additionally exact criteria used bounds indicate value lost core results walsh build framework previously developed showing empirically transfer abstractions structurally related mdps. deﬁnition understand abstraction mapping state space ground abstract using state aggregation scheme. consequently mapping induces abstract mdp. sgatgrg saatara deﬁnition states abstract constructed applying state aggregation function states ground speciﬁcally maps state ground deﬁnition given ground state associated ground states aggregated. similarly abstract state constituent ground states. function retrieves states here introduce formal analysis approximate state abstraction including results bounding error associated abstraction methods. particular demonstrate abstractions based approximate similarity approximate model similarity approximate similarity distributions boltzmann multinomial distributions induce abstract mdps optimal function φq∗ε deﬁnition φq∗ε) approximate function abstraction form equation φq∗ε φq∗ε lemma φq∗ε type abstraction used create abstract proof lemma ﬁrst demonstrate q-values abstract close q-values ground next leverage claim demonstrate optimal action abstract nearly optimal ground lastly claim conclude lemma parameterized integer ﬁrst time steps reward function transition dynamics state space abstract time steps reward function transition dynamics state spaces thus since q-values ground states grouped φmodelε strictly less understand φmodelε type φq∗b. applying lemma yields lemma boltzmann optimal φboltε here introduce φboltε aggregates states similar boltzmann distributions q-values. type particularly interesting abstraction purposes unlikeφq∗ε allows aggregation consequently considerφboltε special case theφq∗b type multinomial optimal φmultε deﬁnition φmultε) φmultε deﬁne type abstraction that ﬁxed satisﬁes apply approximate abstraction example domains—nchain upworld taxi mineﬁeld random. domains selected diversity—nchain relatively simple upworld particularly illustrative power abstraction taxi goal-based hierarchical nature mineﬁeld stochastic random many near-optimal policies. code base provides implementations abstracting arbitrary mdps well visualizing evaluating resulting abstract mdps. graph-visualization library graphstream planning library burlap. experiments provide visuals ground resulting abstract domain. grey circle indicates state colored arrows indicate transitions. thickness arrow indicates much reward associated transition. abstract mdps indicate ground states collapsed abstract state labelling abstract states ground states. nchain simple investigated bayesian literature interesting exploration problem poses implementation normalized rewards used slip probability nchain instance abstraction visualized figure... states agent actions available advance chain return state agent receives reward returning state reward advancing chain. exception agent transitions last state chain receives reward. transitions also small slip probability applied action results opposite dynamics. implementation move left right agent receives positive reward transitioning state grid moving cells self transitions. agent receives reward transitions. consequently moving always optimal action since moving left right change agent’s manhattan distance positive reward. experimentation upworld instance abstraction visualized figure... upworld illustrates compelling property respect state abstraction optimal exact abstraction function always construct abstract |sa| height grid change value optimal policy. consequently letting arbitrarily large upworld oﬀers arbitrary reduction size abstraction cost value optimal policy. result property states values taxi long studied hierarchical literature agent operating grid world style domain move left right down well pick passenger drop passenger. goal achieved agent taken passengers destinations. visualize compression simple taxi instance figure stated above visualize original taxi problem graph representation visualize ground abstract format despite unnatural appearance. mineﬁeld test problem introducing uses grid world dynamics russell norvig slip probability reward function moving grid receives reward; transitions receive reward except transitions random mine-states receive reward. experiments φq∗ε type aggregation functions. provide results φq∗ε because proofs section demonstrate three functions reducible particular φq∗ε functions. solving greedily aggregating ground states abstract states satisﬁed theφq∗ε criteria. since approach represents order-dependent approximation maximum amount abstraction possible randomized order states considered across trials. every ground state equally weighted abstract state. domain report quantities function epsilon conﬁdence bars. first compare number states abstract diﬀerent values shown left column figure figure smaller number abstract states smaller state space agent must plan over. second report value abstract policy initial ground state also shown right column figure figure taxi random domains trials data point whereas trials suﬃcient upworld mineﬁeld nchain. empirical results corroborate thesis—approximate state abstractions decrease state space size retaining bounded error. nchain mineﬁeld observe that increases number states must planned reduced optimal behavior either fully maintained nearly maintained similarly taxi observe reduction number states abstract value fully maintained. increased reduction state space size comes cost value. lastly increased random domain smooth reduction number abstract states corresponding cost value derived policy. reduction state space size whatsoever states identical optimal q-values. experimental results also highlight noteworthy characteristic approximate state abstraction goal-based mdps. taxi exhibits relative stability state space size behavior point fall dramatically. attribute sudden fall quantities goal-based nature domain; information critical achieving optimal behavior lost state aggregation solving goal—and acquiring reward—is impossible. conversely random domain great deal near optimal policies available agent. thus even information optimal behavior lost many near optimal policies available agent remain available. approximate abstraction mdps oﬀers considerable advantages exact abstraction. first approximate abstraction relies criteria imagine planning learning algorithm able learn without solving full mdp. second approximate abstractions achieve greater degrees compression relaxed criteria equality. third methods employ approximate aggregation techniques able tune aggressiveness abstraction incurring bounded error. work proved bounds value lost behaving according optimal policy abstract empirically demonstrate approximate abstractions reduce state space size minor loss quality behavior. provide code base provides implementations abstract visualize evaluate arbitrary promote investigation approximate abstraction. many directions future work. first interested extending approach ortner learning approximate abstraction functions introduced paper online planning setting particularly agent must solve collection related mdps. additionally work presents several suﬃcient conditions achieving bounded error learned behavior approximate abstractions hope investigate conditions strictly necessary approximate abstraction achieve bounded error. further interested characterizing relationship temporal abstractions options approximate state abstractions. lastly interested understanding relationship various approximate abstractions information theoretical limitations degree abstraction achievable mdps.", "year": 2017}