{"title": "Near-Optimal BRL using Optimistic Local Transitions", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Model-based Bayesian Reinforcement Learning (BRL) allows a found formalization of the problem of acting optimally while facing an unknown environment, i.e., avoiding the exploration-exploitation dilemma. However, algorithms explicitly addressing BRL suffer from such a combinatorial explosion that a large body of work relies on heuristic algorithms. This paper introduces BOLT, a simple and (almost) deterministic heuristic algorithm for BRL which is optimistic about the transition function. We analyze BOLT's sample complexity, and show that under certain parameters, the algorithm is near-optimal in the Bayesian sense with high probability. Then, experimental results highlight the key differences of this method compared to previous work.", "text": "model-based bayesian reinforcement learning allows sound formalization problem acting optimally facing unknown environment i.e. avoiding exploration-exploitation dilemma. however algorithms explicitly addressing suﬀer combinatorial explosion large body work relies heuristic algorithms. paper introduces bolt simple deterministic heuristic algorithm optimistic transition function. analyze bolt’s sample complexity show under certain parameters algorithm nearoptimal bayesian sense high probability. then experimental results highlight diﬀerences method compared previous work. acting unknown environment requires trading exploration exploitation model-based bayesian reinforcement learning algorithms achieve maintaining using probability distribution possible models .these algorithms typically fall within three following classes belief-lookahead approaches optimally trade exploration exploitation reformulating problem solving pomdp state pair observed state distribution possible models; problem intractable allowing computationally expensive approximate solutions optimistic approaches propose exploration mechanisms explicitly attempt reduce model uncertainty relying principle optimism face uncertainty. focus optimistic approaches research ﬁeld without loss generality consider uncertainty transition function assuming known reward function. algorithms recent work proves either pac-mdp —with high probability often optimal policy would pac-bamdp —with high probability often ideal belief-lookahead algorithm would paper ﬁrst presents background model-based section pac-mdp pacbamdp analysis section then section introduces novel algorithm bolt which boss optimistic transition model—which intuitively appealing since uncertainty model—and deterministic—which leads better control approach. prove section bolt pac-bamdp inﬁnite horizons generalizing previous results known ﬁnite horizon. experiments section give insight practical behavior algorithms showing particular bolt seems less sensitive parameter tuning beb. consider belief part state resulting belief-mdp solved optimally theory. remarkably modelling problems belief-mdps provides sound dealing explorationexploitation dilemma objectives naturally included optimization criterion. belief-state thus written deﬁnes bayes-adaptive special kind belief-mdp belief-state factored system state belief model. moreover integration possible models value function bamdp transition function given typical algorithms either directly estimate optimal state-action value function learn compute cases major diﬃculty pick actions trade exploitation current knowledge exploration acquire knowledge. consider model-based bayesian reinforcement learning i.e. model-based knowledge model represented using probability distribution possible transition models. initial prior distribution speciﬁed updated using bayes rule. time posterior depends initial distribution state-action history posterior bayes update ﬁnite horizon case reasoning optimal value computed theory ﬁnite inﬁnite horizon performing bayes updates computing expectations. however practice computing value function exactly intractable large branching factor tree expansion. here interested heuristic approaches following optimism face uncertainty principle consists assuming higher return uncertain transitions. solve generated expected model added exploration reward favors transitions lesser known models r-max variance based rewards another approach used boss solve model changed suﬃciently optimistic estimate true pac-mdp property. first algorithm must least near optimistic values high probability. also algorithm must guarantee high probability accurate meaning that known parts model actual evaluation \u0001-close optimal value function. finally number non-\u0001-close steps must bounded polynomial function. several algorithms comply pac-mdp property diﬀering another mainly tightness sample complexity bound. example r-max delayed q-learning classic algorithms property proved whereas boss bayesian algorithm also pac-mdp. pac-mdp analysis policy produced algorithm close optimal policy derived real underlying model. utopic policy cannot computed because impossible learn exactly model ﬁnite number samples possible reason probabilistic error bounds approximation policy. selection suitable prior important issue algorithms direct impact solution quality computing time. naive approach consider independent dirichlet distribution state-action transition known flat-dirichlet-multinomial prior whose deﬁned fdms applied discrete state-action appropriate strong assumption independence state-action pairs transition function. however prior broadly used simplicity computing bayesian update expected value. consider vector parameters counters observed transitions expected value transis θsa) bayesian update evidence transition reduced even though fdms useful analyze benchmark algorithms practice ineﬃcient because exploit structured information problem. example encode fact multiple actions share model factoring multiple dirichlet distributions allow algorithm identify structures using dirichlet distributions combined using chinese restaurant processes indian buﬀet processes alternative pac-mdp approach respect optimal bayesian policy rather using optimal utopic policy. call pac-bamdp analysis guarantee closeness optimal solution bayes-adaptive mdp. type analysis ﬁrst introduced kolter name near-bayesian property shown modiﬁed version pac-bamdp undiscounted ﬁnite horizon case intractable common suboptimal— eﬃcient—algorithms. popular technique maintain posterior belief select representative based posterior according value function. baseline algorithm family called exploit expected model selected time step. therefore algorithm solve diﬀerent horizon h—an algorithm parameter problem horizon— time step seen fig. consider analysis number iterations value iteration performs time step practice convergence reached long theoretically derived inﬁnite horizon case. follows idea exploit adding exploration bonus reward function. contrast boss exploit approach samples diﬀerent models prior uses construct optimistic mdp. advantage almost deterministic algorithm rely sampling boss. hand boss optimistic transitions uncertainty lies meanwhile optimistic reward function even though function known. section introduce novel algorithm called bolt relies acting time step following optimal policy optimistic variant current expected model. variant obtained state-action pair optimistically boosting bayesian updates computing local expected transition model. achieved using bamdp probability bayesian evaluation policy generated algorithm time \u0001-close optimal bayesian policy polynomial number steps bayesian evaluation parametrized belief major conceptual diﬀerence pac-bamdp analysis objective guarantee approximate correctness optimal bayesian policy hard compute pac-mdp analysis approximate correctness guarantee needed optimal utopic policy impossible ﬁnite number steps. modifying transition function seems natural approach modifying reward function since uncertainty consider problems transition function reward function. here optimism controlled positive parameter η—an integer real-valued parameter depending family distributions—and behaviour using diﬀerent parameter values depend used family distributions. however common priors like fdms proved bolt always optimistic respect optimal bayesian value function. lemma current belief-state apply bolt’s value iteration horizon also prior family optimal bayesian value function. then section prove bolt pac-bamdp discounted inﬁnite horizon case using prior. algorithm proved pacbamdp analysis provided kolter ﬁnite horizon domains imposed stopping condition bayes update. therefore include analysis using results section order able compare algorithms theoretically afterwards. deﬁnition must analyze policy generated bolt i.e. argmaxπ show that high probability polynomial number steps policy \u0001-close optimal bayesian policy. theorem denote policy followed bolt time also corresponding state belief prove bolt pac-bamdp introduce preliminary concepts results. first assume analysis maintain vector transition counters even though priors diﬀerent fdms speciﬁc lemma presented section. belief monitored known state-action pairs i.e. state-action pairs enough evidence. also analyze exploit-like algorithm general introduce mixed value function obtained performing exact bayesian update state-action pair update using concepts revisit lemma kolter discounted case. bolt pac-bamdp ˜vat evaluation bolt’s policy using mixed value function reward function a)|bt saσ] bolt transition model obtained policy note that even though apply bolt’s update still monitor belief step presented consider belief time monitored belief bayesian update lemma diﬀerence between optimistic value obtained bolt bayesian value obtained mixed value function policy generated bolt bounded even though hoeﬀding bound assumes samples independent trivially mdps upper bounds case samples dependent. recent results shows tighter bounds achieve elaborated analysis illustrate characteristics bolt present experimental results number domains. domains tried diﬀerent parameters bolt also used ε-greedy variant exploit. however presented problems plain exploit outperforms εgreedy variant. please recall theoretical values parameters η—that ensure optimism—depend horizon mdps solved time step. experiments instead using horizon relied asynchronous value iteration stopping -state chain problem every state connected state taking action every state connected next state action except connected itself. step agent slip probability performing opposite action intended. staying reward coming back reward rewards priors used problems full table shows outperforms algorithms tuned value prior already shown kolter however large value performance decreases dramatically. bolt hand produces results comparable boss tuned parameter decrease much large value indeed value corresponds theoretical bound ensures optimism log)/ unsurprisingly results bolt informative priors much diﬀerent techniques problem degenerates easily solvable problem. nevertheless bolt achieves good results large contrast fails provide competitive result semi prior large variability results depending parameters rises question sensitivity parameter tuning. domain usually cannot tune algorithm parameters problem whole model problem unknown. therefore good algorithm must perform well diﬀerent problems without modifying parameters. fig. shows bolt behave diﬀerent parameters using full prior. resolution analysis beb’s performance decays fast bolt also tends decrease maintaining good results. also conducted experiments values slip probability pattern ampliﬁed near i.e. worse decay almost constant bolt results obtaining almost identical behavior near high resolution results goes near bolt maintains similar behaviour resolution experiment. figure chain problem. averaged total reward trials horizon parameters reference value obtained exploit also plotted. results shown conﬁdence interval. figure paint/polish problem. averaged total reward trials horizon several values using structured prior. reference value obtained exploit also plotted. results shown conﬁdence interval. illustrative example paint/polish problem objective deliver several polished painted objects without scratch using several stochastic actions unknown probabilities. full description problem found walsh here possible outcomes action given agent probabilities outcome not. used structured prior encodes information results summarized fig. using high resolution analyses. also performed experiment prior obtaining similar results chain problem. unsurprisingly using structured prior provides better results using fdms. however high impact overoptimistic shown fig. apply fdms mainly learning phase much shorter using structured prior. again decay much stronger bolt contrast chain problem best parameter bolt beats best parameter beb. last example marble maze problem explicitly encoded possible clusters prior leading little exploration requirements. exploit provides good solutions problem bolt provides similar results several diﬀerent parameters. contrast tested parameters behaves much worse exploit. example best bolt scores best scores exploit scores summary hard know priori algorithm perform better speciﬁc problem speciﬁc prior given certain parameters. however bolt generalizes well larger parameters mainly optimism bounded probability laws free parameter beb. presented bolt novel simple algorithm uses optimistic boost bayes update thus optimistic uncertainty rather face uncertainty. showed bolt strictly optimistic certain parameters used result prove also pacbamdp. sample complexity bounds bolt tighter beb. experiments show bolt eﬃcient using theoretically derived parameters chain problem general bolt seems robust parameter tuning. future work includes using dynamic bonus bolt particularly appropriate ﬁnite horizons exploring general proofs guarantee pac-bamdp property broader family priors fdms.", "year": 2012}