{"title": "Time-Sensitive Bayesian Information Aggregation for Crowdsourcing  Systems", "tag": ["cs.AI", "cs.LG"], "abstract": "Crowdsourcing systems commonly face the problem of aggregating multiple judgments provided by potentially unreliable workers. In addition, several aspects of the design of efficient crowdsourcing processes, such as defining worker's bonuses, fair prices and time limits of the tasks, involve knowledge of the likely duration of the task at hand. Bringing this together, in this work we introduce a new time--sensitive Bayesian aggregation method that simultaneously estimates a task's duration and obtains reliable aggregations of crowdsourced judgments. Our method, called BCCTime, builds on the key insight that the time taken by a worker to perform a task is an important indicator of the likely quality of the produced judgment. To capture this, BCCTime uses latent variables to represent the uncertainty about the workers' completion time, the tasks' duration and the workers' accuracy. To relate the quality of a judgment to the time a worker spends on a task, our model assumes that each task is completed within a latent time window within which all workers with a propensity to genuinely attempt the labelling task (i.e., no spammers) are expected to submit their judgments. In contrast, workers with a lower propensity to valid labeling, such as spammers, bots or lazy labelers, are assumed to perform tasks considerably faster or slower than the time required by normal workers. Specifically, we use efficient message-passing Bayesian inference to learn approximate posterior probabilities of (i) the confusion matrix of each worker, (ii) the propensity to valid labeling of each worker, (iii) the unbiased duration of each task and (iv) the true label of each task. Using two real-world public datasets for entity linking tasks, we show that BCCTime produces up to 11% more accurate classifications and up to 100% more informative estimates of a task's duration compared to state-of-the-art methods.", "text": "crowdsourcing systems commonly face problem aggregating multiple judgments provided potentially unreliable workers. addition several aspects design eﬃcient crowdsourcing processes deﬁning worker’s bonuses fair prices time limits tasks involve knowledge likely duration task hand. bringing together work introduce time–sensitive bayesian aggregation method simultaneously estimates task’s duration obtains reliable aggregations crowdsourced judgments. method called bcctime builds insight time taken worker perform task important indicator likely quality produced judgment. capture this bcctime uses latent variables represent uncertainty workers’ completion time tasks’ duration workers’ accuracy. relate quality judgment time worker spends task model assumes task completed within latent time window within workers propensity genuinely attempt labelling task expected submit judgments. contrast workers lower propensity valid labelling spammers bots lazy labellers assumed perform tasks considerably faster slower time required normal workers. speciﬁcally eﬃcient message-passing bayesian inference learn approximate posterior probabilities confusion matrix worker propensity valid labelling worker unbiased duration task true label task. using real-world public datasets entity linking tasks show bcctime produces accurate classiﬁcations informative estimates task’s duration compared state–of–the–art methods. crowdsourcing emerged eﬀective acquire large amounts data enables development variety applications driven machine learning human computation participatory sensing systems services amazon mechanical turk odesk crowdflower enabled number applications hire pools human workers provide data serve training image annotation galaxy classiﬁcation information retrieval systems applications central problem deal diversity accuracy speed workers exhibit performing crowdsourcing tasks. result uncertainty reliability individual crowd responses many systems collect many judgments diﬀerent workers achieve high conﬁdence quality labels. however incur high cost either time money particularly workers paid judgment delay completion entire crowdsourcing project introduced workers intentionally delay submissions follow work schedule. example typical crowdsourcing scenario requester must specify number requested assignments well time limit completion assignment. must also price paid response usually includes participation bonus based quality submission actual eﬀort required task. however non–trivial problem time limit gives workers suﬃcient time perform task correctly without leading task starvation generally speaking knowledge actual duration assignment useful requesters various reasons. first task’s duration used proxy estimate diﬃculty diﬃcult tasks usually take longer complete second information useful time limit task reduce overall time task completion. third. task requestor task duration fair bonuses workers based diﬃculty task complete. seeking estimate information however important consider workers might perform task immediately might delay submissions accepting task extreme might submit poor annotation rapid time result common heuristic estimates task’s duration account aspects likely inaccurate. given above number challenges addressed various steps designing eﬃcient crowdsourcing workﬂows. first judgments collected uncertainty unknown reliability individual workers must taken account compute ﬁnal labels. aggregated labels often estimated settings true answer task never revealed quantity crowdsourcing process trying discover second estimating task’s duration uncertainty completion time deriving private work schedule worker must taken account third challenges must addressed simultaneously interdependencies workers’ reliability time required complete task ﬁnal labels estimated tasks. attempt address challenges growing interest developing algorithms techniques compute accurate labels minimising possibly unreliable crowd judgements detail simple solutions typically heuristic methods majority voting weighted majority voting however methods consider reliability diﬀerent workers treat judgments equally reliable. sophisticated methods one–coin model glad cubam bayesian classiﬁer combination probabilistic models take reliabilities account potential labelling biases workers e.g. tendency worker consistently underrate items. particular represents worker’s skills based confusion matrix expressing reliability worker possible class objects. works similarly also considers uncertainty confusion matrices aggregated labels using principled bayesian learning framework. representational power enabled successfully applied number crowdsourcing applications including galaxy classiﬁcation disaster response sentiment analysis recently proposed community–based extension improve predictions leveraging groups workers similar confusion matrices. similarly combined language modelling techniques automated text sentiment analysis using crowd judgments. degree applicability performance bcc-based methods promising point departure developing data aggregation methods crowdsourcing systems. however none existing methods reason workers’ completion time learn duration task outsourced crowd. moreover methods learn probabilistic models information contained judgment set. unfortunately strategy challenged datasets arbitrarily sparse i.e. workers provide judgments small sub-set tasks therefore judgments provide weak evidence accuracy worker. contexts hypothesis wider features must leveraged learn reliable crowdsourcing models. work focus time takes worker complete task considered indicator quality work. importantly information workers’ completion time made available popular crowdsourcing platforms including microsoft universal human relevance system crowdflower. therefore seek eﬃciently combine features data aggregation algorithm naturally integrated output data produced platforms. detail present novel time–sensitive data aggregation method simultaneously estimates tasks’ duration obtains reliable aggregations crowdsourced judgments. characteristic time–sensitivity method relates ability jointly reason worker’s completion time together judgments data aggregation process. detail method extension term bcctime. speciﬁcally incorporates newly developed time model enables method leverage observations time spent worker task best inform inference ﬁnal labels. confusion matrices represent labelling accuracy individual workers. model granularity workers’ time proﬁles latent variables represent propensity worker submit valid judgments. further model uncertainty duration task latent thresholds deﬁne time interval within task expected completed workers high propensity valid labelling. then using bayesian message-passing inference method simultaneously infers posterior probabilities confusion matrix worker propensity valid labelling worker true label task upper lower bound duration task. particular latter represents reliable estimate likely duration task obtained automatically ﬁltering contributions workers propensity valid labelling. demonstrate eﬃcacy method using commonly–used public datasets relate important natural language processing application crowdsourcing entity linking tasks. datasets method achieves accurate classiﬁcations compared seven state-of-the-art methods. further show tasks’ duration estimates informative common heuristics consider workers’ completion time correlated quality judgments. show existence diﬀerent types task–speciﬁc quality–time trends e.g. increasing decreasing invariant trends quality judgments time spent workers produce them. also re-conﬁrm existing results showing workers submit judgments quickly slowly entire task typically provide lower quality judgments. data aggregation methods crowdsourcing including cbcc coin majority voting providing accurate classiﬁcations informative estimates task’s duration. rest paper unfolds follows. section describes notation preliminaries bayesian aggregation crowd judgments. section details time analysis real-world datasets. then section formally introduces bcctime details probabilistic inference. section presents evaluation state art. section summarises rest related work areas data aggregation time analysis crowd generated content section concludes. number tasks number workers number true label values observed workers’ completion time observed judgments true label task vector judgment task time spent judging task confusion matrix class proportions tasks propensity making valid labelling attempts labelling probabilities general low-propensity worker vector ψk∀k boolean variable signalling lower-bound threshold duration task upper-bound threshold duration task mean gaussian prior precision hyperparameter gaussian prior mean hyperparameter gaussian prior precision hyperparameter gaussian prior true count hyperparameter beta prior false count hyperparameter beta prior hyperparameter dirichlet prior hyperparameter dirichlet prior hyperparameter dirichlet prior introduce features model relevant method. first introduced method combines multiple judgments produced independent classiﬁers unknown accuracy. speciﬁcally model assumes that task drawn categorical distribution parameters denotes class proportions objects. then worker’s accuracy represented confusion matrix comprising labelling probabilities possible true label value. speciﬁcally matrix vector probability producing judgment object class importantly confusion matrix expresses accuracy biases worker. recognise workers particularly accurate bias speciﬁc class objects. fact accurate workers represented high probabilities diagonal confusion matrix whilst workers bias towards particular class high probabilities corresponding column matrix. example galaxy domain workers classify images celestial galaxies confusion matrices detect workers accuracy classifying spiral galaxies systematically classify every object elliptical galaxies equivalent categorical mixture model mixture parameter parameter c-th categorical component. then assuming judgments independent identically distributed joint likelihood expressed expression possible derive predictive posterior distributions unobserved variable using standard integration rules bayesian inference unfortunately exact derivation posterior distributions intractable non-conjugate form model however shown that particularly models possible compute eﬃcient approximations distributions using standard techniques gibbs sampling variational bayes expectation-propagation building this several extensions proposed various crowdsourcing domains particular cbcc applies community–based techniques represent groups workers similar confusion matrices classiﬁer combination process mechanism enables model transfer learning worker’s reliability communities improve quality inference. result perform full learning confusion matrices task labels using judgments produced workers. mentioned earlier strategy challenged sparse datasets worker labels tasks. case instance crowdﬂower dataset used crowdscale shared task challenge sentiment tweets classiﬁed workers sentiment classes. dataset workers judged tweets i.e. total samples long tail workers less judgments. discussed basic concepts non-time based data aggregation turn analysis relationship time workers spend task quality judgments produce. contrast previous works area extend analysis quality–time responses speciﬁc task instances well entire task set. doing provide insights inform design time–sensitive aggregation model. consider public datasets generated widely used application crowdsourcing entity linking tasks. contains links names entities zencrowd india extracted news articles uniform resource identiﬁers describing entity freebase dbpedia dataset collected using worker asked classify whether single either irrelevant relevant single entity. contains timestamps acceptance submission judgment. moreover gold standard labels collected expert editors tasks. information released regarding restrictions worker pool although workers known living india worker paid judgment. total judgements collected small pool workers giving dataset moderately high number judgements worker detailed table particular figure shows vast majority tasks receive judgements figure shows skewed distribution gold labels links entities uris classiﬁed workers irrelevant such worth noting binary classiﬁers bias towards unrelated classiﬁcation correctly classify majority tasks thus receive high accuracy. therefore detail section important select accuracy metrics evaluate classiﬁer across whole spectrum possible discriminant thresholds. zencrowd dataset also provided demartini contains judgements tasks zc-in although judgements collected workers payment judgement used. however larger pool workers involved lower number judgements collected worker shown table furthermore figure shows similar distribution judgements task india dataset although slightly fewer tasks received judgements remaining tasks receiving judgements judgements. judgement accuracy dataset higher india dataset despite identical crowdsourcing system reward mechanism used. weather sentiment weather sentiment dataset provided crowdflower crowdsourcing scale shared task challenge. includes tweets judgements workers used several experimental evaluations crowdsourcing models detail workers asked classify sentiment tweets respect weather following categories negative neutral positive tweet related weather can’t tell result dataset pertains multi-class classiﬁcation problem. however original dataset used share task challenge contain time information collected judgments. therefore dataset recollected tasks crowdflower shared task dataset using platform acquiring exactly judgements recording elapsed time judgment result ws-amt contains judgements workers shown table restrictions placed worker pool worker paid judgement. furthermore figure shows that original dataset common gold label unrelated tasks assigned gold label can’t tell. wish analyse distribution workers’ completion time judgments’ accuracy. focus datasets zc-us zc-in binary labels. fact binary nature datasets allow analyse accuracy higher level figure histograms recall entity linking tasks positive gold standard labels least judgments zencrowd datasets. show diﬀerent trends recall-time curves various tasks. detail i.e. terms precision recall workers’ judgments time spent produce them. speciﬁcally figure shows cumulative distribution precision recall judgments selected speciﬁc time threshold respect gold standard labels. here precision fraction true positive classiﬁcations returned positive classiﬁcations recall number true positive classiﬁcations divided number positive samples. similarly demartini accuracy lower extremes time distributions. zc-us precision recall higher sub-set judgments produced seconds less seconds. figure pearson’s correlation coeﬃcient p-value linear correlation workers’ completion time judgments accuracy entity linking tasks positive gold standard labels judgments zencrowd datasets. addition figure shows distribution recall execution time sample positive task instances least judgments. example figure shows time distribution judgments freebase.com/united states associated entity american. graphs samples increasing quality-time curve i.e. workers spending time produce better judgments samples decreasing quality-time curve i.e. workers spending time produce worse judgments finally last samples approximately constant quality-time curve i.e. worker’s quality invariant time spent also seen trends naturally correlate diﬃculty task instance. instance freebase.com/m/hkhgs linked entity southern avenue diﬃcult judge dbpedia.org/page/switzerland linked entity switzerland. fact southern avenue ambiguous entity name lead worker open check content able issue correct judgment. instead relevance second entity switzerland judged easily visual inspection uri. addition task speciﬁc time interval includes sub-set judgments highest precision. example zc-in judgments highest precision dbpedia.org/page/switzerland entity switzerland submitted sec. sec. instead zc-us best judgments dbpedia.org/page/european linked entity european submitted interval sec. sec. result clear task instance speciﬁc quality–time proﬁle relates diﬃculty labelling instance. better analyse trends figure shows pearson’s correlation coeﬃcient entities positive links judgments across datasets. time spent worker always correlated quality judgment across task instances. tasks signiﬁcantly positive correlation others signiﬁcantly negative correlation whilst tasks less signiﬁcant correlation accuracy judgments time spent workers. conﬁrms diﬀerent task instances substantially diﬀerent quality-time responses based diﬃculty sample. thus insight signiﬁcantly extends previous ﬁndings reported demartini quality–time trend observed across entire task set. moreover empirically supports theory several existing data aggregation models make task–speciﬁc features achieve accurate classiﬁcations number crowdsourcing applications concerning among others galaxy classiﬁcation image labelling problem solving based results time analysis workers’ judgments observed diﬀerent types quality–time trends occur speciﬁc task instances. however standard well existing aggregation models consider information unable perform inference likely duration task. rectify this need extend able include trends aggregation crowd judgments. model must ﬂexible enough identify workers addition imperfect skills also intention make valid attempt complete task. increases uncertainty data reliability. section describe bayesian classiﬁer combination model time particular describe three components model concerning representation unknown workers’ propensity valid labelling reliability workers’ judgments uncertainty worker’s completion time followed details probabilistic inference. valid labelling attempt given task. model able naturally explain unreliability worker based imperfect skills also attitude towards approaching task correctly. particular close means worker tendency exert best eﬀort provide valid judgments even though judgments might still noisy consequence imperfect skills possesses. contrast close zero means worker tends provide valid judgments tasks means behaves similarly spammer. speciﬁcally workers high propensity valid labelling provide inputs meaningful task’s true label task’s duration. capture this deﬁne per-judgment boolean variable meaning made valid labelling attempt submitting invalid annotation. setting number valid labelling attempts made worker derives propensity valid labelling. thus model assuming random draw bernoulli distribution parametrised describe part model concerned generative process crowd judgments confusion matrix propensity workers. intuitively judgments associated valid labelling attempts considered estimate ﬁnal labels. means judgment generated diﬀerent processes depending whether comes valid labelling attempt. capture generative model bcctime mixture model used switch cases conditioned judgment generated worker’s confusion matrix standard model. therefore assume generated model described including second case judgment produced invalid labelling attempt i.e. natural assume judgment contribute estimation true label. formally assumption represented general random vote model vector labelling probabilities general worker propensity make valid labelling attempts. notice equation depend means judgments coming invalid labelling attempts treated noisy responses uncorrelated shown section duration task deﬁned interval workers likely submit high-quality judgments. however dependency duration task’s characteristics requirement interval must non-constant across tasks. model this deﬁne lower-bound threshold upper-bound threshold time interval representing duration per–task thresholds latent variables must learnt training time. then tasks lower higher variability duration represented based values time thresholds. setting valid labelling attempts made workers expected completed within task’s duration interval detailed thresholds. formally represent probability greater using standard greaterthan probabilistic factor introduced trueskill bayesian ranking model factor deﬁnes non-conjugate relationship posterior distribution form prior distribution therefore posterior distribution needs approximated. moment matching gaussian distribution posterior distribution shown table herbrich similar model probability greater factor graph bcctime illustrated figure speciﬁcally shaded variables observed inputs unobserved random variables unshaded. graph uses gate notation introduced represent mixture models bcctime. speciﬁcally outer gate represents workers’ judgments completion times generated either random vote model using gating variable. inner gate mixture model generating workers’ judgments rows confusion matrix using gating variable. select conjugate distributions parameters enable tractable inference posterior probabilities. therefore prior dirichlet distributed hyperparameter expression compute marginal posterior distributions latent variable integrating remaining variables. unfortunately integrations intractable non–conjugate form model. however still compute approximations posterior distributions using standard techniques family approximate bayesian inference methods particular well-known algorithm shown provide good quality approximations models method leverages factorised distribution joint probability approximate marginal posterior distributions iterative message passing scheme implemented factor graph. speciﬁcally implementation provided infer.net standard framework running bayesian inference probabilistic models. using infer.net able train bcctime largest dataset judgments within seconds using approximately standard laptop. described model test performance terms classiﬁcation accuracy ability learn tasks’ duration real crowdsourcing experiments. using datasets described section conduct experiments following experimental setup. consider benchmarks consisting three popular baselines three state–of–the–art aggregation methods commonly employed crowdsourcing applications. detail parameter assuming worker return correct answer probability speciﬁed coin incorrect answer inverse probability. result method applicable binary datasets. crucially model represents core mechanism several existing methods including image annotation disaster response learnt. benchmark used assess contribution inferring worker’s propensity versus joint learning tasks’ time thresholds quality ﬁnal labels. note bccpropensity easy obtain bcctime setting time thresholds static observations max.value. confusion matrices described section given judgment cbcc able learn confusion matrix community worker well task label. method also used number crowdsourcing applications including search evaluation sentiment analysis experiments cbcc number worker types communities order infer groups reliable workers less reliable workers similar results observed higher number communities. aggregated label receives votes assigns point mass label highest consensus among judgments. thus algorithm represent uncertainty around classiﬁcation considers judgments coming reliable workers. note alternative variant bcctime captures time spent redundant. fact workers’ propensity modelled together time spent workers’ accuracy captured confusion matrices. means model equivalent already included benchmarks. benchmarks also implemented infer.net trained using algorithm. particular refer coin unconstrained version zencrowd without unicity sameas constraints deﬁned original method. suggests version suitable fair comparison methods. experiments hyperparameters bcctime reproduce typical situation task requester prior knowledge true labels labelling probabilities workers basic prior knowledge accuracy workers representing that priori assumed better random annotators therefore workers’ confusion matrices initialised slightly higher value diagonal lower values rest matrix. then dirichlet priors uninformatively uniform counts. priors confusion matrices initialised higher diagonal value meaning priori workers assumed better random. gaussian priors tasks’ meaning priori entity linking task expected completed within seconds. furthermore initialise beta prior function number tasks represent fact priori worker considered reliable makes valid labelling attempts tasks. importantly given shape distribution worker’s time completion data observed datasets apply logarithmic transformation order obtain uniform distribution workers’ completion time training data. finally priors benchmarks equivalently bcctime. evaluate classiﬁcation accuracy tested methods measured area curve zc-us zc-in average recall ws-amt. particular former standard accuracy metric evaluate performance binary classiﬁers range discriminant thresholds applied predictive class probabilities well suited zencrowd binary datasets. latter recall averaged class categories main metric used score probabilistic methods competed crowdflower shared task challenge dataset equivalent ws-amt table reports seven algorithms zencrowd datasets. speciﬁcally shows bcctime bccpropensity highest accuracy datasets higher zc-in higher zc-us respectively compared methods. among bcctime best method improvement zc-in zc-us. similarly table reports average recall methods ws-amt showing bcctime highest average recall higher second best benchmark higher bccpropensity. means inference time thresholds already provides valuable information tasks extracted judgments also adds extra quality improvement aggregated labels addition modelling workers’ propensities. important observation proves information workers’ completion time eﬀectively data aggregation. altogether information allows model correctly ﬁlter unreliable judgments consequently provide accurate classiﬁcations. figure shows curve methods zencrowd datasets namely plot false positive rate true positive rate obtained diﬀerent discriminant thresholds. graph shows true positive rate bcctime generally higher benchmarks false positive rate. detail majority vote vote distribution perform worse random datasets methods clearly penalised presence less reliable workers treat workers equally reliable. interestingly coin performs better cbcc meaning confusion matrix better approximated single parameter datasets. also looking percentages workers’ propensities inferred bcctime reported table found workers zc-us workers zc-in workers ws-amt propensity greater means that zc-us ws-amt workers identiﬁed suspected spammers majority estimated reliable diﬀerent propensity values. zc-in percentage suspected spammers higher also reﬂected lower accuracy judgments respect gold standard labels. figure shows mean value inferred upper-bound time threshold workers’ maximum completion time task three datasets. looking data zencrowd datasets average maximum time spent workers higher indian workers also seen datasets signiﬁcant portion outliers reach minutes. however discussed section know many entity linking tasks fairly simple easily solved visual inspection candidate uri. imply normal worker completes task single session take long time issue judgment. interestingly bcctime eﬃciently removes outliers recovers realistic estimates maximum duration entity linking task. fact estimated upper-bound time thresholds within smaller time band i.e. around seconds zc-us seconds zc-in. similar results also observed ws-amt average observed maximum time signiﬁcantly higher average inferred maximum time thus suggesting bcctime estimates also realistic dataset. addition figure shows plot worker’s completion time task. graphs show bcctime estimates similar micro-tasks three datasets i.e. estimates obtained worker’s completion time data much higher seconds zc-us seconds zc-in seconds ws-amt. again presence outliers original data signiﬁcantly bias empirical average times towards high values. moreover measuring variability sets estimates bcctime estimates much smaller standard deviation lower empirical averages. means estimates informative compared normal average times obtained workers’ completion time data. accurate sparse judgments zc-in zc-us ws-amt clear winner since methods except random similar average recall trained sparse judgments. shows bcctime current form necessarily outperform methods sparse data. explained fact extra latent variables used improve quality ﬁnal labels also require larger judgments accurately learnt. however address issue possible draw community-based models design hierarchical extension bcctime over example workers’ confusion matrices improve robustness sparse data. here simplicity bcctime presented based simpler instance bayesian classiﬁer combination framework community-based version considered trivial extension. experimental evaluation. recent years large body literature focussed development smart data aggregation methods requesters combining judgments multiple workers. general existing methods vary assumptions complexity modelling diﬀerent aspects labelling noise. interested reader refer survey sheshadri lease well summary table lists popular methods comparison approach. particular methods able handle binary classiﬁcation problems i.e. workers vote objects possible classes multi-class classiﬁcation problems i.e. workers vote objects classes. among these many approaches coin model introduced benchmarks. detail model represents worker’s reliability single parameter deﬁned within range speciﬁcally combines model budget–limited task allocation framework provides strong theoretical guarantees asymptotical optimality inference workers’ reliability worker-task matching. uses general variational inference model reduces karger al.’s method well algorithms under special conditions. methods coin model represents bias worker towards positive labelling class towards negative class then quantities inferred using logistic regression maximum–a–posteriori approaches alternatively uses coin model embedded gaussian process classiﬁcation framework compute predictive probabilities aggregated labels workers’ reliability using along lines models reason diﬃculty task aﬀects quality judgment improve reliability aggregated labels area logistic regression model incorporate task’s diﬃculty together expertise worker labelling images. contrast diﬀerence quantities quantify advantage worker classifying object within joint diﬃculty-ability-response model. similar setting exploit convex problem formulation model improve eﬃciency inferring quantities numerical optimisation method. additional factors worker’s motivation propensity particular task taken account sophisticated models introduced recently devised method leverage fact error rates workers directly aﬀected access path follow access path represents several contextual features task however unlike work none methods learn confusion matrix worker. result represent reliability considering accuracy potential biases worker single data structure. ghahramani venanzi particular introduced ﬁrst confusion matrix-based model confusion matrices inferred using expectation-maximisation unsupervised manner. then extended work include task–speciﬁc latent matrix representing confusability task perceived workers. however neither methods consider uncertainty worker’s reliability parameters models. example label obtained worker methods infer worker perfectly reliable totally incompetent when reality worker neither. overcome limitation methods cbcc capture uncertainty worker’s expertise true labels using bayesian learning framework. methods extensively discussed earlier included benchmarks experiments. similarly cbcc methods leverage groups workers equivalent reliability improve quality aggregated labels limited data however already noted methods extra information workers’ judgments learn probabilistic models. result unlike approach cannot take full advantage time information provided crowdsourcing platform improve quality inference results. turn problem time analysis crowd generated content. recently introduced metric measuring eﬀort required complete crowdsourced task based area error-time curve such metric supports idea considering time important factor crowdsourcing eﬀort. regard closely related work analysis zencrowd datasets presented work showed workers complete tasks fast slow typically less accurate others. ﬁndings also conﬁrmed work. however addition extended analysis showing judgment’s quality correlated time spent workers diﬀerent ways speciﬁc task instances. intuition method exploits eﬃciently combine workers’ completion time features data aggregation process. furthermore earlier work introducing method predicts duration task based number available features using survival analysis model presented however method deal aggregating labels learning accuracy workers approach. presented evaluated bcctime time–sensitive aggregation method simultaneously merges crowd labels estimates duration individual task instances using principled bayesian inference. innovation method leverage extended features comprising workers’ completion time judgment set. appropriately correlated together features become important indicators reliability worker that turn allow estimate ﬁnal labels tasks’ duration workers’ reliability accurately. speciﬁcally introduced representation accuracy proﬁle worker consisting worker’s confusion matrix bcctime-proposedmethod apm-nushietal. cbcc-venanzietal. wm-lietal. lu-liuetal. gp-rodriguezetal. bm-bietal. mlnb-braggetal. mss-qietal. bcc-kim&ghahramani minmaxentropy-zhouetal. dare-bachrachetal. zencrowd-demartinietal. kj-kajinoetal. lda-wangetal. yu-yanetal. cubam-welinderetal. ry-raykaretal. glad-whitehilletal. ds-dawid&skene majorityvoting accounts worker’s labelling probabilities class worker’s propensity valid labelling represents worker’s intention meaningfully participate labelling process. furthermore used latent variables represent duration task using pairs latent thresholds capture time interval best judgments task likely submitted honest workers. model deal diﬀerences time length task instance relating diﬀerent type correlation quality received judgments time spent workers. fact task–speciﬁc correlations observed experimental analysis crowdsourced datasets various task instances showed diﬀerent types quality–time trends. thus main idea behind bcctime model trends aggregation crowd judgments make reliable inference quantities interest. extensive experimental validation real-world datasets showed bcctime produces signiﬁcantly accurate classiﬁcations estimates tasks’ duration considerably informative common heuristics obtained workers’ completion time data. background several implications work concerning various aspects reliable crowdsourcing systems. firstly process designing task take exploit unbiased task’s duration estimated bcctime. shown information valid proxy assess diﬃculty task therefore supports number decision–making problems fair pricing diﬃcult tasks deﬁning fair bonuses honest workers. secondly worker’s propensity valid labelling uncovers additional dimension workers’ reliability enables score attitude towards correctly approaching given task. information useful select diﬀerent task designs engaging tasks workers systematically approach task incorrectly. thirdly method uses features readily available common crowdsourcing systems allows faster take technology real applications. building advances several aspects current model indicate promising directions improvements. example consider time– dependencies accuracy proﬁle worker capture fact workers typically improve skills time performing sequence tasks. doing possible take advantage temporal dynamics potentially improve quality ﬁnal labels. addition crowdsourcing settings involve continuous-valued judgments currently supported method. deal cases number non– trivial extensions generative model turn treatment probabilistic inference required.", "year": 2015}