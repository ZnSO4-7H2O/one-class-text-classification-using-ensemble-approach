{"title": "Self-Organization of the Neuron Collective of Optimal Complexity", "tag": ["cs.NE", "cs.AI"], "abstract": "The optimal complexity of neural networks is achieved when the self-organization principles is used to eliminate the contradictions existing in accordance with the K. Godel theorem about incompleteness of the systems based on axiomatics. The principle of S. Beer exterior addition the Heuristic Group Method of Data Handling by A. Ivakhnenko realized is used.", "text": "abstract optimal complexity neural networks achieved self-organization principles used eliminate contradictions existing accordance godel’s theorem incompleteness systems based axiomatics. principle beer’s exterior addition heuristic group method data handling ivakhnenko realized used. introduction many cases neural networks must synthesized unrepresentative learning composed small number classified instances. instructions teacher used learn neural network usually exhaust multitude possible states. therefore causal relations represent learning classified. relations neural network inputs output represented functions belong selected class transformations. growing neural network complexity estimated number possible states increases number functions. however variety neural network uselessly increase value. similarly number functions used describe input-output relationships must limited. number accord ashby’s principle adequate variety. hand learned neural networks regarded causal systems based upon axiomatics. learning belong kind initial statements composed teacher instructions. require learning axioms would included contradictory instances. however functions whose necessity possible prove refuse within accepted axiomatics included desired collective functions describing relations neural network inputs outputs. follows fundamental theorem godel concerning incompleteness functions whose necessity possible prove refuse generate contradictory decisions input values close represented learning set. contradictions discovered testing consisting classified instances included learning set. thus neural network represented collective neurons describe discovered relations inputs output. number neurons collective equaled number relations must minimal. neuron local field competence number errors minimal. borders fields determined learning set. input values close borders fields results whole collective neurons taken account. decisions neurons weighted accordingly efficiency estimated learning set. efficiency neurons equal rule majority votes used order evaluate plausibility taken decision. example value plausibility equal ratio number neurons voted taken decision total number. complexity learned neural network optimal number errors occurred leaning well number neurons collective number inputs minimal. therefore number functions able generate contradictory decisions also minimal. neural network optimal complexity synthesized unrepresentative learning using fundamental principles self-organization order exclude contradictory relations trained neural network represent principle exterior addition beer would applied principle heuristic group method data handling ivakhnenko used synthesize neural networks consisting optimal number layers many cases efficiency neural networks increased instead quantitative input variables products generalized variables. variables nonlinear transformation formed preferable expanding structure input variables below methods selforganizing neural networks whose optimal complexity achieved nonlinear transformations discussed. basic stages selforganization neural network behavior described logical functions mccaloch pitts suggested number input variables exist logical functions note number searching variants logical functions variables requires huge computational expenses. order reduce expenses several methods suggested. method willis initial logical function many variables decomposes several functions smaller number arguments however method applicable narrow class logical functions. another approach suggested based gmdh applicable wider class functions heuristics hmdh used synthesize efficient neural networks unrepresentative learning set. using hmdh learning include even instances «teacher» classified within gmdh neural network represented multilayered scheme rosenblatt perceptron. synthesized reference function arguments usually reference function belong arbitrary class functions first layer next ones. note number means best functioncandidates selected next layers. using this gabor’s principle inconclusive decisions realized gmdh criterion used select function-candidate supposes learning divided non-conjunctive subsets length. subsets used learn neural networks realize heuristics. heuristics true function accurately describes neural network behavior would depend choice learning subsets efficiency function-candidate synthesized subset estimated upon forsince efficiency function-candidate estimated upon instances belong subset criterion called exterior. similar structure criterion needs realize principle exterior addition introduced exclude abovementioned contradictions. function-candidates synthesized computed layers gmdh algorithms convolution criteria done. cases function-candidate layer efficient smallest value criterion layers decreased increased. since complexity function exterior addition principle used value criterion goes minimum points desired function however indeterminate components presenting input variables founded minimum local. avoid possible reducing neural network efficiency positive variable introduced stopping rule nevertheless rule probably fulfilled desired function noise distortion input variables typically rule points function whose efficiency lower. generally advantages gmdh algorithms mostly concerned structure learned neural network. self-organizing neural network must assign number layers number neurons well activation function. also learned neural network comprises input variables called features useful separate patterns classified instances. explored gmdh-type algorithms selforganizing concluded structure synaptic weights learned neural network really depend following conditions. results depend firstly variants dividing learning subsets secondly choice number best function-candidates well value thirdly choice exterior criterion structure also class logical functions attractive self-organizing neural networks. trained neural network easily interpreted symbolic then rules used expert systems realized universal logical elements etc. direct self-organizing logical neural network optimal complexity exterior criteria drawbacks suggested criteria number neural network errors occurred learning used. statement apparently number layers increases number errors decreases minimum zero. number layers increases properties exterior addition useful. similarly condition following rule stopping algorithm formulated. second rule meets i.e. value case input variables added doubtful instances trained neural network erroneously classified excluded learning set. number neurons voted taken decision; total number neurons collective. closer value coherence decisions. usually value maximal learning instances. value decreased upon testing instances included less value decision neuron collective refused. typically value less analyzing values quality learning controlled. apparently value necessary either involve input variables modify learning set. computed boolean input variables. contains combinations input variables. however realizing logical neural network input quantitative variables must quantized represented boolean ones. easiest manner introduce threshold i.e. value must selected quantized variable could generate minimum number errors learning set. apparently rough approximation worsen separating ability features. example generate plain variables points classes linearly separated. parallel coordinates draw lines indicating quantized features. obviously features able exactly classify points set. would improve separating ability transforming input variables quantization. order solve problem suggested using nonlinear transformation among nonlinear function chosen non-parametric ones reasons. first non-parametric functions robust uncertainty unrepresentative learning comprised second formed features easily interpreted. suggested transformation analogously s.j. farlow gmdh algorithm ivakhnenko american statistician v.g. schetinin researching methods multilayered self-organization separating functions ph.d. thesis computer science penza state university v.g. schetinin synthesizing minimal decision rules principle exterior addition proc. conf. mathematical methods pattern recognition moscow v.g. schetinin synthesizing minimal neural network diagnosing illness proc. conf. neuroinformatics applications krasnojarsk suggested method criteria self-organization used clinical laboratory diagnostics pathologies neural networks logical type used essentially facilitate clinical interpretation discovered rules. efficiency suggested method demonstrated simple example recognizing human know size weight females males. classes instances linearly separable therefor errors recognition apply neural network logical type required quantize variables find corresponding thresholds note that particularly variables ensure unerring recognition. happen transform quantitative variables generalized variable condition generalized variable quantized variable able reduce number neurons decrease number errors learning thus transformation used example able raise efficiency neural network. suggested method criteria self-organization able synthesize logical neural network optimal complexity. neural networks type advantages since decisions robust easily interpreted. however quantization quantitative input variables reduce efficiency trained neural network. suggested nonlinear transformation variables compensate efficiency decrease logical neural network.", "year": 2005}