{"title": "Automated Variational Inference in Probabilistic Programming", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We present a new algorithm for approximate inference in probabilistic programs, based on a stochastic gradient for variational programs. This method is efficient without restrictions on the probabilistic program; it is particularly practical for distributions which are not analytically tractable, including highly structured distributions that arise in probabilistic programs. We show how to automatically derive mean-field probabilistic programs and optimize them, and demonstrate that our perspective improves inference efficiency over other algorithms.", "text": "present algorithm approximate inference probabilistic programs based stochastic gradient variational programs. method efﬁcient without restrictions probabilistic program; particularly practical distributions analytically tractable including highly structured distributions arise probabilistic programs. show automatically derive mean-ﬁeld probabilistic programs optimize them demonstrate perspective improves inference efﬁciency algorithms. probabilistic programming languages simplify development probabilistic models allowing programmers specify stochastic process using syntax resembles modern programming languages. languages allow programmers freely deterministic stochastic elements resulting tremendous modeling ﬂexibility. resulting programs deﬁne prior distributions running program forward many times results distribution execution traces trace generating sample data prior. goal inference programs reason posterior distribution execution traces conditioned particular program output. examples include blog prism bayesian logic programs stochastic logic programs independent choice logic ibal probabilistic scheme church stochastic matlab hansei easy sample prior deﬁned probabilistic program simply program. inference languages hard given known value subset variables inference must essentially program ‘backwards’ sample probabilistic programming environments simplify inference providing universal inference algorithms; usually sample based universality ease implementation. variational inference offers powerful deterministic approximation exact bayesian inference complex distributions. goal approximate complex distribution simpler parametric distribution inference therefore becomes task ﬁnding closest measured divergence. easy distribution optimization often done tractably; example mean-ﬁeld approximation assumes product marginal distributions easy work with. since variational inference techniques offer compelling alternative sample based methods interest automatically derive them especially complex models. unfortunately intractable programs. even models closed-form coordinate descent equations derivations often complex cannot done computer. however paper show tractable construct stochastic gradient-based variational inference algorithm automatically leveraging compositionality. unconditional probabilistic program deﬁned parameterless function arbitrary deterministic stochastic elements. stochastic elements either belong ﬁxed known atomic random procedures called erps deﬁned function stochastic elements. syntax evaluation valid program well deﬁnition library erps deﬁne probabilistic programming language. program runs encounter sequence erps sample values each. sampled values called trace program. value taken erp. probability trace given probability taking particular value observed trace history program probability distribution history-dependent parameter trace-based probabilistic programs therefore deﬁne directed graphical models general many classes models since language allow complex programming concepts control recursion external libraries data structures etc. goal variational inference approximate complex distribution simpler distribution done adjusting parameters order maximize reward function typically given divergence minimizing typically done computing derivatives analytically setting equal zero solving coupled nonlinear equations deriving iterative coordinate descent algorithm. however approach works conjugate distributions fails highly structured distributions analytically tractable. purpose adding constant possible approximately estimate value variance monte-carlo estimate expression minimized. choosing appropriate value drastic effects quality gradient estimate. consider distribution induced arbitrary unconditional probabilistic program. goal estimate marginals conditional distribution call target program. introduce variational distribution deﬁned another probabilistic program called variational program. distribution unconditional sampling easy running derive variational program target program. easy partial mean-ﬁeld approximation target probabilistic program forward time encountered variational parameter used place whatever parameters would ordinarily passed erp. instead sampling instead sample auxiliary variational parameter ignored). fig. illustrates pseudocode probabilistic program variational equivalent upon encountering normal line instead using parameter variational parameter used instead uniform default argument). note dependency exists control logic parameterization. thus general stochastic dependencies parameters variable depending outcome another variable disappear dependencies control logic remain thus requirement probabilistic program able compute likelihood value well gradient respect parameters. highlight able compute gradient log-likelihood respect natural parameters additional requirement compared mcmc sampler. note everything holds regardless conjugacy distributions stochastic program; regardless control logic stochastic program; regardless actual parametrization particular emphasize need gradients deterministic structures learning inference transfer. assume wish variational inference distinct datasets ideally solve distinct inference problem each yielding distinct unfortunately ﬁnding help dataset perhaps approach used learn ‘approximate samplers’ instead depending implicitly optimization algorithm suppose instead depends ﬁxed parameters observations variational distribution decent approximate sampler gradient estimates derived similarly arbitrary probabilistic programs. structured mean-ﬁeld approximations. sometimes case vanilla mean-ﬁeld distribution poor approximation posterior case structured approximations used deriving variational update structured mean-ﬁeld harder vanilla mean-ﬁeld; however probabilistic program point view structured mean-ﬁeld approximation simply complex variational program could derived program analysis gradients computed mean-ﬁeld case. online probabilistic programming advantage stochastic gradients probabilistic programs simple parallelizability. also done online fashion similar fashion recent work stochastic variational inference blei suppose variables observations separated main large number independent sets latent variables observations allowed depend instance represents topic distributions represent document distribution topics topic recall gradient variational parameters wards erps rewards erps rewritten random integer expectation approximately computed online fashion allowing update estimate without manipulating entire data ﬁrst vanilla stochastic gradient descent gradients given episodic natural actor critic algorithm version algorithm connecting variational inference reinforcement learning details reserved longer version paper. important feature enac optimizing baseline constant algorithm three algorithms given budget samples; used different ways. three algorithms estimated gradient used steepest descent optimizer stepsize three algorithms used stepsize; addition gradients scaled unit norm. experiment thus directly compares quality direction gradient estimate. fig. shows results. enac algorithm shows faster convergence lower variance steepest descent sogd fares poorly fig. also shows gradients enac used either steepest descent conjugate gradient optimizer; conjugate gradients converge faster. sogd enac estimate conclude performance advantage enac solely second-order information additional step estimating baseline improves performance signiﬁcantly. converged estimated variational program allows fast approximate sampling posterior fraction cost sample obtained using mcmc sampling. samples variational program also used warm starts mcmc sampling. natural conjugate gradients variational inference investigated analysis mostly devoted case variational approximation gaussian resulting gradient equation involves integral necessarily tractable. variational inference probabilistic programming explored authors similarly note easy sample variational program. however observation estimate free energy variational program estimate gradient free energy. highlight need optimizing parameters variational program offer general algorithm instead suggesting rejection sampling importance sampling. stochastic approximations variational inference also used carbonetto approach different ours sequential monte carlo reﬁne gradient estimates require family variational distributions contains target distribution. approach fairly general cannot automatically generated arbitrarily complex probabilistic models. finally stochastic gradient methods also used online variational inference algorithms particular work blei stochastic variational inference online generally conjugacy assumptions reﬁne estimates latent variable distributions without processing observations. however approach requires manual derivation variational equation coordinate descent possible conjugacy assumptions general hold arbitrary probabilistic programs.", "year": 2013}