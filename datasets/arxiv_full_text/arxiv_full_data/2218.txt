{"title": "Boosting Variational Inference: an Optimization Perspective", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Variational inference is a popular technique to approximate a possibly intractable Bayesian posterior with a more tractable one. Recently, boosting variational inference has been proposed as a new paradigm to approximate the posterior by a mixture of densities by greedily adding components to the mixture. However, as is the case with many other variational inference algorithms, its theoretical properties have not been studied. In the present work, we study the convergence properties of this approach from a modern optimization viewpoint by establishing connections to the classic Frank-Wolfe algorithm. Our analyses yields novel theoretical insights regarding the sufficient conditions for convergence, explicit rates, and algorithmic simplifications. Since a lot of focus in previous works for variational inference has been on tractability, our work is especially important as a much needed attempt to bridge the gap between probabilistic models and their corresponding theoretical properties.", "text": "variational inference popular technique approximate possibly intractable bayesian posterior tractable one. recently boosting variational inference proposed paradigm approximate posterior mixture densities greedily adding components mixture. however case many variational inference algorithms theoretical properties studied. present work study convergence properties approach modern optimization viewpoint establishing connections classic frank-wolfe algorithm. analyses yields novel theoretical insights regarding sufﬁcient conditions convergence explicit rates algorithmic simpliﬁcations. since focus previous works variational inference tractability work especially important much needed attempt bridge probabilistic models corresponding theoretical properties. variational inference method approximate complicated probability distributions simpler ones. many applications calculating exact posterior distribution intractable methods like mcmc ﬂexible also prohibitively expensive. variational inference restricts posterior member simpler tractable family distributions inference problem reduces ﬁnding member closely represent true underlying posterior. closeness typically measured sense. tractable called mean ﬁeld family assumes factored structure. example family gaussian distributions diagonal covariance matrices. inference computationally efﬁcient properties gaussian distributions family restrictive. such approximated distribution often good representation true posterior. simple example multi-modal distribution. mean ﬁeld family able capture modes. number efforts improve approximation retaining simplicity gaussian distributions. example could consider approximating mixture gaussian distributions allowing isotropic structures. mixture isotropic gaussian distributions already much powerful ﬂexible model single isotropic gaussian. fact ﬂexible enough model distribution arbitrarily closely signiﬁcant algorithmic empirical development studying variational inference using mixture models limited theoretical studies. work bridge gap. study optimization perspective approximation posterior iteratively adding simpler distributions necessarily gaussians greedily given components mixtures building mixture convex problem show efﬁcient algorithms converging global optimum. hand ﬁnding individual components non-convex known exhibit several local optima however show need solve inner non-convex problem exactly achieve strong convergence guarantees. analyses establishing connections functional variant well known frank-wolfe algorithm connection helps provide convergence rate greedy variational boosting algorithm explicit constants terms properties distributions. best knowledge explicit rates known context variational inference. moreover also able provide novel insights including sufﬁcient conditions linear convergence opposed connect boosting variational inference frank-wolfe framework enabling carefully analyze convergence. also thoroughly analyze assumptions essential ensure global convergence present explicit rate conjectured rate. propose simpler variants algorithm retain strong theoretical properties provide sufﬁcient conditions greedy algorithms achieve linear convergence therefore much faster previously conjectured. revisit norm-corrective frank-wolfe algorithm give linear convergence guarantees cost slightly larger computational cost. algorithm allows selectively reoptimize weights mixture efﬁciently every iteration resulting much faster convergence practice. variational approximations using mixture models extensively studied applied. perhaps closest algorithmic setup work iteratively components mixture greedily similar gradient boosting. require boosting subroutine return optimal density show required obtaining conjectured convergence rate number components added. also similar algorithm setup. traditional approaches directly target non-convex problem ﬁnding exactly ﬁrst density mixture. problem convergence analysis carried rates applicable locally depend smoothness assumption divergence hold globally unless iterate close optimum greedy methods clear advantage need perfectly best approximating distribution family previously considered rough approximate solution enough ensure convergence. frank-wolfe algorithm popular algorithm convex constrained minimization specially attractive cheap projection-free iterations. algorithm well studied theoretically empirically even applied non-euclidean spaces. example consider variational objective approximate marginal inference marginal polyrest paper organized follows. review variational inference problem optimization perspective section necessary sufﬁcient assumptions required show convergence section present algorithmic contributions framework section conclude paper experimental proof concept showing proposed methods converge expected. notation. represent vectors small letters bold e.g. matrices capital bold e.g. non-empty subset hilbert space conv denote convex hull. often called atom literature elements called atoms. given closed call diameter diam maxzz∈a radius radius maxz∈a support density function measurable denoted capital letters sans serif i.e. sometimes write domain density function notation domain support coincide would made explicit. inner product density functions observe data points space. bayesian modelling approach consists specifying prior data likelihood parameter vector measurable example challenges bayesian inference posterior obtained bayes theorem could intractable hard calculate normalization constant. instead joint distribution usually easier evaluate i.e. functional perspective posterior written assume represent posterior joint distribution. goal variational inference density constrained tractable densities support close sense true posterior. respective optimization problem note unconstrained minimization would yield equal true posterior. thus would ideally want able represent parameter space well still retaining tractability. objective equation computable requires access instead common practice maximize called evidence lower bound given well known strictly convex smoothness strong convexity depends choice showed smoothness constant bounded minimal value obtained functions densities domain showed strong convexity constant equal respective maximal value. simplicity following write instead dkl. sufﬁcient condition smoothness density bounded away zero extend result showing necessary condition global smoothness hold entire support lemma lipschitz smooth constant q/px i.e. bounded away zero sufﬁcient condition smoothness i.e. bounded away zero smoothness typical assumption useful measure convergence optimization algorithms employed also variational inference setting lemma entails proofs based smoothness valid regions space. lemma states good approximation smooth. consider general density simple ensure smoothness bound away zero. therefore restrict support approximating densities compact sets. practice algorithms initialized well enough q/px bounded away zero. example consider mixture gaussians mean sufﬁciently apart. boosting approach place density modes ﬁrst other. therefore gradient second iteration arbitrarily large parts domain depending modes covariance matrix unfortunately precisely parts method targets. thus need ensure signiﬁcant mass placed second mode well. family densities bounded away zero truncating support seen smoothing condition. initializing solution mean ﬁeld variational inference would place mass modes would appear smooth algorithm truncation might necessary. valid practice focus truncated densities need ensure rates present work valid density independently initial approximation. following line work introduce information projection another densities obtained truncating densities therefore bounded support intuitively variational inference aims projecting true posterior tractable densities instead boosting variational inference considers mixtures densities i.e. optimization constrained conv. underlying intuition conv expressive example density approximated mixture gaussian distributions appropriate covariance matrix. order comment rates convergence restrict densities truncated support call therefore distinguish density truncated version write former latter. therefore solve following optimization problem original posterior support choice conv optimization domain suboptimal conv support subset measure exactly error introduced truncating support. error represent tradeoff smoothness objective quality approximation. hope conv richer family distributions tractable conv optimization perspective. note conv. contains non-degenerate truncated gaussian distributions appropriate covariance matrix conv contains becomes minimizer equation proof deferred appendix convenient form also compute diameter corollary given distribution holds diam maxq∈a lebesgue measure support bounded assumptions theorem section explain foundations boosting frank-wolfe function spaces. analysis authors enforce bounded polytope using functions bounded norm. following traditional approaches assume functions must bounded norm. given computing exact solution depending often hard practice desirable rely approximate returns approximate minimizer accuracy parameter current iterate that general hard optimization problem. therefore approximate solution commonly employed. discuss simple algorithm implement section frank-wolfe algorithm depicted algorithm note algorithm variant algorithm algorithm known converge sublinearly following rate. theorem compact convex function bounded curvature then afﬁne invariant frank-wolfe algorithm converges cases convergence might actually faster stated below. theorem compact strongly convex function bounded curvature further assume lies within relative interior conv. then afﬁne invariant frank-wolfe algorithm produces sequence iterates converges goemetrically discussion recall diam. theorem showed degenerate truncated theorem degenerate truncated gaussian distribution compact support further assume means covariance matrix truncation given σmin σmin small enough conv. vertices diameter then information loss afﬁne invariant frank-wolfe algorithm choice compact support converges explicit dependency rate artifact proof technique consequence using norm. note depends implicitly decreases whenever increases support remains ﬁxed understanding whether rate meaningful high dimensions challenging question. better rates might achieved different notion distance left future work. solve problem revisit technique well known stochastic variational inference framework account constrained scenario. rewrite optimization problem equation exploiting parametric form distributions order obtain valid solution problem perform projected gradient descent parameters stochastic approximation gradient. proja operator proja holds. operator easy implement gaussian case reduced constraint mean constraint eigenvalues covariance matrix truncation. therefore sample points distributions bounded lemma showed exhibits bounded curvature results important theoretically justify successfully build mixture distributions approximating posterior boosting-like approach. optimization subtleties addressed essential convergence algorithm theorem introduce idea greedily adding density boosting fashion converging linearly additional assumptions. check whether optimum relative interior focus sublinear rate trying understand assumptions made target family distributions inﬂuence convergence. discussion expected rate depends main assumptions introduced compact support degenerate distributions. support covariance matrix directly inﬂuence values substantially different presented explicit assumptions make allows understand choices distribution family inﬂuences rate. particular consider importance bounded supports show vital conjecture hold. similarly sublinear convergence analysis variational inference holds ratio q/px bounded contains truncated gaussian distributions non-degenerate covariance matrix small enough determinant perfectly approximate density deﬁned bounded support also satisﬁes write suboptimality boosting approach making tradeoff support approximation error term explicit. indeed equation compute information lost projection compact support. hand represent projection onto support well. therefore ﬁnally give theorem measures total information loss boosting variational inference frank-wolfe. hibits convergence guarantees. alternative become attractive whenever line search expensive computationally. consider smoothness quadratic upper bound section review norm-corrective frankwolfe presented algorithm main limitation algorithm iteration uniformly reduces weights atoms active undesirable especially variational inference setting ﬁrst approximating densities carries information. hand early iterations suboptimal choices made considered optimal greedy strategy lose signiﬁcance optimization proceeds. therefore useful selectively update weights mixtures time. efﬁciency reasons update weights every iteration rather minimizing directly target quadratic upper bound previous section. results quadratic programming problem probability simplex many efﬁcient solutions known typically small. stepsize note ˆ∇θez∼sl unbiased estimator gradient showed approximation possible data domain sampling process i.i.d. stochastic algorithm depicted algorithm notably approximate solution sufﬁcient ensure convergence even δ-approximate expectation therefore relying cheap estimates gradient well posed framework. note linear problem equation without constraints would trivially solved degenerate distribution placed minimum value gradient. therefore contains truncated normal distributions local minimum covariance σmini. therefore experiments learn covariance matrix. recall approximate solution problem enough converge. constant procedure allows efﬁcient optimization using standard convex solvers. finding closest point norm typically performed much efﬁciently solving general optimization problem domain fully-corrective algorithm variants require iteration variant algorithm equivalent variant algorithm line search quadratic upper bound performed active atoms rather added current iteration hence name corrective. authors showed sublinear convergence algorithm work show additional assumptions convergence actually linear. theorem compact l-smooth µ-strongly convex optimization domain. then suboptimality iterates variant algorithm decreases geometrically step depth description pwidth continuous setting pyramidal width arbitrarily small. reason quantization mean vector sufﬁcient ensure pyramidal width bounded away zero. obtain linear convergence rate variant algorithm needs upper-bound number steps. notion comes pairwise away step frank-wolfe away vertex exponential decay guaranteed remove weight |st| |st+|. unfortunately tightest known bound variant number good steps rate variant given appendix. approach unsatisfactory linear convergence frank-wolfe active ﬁeld research beyond scope paper. case algorithm potentially much faster algorithm cost greater computation complexity iteration. furthermore algorithm already linearly convergent optimum lies relative interior conv shown therefore practice norm corrective variant achieve linear convergence general converges faster algorithm variational inference problem full framework allowing potentially globally linearly convergent algorithms. quantization mean values convergence linear conv ﬁnite number faces. best knowledge results ﬁrst linearly convergent algorithms boosting variational inference problem. furethermore identify assumptions depends development frank-wolfe analysis algorithm relation pwidth diam also known condition number related eccentricity. intuitively smaller diameter helps optimization reducing size search space. hand continuous setting contain atoms forming narrow pyramid limit gives vanishing pyramidal width. unfortunately computing constant challenging known examples synthetic data section empirically observe convergence algorithms task verifying convergence follows analysis. particular consider simple forms posterior distribution dimension heavy tailed cauchy distribution mixture gaussian distributions. approximate distributions using line search fully corrective variants expected even rough approximations performed fully corrective perfectly target distribution limited number iterations. ensure linear convergence performed quantization mean vectors examples used line search fully corrective respectively. weight fully corrective used standard semideﬁnite-quadratic programming expected expensive iteration algorithm converges much faster terms number iterations. therefore showed linear convergence achievable using algorithm minimizing dkl. discussion authors perform extensive experimental evaluation showing remarkable practical performances algorithm hand truncate gaussian distributions experiments still observe excellent convergence properties. note that provided algorithm initialized well enough bounded away zero entails exist ﬁnite upper bounds smoothness constant ﬁxed ﬁnite number iterations. regularize determinant covariance matrix bounded diameter. therefore algorithm linearly convergent whenever true posterior relative interior conv sublinear otherwise. sians mean ﬁeld ﬁeld family gives better training testing accuracy vanilla mean ﬁeld inference. reduce variance gradient estimator rao-blackwellization illustrate importance connections frank wolfe algorithm implement three different methods optimizing weights mixture. first implement line search technique minimizing original objective already proposed however simpler ﬁxed step size also guarantees convergence analysis fully corrective step optimizes previous weights. illustrated figure speciﬁcally report training data log-likelihood values show three different techniques offer varying rates training data expected. training data also translates test data accuracy present area curve receiver operator characteristic. presented in-depth theoretical convergence analysis boosting variational inference paradigm delineating explicitly rates assumptions required previously conjectured sublinear presented linear convergence rates. real data illustrate practical utility boosting framework implement algorithm real world application predicting whether chemical reactive features chemreact dataset contains chemicals features. training data contains points rest forms testing dataset. prediction task employ bayesian logistic regression spherical prior regression coefﬁcients feature vector response value respectively logistic likelihood function written represent feature matrix formed stacking response vector sigmoid function sigmoid +exp. straightforward posterior model closed form expression easy sample typically even relatively simple model mcmc techniques prohibitively slow mean ﬁeld variational inference often used. thesis yale university francesco locatello rajiv khanna michael tschannen martin jaggi. uniﬁed optimization view generalized matching pursuit frank-wolfe. proc. international conference artiﬁcial intelligence statistics stephan mandt james mcinerney farhan abrol rajesh ranganath david blei. variational tempering. proceedings international conference artiﬁcial intelligence statistics pages andrew miller nicholas foti ryan adams. variational boosting iteratively reﬁning posterior approximations. arxiv preprint arxiv. frank nielsen vincent garcia. statistical exponential families digest ﬂash cards. arxiv preprint arxiv.", "year": 2017}