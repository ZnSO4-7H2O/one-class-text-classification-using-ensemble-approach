{"title": "Right for the Right Reasons: Training Differentiable Models by  Constraining their Explanations", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.", "text": "using dataset containing attributes patients hospitalized least pneumonia. counterintuitively model learned presence asthma negative predictor readmission reality pneumonia patients asthma greater medical risk. model would presented grave safety risk used production. problem occurred outcomes dataset reﬂected severity patients’ diseases quality care initially received higher patients asthma. case others like motivated recent work interpretable machine learning algorithms provide explanations domain experts inspect correctness trusting model predictions. however limited work optimizing models right prediction also right explanation. toward work makes following contributions annotations available sequentially discover classiﬁers similar accuracies qualitatively different decision boundaries domain experts inspect validity. related work ﬁrst deﬁne several important terms interpretable machine learning. classiﬁers implicit decision rules converting input decision though rules opaque. model interpretable provides explanations predictions form humans understand; explanation provides reliable information model’s implicit decision rules given prediction. contrast machine learning model accurate predictions correct right right reasons implicit rules learned generalize well conform domain experts’ knowledge problem. explanations take many forms evaluating quality explanations interpretability model difﬁcult however within machine learning community recently neural networks among accurate supervised learning methods today. however opacity makes difﬁcult trust critical applications especially conditions training differ test. recent work explanations black-box models produced tools show implicit rules behind predictions. tools help identify models right wrong reasons. however methods scale explaining entire datasets cannot correct problems reveal. introduce method efﬁciently explaining regularizing differentiable models examining selectively penalizing input gradients. apply penalties based expert annotation unsupervised fashion produces multiple classiﬁers qualitatively different decision boundaries. multiple datasets show approach generates faithful explanations models generalize much better conditions differ training test. introduction high-dimensional real-world datasets often full ambiguities. train classiﬁers data frequently possible achieve high accuracy using classiﬁers qualitatively different decision boundaries. narrow choices encourage robustness usually employ regularization techniques also structure models ensure domain-speciﬁc invariances however solutions address situations training dataset contains subtle confounds differs qualitatively test dataset. cases model fail generalize matter well tuned. generalization gaps particular concern uninterpretable models neural networks especially sensitive domains. example caruana describe model intended prioritize care patients pneumonia. model trained predict hospital readmission risk ness. svms speciﬁc text classiﬁcation architectures exists work incorporating human input decision boundaries form annotator rationales unlike approach works either tailored speciﬁc domains fully close loop generating explanations constraining them. background input gradient explanations consider differentiable model parametrized inputs rn×d probability vector outputs rn×k corresponding one-hot labels rn×k. input gradient given vector normal model’s decision boundary thus serves ﬁrst-order description model’s behavior near gradient shape vector largemagnitude values input gradient indicate elements would affect changed. visualize explanations highlighting portions locations high input gradient magnitudes. approach wish develop method train models right right reasons. explanations faithfully describe model’s underlying behavior constraining explanations match domain knowledge cause underlying behavior closely match knowledge too. ﬁrst describe input gradient-based explanations lend themselves efﬁcient optimization correct explanations presence domain knowledge describe used efﬁciently search qualitatively different decision boundaries knowledge available. constraining explanations loss function constraining input gradient explanations basic options either constrain large relevant areas small irrelevant areas. however input gradients relevant inputs many models small decision boundary know advance large shrink irrelevant gradients instead. formally deﬁne annotation matrix }n×d binary masks indicating whether dimension irrelevant predicting observation would like near locations. optimize loss function form convergence around local counterfactual explanations show perturbing input various ways affect model’s prediction approach explanations domainmodel-speciﬁc zhang alternatively explanations model-agnostic relatively domain-general exempliﬁed lime trains presents local sparse models predictions change inputs perturbed. per-example perturbing ﬁtting process used models lime computationally prohibitive especially seek explain entire dataset training iteration. underlying model differentiable alternative input gradients local explanations provides particularly good introduction; also selvaraju simonyan hechtlinger idea simple gradients model’s output probabilities respect inputs literally describe model’s decision boundary similar spirit local linear explanations lime much faster compute. input gradient explanations perfect use-cases— points decision boundary uniformatively small always capture idea salience bach montavon sundararajan shrikumar fong vedaldi however exactly required constraining decision boundary. past drucker showed applying penalties input gradient magnitudes improve generalization; knowledge application input gradients constrain explanations alternate explanations novel. figure input gradients normal model’s decision boundary. examples simple twothreeclass datasets input gradients taken respect hidden layer multilayer perceptron relu activations. probability input gradients sharpest near decision boundaries probabilities input gradients consistent within decision regions. probability gradients contains information full model. input gradient large regions marked term regularization parameter right answers right reasons terms similar orders magnitude; appendix details. note loss penalizes gradient probability performed best practice though many visualizations show gradient predicted probability itself. summing across classes slightly stable results using predicted class probability perhaps discontinuities near decision boundary explore regularizing input gradients speciﬁc class probabilities though would natural extension. loss function differentiable respect easily optimize gradient-based optimization methods. need annotations every input case explanation term effect loss. extreme matrix encourages model small gradients respect inputs; improve generalization extremes biases model particular implicit rules. penalization approach enjoys several desirable properties. alternatives specify single examples presuppose coherent notion global feature importance decision boundaries nonlinear many features relevant context speciﬁc examples. alternatives simulate perturbations entries known irrelevant require deﬁning domain-speciﬁc perturbation logic; approach not. alternatives apply hard constraints completely remove elements identiﬁed miss fact entries imprecise even human-provided. thus preserve potentially misleading features softly penalize use. find-another-explanation discovering many although obtain annotations experts zaidan always extra information know right reasons. cases propose approach iteratively adapts discover multiple models accurate qualitatively different reasons; domain expert could examine determine right best reasons. speciﬁcally generate spectrum models different decision boundaries iteratively training models explaining training next model differ previous iterations fx|θi. words regularize input gradients largest magnitude previously. repeated iterations accuracy decreases explanations stop changing spanned space possible models. resulting models accurate different reasons; although know reasons best present domain expert inspection selection. also prioritize labeling reviewing examples ensemble disagrees. finally size ensemble provides rough measure dataset redundancy. empirical evaluation demonstrate explanation generation explanation constraints ﬁnd-another-explanation method color dataset three real-world datasets. cases used multilayer perceptron hidden layers size relu nonlinearities softmax output penalty trained network using adam autograd experiments used explanation penalty gave right answers right reasons loss terms similar magnitudes. details cross-validation included appendix cutoff value described section used display often chose tended preserve gradient components code experiments available https//github.com/dtak/rrr. color dataset created dataset images four possible colors. images fell classes independent decision rules model could implicitly learn whether four corner pixels color whether top-middle three pixels different colors. images class satisﬁed conditions images class satisﬁed neither. corner top-row pixels relevant expect faithful explanation accurate model highlight them. figure gradient lime explanations nine perceptron predictions color dataset. gradients plot dots pixels identiﬁed lime select features methods suggest model learns corner rule. finally figure shows ﬁnd-anotherexplanation technique sec. discover rule without given rules lead high accuracy test model performs better random guessing prevented using either lastly though directly relevant discussion interpretability explanation demonstrate potential explanations reduce amount data required training appendix real-world datasets demonstrate real-world cross-domain applicability test approach variants three familiar machine learning text image tabular datasets test input gradients alt.atheism soc.religion.christian subset newsgroups dataset used twohidden layer network architecture tf-idf vectorizer components gave accurate model iris-cancer concatenated examples classes iris dataset ﬁrst examples class breast cancer wisconsin dataset create composite dataset despite dataset’s small size network still obtains average test accuracy across random training-test splits. however modify test remove iris components average test accuracy falls higher variance suggesting model learns depend iris features suffers without them. verify explanations reveal dependency regularizing avoids decoy mnist baseline mnst dataset network obtains train test accuracy. however decoy mnist images gray swatches randomly chosen corners whose shades functions digits training random test. dataset model higher train accuracy much lower test accuracy indicating decoy rule misleads verify gradient lime explanations users detect issue explanation regularization lets overcome input gradients consistent sample-based methods lime faster. newsgroups input gradients less sparse identify words document similar weights. note input gradients also identify words outside document would affect prediction added. decoy mnist lime input gradients reveal model predicts rather color swatch corner. ﬁne-grained resolution input gradients sometimes better capture counterfactual figure implicit rule transitions increase number nonzero rows pairs points represent fraction large-magnitude gradient components corners top-middle test examples almost always note wide regime model learns hybrid rules. figure rule discovery using ﬁnd-another-explanation method cutoff note ﬁrst iterations produce explanations corresponding rules dataset third produces noisy explanations relevant pixels suggests methods effective explaining model predictions model learned corner rather top-middle rule consistently across random restarts. however train model nonzero able cause rule. figure shows model transitions rules vary number examples penalized result demonstrates model made learn multiple rules despite commonly reached standard gradient-based optimization methods. however depends knowing good setting case would still require annotating order examples dataset aspects debatable sample-based method lime often overly sparse many words identiﬁed signiﬁcant input gradients lime ignores. number features lime selects must passed parameter beforehand also lime samples ﬁxed number times. sufﬁciently long documents unlikely sample-based approaches mask every word even once meaning output becomes increasingly nondeterministic—an undesirable quality explanations. resolve issue could increase number samples would increase computational cost since model must evalutated least behavior extending adding lines outside digit either reinforce transform another digit would change predicted probability lime hand better captures fact main portion digit salient iris-cancer input gradients actually outperform lime. know accuracy difference iris features important model’s prediction lime identiﬁes single important feature breast cancer dataset example tabular contains continuously valued rather categorical features represent pathological case lime operates best selectively mask small number meaningful chunks inputs generate perturbed samples. truly continuous inputs surprising explanations based gradients perform best. figure iris-cancer features identiﬁed input gradients lime iris features highlighted red. input gradient explanations faithful model. note gradients change sign switching magnitudes input gradients different across examples provides information examples’ proximity decision boundary. sample local surrogate. input gradients hand require order model evaluation total generate explanation similar quality furthermore complexity based vector length document length. issue highlights inherent scalability advantages input gradients enjoy sample-based perturbation methods. table gradient lime runtimes explanation. note method uses different version lime; iriscancer colors lime tabular continuous quartile-discrete perturbation methods respectively decoy mnist uses lime image newsgroups uses lime text. code executed laptop input gradient calculations optimized performance runtimes meant provide sense scale. figure overcoming confounds using explanation constraints iris-cancer default input gradients tend large iris dimensions results lower accuracy iris removed test set. models trained iris dimensions almost exactly test accuracy without iris. figure training explanation constraints decoy mnist. accuracy swatch colorrandomized test unless model trained swatches case test accuracy matches architecture’s performance standard mnist dataset given annotations input gradient regularization ﬁnds solutions consistent domain knowledge. another advantage using explanation method closely related model incorporate explanations training process useful model faces ambiguities classify inputs. deliberately constructed decoy mnist iris-cancer datasets kind ambiguity rule works training generalize test. train network confounded datasets test accuracy better random guessing part decoy rules simple primary rules complex performance still signiﬁcantly worse baseline test decoy rules. penalizing explanations know incorrect using loss function deﬁned section able recover baseline test accuracy demonstrate figures annotations unavailable ﬁnd-anotherexplanation method discovers diverse classiﬁers. color dataset even almost every still beneﬁt explanation regularization however annotation never free cases either know right explanation cannot easily encode additionally interested exploring structure model dataset less supervised fashion. real-world datasets usually overdetermined ﬁnd-another-explanation discover shallower local minima would normally never explore. given enough models right different reasons hopefully least right right reasons. figure shows ﬁnd-another-explanation results three real-world datasets example explanations iteration model train test accuracy below. iris-cancer initial iteration model heavily relies iris features high train test accuracy subsequent iterations lower train higher test accuracy words spontaneously obtain generalizable model without predeﬁned alerting ﬁrst four features misleading. find-another-explanation also overcomes confounds decoy mnist needing iteration recover baseline accuracy. bumping high results erratic behavior. interestingly process remniscent distillation gradients become evenly intuitively distributed later iterations. many cases indicate probabilities certain digits increase brighten pixels along extend distinctive strokes decrease unrelated dark areas seems desirable. however last iteration start revert using decoy swatches cases. figure find-another-explanation results iris-cancer newsgroups decoy mnist real-world datasets often highly redundant allow diverse models similar accuracies. iris-cancer decoy mnist explanations accuracy results indicate overcome confounds iterations without prior knowledge encoded input gradients provide faithful information model’s rationale prediction trade interpretability efﬁciency. particular input features individually meaningful users input gradients difﬁcult interpret difﬁcult specify. additionally decision boundary capture idea salience well methods however necessarily faithful model easy incorporate loss function. input gradients ﬁrst-order linear approximations model; might call ﬁrst-order explanations. demonstrated training models input gradient penalties makes possible learn generalizable decision logic even dataset contains inherent ambiguities. input gradients consistent sample-based methods lime faster compute sometimes faithful model especially inputs continous. ﬁnd-another-explanation method present range qualitatively different classiﬁers detailed annotations available useful practice suspect model right right reasons certain regions. consistent results several diverse datasets show input gradients merit investigation scalable tools optimizable explanations; exist many options further advancements weighted annotations different penalty norms general speciﬁcations whether features positively negatively predictive speciﬁc classes speciﬁc inputs. finally right right reasons approach solving related problems e.g. maintaining robustness despite presence adversarial examples seeing whether explanations explanation constraints goals fairness accountability transparency machine learning constraining models avoid building ﬁnd-another-explanation results another promising direction include humans loop interactively guide models towards correct explanations. overall feel developing methods ensuring models right better reasons essential overcoming inherent obstacles generalization posed ambiguities real-world datasets. acknowledgements acknowledges support darpa wnf--- afosr fa--- acknowledges support oracle labs. authors thank arjumand masood gershman paul raccuglia mali akmanalp harvard dtak group many helpful discussions insights. references philip adler casey falk sorelle friedler gabriel rybeck carlos scheidegger brandon smith suresh venkatasubramanian. auditing black-box models obscuring features. arxiv preprint arxiv. sebastian bach alexander binder gr´egoire montavon frederick klauschen klaus-robert m¨uller wojciech samek. pixelwise explanations non-linear classiﬁer decisions layer-wise relevance propagation. plos david baehrens timon schroeter stefan harmeling motoaki kawanabe katja hansen klaus-robert ˜aˇzller. explain individual classiﬁcation decisions. journal machine learning research rich caruana johannes gehrke paul koch marc sturm noemie elhadad. intelligible models healthcare predicting pneumonia risk hospital -day readmission. proceedings sigkdd international conference knowledge discovery data mining pages cynthia dwork moritz hardt toniann pitassi omer reingold richard zemel. fairness awareness. proceedings innovations theoretical computer science conference pages gr´egoire montavon sebastian lapuschkin alexander binder wojciech samek klaus-robert m¨uller. explaining nonlinear classiﬁcation decisions deep taylor decomposition. pattern recognition nicolas papernot patrick mcdaniel somesh ananthram swami. distillation defense adversarial perturbations deep neural networks. security privacy ieee symposium pages ieee marco tulio ribeiro sameer singh carlos guestrin. trust you? explaining predictions classiﬁer. proceedings sigkdd international conference knowledge discovery data mining pages avanti shrikumar peyton greenside anna shcherbina anshul kundaje. black learning important features propagating activation differences. arxiv preprint arxiv. karen simonyan andrea vedaldi andrew zisserman. deep inside convolutional networks visualising image classiﬁcation models saliency maps. arxiv preprint arxiv. muhammad bilal zafar isabel valera manuel gomez rodriguez krishna gummadi. fairness beyond disparate treatment disparate impact learning classiﬁcation without disparate mistreatment. arxiv preprint arxiv. cross-validation regularization parameters selected maximize accuracy validation set. however training validation sets share misleading confounds validation accuracy good proxy test accuracy. instead recommend increasing explanation regularization strength cross-entropy right reasons terms roughly equal magnitudes intuitively balancing terms push optimization away cross-entropy minima violate explanation constraints speciﬁed towards ones correspond better reasons. increasing much makes cross-entropy term negligible. case model performs better random guessing. data. penalizing corners however reduces accuracy reach threshold corner pixels match ways top-middle pixels differ ways suggesting rule could inherently harder learn data positional explanations alone. figure cross-validating regime highest accuracy also initial cross-entropy loss terms similar magnitudes. exact equality required; order magnitude signiﬁcantly affect accuracy. pro-rule mask penalizes pixels except corners reach accuracy fewer examples penalizing top-middle pixels pixels except top-middle also consistently improves accuracy relative figure longer newsgroups examples. blue supports predicted label orange opposes opacityi |wi|/ max|w|. lime input gradients never disagree gradients provide fuller picture model’s behavior lime’s limits features samples", "year": 2017}