{"title": "Review Networks for Caption Generation", "tag": ["cs.LG", "cs.CL", "cs.CV"], "abstract": "We propose a novel extension of the encoder-decoder framework, called a review network. The review network is generic and can enhance any existing encoder- decoder model: in this paper, we consider RNN decoders with both CNN and RNN encoders. The review network performs a number of review steps with attention mechanism on the encoder hidden states, and outputs a thought vector after each review step; the thought vectors are used as the input of the attention mechanism in the decoder. We show that conventional encoder-decoders are a special case of our framework. Empirically, we show that our framework improves over state-of- the-art encoder-decoder systems on the tasks of image captioning and source code captioning.", "text": "propose novel extension encoder-decoder framework called review network. review network generic enhance existing encoderdecoder model paper consider decoders encoders. review network performs number review steps attention mechanism encoder hidden states outputs thought vector review step; thought vectors used input attention mechanism decoder. show conventional encoder-decoders special case framework. empirically show framework improves state-ofthe-art encoder-decoder systems tasks image captioning source code captioning. encoder-decoder framework learning transformation representation another. framework encoder network ﬁrst encodes input context vector decoder network decodes context vector generate output. encoder-decoder framework recently introduced sequence-to-sequence learning based recurrent neural networks applications machine translation input text sequence language output text sequence language. generally encoder-decoder framework restricted rnns text; e.g. encoders based convolutional neural networks used image captioning since often difﬁcult encode necessary information single context vector attentive encoder-decoder introduces attention mechanism encoder-decoder framework. attention mechanism modiﬁes encoder-decoder bottleneck conditioning generative process decoder encoder hidden states rather single context vector only. improvements attention mechanism shown various tasks including machine translation image captioning text summarization however remain important issues address attentive encoder-decoder models. first attention mechanism proceeds sequential manner thus lacks global modeling abilities. speciﬁcally generation step decoded token conditioned attention results current time step information future attention results example multiple objects image caption tokens generated beginning focuses ﬁrst objects unaware objects potentially suboptimal. second previous works show discriminative supervision beneﬁcial generative models clear integrate discriminative supervision encoder-decoder framework end-to-end manner. address questions propose novel architecture review network extends existing encoder-decoder models. review network performs given number review steps attention encoder hidden states outputs thought vector step thought vectors introduced capture global properties compact vector representation usable attention mechanism decoder. intuition behind review network review information encoded encoder produce vectors compact abstractive global representation original encoder hidden states. another role thought vectors focus multitask learning. instance thought vectors inputs secondary prediction task predicting discriminative signals addition objective generative model. paper explore multitask review network also explore variants weight tying. show conventional attentive encoder-decoders special case review networks indicates model strictly expressive attentive encoder-decoders. experiment different tasks image captioning source code captioning using cnns rnns encoders respectively. results show review network consistently improve performance attentive encoder-decoders datasets obtain state-of-the-art performance. encoder-decoder framework context sequence-to-sequence learning recently introduced learning transformation text sequences rnns used encoding decoding. encoder-decoders general refer models learn representation transformation using network components encoder decoder. besides rnns convolutional encoders developed address multi-modal tasks image captioning attention mechanisms later introduced encoder-decoder framework machine translation attention providing explanation explicit token-level alignment input output sequences contrast vanilla encoder-decoders attentive encoder-decoders condition decoder encoder’s hidden states. generation step decoder pays attention speciﬁc part encoder generates next token based current hidden state decoder attended hidden states encoder. attention mechanisms considerable success applications well including image captioning text summarization work also related memory networks memory networks take question embedding input perform multiple computational steps attention memory usually formed embeddings group sentences. dynamic memory networks extend memory networks model sequential memories memory networks mainly used context question answering; review network hand generic architecture integrated existing encoder-decoder models. moreover review network learns thought vectors using multiple review steps facts provided input memory networks. another difference review network outputs sequence thought vectors memory networks last hidden state generate answer. presented processor unit runs encoder multiple times model mainly focuses handling non-sequential data approach differs many ways model proposed performs number sub-steps inside standard recurrent step decoder generates output attention thought vectors. given input representation output representation goal learn function mapping example image captioning aims learn mapping image caption notation simplicity denote tensor sequence tensors. example d-tensor represents image channels image captioning sequence d-tensors machine translation denotes one-of-k embedding t-th word input sequence length contrast conventional encoder-decoder models model consists three components encoder reviewer decoder. comparison architectures shown figure describe three components detail. encoder encoder encodes input context vector hidden states {ht}t. discuss types encoders encoders encoders. encoder length input sequence. encoder processes input sequence sequentially. time step encoder updates hidden state work implement using lstm unit. context vector deﬁned ﬁnal hidden state htx. cell state hidden state ﬁrst lstm unit initialized zero. encoder take widely-used architecture—vggnet example describe cnns encoders. given vggnet output last fully connected layer context vector columns convolutional output conv hidden states conv. case hyperparameter speciﬁes number review steps. intuition behind reviewer module review information encoded encoder learn thought vectors compact abstractive global representation original encoder hidden states. reviewer performs review steps encoder hidden states outputs thought vector step. speciﬁcally modiﬁed lstm unit attention mechanism review step study variants attentive input reviewers attentive output reviewers. attentive input reviewer inspired visual attention commonly used images; attentive output reviewer inspired attention text commonly used sequential tokens. attentive input reviewer review step attentive input reviewer ﬁrst applies attention mechanism attention result input lstm unit function determines weight i-th hidden state. implemented product multi-layer perceptron takes concatenation input attentive output reviewer contrast attentive input reviewer attentive output reviewer uses zero vector input lstm unit thought vector computed weighted attention results output lstm unit speciﬁcally attentive output reviewer formulated attention mechanism follows deﬁnition denotes zero vector model parameter matrix lstm unit step note performing attention unit commonly used sequence-to-sequence learning apply linear transformation matrix since dimensions weight tying study variants weight tying reviewer module. denote parameters unit ﬁrst variant follows common setting rnns weights shared among units; i.e. wtr. also observe reviewer unit sequential input experiment second variant weights untied; i.e. wj∀i cell state hidden state ﬁrst unit hidden states passed reviewer units cases weight tying. decoder {ft}t thought vectors output reviewer. decoder formulated lstm network attention thought vectors hidden state t-th lstm unit decoder. decoder formulated follows st−) denotes concatenation vectors denotes decoder lstm softmaxy probability word given softmax layer t-th decoded token word embedding attention mechanism follows deﬁnition initial cell state hidden state decoder lstm review vector model parameter matrix. conventional encoder-decoders supervision provided generative manner; i.e. model aims maximize conditional probability generating sequential output however discriminative supervision shown useful model guided predict discriminative objectives word occurrences output argue review network provides natural incorporating discriminative supervision model. take word occurrence prediction example describe incorporate discriminative supervision. shown blue components figure ﬁrst apply linear layer thought vector compute score word review step. apply max-pooling layer review units extract salient signal word multi-label margin loss discriminative supervision. score word pooling layer words occur discriminative loss written training loss single training instance deﬁned weighted negative conditional likelihood discriminative loss. length output sequence loss written deﬁnition softmaxy follows formulation follows constant weighting factor. adopt adaptive stochastic gradient descent train model end-to-end manner. loss training batch averaged instances batch. show model reduced conventional encoder-decoders special case. attentive encoder-decoders decoder takes context vector encoder hidden states {ht}t input review network input decoder instead review vector thought vectors {ft}t. show model reduced attentive encoder-decoders need construct case since reduced speciﬁc setting deﬁne reviewer unit identity mapping satisﬁes deﬁnition attentive input reviewer attentive output reviewer untied weights. setting ft∀t i.e. thus model reduced attentive encoder-decoders special case. similarly show model reduced vanilla encoder-decoders constructing case therefore model expressive encoder-decoders. though construction practice number review steps much smaller compared since review network learn compact effective representation. experiment datasets different tasks image captioning source code captioning. since tasks quite different test robustness generalizability model. evaluate model mscoco benchmark dataset image captioning. dataset contains images least captions image. ofﬂine evaluation data split reserve images development test respectively rest training. models evaluated using ofﬁcial mscoco evaluation scripts. report three widely used automatic evaluation metrics bleu- meteor cider. remove non-alphabetic characters captions transform letters lowercase tokenize captions using white space. replace words occurring less times unknown token <unk> obtain vocabulary words. truncate captions longer tokens. number review steps weighting factor dimension word embeddings learning rate dimension lstm hidden states hyperparameters tuned development set. also early stopping strategies prevent overﬁtting. speciﬁcally stop training procedure bleu- score development reaches maximum. hidden layer size deﬁne function attention mechanism attentive input reviewer table comparison model variants mscoco dataset. results obtained single model using vggnet. scores brackets without beam search. rnn-like tied weights review network unless otherwise indicated. disc means discriminative supervision. table comparison state-of-the-art systems mscoco evaluation server. indicates ensemble models. feat. means using task-speciﬁc features attributes. fine. means using ﬁne-tuning. experiments consistent visual attention models beam search beam size decoding. guide model predict words occurring caption discriminative supervision without introducing extra information. parameters encoders training. compare model encoder-decoders study effectiveness review network. also compare different variants model evaluate effects different weight tying strategies discriminative supervision. results reported table results table obtained using vggnet encoders described section table review network improve performance conventional attentive encoder-decoders consistently three metrics. also observe adding discriminative supervision boost performance demonstrates effectiveness incorporating discriminative supervision end-to-end manner. untying weights reviewer units improve performance. conjecture models untied weights expressive shared-weight models since unit parametric function compute thought vector. addition table experiment shows applying discriminative supervision attentive encoder-decoders improve cider score without beam search. experiments development performances slightly worse also experimented development gives best performance. also compare model state-of-the-art systems mscoco evaluation server table submission uses inception-v encoder ensemble three identical models different random initialization. take output last convolutional layer encoder states. table among state-of-the-art published systems review network achieves best performance three four metrics close performance semantic attention bleu- score. google system employs several tricks ﬁne-tuning scheduled sampling takes weeks train; semantic attention system requires hand-engineering task-speciﬁc features/attributes. unlike methods approach review network generic end-to-end encoder-decoder model trained within hours titan gpu. figure corresponds test image ﬁrst original image caption output model following three images visualized attention weights ﬁrst three reviewer units. also list top- words highest scores unit. colors indicate semantically similar words. better understand review network visualize attention weights review network figure visualization based review network untied weights discriminative supervision. also list top- words highest scores reviewer unit. words highest scores uncover reasoning procedure underlying review network. example ﬁrst image ﬁrst reviewer focuses motion giraffe tree near second reviewer analyzes relative position giraffe tree third reviewer looks picture infers scene based recognizing fences enclosures. information stored thought vectors decoded natural language decoder. different attentive encoder-decoders attend single object time generation clearly seen figure review network captures global signals usually combining multiple objects thought including objects ﬁnally shown caption thoughts sometimes abstractive motion relative position quantity scene also order review restricted order natural language. table comparison model variants habeascorpus code captioning dataset. bidir indicates using bidirectional encoders refers log-likelihood cs-k refers top-k character savings. dataset source code captioning habeascorpus habeascorpus collects nine popular opensource java code repositories apache lucene. dataset contains java source code ﬁles source code tokens comment word tokens. randomly sample ﬁles test development rest training. development early stopping hyperparameter tuning. evaluation follows previous works source code language modeling captioning report log-likelihood generating actual code captions based learned models. also evaluate approaches perspective code comment completion compute percentage characters saved applying models predict next token. speciﬁcally metric top-k character savings minimum number preﬁx characters needed ﬁltered actual word ranks among top-k based given model. length actual word. number saved characters compute average percentage saved characters comment obtain metric cs-k. follow tokenization used transform camel case identiﬁers multiple separate words remove non-alphabetic characters. truncate code sequences comment sequences longer tokens. encoder attentive output reviewer tied weights. number review steps dimension word embeddings dimension lstm hidden states report log-likelihood top-k character savings different model variants table baseline model language model lstm decoder whose output sensitive input code sequence. preliminary experiment showed lstm decoder signiﬁcantly outperforms ngram models used lstm decoder baseline comparison. also compare different variants encoder-decoders including incorporating bidirectional encoders attention mechanism. seen table bidirectional encoders attention mechanism improve vanilla encoder-decoders. review network outperforms attentive encoder-decoders consistently metrics indicates review network effective learning useful representation. present novel architecture review network improve encoder-decoder learning framework. review network performs multiple review steps attention encoder hidden states computes thought vectors summarize global information input. empirically show consistent improvement conventional encoder-decoders tasks image captioning source code captioning. future interesting apply model tasks modeled encoder-decoder framework machine translation text summarization. acknowledgements work funded grants ccf- iis- google disney research grant adelaide grant fa-c-.", "year": 2016}