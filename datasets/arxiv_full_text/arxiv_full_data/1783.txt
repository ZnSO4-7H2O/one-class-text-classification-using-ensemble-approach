{"title": "Semi-supervised Learning with Sparse Autoencoders in Phone  Classification", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "We propose the application of a semi-supervised learning method to improve the performance of acoustic modelling for automatic speech recognition based on deep neural net- works. As opposed to unsupervised initialisation followed by supervised fine tuning, our method takes advantage of both unlabelled and labelled data simultaneously through mini- batch stochastic gradient descent. We tested the method with varying proportions of labelled vs unlabelled observations in frame-based phoneme classification on the TIMIT database. Our experiments show that the method outperforms standard supervised training for an equal amount of labelled data and provides competitive error rates compared to state-of-the-art graph-based semi-supervised learning techniques.", "text": "propose application semi-supervised learning method improve performance acoustic modelling automatic speech recognition based deep neural networks. opposed unsupervised initialisation followed supervised tuning method takes advantage unlabelled labelled data simultaneously minibatch stochastic gradient descent. tested method varying proportions labelled unlabelled observations frame-based phoneme classiﬁcation timit database. experiments show method outperforms standard supervised training equal amount labelled data provides competitive error rates compared state-of-the-art graph-based semi-supervised learning techniques. deep learning revolutionised research automatic speech recognition well many ﬁelds application machine learning extensive reviews). despite recent signiﬁcant improvements made word error rates experiments reported large fully-labelled data sets. initial paradigm unsupervised initialisation network weights followed supervised ﬁne-tuning parameters abandoned favour fully supervised methods efﬁcient models however under-resources languages large amounts labelled data available fully supervised learning techniques still relevant. unsupervised learning limit ﬁnding initial weights consequently data representations speciﬁcally optimised problem hand. example would representations speech speaker recognition orthogonal objectives. alternative learning paradigm recently applied ﬁeld computer vision well semi-supervised learning labelled unlabelled observations used jointly semi-supervised learning using neural network also explored means self-training scheme. self-training scheme however based heuristics prone reinforcing poor predictions. work done ﬁrst attempts using semi-supervised learning asr. authors propose number algorithms employing graph based learning obtain better wers baseline neural network. authors extend initial results frame based phoneme classiﬁcation large vocabulary asr. graph based learning however computationally intensive addition point data requires reevaluation graph laplacian. ranzato szummer propose semi-supervised learning method based linearly combining supervised cost function deep classiﬁer unsupervised cost function deep autoencoder minimising combination costs mini-batch stochastic gradient descent standard backpropagation. authors apply method ﬁnding representations text documents information retrieval classiﬁcation. propose similar approach frame-based phoneme recognition asr. although objective function proposed different number ways. firstly instead compact lower dimensional encoding used employ sparse encoding. secondly instead stacking number encoders decoders classiﬁers deep architecture single layer model. motivated work authors analyse effect several model parameters unsupervised learning neural networks computer vision benchmark data sets cifar- norb. conclude state-of-the-art results achieved single layer networks regardless learning method optimal model setup chosen. unsupervised path model cost function second degree norm difference original input reconstructed input input output dimensions. found adding noise original input process called ’corruption’ dimensions input vector randomly picked zero helps network learn even better representation described case encoded vector deﬁned decoder layer produce ﬁnal output form. input single datapoint reconstructed output single datapoint. cost function regular auto-encoder. practice compute cost averaged batch points optimisation called mini-batch stochastic gradient descent input datapoint accompanied label classiﬁer part layer updated loss function simply reduces model iteratively applied several layers. however experiments single layer feature representation. important note update encoder weights dependent decoder weights classiﬁer weights delta propagated backpropagation algorithm linear combination deltas calculated parts. used adaptive learning rate scheme linear decay learning rate decays linearly certain number epochs. fig. flow chart cost calculation single layer network. three components considered encoder decoder classiﬁer. loss weighted crossentropy reconstruction loss several layers stacked together encoder/decoder pairs retained training. paper organised follows section describes method. section reports details experimental set-up. section reports results ﬁnally section concludes paper. architecture single layer model depicted figure remove bottom path equivalent autoencoder encoding weights logistic layer subsequent decoding weights nonlinearity. model representation obtained encoder also used classiﬁer parallel regular decoder. combining unsupervised supervised cost functions unlabelled labelled data efﬁcient order obtain good representations input well good prediction discriminative abilities network. model trained optimising combined cost reconstruction error classiﬁcation errors given respectively autoencoder classiﬁcation network. combination linear deﬁned hyper-parameter controlling proportion costs objective function. optimised validation independent training set. optimal value depends general proportion labelled versus unlabelled examples training also shown section size hidden representation larger input size experiments. consequently promote sparsity feature representation. autoencoders encoding decoding weights often tied means decoder weight matrix transpose encoder weight matrix reduces amount free parameters available also expressive power model. experiments instead optimise independently. makes model expressive cost computational overhead possible delayed convergence. another aspect increases computational cost model sparse autoencoders opposed autoencoders bottleneck architecture fewer nodes hidden layer consequently reduced memory computational complexity. however computational cost linear number training samples thus efﬁcient graph based semi-supervised learning algorithms cubic complexity performed experiments standard timit data frame-based phoneme classiﬁcation. used standard core test sentences development/validation sentences. training sentences. similarly part standard procedure experiments timit glottal stop segments excluded. data created help standard recipes given input network created ﬁrst extracting -dimensional feature vectors frame. feature vector made mfcc coefﬁcients computed rate overlapping window energy coefﬁcient deltas delta-deltas. time step features obtained frames left frames right concatenated together form ﬁnal vector dimension coefﬁcients speakerdependent mean variance normalisation also applied. total number frames training validation frames total test frames. counts line experiments training used standard phone phones collapsed phones evaluation means output layer nodes time evaluation phonemes reduced phonemes. procedure also used although common senones target labels classiﬁcation network output classiﬁcation network based phonemes order able compare studies semi-supervised learning speech. fig. frame-based phoneme recognition accuracy versus percentage labelled training examples timit database. neural network trained supervised backpropagation. sssae method. validation test accuracy rates shown. table corresponding numerical values. labelled portion data set. percentage labelled frames training varied intermediate steps conditions optimised hyper-parameter validation set. accuracy results reported optimal value finally number nodes encoder network also optimised validation resulting optimal value nodes. baseline compare results obtained method obtained similar neural network trained supervised backpropagation amount labelled examples. also compare results obtained literature semi-supervised learning. table figure show frame-level classiﬁcation accuracy rates neural network trained supervised proposed single layer semi-supervised sparse auto-encoder varying percentage labelled data. validation accuracy test accuracy reported. hyper-parameters neural network learning rate tuned using validation also shown table results frame-based phoneme classiﬁcation validation test sets timit material. method compared neural network trained supervised backpropagation amount labelled data. total number training frames value optimised validation proportion labelled examples varied. table figure show method always performs better supervised baseline much absolute improvement. expected advantage decreases proportion labelled training examples increased. validation test errors always close indicating parameters optimised validation generalise well test set. expected optimal value strongly dependent proportion labelled material. higher proportion weight algorithm gives classiﬁcation error compared unsupervised reconstruction error. table compare performance system results obtained graph based semi-supervised learning methods published labelled data. observe system performs better techniques mentioned except prior regularised measure propagation algorithm. reported results frame based phoneme classiﬁcation timit database using semi-supervised learning based sparse autoencoders. observe method outperforms neural network trained supervised backpropagation amount labelled training data experimental conditions. results also outperform many semi-supervised learning methods proposed literature similar task exception prior-regularised measure propagation method. expected advantage using method decreases proportion labelled training observations increased. however argue realistic situations always abundance unlabelled data compared data carefully annotated. consequence becomes important investigate model percentage labelled data low. spite promising results order draw general conclusions would need test method word recognition task particular large-vocabulary asr. however improvements frame-level phoneme classiﬁcation incentive continue work direction. possible improvements obtained using alternative features instead mfccs used allow comparison previous results literature. also test results vary depth model stacking several blocks autoencoders/classiﬁers. bengio lamblin popovici larochelle greedy layer-wise training deep networks advances neural information processing systems sch¨olkopf platt hoffman eds. press cambridge william fisher george doddington kathleen goudie-marshall darpa speech recognition research database speciﬁcations status proceedings darpa workshop speech recognition daniel povey arnab ghoshal gilles boulianne burget kaldi speech recognition toolkit ieee workshop automatic speech recognition understanding. dec. ieee signal processing society ieee catalog cfpsrw-usb. abdel rahman mohamed tara sainath george dahl bhuvana ramabhadran geoffrey hinton michael picheny deep belief networks using discriminative features phone recognition. icassp. ieee. dumitru erhan yoshua bengio aaron courville pierre-antoine mansagol pascal vincent unsupervised pre-training help deep learning? journal machine learning research vol. coates a.y. analysis singlelayer networks unsupervised feature learning proceedings fourteenth international conference artiﬁcial intelligence statistics geoffrey gordon david dunson miroslav dud´ık eds. vol. jmlr workshop conference proceedings jmlr w&cp. yuzong katrin kirchhoff graph-based semisupervised acoustic modeling dnn-based speech recognition proceedings ieee spoken language technology workshop graph-based semisupervised learning acoustic modeling automatic speech recognition ieee/acm transactions audio speech language processing vol. karel vesel´y mirko hannemann luk´aˇs burget semi-supervised training deep neural networks. proceedings ieee conference automatic speech recognition understanding marc’aurelio ranzato martin szummer semisupervised learning compact document representations deep networks. icml william cohen andrew mccallum roweis eds.", "year": 2016}