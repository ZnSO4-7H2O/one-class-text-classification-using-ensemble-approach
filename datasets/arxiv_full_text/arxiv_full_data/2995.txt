{"title": "UberNet: Training a `Universal' Convolutional Neural Network for Low-,  Mid-, and High-Level Vision using Diverse Datasets and Limited Memory", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "In this work we introduce a convolutional neural network (CNN) that jointly handles low-, mid-, and high-level vision tasks in a unified architecture that is trained end-to-end. Such a universal network can act like a `swiss knife' for vision tasks; we call this architecture an UberNet to indicate its overarching nature.  We address two main technical challenges that emerge when broadening up the range of tasks handled by a single CNN: (i) training a deep architecture while relying on diverse training sets and (ii) training many (potentially unlimited) tasks with a limited memory budget. Properly addressing these two problems allows us to train accurate predictors for a host of tasks, without compromising accuracy.  Through these advances we train in an end-to-end manner a CNN that simultaneously addresses (a) boundary detection (b) normal estimation (c) saliency estimation (d) semantic segmentation (e) human part segmentation (f) semantic boundary detection, (g) region proposal generation and object detection. We obtain competitive performance while jointly addressing all of these tasks in 0.7 seconds per frame on a single GPU. A demonstration of this system can be found at http://cvn.ecp.fr/ubernet/.", "text": "work introduce convolutional neural network jointly handles low- mid- high-level vision tasks uniﬁed architecture trained end-to-end. universal network like ‘swiss knife’ vision tasks; call architecture ubernet indicate overarching nature. address main technical challenges emerge broadening range tasks handled single training deep architecture relying diverse training sets training many tasks limited memory budget. properly addressing problems allows train accurate predictors host tasks without compromising accuracy. advances train end-to-end manner simultaneously addresses boundary detection normal estimation saliency estimation semantic segmentation human part segmentation semantic boundary detection region proposal generation object detection. obtain competitive performance jointly addressing tasks seconds frame single gpu. demonstration system found cvn.ecp.fr/ubernet/. computer vision involves host tasks boundary detection semantic segmentation surface estimation object detection image classiﬁcation name few. convolutional neural networks method choice text recognition decades recently shown successful handling effectively most vision tasks. considering works apply single static image indicatively list successes cnns superresolution colorization boundary detection symmetry detection interest point detection image descriptors surface table single training accommodate vision tasks several datasets contain annotations multiple tasks even extended e.g. number task grows becomes impossible dataset all. normal estimation depth estimation intrinsic image decomposition shadow detection texture classiﬁcation material classiﬁcation saliency estimation semantic segmentation region proposal generation instance segmentation pose estimation part segmentation landmark localization well large body works around object detection image classiﬁcation e.g. works rely ﬁnetuning common pretrained network others indicates broad potential cnns. however works ends taskspeciﬁc potentially mildly different architecture. wanted perform tasks would need train test separate networks. understanding joint treatment multiple problems result simpler faster better systems also catalyst reaching ﬁelds. expect allin-one swiss knife architectures become indispensable general involving instance robots able recognize scene recognize objects navigate towards them manipulate them. furthermore single visual module address multitude tasks make possible explore methods improve performance them rather developping narrow problem-speciﬁc techniques. apart simplicity efﬁciency problem also motivated arguing training network accomplish multiple tasks leaves smaller space ‘blindspots’ effectively providing complete speciﬁcation network duties. finally particular motivation research interest studying synergy different visual tasks work understood ﬁrst step direction. problem using single network solve multiple tasks repeatedly pursued context deep learning computer vision. used joint localization detection classiﬁcation propose network jointly solves surface normal estimation depth estimation semantic segmentation train system joint detection pose estimation region proposal generation; study effects sharing information across networks trained complementary tasks recently propose introduction intertask connections improve performance task synergy propose architecture encompassing host face-related tasks. inspired advances work introduce techniques allow expand range tasks handled single deep network thereby make possible train single network multiple diverse tasks without sacriﬁcing accuracy. ﬁrst contribution consists exploring trained diverse datasets. problem inevitably shows breadth since single dataset currently contains ground-truth possible tasks. shown table. high-level annotation often missing datasets used low-level tasks vice versa. consider instance network supposed predicting human landmarks surface normals dataset image comes annotations tasks rather disjoint datasets pascal pose estimation dataset keypoints) providing every image annotations two. order handle challenge introduce sec. loss function relies ground truth available training sample shunning losses tasks ground truth available sample. combine loss function stochastic gradient descent updating network parameter observed sufﬁcient number training samples related parameter. results asynchronous variant backpropagation allows train end-to-end manner. particular pool features layers performance. conv conv conv conv conv vgg- network show fig. skip-layer normalization modifying slightly batch normalization prior forming inner product intermediate layers; alleviates need learning rates case exception last layer already trained appropriate argument linear classiﬁer therefore seemed better without normalization. cumulative task-speciﬁc operations scaling many tasks requires keeping task-speciﬁc memory computation budget therefore choose process outputs skip-pooling task-speciﬁc layers perform linear operations. particular denote neuron activations layers used obtain score given image position output task linear function rather explicitly allocating memory vector formed contactenating intermediate activations forming matrix product instead compute tkfk intermediate results layer yields result acts like low-memory online accumulation scores across skip layers. fusion layers fusion layers denoted circular connections fig. observe instead simply adding scores accelerate training concatenating score maps learning linear function operates concatenated score maps originally done scheme clearly still learning linear function number free parameters decomposition intuitively understood form preconditioning seems effective. also back-propagating intermediate layers typically also results better performance. note simplicity assume correspondence across layer positions; handled network appropriate interpolation layers diagram understood included circular nodes. atrous convolution also convolution holes allows control spatial resolution output layer. particular trous convolution obtain output stride gives moderate boost tasks boundary detection semantic segmentation. ticular limited memory available modern graphics processing units number tasks increases memory demand naively implemented back-propagation algorithm increase linearly number tasks factor proportional memory requested task-speciﬁc network layers. instead build recent developments learning deep architectures shown possible efﬁciently train deep memory complexity sublinear number layers. develop variant customized multi-task architecture allows perform end-to-end network training practically unlimited number tasks since memory complexity independent number tasks. current architecture systematically evaluated following tasks boundary detection normal estimation saliency estimation semantic segmentation semantic part segmentation semantic boundary detection proposal generation object detection. present system operates seconds frame delivers results competititve state-of-the-art tasks. introduce architecture network shown fig. aiming simplicity introduce minimal number additional task-speciﬁc layers common trunk based network. clearly always include additional layers parameters e.g. u-net type architectures dense post-processing bilateral ﬁltertype smoothing well general structured prediction cnn-based pairwise terms leave future work. starting point using standard ‘fully’ convolutional network namely provides ﬁeld decision variables rather single classiﬁciation output; used accomplish dense labelling regression task boundary detection normal estimation semantic segmentation. describe modiﬁcations basic architecture. skip layers ﬁrst deviation standard architecture skip layers combine top-layer neurons activations intermediate neurons form network output. tasks boundary detection clearly proﬁt smaller degree spatial abstraction lower-level neurons even high-level tasks semantic segmentation shown skip layers improve figure ubernet architecture jointly solving multiple labelling tasks image pyramid formed successive downsampling operations image processed tied weights; skip layer pooling different network layers network combined batch normalization provide features used form taskspeciﬁc responses combined across network layers resolutions form task-speciﬁc decisions. loss functions individual-scale fused responses used train task responses task-speciﬁc manner. simplicity omit interpolation normalization object detection layers details provided text. pyramid pass scaled versions image cnns shared weights. allows deal scale variability image patterns. even though max-fusion scheme shown yield higher accuracy sum-fusion understanding particular case semantic segmentation large score scale sufﬁces assign object label pixel. case boundaries score determined accumulation evidence multiple scales normals maximization scales normal vector entries make sense. therefore concatenation scores followed linear operation case fusing skip-layers described above leave exploration scale-aware processing future. described network’s architecture turn parameter estimation. objective train end-to-end manner vgg-based trunk delivers features tasks weights taskspeciﬁc layers. described introduction main challenge face diversity tasks wish cover. order handle diversity available datasets needs handle missing ground truth data training. certain recent works manage impute missing data em-type approach exploiting domain-speciﬁc knowledge e.g. requesting ﬁxed percentage pixels contained bounding object obtain label object. however possible arbitrary tasks e.g. normal estimation. instead propose adapt loss function information sample zero loss tasks ground-truth. idea straightforward describe care needs taken optimizing resulting loss backpropagation ensure estimates parameter gradients accumulate evidence sufﬁcient number training samples. index tasks; denotes weights common trunk task-speciﬁc weights; hyperparameter determines relative imporw∗ regularization tance task relevant network weights taskspeciﬁc loss function. index training samples denote task-speciﬁc network prediction ground truth i-th example respectively task-speciﬁc network parameters indicate whether example comes ground-truth task arbitrary value without affecting loss i.e. need impute ground truth. network deﬁnition accounted end-to-end training. pyramid highest resolution image similar smallest image dimension pixels largest dimension exceed task-speciﬁc deviations choices separately validated individual tasks integrated common architecture shown fig. still however task-speciﬁc deviations. exception uniform architecture outlined detection follow work learn convolutional region proposal network followed fully-connected subnetwork classiﬁes region proposals labels recent advances however make exception unnecessary. output task-speciﬁc streams penalized loss function adapted task hand. region labelling tasks object detection softmax loss function common recent works semantic segmentation object detection regression tasks smooth loss normal estimation apply normalization prior penalizing loss since surface normals unit-norm vectors. tasks want estimate thin structures mil-based loss function introduced order accommodate imprecision placement boundary annotations. tasks also class imbalance problem many negatives positives; mitigate using weighted cross-entropy loss attribute weight positives negatives. furthermore low-level tasks boundary detection normal estimation saliency estimation well semantic boundary detection spatial resolution scores equal image allows train loss function higher degree spatial accuracy allows accurately localize small structures. region labelling tasks semantic segmentation human part segmentation realized really need level spatial accuracy instead train score maps employ lower spatial resolution using downsampling factor respect original image dimensions. results -fold reduction task-speciﬁc computation memory demands. table pseudocode asynchronous stochastic gradient descent algorithm back-propagation training. update task-speciﬁc parameter observing sufﬁcient many training samples pertain task. often lead erratic behaviour originally handled increasing minibatch size quite large numbers even though mitigates problem partially ﬁrstly highly inefﬁcient timewise also scale solving tasks simultaneously. instead brute-force approach propose modiﬁed variant backpropagation naturally handles problem updating parameters task sufﬁciently many relevant images observed. pseudocode presenting scheme contrast standard scheme provided table. particular longer minibatch’ rather treat images streaming mode keeping counter task seen rather outer loops number minibatches outer loops equalling number images treated original scheme. whenever process training sample contains ground truth task increment task counter current gradient cumulative gradient sum. table pseudocode standard synchronous stochastic gradient descent algorithm back-propagation training. update parameters time observing ﬁxed number samples. gradient descent since want train network end-to-end manner tasks consider training union different training sets containing pairs images ground-truth distinct tasks. images sampled uniformly random; common practice multiple epochs within epoch sample without replacement. weight decay term results regularization denotes gradient loss task respect parameter vector difference update terms parameters common trunk affecting tasks accumulate gradients tasks task-speciﬁc parameters affected subset images figure vanilla backpropagation single task; memory lookup operations indicated black arrows storage operations indicated orange blue arrows forward backward pass respectively. forward pass layer stores activation signals bottom boxes. backward pass activation signals combined gradient signals computed recursively starting loss layer. task counter exceeds threshold update task parameters reset counter cumulative gradient zero. clearly common parameters updated regularily since counter incremented every single training image. however case tasks affected subset training images. results asynchronous variant backpropagation sense parameter updated time instance independent others. note apart implementing necessary book-keeping scheme requires additional memory computation. also clear ‘asynchronous’ term relates manner parameters different tasks updated rather computation itself implementation single-node. also note according pseudocode allow different ‘effective batchsizes’ observed useful training. particular detection tasks reported batchsize sufﬁces dense labelling tasks semantic segmentation batchsize even often used training effective batchsize detection task-speciﬁc parameters shared features reasoning behind using larger batch size shared features want updates absorm information larger number images containing multiple tasks task-speciﬁc idiosyncracies cancel out. becomes likely average gradient serve tasks avoid ‘moving target’ problem every task quickly changes shared representation tasks making optimization harder. itbp} indicates subsequence samples contain ground-truth task realize ﬁrst estimate expected typically smaller magnitude second since several terms averaged equal zero. implies somehow modiﬁed original cost function since stochastic gradient estimates match. however effect absorbed hyperparameters estimates expected magnitude consider algorithms optimizing quantity. figure low-memory backpropagation single task ﬁrst store subset activations memory serve ‘anchor’ points running backpropagation smaller networks. reduces number layer activations/gradients simultaneously stored memory. figure vanilla backpropagation multi-task training naive implementation memory complexity depth common trunk depth task-speciﬁc branches number tasks. turn handling memory limitations turns major problem training network many tasks. order handle problems build recent advances memory-efﬁcient backpropagation deep networks adapt task multitask learning. start describing basic idea behind algorithm paving presentation extension multi-task learning. baseline implementation back-propagation algorithm maintains intermediate layer activations computed forward pass. illustrated fig. during backward pass layer combines stored activations back-propagated gradients coming layer above ﬁnds gradients parameters back-propagates gradients layer below. strategy achieves computational efﬁciency reusing computed activation signals memorydemanding since requires storing intermediate activations. popular caffe library memory also allocated gradient signals since priori could feed multiple layers network. consider simplicity every layer requires bytes memory activations gradient signals network total layers memory complexity naive implementation would become prohibitive large values memory-efﬁcient alternative described ﬁrst step shown fig. shown fig. perform ﬁrst forward pass network store activations subset layers network depth activations stored lying layers apart intermediate activations shown grey discarded soon used. ﬁrst stage accomplished times backpropagation sub-networks length shown fig. stored activations help start backpropagation deeper layer network acting like anchor points computation subnetwork requires activation lowest level gradient signal highest layer. seen scheme total complexity reduced since retain activation signals step perform back-propagation subnetwork length naive application algorithm presented would result reduction memory complexity n√lc however realize branching point different tasks computations practially decoupled taskspeciﬁc branch works effectively returns gradient signal layer gradient signals accumulated tasks since cost additive task-speciﬁc losses. allowed load increasing number tasks network without encountering memory issues. using nvidia card able three-layer pyramid largest image size using skip-layer connections network layers pyramid levels tasks seven tasks. largest dimension would possible without memory-efﬁcient option present number tasks would would decrease tasks used. apart reducing memory demands notice also reduce computation time performing lazy evaluation gradient signals accumulated branching point. particular training sample contain ground-truth certain tasks contribute gradient term common trunk; computation task-speciﬁc branches avoided instance contain ground-truth task. results substantial acceleration training would essential scale training even tasks. experimental evaluation objectives ﬁrst show generic ubernet architecture introduced sec. successfully addresses broad range tasks. order examine compare primarily results obtained methods rely network recent works e.g. detection semantic segmentation shown improvements deeper resnets consider choice network sense orthogonal goal section. second objective explore incorporating tasks affects performance individual tasks. order remove erroneous sources variation common initialization singlemultitask networks obtained pretraining network joint semantic segmentation object detection detailed sec. furthermore multi-task network trained union datasets corresponding multiple tasks solving. used particular proportion images dataset moderately favor high-level tasks detailed sec. even though using larger task-speciﬁc dataset boost performance particular task single task networks trained subset multi-task dataset pertains particular task. sacriﬁcing performance respect competing methods ensures loss term pertaining task unaffected singleversus multitask training facilitates comparison. experimental settings optimization single-task experiments momentum minibatch size exception detection minibatch size following multi-task experiments asynchronous algorithm effective minibatch sizes detection-related parameters task-speciﬁc parameters shared features justiﬁed sec. exception initialization experiment described right below always iterations starting learning rate decrease learning rate factor iterations. optimization schemes explored future version work. initialization labelling detection network common initialization experiments requires disposal parameters convolutional labelling tasks region-based detection task. could imagenet-pretrained network this exploiting pretraining ms-coco shown yield boosts performance e.g. leaving joint pretraining ms-coco future version work take shortcut instead form ‘frankentwo-task network common convolutional trunk ﬁfth convolutional layer detection branch combining spp-pooling layer followed fully-connected layers fully-convolutional branch used semantic segmentation. fully-connected branches initialized parameters respective pretrained networks coco-d coco-s initialize parameters common layers coco-d parameters. ﬁnetune network iterations voc++ stands union pascal trainval pascal trainval sets; start learning rate decrease iterations. datasets summary datasets used experiments provided table. images dataset correspond dataset augmentation trainval images additional rotations. numbers effectively doubled ﬂipping-based dataset augmentation voc-related datasets used twice amounts placing higher emphasis high-level tasks. note voc’ validation used evaluation human part segmentation semantic boundary detection saliency estimation tasks. means general report numbers distinct networks validation included training based report results detection semantic segmentation; validation excluded training gives results human parts semantic boundaries saliency. tialization described above start ms-coco pretrained network ﬁnetune dataset test test dataset. differences minimal image side rather maximal side rather comply restriction dimensions convolution holes followed appropriately modiﬁed roi-pooling layers effectively identical results adding holes network seem help following measure performance network obtained training joint segmentation detection task mentioned sec. serve starting point ensuing experiments. ﬁnetuning voc++ observe actually small boost performance quite promising since likely telling additional supervision signal semantic segmentation helped detection subnetwork learn something better detection. however increasing number tasks performance drops still comparable strong baseline sec. necessarily obvious difference choice task weight parameters adversely inﬂuence detection performance favoring tasks. semantic segmentation second task tried semantic segmentation. even though really broad range techniques devised problem recent comparison) compare methods lying closest turns relies ‘deeplab-large field view architecture remind that detailed sec. deviate deeplab architecture using linear operations skip layers using multi-scale architecture using densecrf post-processing. ﬁrst observe thanks multi-scale processing similar improvement singlescale architecture obtained understandably ranks latest state-of-the-art results ones obtained e.g. resnets atrous spatial pyramid pooling; advances complementary easy include network’s archifigure qualtitative results continued please note leftmost image practically color information justify mistake semantic segmentation object detection tasks left cactus incorrectly labelled chair apparently mistaken thorny throne. turning results two-task architecture observe quite surprisingly effectively performance. obvious given twotask network starting point vgg-type network uses detection network parameters ﬁfth convolutional layer rather segmentation parameters. apparently iterations ﬁne-tuning shared representation modiﬁed appropriate semantic segmentation task. turning multi-task network performance observe performance drops number tasks increases. still even without using post-processing fare comparably strong baseline human part segmentation task understood special case semantic segemntation assigning human part labels. recent work shown semantic part segmentation task solved cnns dataset introduced train network architecturally identical used semantic segmentation ﬁnetuned task segmenting human parts. general comment task observe structured prediction yields quite substantial improvements apparently highly conﬁned structure output space labelling task. since post-processing fair seen table. single-task case perform comparably. however multi-task case performance drops substantially scarcity data contain annotation task training single task network data contain annotations human parts training multi-task network human part annotations images used train whole network potential remedy increase weight task’s loss learning rates task-speciﬁc parameters parameter updates effective; another alternative give multi-task network training iterations pass times part annotations. exploring options. semantic boundary detection evaluate method semantic boundary detection task deﬁned goal instances pascal classes discontinuities. understood combination semantic segmetnation boundary detection tackled head-on fully convolutional networks. train train evaluate val. compare original method situational boundary detector high-for-low method authors beyond individual task boundary detection explore gains obtained providing inputs task results separate semantic segmentation system even though combining outputs different tasks immediate next goals consider here. still observe even applying architecture out-of-the-box reasonably close results substantially better standalone semantic boundary detection result. performance deteriorates multitask case remains quite close current ‘standalone’ boundary detection train network union trainval boundary images context dataset evaluate test berkeley segmentation dataset and. compare method best-established methods boundary detection well recent deep learning-based ones method gpb-owt-ucm se-var deepnets n-fields deepedge cscnn deepcontour hed-fusion hed-late merging multi-scale multi-scale +spb ours training setup ours -task ours -task table boundary detection results report maximal meaure obtained optimal dataset scale optimal image scale well average precision test dataset ﬁrst experiment train network exact experimental setup used including graduated deep supervised network training images obtained dataset augmentation images voc-context differences batch normalization allows increase layer-speciﬁc learning rates also ‘convolutionalized’ fully-connected layers network. improvement performance quite substantial maximal f-measure increases surpassing even performance would using spectral boundaries top. still settings successful remaining tasks using data images three times images would skew performance substantially favor low-level task boundary detection since training objective clearly affected number images containing ground truth task therefore remove side losses skip layers reduce layer-speciﬁc learning rate particular data used train ubernet multi-task setup. means that leftright ﬂipping boundary samples samples voc. shown ‘ours task’ table. substantially affect performance still remain competitive previous state-of-the-art works multi-task training case performance drops more always stays reasonably good level compared standard strong baselines saliency estimation train msra-k dataset evaluate pascal-s dataset subset pascal validation annotated. additional datasets typically used benchmark task e.g. explore performance method datasets future. ﬂipping dataset augmentation training. compare classic methods well recent ones typically rely deep learning. note method sets state-of-the-art dataset even multi-task training case method outperforms previous state-ofthe-art crf-based variant surface normal estimation task surface normals typically estimated point cloud data rather directly measured. training also evaluation normal estimation algorithm therefore affected step. train training normals estimated extend images normal ground truth estimated images training scenes since method publicly available surrogate method competing methods e.g. using alternative normal estimation methods extended data would expect differences large. coarse-to-ﬁne cascade even though work limited using linear functions skip layers sake simplicity efﬁciency successes suggest adding instead nonlinearities could improving performance task well potentially also tasks. effect task weights report multiple results single-task training case obtained setting values weight loss term observe quite substantial impact performance. setting large weight directly compete current state-of-the-art weight reduce performance substantially. however following subsection becomes necessary reasonably weight else adverse effects performance remaining tasks. using lower weight witness drop performance multi-task case. even though multi-task network’s performance different plain cnn-based result clear somewhat unique performance compared seen remaining tasks. conjencture geometric continuous nature task quite different remaining labelling tasks. intermediate ﬁnal features network appropriate task ‘out-of-the-box’ takes substantially large-scale modiﬁcations inner workings network nonlinearities within network accommodate task. however interesting competing methods vggcascade address task using additional layers network performance network multitude task adresses depends weights assigned losses different tasks weight task substantially larger expect skew internal representation network favor particular task negecting others. motivated empirical results previous paragraphs explored impact modifying weight attributed normal estimation task case solving multiple rather individual tasks. table. report performance changes increase weight normal estimation task realize that least particular experimental settings free lunch’ performance measures different tasks like communicating vessels. evaluation arguably affected optimization choices; using e.g. larger batch sizes iterations polynomial schedule could help. present results indicate common trunk apparently bounded learning capacity suggests inserting parameters potentially additional nonlinear layers skip layers needed maintain high performance across tasks. explore directions future well whether effect persists working deeper networks resnets. work introduced techniques allow train tackles broad computer vision problems uniﬁed architecture. shown effectively scale many diverse tasks since memory complexity independent number tasks incoherently annotated datasets combined training. certain straightforward directions future work considering tasks symmetry human landmarks texture segmentation tasks indicated introduction using deeper architectures resnets combining dense labelling results structured prediction research directions underway importantly consider work ﬁrst step direction jointly tackling multiple tasks exploiting table impact weight used normal estimation loss training -tasks improving normal estimation comes cost decreasing performance remaining tasks synergy recurring theme computer vision e.g. integrating segmentation recognition believe successfully addressing imperative single network succesfully handle involved tasks. code work soon made publicly available http//cvn.ecp.fr/ubernet/. work supported fp-reconfig fp-mobot h-isupport projects equipment donated nvidia. thank george papandreou pointing low-memory backpropagation implemented pierre-andr´e savalle showing handle prototxt ﬁles ross girshick making fasterrcnn system publicly available nikos paragios creating environment work took place.", "year": 2016}