{"title": "Adaptivity and Computation-Statistics Tradeoffs for Kernel and Distance  based High Dimensional Two Sample Testing", "tag": ["math.ST", "cs.AI", "cs.IT", "cs.LG", "math.IT", "stat.ML", "stat.TH"], "abstract": "Nonparametric two sample testing is a decision theoretic problem that involves identifying differences between two random variables without making parametric assumptions about their underlying distributions. We refer to the most common settings as mean difference alternatives (MDA), for testing differences only in first moments, and general difference alternatives (GDA), which is about testing for any difference in distributions. A large number of test statistics have been proposed for both these settings. This paper connects three classes of statistics - high dimensional variants of Hotelling's t-test, statistics based on Reproducing Kernel Hilbert Spaces, and energy statistics based on pairwise distances. We ask the question: how much statistical power do popular kernel and distance based tests for GDA have when the unknown distributions differ in their means, compared to specialized tests for MDA?  We formally characterize the power of popular tests for GDA like the Maximum Mean Discrepancy with the Gaussian kernel (gMMD) and bandwidth-dependent variants of the Energy Distance with the Euclidean norm (eED) in the high-dimensional MDA regime. Some practically important properties include (a) eED and gMMD have asymptotically equal power; furthermore they enjoy a free lunch because, while they are additionally consistent for GDA, they also have the same power as specialized high-dimensional t-test variants for MDA. All these tests are asymptotically optimal (including matching constants) under MDA for spherical covariances, according to simple lower bounds, (b) The power of gMMD is independent of the kernel bandwidth, as long as it is larger than the choice made by the median heuristic, (c) There is a clear and smooth computation-statistics tradeoff for linear-time, subquadratic-time and quadratic-time versions of these tests, with more computation resulting in higher power.", "text": "nonparametric sample testing decision theoretic problem involves identifying differences random variables without making parametric assumptions underlying distributions. refer common settings mean difference alternatives testing differences ﬁrst moments general difference alternatives testing difference distributions. large number test statistics proposed settings. paper connects three classes statistics high dimensional variants hotelling’s t-test statistics based reproducing kernel hilbert spaces energy statistics based pairwise distances. following question much statistical power popular kernel distance based tests unknown distributions differ means compared specialized tests mda? answer this formally characterize power popular tests like maximum mean discrepancy gaussian kernel bandwidth-dependent variants energy distance euclidean norm high-dimensional regime. prove several interesting properties relating classes tests include gmmd asymptotically equal power; furthermore also enjoy free lunch because additionally consistent power specialized high-dimensional t-tests mda. tests asymptotically optimal spherical covariances according simple lower bounds. three observations practically important since point implies gmmd consistent alternatives also automatically adaptive simpler alternatives point suggests median heuristic theoretical justiﬁcation default bandwidth choice point implies expending computation yield direct statistical beneﬁt orders magnitude. nonparametric sample testing deals detecting differences distributions given samples both without making parametric distributional assumptions. formally given samples distributions common types sample tests involve testing following sets null alternate hypotheses problem sustained interest statistics machine learning literature applications sample size might limited compared dimensionality experimental computational costs. example used answer questions medicine neuroscience assume simplicity though results extended case converges constant test function ...xn reject consider tests asymptotic type-i error call tests also needs specify relative rate increase. central question considered paper what power tests designed compared designed distributions truly differ means?. explain related questions detail section two-sample testing fundamental decision-theoretic problem long history statistics example past century seen wide adoption t-statistic hotelling decide samples different population means introduced parametric setting univariate gaussians generalized multivariate non-gaussian settings well. sample means joint sample covariance matrix statistician using multivariate t-test calculates seminal paper saranadasa authors proved asymptotic power tending high-dimensional setting motivating study alternative test statistics. despite increasing popularity usage many interesting questions remain unanswered discussed section partially answered paper. paper deals high-dimensional nonparametric two-sample testing grow polynomially explicit parametric assumptions section experimentally validate claims variety distributions even quite small sample sizes dimensions. shows asymptotics accurately describe even ﬁnite sample behavior tests. paper outline. rest paper organized follows. section introduce three classes tests literature hotelling-based tests kernel-based distance-based tests discuss related open questions section section prove three popular tests asymptotic power showing free adaptivity gda-based tests simpler problem. section show classes tests optimal diagonal covariance setting adapting lower bound normal means problem. section discusses computation-statistics tradeoffs compare power linear-time sub-quadratic time quadratic-time versions tests. section experiments discuss practical implications work. proofs section notation standard notation extensively. also non-random sequences negation negation mean absolute constant trace matrix k-th power trace. elementwise hadamard product refers total elements matrix i-th standard basis vector vector ones. convergence distribution indicator function. tests mda. mentioned introduction saranadasa prove hotelling’s power tending small explained inherent difﬁculty accurately estimating parameters samples avoid problem proposed test statistic showed non-trivial power whenever important precursor nonparametric work saranadasa dempster proposed high-dimensional t-test gaussians. srivastava srivastava proposed instead diag− analyzing power covariances also unequal without explicit restrictions rather terms conditions stated terms mean difference return conditions later paper since assumptions similar ﬂavor. hence unbiased estimator paper instead using directly analyze minor variant u-statistic also large literature so-called parametric behrens-fisher problem parametric problem distributions gaussian heteroskedastic also nonparametric behrensfisher problem deals nonparametric mean-scale families univariate multivariate settings. belloni didier lopes recent works references therein. another related line work analyzes setting could exponentially larger assuming kind sparsity example. tests gda. well known kolmogorov-smirnov test kolmogorov smirnov involves differences empirical cdfs. test related cramer von-mises criterion cram´er mises anderson-darling test anderson darling popular dimension usage restricted higher dimensions. mostly curse dimensionality involved estimating multivariate empirical cdfs. work generalizing popular one-dimensional higher dimensions like bickel seemingly common multivariate tests. examples univariate tests include rank based tests covered book lehmann d’abrera runs test wald wolfowitz interesting multivariate tests include spanning tree methods friedman rafsky nearest-neighbor based tests schilling henze cross-match tests rosenbaum proved consistent ﬁxed setting much known power high-dimensional setting. popular class tests multivariate problem emerged last decade kernel-based tests introduced parallel fern´andez gretton expanded gretton maximum mean discrepancy deﬁned note form gmmd statistic summations like technical convenience mimic form u-statistic asymptotic properties same. note linear kernel popular kernel gaussian kernel bandwidth parameter leading test statistic henceforth call gmmd apart fact population gmmd fact makes useful test statistic estimation error i.e. independent gretton detailed proof fact. unlike divergence example hard estimate high-dimensions. however recently argued ramdas study estimation error covers side story test power still degrades even estimation error not. related different class tests distance-based energy statistics introduced parallel baringhaus franz sz´ekely rizzo generalized kinds metrics denoted related independence testing problem lyons test statistic called cramer statistic former paper term energy distance done latter more study u-statistic form eedu leads tests consistent ﬁxed setting ﬁxed alternatives fairly general conditions results found associated references. however much known high dimensional regime. remark paper deal largely gmmd popular choices kernel distance used practice similar inferences possibly made kernels distances using proof technique. similarly focus though draw similar inferences corresponding variants. test statistics like analysed high-dimensional setting. however presently poor understanding gmmd high dimensions. list open questions going answer paper followed partial answers questions. explanation ﬁxed setting gmmd well understood null alternate distributions given gretton sz´ekely rizzo respectively. however behavior high dimensions seems essentially unanswered current literature. general characterization power impossible since could different arbitrarily similar formal statement proof claim). reason somewhat restricted trying characterize power limited settings. example hope characterize power parameterizing problem terms smallest moment differ. result propose analyze consider nonparametric distributions differ speciﬁc moment much power gmmd identify difference reject null. ﬁrst step paper characterize power differ ﬁrst moment. explanation popular choice bandwidth median heuristic chosen median euclidean distance pairs points however effect choice test power unclear. gretton also make suggestions choosing bandwidth parameter linear-time gmmd also guarantees ﬁxed setting. hence study kernel bandwidth affects power work progress current literature. ﬁxed consistency proved gretton further power gmmd ﬁxed alternative also explicitly derived ﬁxed setting ignoring constants gaussian cdf. notice consistency gmmd test ﬁxed stark contrast using gaussian kernels density estimation must bandwidth zero increasing hence gmmd statistic behave l-distance kernel density estimates done anderson explanation sejdinovic describes connections kernel distance based tests independence testing. informally speaking near one-to-one correspondence class kernels distances tests make sense. however metric/semimetric corresponding gaussian kernel metric/semimetric euclidean distance seems popular statistics literature gmmd machine learning practical importance ﬁelds know choose gmmd. explanation given nonparametric two-sample testing problem generally know distributions differed means not. differ means presumably former statistics perform worse latter since latter designed speciﬁcally purpose concentrate power detecting ﬁrst moment differences. much worse? price must extra generality gmmd eed? main questions considered paper actually comparing powers gmmd ucq. eedu gmmd eedu power high dimensions theory practice even though gmmd eedu also consistent whereas not. would like note result actually observed practice seemingly explicitly acknowledged conjectured. figures baringhaus franz quite convincing authors explicitly point experiments conclusion sections figures lopes also show phenomenon gmmd though latter authors comment experimental observation. know paper ﬁrst rigorous justiﬁcation phenomenon. noting gmmd introduce linear-time block-based subquadratic-time statistics gmmd related work regard reddi analyses linear-time version gmmd high-dimensional setting. discuss last question detail section result section show expending computation yields direct statistical beneﬁt higher power; clear smooth statistics-computation tradeoff family earlier proposed sub-quadratic linear time sample tests. model. k-dimensional independent zero mean identity covariance random variables unknown full-rank deterministic transformation matrix satisfying denote mean difference remark assumption implies means covariances like saranadasa assume different covariances like chen reason choice follows. gmmd detect differences distributions population projected along direction surface dimensional unit sphere; sz´ekely rizzo proof. this gmmd sensitive differences second moments distributions. analyze power makes sense nullify sources signal like remark assumption made essentially form saranadasa chen calculations explicitly involve much moments deviate standard gaussian. show section many results hold experimentally variety non-gaussian distributions. remark assumption essentially means fairly well conditioned also made aforementioned earlier works. this note conditions reduce requiring eigenvalues bounded assumption still met. eigenvalues bounded condition satisﬁed long terribly conditioned. assumption discussed detail several nontrivial examples chen similarly reﬂects conditioning number moments best case independent coordinates i.e. identity covariance inﬁnite moments assume fewer moments deviate away diagonal covariance ill-conditioned matrices strays away half assume fairly well-conditioned least think good conditioning necessary theorems hold scalar lowered. remark first recall assumed full rank assumption λmin assumption essentially means signal strength large relative noise. example assumption requires indeed generally implies need assumption technical reasons conjecture results hold weaker assumption. even present form strong assumption since signal strength large decision problem becomes easy regime rather uninteresting. note implies cauchy-schwarz theorem follows main result chen hence reproduce here. there authors prove asymptotically normally distributed variance null gives rise expression power fairly easily except authors made small mistake interchanging crucial expression another minor difference write power single expression chen prefer write aforementioned special cases high snr. remark null distribution asymptotically gaussian high-dimensional setting. stark contrast ﬁxed-d increasing-n setting null distribution inﬁnite weighted chi-squared distributions properties degenerate u-statistics seems ﬁrst proved saranadasa using martingale central limit theorem this holds refer nuclear operator norms respectively. proof theorem covered section conjecture result like claims karoui gaussian kernel often behaves like linear kernel high dimensions results hold true further also interpret results rather pessimistically saying kernels provide advantage high-dimensional setting demonstrate experiments linear kernel sufﬁce trivial power gmmd’s power tends reasonable scenarios. course samples probably needed detect differences second moments compared differences ﬁrst moments.hence choose interpret result optimistically gmmd capable detecting difference distributions also detects differences means well designed test mean differences. section proof theorem remark remark inability prove theorems limiting case proofs theorems based taylor expansion respectively deﬁnition). leads dominant taylor term u-statistic remainder term u-statistic easily observe −hcq hence behavior immediately captured behavior important fact always gaussian null alternative prove however results suggest however know asymptotically gaussian know limiting distribution even though undertake tedious calculations mean variance hence allows make arguments mean variance gmmd cannot make power claims since purpose require knowing limiting distribution null. conjecture indeed gaussian simulations support this proof vastly complicated number terms controlled martingale central limit theorem larger proving theorem statements limiting case important direction future work require development theory u-statistics high dimensional variables. however moment show variety experiments support conjecture implying borderline case probably technical limitation. think problem-dependent constant determines hard testing problem course larger easier distributions distinguish. indeed special case spherical gaussians kl-divergence distributions. then expression power simpliﬁes show form power achieved theorem improvable certain assumptions. example case provide matching lower bounds using techniques ingster suslina designed gaussian normal means problem. proof relies gaussian approximations central noncentral chi-squared distributions. proposition central chi-squared distribution degrees freedom noncentral chi-squared distribution degrees freedom noncentrality parameter uniformly next deﬁne surface d-dimensional sphere radius normal means problem given test recalling deﬁnition analogously deﬁne normal means problem tests expected type- error deﬁne minimax power level proof. proposition almost verbatim proposition ingster suslina proof given example ingster suslina example yielding expression power difference proposition statement directly expression instead approximation eq.. proposition directly yields lower bound sample testing represent pairs d-dimensional distributions whose means differ whose covariances deﬁne minimax power level remark lower bound expression exactly matches upper bound expression including matching constants showing discussed tests minimax optimal setting even though current lower bounds possibly strengthened include nondiagonal remark able even diagonal-covariance lower bounds sample testing literature especially accurate even constants. section consider computationally cheaper alternatives computing quadratic time gmmd suggested gretton zaremba namely block-based gmmd linear-time gmmd clear gmmd minimum variance unbiased estimator clear much worse options slightly worse computational beneﬁts could worth large amount data. lack high-dimensional analysis gretton inferred suffers cheaper computation power worse constant factor compared power gmmd. show that power worse constants exponents points assumptions section assumed hold wherever needed proceed directly comparisons. assume divide data blocks size gmmd gmmd statistic evaluated samples block block-based deﬁned yn}. idea order magnitude choice median heuristic makes make reasonable supposition choice similar meanheuristic chooses average distance pairs points i.e. assume argument’s sake following proposition captures order magnitude bandwidth choice made common median heuristic. proposition average distance pairs points hence median-heuristic chooses remark proposition implies choice made median heuristic borderline satisfying condition main theorem holds practically experiments follow seems like claims still seem hold even implies conditions currently needed theory possibly stronger needed. hence heuristic actually provides reasonable default bandwidth choice since usually unknown. different test statistics considered simulation suite ummd. gmmd i.e. ummd median gmmd chosen aforementioned median heuristic. ummd. gmmd i.e. energy distance eedγ u-statistic chen lmmd linear-time gmmd linear-time version ucq. plot power tests statistics various running repetitions sample test parameter setting. sentence summary experiments follow u-statistics exactly power mean-differences claimed theorems i.e. φgmmd choices bandwidth lineartime statistics perform signiﬁcantly worse also predicted theory values shifts covariance matrix chosen keep asymptotic power distribution figure shows performance various estimators aforementioned sample test settings. clear power gmmd coincide quadratic time statistic staying constant linear large) bandwidth increasing experiment previous experiment seen performance estimators diagonal covariance matrix. here empirically verify similar effects observed distributions non-diagonal covariance matrix. consider distributions matrix random unitary matrix obtained eigenvectors random gaussian matrix. follows. diagonal matrix entries equally spaced raised power experimental setup similar used lopes matrix figure shows qualitative performance statistics similar observed previous experiment experiment experiment study performance statistics distributions differ covariances rather means. experiment here positive deﬁnite matrix generated described experiment again experimental setup similar used lopes surprisingly seen figure gmmd perform better experiment demonstrates gmmd dominate sense. fact designed mean-shift alternatives rest work general alternatives. hence achieve power distributions differ means strictly higher power distributions differ means higher moment. also powers different statistics longer equal bandwidth matter situation. experiment finally verify nature asymptotic power ﬁxed dimension. purpose experiment hold ﬁxed value vary here consider sample tests normal distributions diagonal non-diagonal covariance matrices figure illustrates power tests scenario. seen power increases manner similar ones observed previous experiments. experiment suggests assumption probably relaxed dropped theory. need bound certain taylor remainder term proof theorems follows perhaps possible better bound term. hence distribution gmmd matches distribution null alternative statistics hence also power. argument also holds studentized statistics calculated practice. rest proof devoted proving three steps proof. ﬁrst four claims follow directly detailed work ullah last claim follows. first note ﬁrst inequality follows second follows πopπ∗ cauchy-schwarz. also hadamard product identity diagdiag πdiag) horn johnson since matrices similarly proof. statements hold simply expansion substitution previous proposition. remembering last claim holds. indeed assumption implies hence since similarly fashion deduce dominant term since alternately results obtained fashion similar proposition variance uncentered quadratic forms proposition results ullah momnents products non-normal quadratic forms gaussian case). hence bounding variances terms expansion remark recall typically stated textbooks like serﬂing degenerate u-statistics variance null variance alternative true asymptotically ﬁxed setting variance alternative still high-dimensional setting depending signal noise ratio dimension difference proof instead taking taylor expansion gaussian kernel take expansion euclidean distance. gives rise exact terms bound different constants. indeed exact form taylor’s theorem", "year": 2015}