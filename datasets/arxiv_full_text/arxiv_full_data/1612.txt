{"title": "Neural Belief Tracker: Data-Driven Dialogue State Tracking", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user's goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted lexicons for capturing some of the linguistic variation in users' language. We propose a novel Neural Belief Tracking (NBT) framework which overcomes these problems by building on recent advances in representation learning. NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.", "text": "core components modern spoken dialogue systems belief tracker estimates user’s goal every step dialogue. however current approaches difﬁculty scaling larger complex dialogue domains. dependency either spoken language understanding models require large amounts annotated training data; hand-crafted lexicons capturing linguistic variation users’ language. propose novel neural belief tracking framework overcomes problems building recent advances representation learning. models reason pre-trained word vectors learning compose distributed representations user utterances dialogue context. evaluation datasets shows approach surpasses past limitations matching performance state-of-the-art models rely hand-crafted semantic lexicons outperforming lexicons provided. spoken dialogue systems allow users interact computer applications conversation. task-based systems help users achieve goals ﬁnding restaurants booking ﬂights. dialogue state tracking component serves interpret user input update belief state system’s internal representation state conversation probability distribution dialogue states used downstream dialogue manager decide action system user looking cheaper restaurant inform system sure. kind where? user thai food somewhere downtown inform system house serves cheap thai food user inform; request system house regent street dialogue state tracking challenge series shared tasks provided common evaluation framework accompanied labelled datasets framework dialogue system supported domain ontology describes range user intents system process. ontology deﬁnes collection slots values slot take. system must track search constraints expressed users questions users search results taking account user utterance dialogue context example figure shows true state user utterance three-turn conversation. seen example models depend identifying mentions ontology items user utterances. becomes non-trivial task confronted lexical variation dynamics context noisy automated speech recognition output. traditional statistical approaches separate spoken language understanding modules address lexical variability within single dialogue turn. however training models requires substantial amounts domain-speciﬁc annotation. alternatively turn-level cross-turn coalesced single model achieve superior belief tracking performance shown henderson coupled models typically rely manually constructed semantic dictionaries identify alternative mentions ontology items vary lexically morphologically. figure gives example dictionary three slot-value pairs. approach term delexicalisation clearly scalable larger complex dialogue domains. importantly focus english research understates considerable challenges morphology poses systems based exact matching morphologically richer languages italian german paper present models collectively called neural belief tracker family. proposed models couple efﬁciently learning handle variation without requiring hand-crafted resources. that models move away exact matching instead reason entirely pre-trained word vectors. vectors making user utterance preceding system output ﬁrst composed intermediate representations. representations used decide ontologydeﬁned intents expressed user point conversation. best knowledge models ﬁrst successfully pre-trained word vector spaces improve language understanding capability belief tracking models. evaluation datasets show that models match performance delexicalisation-based models make hand-crafted semantic lexicons; models signiﬁcantly outperform models resources available. consequently believe work proposes framework better-suited scaling belief tracking models deployment real-world dialogue systems operating sophisticated application domains creation domain-speciﬁc lexicons would infeasible. models probabilistic dialogue state tracking belief tracking introduced components spoken dialogue systems order better handle noisy speech recognition sources uncertainty understanding user’s goals modern dialogue management policies learn tracker’s distribution intents decide whether execute action request clariﬁcation user. mentioned above dstc shared tasks spurred research problem established standard evaluation paradigm setting task deﬁned ontology enumerates goals user specify attributes entities user request information about. many different belief tracking models proposed literature generative discriminative statistical models rule-based systems motivate work presented here categorise prior research according reliance separate module interpreting user utterances separate traditional pipelines spoken language understanding decoders detect slot-value pairs expressed automatic speech recognition output. downstream model combines information past dialogue context update belief state figure architecture model. implementation three representation learning subcomponents modiﬁed long produce adequate vector representations downstream model components decide whether current candidate slot-value pair expressed user utterance dstc challenges systems used output template-based matching systems phoenix however robust accurate statistical systems available. many discriminative approaches spoken dialogue train independent binary models decide whether slot-value pair expressed user utterance. given enough data models learn lexical features good indicators given value capture elements paraphrasing line work later shifted focus robust handling rich output also treated sequence labelling problem word utterance labelled according role user’s intent; standard labelling models crfs recurrent neural networks used approaches adopt complex modelling structure inspired semantic parsing drawback shared methods resource requirements either need learn independent parameters slot value need ﬁne-grained manual annotation word level. hinders scaling larger realistic application domains. joint slu/dst research belief tracking found advantageous reason jointly taking predictions input generating belief states output dstc systems used external module outperformed systems used external features. joint models typically rely strategy known delexicalisation whereby slots values mentioned text replaced generic labels. dataset transformed manner extract collection template-like n-gram features perform belief tracking shared model iterates slot-value pairs extracting delexicalised feature vectors making separate binary decision regarding pair. delexicalisation introduces hidden dependency rarely discussed identify slot/value mentions text? domains manually construct semantic dictionaries list potential rephrasings slot values. shown mrkˇsi´c dictionaries essential performance current delexicalisation-based models. though scale rich variety user language general domains. able data leveraging semantic information pre-trained word vectors resolve lexical/morphological ambiguity; maximising number parameters shared across ontology values; ﬂexibility learn domainspeciﬁc paraphrasings kinds variation make infeasible rely exact matching delexicalisation robust strategy. neural belief tracker model designed detect slot-value pairs make user’s goal given turn dialogue. input consists system dialogue acts preceding user input user utterance itself single candidate slot-value pair needs make decision about. instance model might decide whether goal food=italian expressed ‘i’m looking good pizza’. perform belief tracking model iterates candidate slot-value pairs decides ones expressed user. figure presents information model. ﬁrst layer hierarchy performs representation learning given three model inputs producing vector representations user utterance current candidate slot-value pair system dialogue acts subsequently learned vector representations interact context modelling semantic decoding submodules obtain intermediate interaction summary vectors used input ﬁnal decision-making module decides whether user expressed intent represented candidate slot-value pair. given user utterance system candidate slot-value pair representation learning submodules produce vector representations input downstream components model. representation learning subcomponents make pre-trained collections word vectors. shown mrkˇsi´c specialising word vectors express semantic similarity rather relatedness essential improving belief tracking performance. reason semantically-specialised paragram-sl word vectors throughout work. training procedure keeps vectors ﬁxed test time unseen words semantically related familiar slot values recognised purely position original vector space means model parameters shared across values given slot even across slots. represent user utterance consisting words uku. word associated word vector uku. propose model variants differ method used produce vector representations nbt-dnn nbt-cnn. constituent ngrams utterance. concatenation word vectors starting index that denotes vector concatenation. simpler models term nbt-dnn shown figure model computes cumulative n-gram representation vectors n-gram ‘summaries’ unigrams bigrams trigrams user utterance weight matrices bias terms cumulative n-grams vectors dimensionality denotes sigmoid activation function. maintain separate parameters slot three vectors summed obtain single representation user utterance cumulative n-gram representations used model unweighted sums word vectors utterance. ideally model learn recognise parts utterance relevant subsequent classiﬁcation task. instance could learn ignore verbs stop words attention adjectives nouns likely express slot values. figure nbt-dnn model. word vectors n-grams summed obtain cumulative n-grams passed another hidden layer summed obtain utterance representation figure nbt-cnn model. convolutional ﬁlters window sizes applied word vectors given utterance convolutions followed relu activation function max-pooling produce summary n-gram representations. summed obtain utterance representation nbt-cnn second model draws inspiration successful applications convolutional neural networks language understanding models typically apply number convolutional ﬁlters n-grams input sentence followed non-linear activation functions max-pooling. following approach nbt-cnn model applies different ﬁlters n-gram lengths ﬁlters value word vector dimensionality. denotes concatenation word vectors starting index list n-grams convolutional ﬁlters length over. three intermediate representations given bias term broadcast across ﬁlters. finally three summary n-gram representations summed obtain ﬁnal utterance representation vector nbt-cnn model better suited longer utterances convolutional ﬁlters interact directly subsequences utterance noisy summaries given nbt-dnn’s cumulative n-grams. diagram figure shows utterance representation candidate slotvalue pair representation directly interact semantic decoding module. component decides whether user explicitly expressed intent matching current candidate pair examples matches would want thai food’ food=thai demanding ones pricey restaurant’ price=expensive. high-quality pre-trained word vectors comes play delexicalisation-based model could deal former example would helpless latter case unless human expert provided semantic dictionary listing potential rephrasings value domain ontology. vector space representations candidate pair’s slot name value given model learns tuple single vector dimensionality utterance representation representations forced interact order learn similarity metric discriminates interactions utterances slot-value pairs either express denotes element-wise vector multiplication. product seem like intuitive similarity metric would reduce rich features single scalar. element-wise multiplication allows downstream network make better parameters learning non-linear interactions sets features context modelling ‘decoder’ sufﬁce extract intents utterances human-machine dialogue. understand queries belief tracker must aware context i.e. dialogue leading latest user utterance. previous system user utterances important relevant last system utterance dialogue system could performed following system acts also tried concatenate pass vector downstream decision-making neural network. however set-up weak performance since relatively small datasets sufﬁce network learn model interaction feature vectors. system conﬁrm system asks user conﬁrm whether speciﬁc slot-value pair part desired constraints. example user responds ‘how turkish food?’ ‘yes’ model must aware system order correctly update belief state. make markovian decision consider last system acts incorporate context modelling nbt. word vectors arguments system request conﬁrm acts model computes following measures similarity system acts candidate pair utterance representation denotes product. computed similarity terms gating mechanisms pass utterance representation system asked current candidate slot slot-value pair. type interaction particularly useful conﬁrm system system asks user conﬁrm user likely mention slot values respond afﬁrmatively negatively. means model must consider three-way interaction utterance candidate slot-value pair slot value pair offered system. latter model consider afﬁrmative negative polarity user utterance making subsequent binary decision. binary decision maker intermediate representations passed another hidden layer combined. φdim layer maps input vector vector size input ﬁnal binary softmax given dstc transcriptions hypotheses turn-level semantic labels provided dialogue state tracking challenge ofﬁcial transcriptions contain various spelling errors corrected manually; cleaned version available mi.eng.cam.ac.uk/˜nm/ dstc-clean.zip. training data contains dialogues test consists dialogues. train models transcriptions report belief tracking performance test hypotheses provided original challenge. performed wizard style experiment amazon mechanical turk users assumed role system user task-oriented dialogue system based dstc ontology. users typed instead using speech means performance experiments indicative model’s capacity semantic understanding robustness errors. whereas dstc dialogues users would quickly adapt system’s language understanding capability experimental design gave freedom sophisticated language. expanded original dataset using data collection procedure yielding total dialogues. divided training validation test dialogues. dataset available mi.eng.cam.ac. uk/˜nm/woz_..zip. training examples corpora used create training data separate experiments. dataset iterate train utterances generating example slotvalue pairs ontology. example consists transcription context candidate slot-value pair. binary label example indicates whether utterance context express example’s candidate pair. instance would like irish work deﬁne simple rule-based belief state update mechanism applied n-best lists. dialogue turn syst− denote preceding system output denote list hypotheses posterior probabilities hypothesis slot slot value models estimate probability expressed given hypothesis. predictions hypotheses combined informable slots value highest probability chosen {∅}). requests slots current goal slots food area price. users specify values slots order restaurants models evaluate model variants nbt-dnn nbt-cnn. train models adam optimizer crossentropy loss backpropagating subcomponents keeping pre-trained word vectors ﬁxed model trained separately slot. high class bias incorporate ﬁxed number positive examples mini-batch. baseline system implements wellknown competitive delexicalisation-based model dataset. dstc model henderson model n-gram based neural network model recurrent connections between turns replaces occurrences slot names values generic delexicalised features. compare models sophisticated belief tracking model presented model uses belief state updates turn-level feature extraction. unlike nbtcnn operates vectors model hyperparameters tuned respective validation sets. datasets initial adam learning rate positive examples included mini-batch. batch size affect performance experiments. gradient clipping used handle exploding gradients. dropout used regularisation models implemented tensorflow baseline model supplemented task-speciﬁc semantic dictionary dictionaries available mi.eng.cam. ac.uk/˜nm/sem-dict.zip. dstc dictionary contains three rephrasings. nonetheless rephrasings translates substantial gains performance believe result supports claim vocabulary used mechanical turkers dstc constrained system’s inability cope lexical variation noise. dictionary includes rephrasings showing unconstrained language used mechanical turkers wizard-of-oz setup requires elaborate lexicons. baseline models exact matches ontology-deﬁned intents one-hot delexicalised ngram features. means pre-trained vectors cannot incorporated directly models. belief tracking performance table shows performance models trained evaluated dstc datasets. models outperformed baseline models terms joint goal request accuracies. goals gains always statistically signiﬁcant moreover statistically signiﬁcant variation lexicon-supplemented models showing handle semantic relations otherwise explicitly encoded semantic dictionaries. performs well across board compare performance datasets understand strengths. improvement baseline greater corroborates intuition nbt’s ability learn linguistic variation vital dataset containing longer sentences richer vocabulary errors. comparison language subjects dstc dataset less rich compensating errors main hurdle given access dstc test transcriptions models’ goal accuracy rises importance word vector spaces models semantic relations embedded pre-trained word vectors handle semantic variation produce high-quality intermediate representations. table shows performance nbt-cnn models making three different word vector collections ‘random’ word vectors initialised using xavier initialisation distributional glove vectors trained using co-occurrence information large textual corpora; semantically specialised paragramsl vectors obtained injecting semantic similarity constraints paraphrase database distributional glove vectors order improve semantic content. results table show semantically specialised word vectors leads considerable performance gains paragram-sl vectors outperformed glove xavier vectors goal tracking datasets. gains particularly robust noisy dstc data collections pre-trained vectors consistently outperformed random initialisation. gains weaker noise-free dataset seems large enough model learn task-speciﬁc rephrasings compensate lack semantic content word vectors. dataset glove vectors improve randomly initialised ones. believe happens distributional models keep related antonymous words close together offsetting useful semantic content embedded vector spaces. table dstc test performance nbt-cnn model making three different word vector collections. asterisk indicates statistically signiﬁcant improvement baseline xavier word vectors paper proposed novel neural belief tracking framework designed overcome current obstacles deploying dialogue systems real-world dialogue domains. models offer known advantages coupling spoken language understanding dialogue state tracking without relying hand-crafted semantic lexicons achieve state-of-the-art performance. evaluation demonstrated beneﬁts models match performance models make lexicons vastly outperform available. finally shown performance models improves semantic quality underlying word vectors. best knowledge ﬁrst move past intrinsic evaluation show semantic specialisation boosts performance downstream tasks. future work intend explore applications multi-domain dialogue systems well languages english require handling complex morphological variation. authors would like thank ivan vuli´c ulrich paquet cambridge dialogue systems group anonymous reviewers constructive feedback helpful discussions. references mart´ın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg man´e rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda vi´egas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng. tensorflow large-scale machine learning heterogeneous systems. bohus alex rudnicky. hypotheses other belief updating model. proceedings aaai workshop statistical empirical methods spoken dialogue systems. asli celikyilmaz dilek hakkani-tur. convolutional neural network based semantic tagging proceedings nips entity embeddings. workshop machine learning spoken language understanding interaction. ronan collobert jason weston leon bottou michael karlen koray kavukcuoglu pavel kuksa. natural language processing scratch. journal machine learning research matthew henderson milica gaˇsi´c blaise thomson pirros tsiakoulis steve young. discriminative spoken language understanding using word confusion networks. spoken language technology workshop ieee. youngsoo jang jiyeon byung-jun youngjae chang kee-eung kim. neural dialog state tracker large ontologies attention mechanism. proceedings ieee slt. mairesse gasic jurcicek keizer thomson young. spoken language understanding unaligned data using discrimproceedings inative classiﬁcation models. icassp. gr´egoire mesnil yann dauphin kaisheng yoshua bengio deng dilek hakkani-tur xiaodong larry heck dong geoffrey zweig. using recurrent neural networks slot ﬁlling spoken language understanding. ieee/acm transactions audio speech language processing nikola mrkˇsi´c diarmuid s´eaghdha blaise thomson milica gaˇsi´c lina rojas-barahona pei-hao david vandyke tsung-hsien steve young. counter-ﬁtting word vectors linguistic constraints. proceedings hlt-naacl. nikola mrkˇsi´c diarmuid s´eaghdha blaise thomson milica gaˇsi´c pei-hao david vandyke tsung-hsien steve young. multidomain dialog state tracking using recurrent neural networks. proceedings acl. baolin peng kaisheng jing kam-fai wong. recurrent neural networks external memory language understanding. proceedings national conference natural language processing chinese computing. iman saleh shaﬁq joty llu´ıs m`arquez alessandro moschitti preslav nakov scott cyphers glass. study using syntactic semantic structures concept segmentation labeling. proceedings coling. hongjie takashi ushio mitsuru endo katsuyoshi yamagami noriaki horii. convolutional neural networks multi-topic dialog state tracking. proceedings iwsds. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research pei-hao milica gaˇsi´c nikola mrkˇsi´c lina rojasbarahona stefan ultes david vandyke tsunghsien steve young. continuously arxiv learning neural dialogue management. preprint pei-hao milica gaˇsi´c nikola mrkˇsi´c lina rojasbarahona stefan ultes david vandyke tsunghsien steve young. on-line active reward learning policy optimisation spoken dialogue systems. proceedings acl. ngoc thang pankaj gupta heike adel hinrich sch¨utze. bi-directional recurrent neural network ranking loss spoken language understanding. proceedings icassp. ivan vuli´c nikola mrkˇsi´c reichart diarmuid s´eaghdha steve young anna korhonen. morph-ﬁtting fine-tuning word vector spaces proceedings simple language-speciﬁc rules. acl. zhuoran wang oliver lemon. simple generic belief tracking mechanism dialog state tracking challenge believability observed information. proceedings sigdial. tsung-hsien milica gaˇsi´c dongho nikola mrkˇsi´c pei-hao david vandyke steve young. stochastic language generation dialogue using recurrent neural networks convolutional sentence reranking. proceedings sigdial. tsung-hsien milica gaˇsi´c nikola mrkˇsi´c pei-hao david vandyke steve young. semantically conditioned lstm-based natural language generation spoken dialogue systems. proceedings emnlp. tsung-hsien david vandyke nikola mrkˇsi´c milica gaˇsi´c lina rojas-barahona pei-hao stefan ultes steve young. networkbased end-to-end trainable task-oriented dialogue system. proceedings eacl. kaisheng baolin peng zhang dong geoffrey zweig yangyang shi. spoken language understanding using long short-term memory neural networks. proceedings asru. steve young milica gaˇsi´c simon keizer franc¸ois mairesse jost schatzmann blaise thomson hidden information state model practical framework pomdp-based spoken dialogue management. computer speech language", "year": 2016}