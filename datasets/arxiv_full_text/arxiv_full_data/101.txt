{"title": "Towards better decoding and language model integration in sequence to  sequence models", "tag": ["cs.NE", "cs.CL", "cs.LG", "stat.ML"], "abstract": "The recently proposed Sequence-to-Sequence (seq2seq) framework advocates replacing complex data processing pipelines, such as an entire automatic speech recognition system, with a single neural network trained in an end-to-end fashion. In this contribution, we analyse an attention-based seq2seq speech recognition system that directly transcribes recordings into characters. We observe two shortcomings: overconfidence in its predictions and a tendency to produce incomplete transcriptions when language models are used. We propose practical solutions to both problems achieving competitive speaker independent word error rates on the Wall Street Journal dataset: without separate language models we reach 10.6% WER, while together with a trigram language model, we reach 6.7% WER.", "text": "mistakes introduced decoding cause model skip words jump another place recording. problem incomplete transcripts especially apparent external language models used. speech recognition system builds recently proposed listen attend spell network attention-based seqseq model able directly transcribe audio recording space-delimited sequence characters similarly seqseq neural networks uses encoder-decoder architecture composed three parts listener module tasked acoustic modeling speller module tasked emitting characters attention module serving intermediary speller listener listener multilayer bi-lstm network transforms sequence frames acoustic features possibly shorter sequence hidden activations hn/k time reduction constant emit character speller uses attention mechanism relevant activations listener summarize context history previously emitted characters encapsulated recurrent state implement recurrent step using single lstm layer. attention mechanism sensitive location frames selected previous step employs convolutional ﬁlters previous attention weights output character distribution computed using softmax function. recently proposed sequence-to-sequence framework advocates replacing complex data processing pipelines entire automatic speech recognition system single neural network trained end-to-end fashion. contribution analyse attention-based seqseq speech recognition system directly transcribes recordings characters. observe shortcomings overconﬁdence predictions tendency produce incomplete transcriptions language models used. propose practical solutions problems achieving competitive speaker independent word error rates wall street journal dataset without separate language models reach together trigram language model reach wer. index terms attention mechanism recurrent neural networks lstm deep learning many breakthroughs including speech image recognition subfamily deep models sequence-to-sequence neural networks proved successful complex transduction tasks machine translation speech recognition lip-reading seqseq networks typically decomposed modules implement stages data processing pipeline encoding module transforms inputs hidden representation decoding module emits target sequences attention module computes soft alignment hidden representation targets. training directly maximizes probability observing desired outputs conditioned inputs. discriminative training mode fundamentally different generative \"noisy channel\" formulation used build classical state-of-the speech recognition systems. such beneﬁts limitations different classical systems. understanding preventing limitations speciﬁc seqseq models crucial successful development. discriminative training allows seqseq models focus informative features. however also increases risk overﬁtting distinguishing characteristics. observed seqseq models often yield sharp predictions hypotheses need considered likely transcription given utterance. however high conﬁdence reduces diversity transcripts obtained using beam search. typical training models conditioned ground truth transcripts scored one-step ahead predictions. itself training criterion ensure relevant fragments input utterance transcribed. subsequently analysed impact model conﬁdence separating effects model accuracy beam search effectiveness. also propose practical solution partial transcriptions problem relating coverage input utterance. model conﬁdence promoted cross-entropy training criterion. baseline network training loss minimized model concentrates output distribution correct ground-truth character. leads peaked probability distributions effectively preventing model indicating sensible alternatives given character homophones. moreover overconﬁdence harm learning deeper layers network. derivative loss backpropagated softmax function logit corresponding character equals approaches network’s output becomes concentrated correct character. therefore whenever spelling makes good prediction little training signal propagated attention mechanism listener. model overconﬁdence consequences. first next-step character predictions accuracy overﬁtting. second overconﬁdence impact ability beam search good solutions recover errors. ﬁrst investigate impact conﬁdence beam search varying temperature softmax function. without retraining model change character probability distribution depend temperature hyperparameter increased temperatures distribution characters becomes uniform. however preferences model retained ordering tokens least probable preserved. tuning temperature therefore allows demonstrate impact model conﬁdence beam search without affecting accuracy next step predictions. decoding results baseline model data presented figure haven’t used language model. high temperatures deletion errors dominated. didn’t want change beam search cost instead constrained search emit token probability within narrow range probable token. compare default setting sharper distribution smoother distributions strategies lead greedy decoding accuracy temperature changes affect selection probable character. temperature increases beam search ﬁnds better solutions however care must taken prevent truncated transcripts. elegant solution model overconﬁdence problem proposed inception image recognition architecture purpose computing training cost ground-truth label distribution smoothed fraction probability mass assigned classes correct one. turn prevents model learning concentrate probability mass single token. additionally model receives training signal error function cannot easily saturate. originally uniform label smoothing scheme proposed model trained assign probability mass speech recognizer computes probability character conditioned partially emitted transcript whole utterance. thus trained minimize cross-entropy ground-truth characters model predictions. training loss single utterance denotes target label function. baseline model indicator i.e. value correct character otherwise. label smoothing used encodes distribution characters. recurrent formulation speller function probable transcript cannot found exactly using viterbi algorithm. instead approximate search methods used. typically best results obtained using beam search. search begins hypotheses containing empty transcript. every step candidate transcripts formed extending hypothesis beam character. candidates scored using model certain number top-scoring candidates forms beam. model indicates transcript considered ﬁnished emitting special token. identiﬁed challenges adding language model. first model overconﬁdence deviations best guess network drastically changed term made balancing terms difﬁcult. second incomplete transcripts produced unless recording coverage term added. equation heuristic involving multiplication conditional unconditional probabilities transcript tried justify adding intrinsic language model suppression term would transform p/p. estimated language modeling capability speller replacing encoded speech constant separately trained biasing vector. character perplexity obtained didn’t observe consistent gains extension beam search criterion. figure inﬂuence beam width softmax temperature decoding accuracy. baseline case increasing temperature reduces error rate. label smoothing used next-character prediction improves witnessed beam size= tuning temperature bring additional beneﬁts. compare three strategies designed prevent incomplete transcripts. ﬁrst strategy doesn’t change beam search criterion forbids emitting token unless probability within range probable token. strategy prevents truncations inefﬁcient omissions middle transcript failure shown table alternatively beam search criterion extended promote long transcripts. term depending transcript length proposed seqseq networks usage reported difﬁcult beam search looping parts recording additional constraints needed prevent looping propose coverage term counts number frames received cumulative attention greater coverage criterion prevents looping utterance cumulative attention bypasses threshold frame counted selected subsequent selections frame reduce decoding cost. implementation coverage recomputed beam search iteration using attention weights produced step. figure compare effects three methods decoding network uses label smoothing trigram language model. unlike didn’t experience looping beam search promoted transcript length. hypothesize label smoothing increases cost correct character emissions helps balancing terms used beam search. observe large beam widths constraining emissions sufﬁcient. contrast promoting coverage transcript length yield improvements increasing beams. however simply maximizing transcript length yields word insertion errors achieves overall worse wer. \"chase nigeria’s registrar society independent organization hired count votes\" society independent organization hired count votes\" \"chase nigeria’s registrar\" \"chase’s nature register\" correct label spread probability mass uniformly classes better results obtained unigram smoothing distributes remaining probability mass proportionally marginal probability classes contribution propose neighborhood smoothing scheme uses temporal structure transcripts remaining probability mass assigned tokens neighboring transcript. intuitively smoothing scheme helps model recover beam search errors network likely make mistakes simply skip character transcript. repeated analysis softmax temperature beam search accuracy network trained neighborhood smoothing figure observe effects. first model regularized greedy decoding leads nearly percentage smaller error rate. second entropy network predictions higher allowing beam search discover good solutions without need temperature control. moreover since model trained evaluated didn’t control emission token. language model used wide beam searches often yield incomplete transcripts. narrow beams problem less visible implicit hypothesis pruning. illustrate failed decoding table ground truth least probable transcript according network language model. width beam search trigram language model ﬁnds second transcript misses beginning mance achieved deep sophisticated encoders table gathers results extended trigram language model. report averages runs. tuned beam search parameters validation applied test set. typical setup used beam width language model weight coverage weight coverage threshold best result surpasses ctcbased networks matches results dnn-hmm ensemble label smoothing proposed efﬁcient regularizer inception architecture several improved smoothing schemes proposed including sampling erroneous labels instead using ﬁxed distribution using marginal label probabilities using early errors model smoothing techniques increase entropy model’s predictions technique used promote exploration reinforcement learning label smoothing prevents saturating softmax nonlinearity results better gradient lower layers network similar concept training targets slightly range output nonlinearity proposed seqseq networks locally normalized i.e. speller produces probability distribution every step. alternatively normalization performed globally whole transcripts. discriminative training classical systems normalization performed lattices case recurrent networks lattices replaced beam search results. global normalization yielded important beneﬁts many tasks including parsing translation global normalization expensive training step requires running beam search inference. remains established whether globally normalized models approximated cheaper train locally normalized models proper regularization label smoothing. using source coverage vectors investigated neural machine translation models. past attentions vectors used auxiliary inputs emitting either directly set. models trained -dimensional mel-scale ﬁlterbanks extracted every form windows extended temporal ﬁrst second order differences per-speaker mean variance normalization. character consisted lowercase letters space apostrophe noise marker startendsequence tokens. comparison previously published results experiments involving language models used extended-vocabulary trigram language model built kaldi recipe framework compose language model \"spelling lexicon\" models implemented using tensorﬂow framework base conﬁguration implemented listener using bidirectional lstm layers units direction interleaved time-pooling layers resulted -fold reduction input sequence length approximately equating length hidden activations number characters transcript. speller single lstm layer units. input characters embedded dimensions. attention used hidden units previous attention weights accessed using convolutional ﬁlters spanning frames. lstm weights initialized uniformly range networks trained using asynchronous replica workers employing adam algorithm default parameters learning rate initially reduced training steps respectively. static gaussian weight noise standard deviation applied weight matrices training steps. also used small weight decay compared label smoothing methods unigram smoothing probability correct label neighborhood smoothing probability correct token remaining probability mass distributed symmetrically neighbors distance ratio. tuned smoothing parameters small grid search found good results obtained broad range settings. gathered results obtained without language models table used beam size mechanism promote longer sequences. report averages runs taken epoch lowest validation wer. label smoothing brings large error rate reduction nearly matching perforcumulative coverage information coverage embeddings vectors associated source words modiﬁed training proposed solution employs coverage penalty decode time similar used google translation system hinton deng dahl a.-r. mohamed jaitly senior vanhoucke nguyen sainath deep neural networks acoustic modeling speech recognition shared views four research groups ieee signal processing magazine vol. hannun case casper catanzaro diamos elsen prenger satheesh sengupta coates deep speech scaling end-to-end speech recognition arxiv preprint arxiv. amodei anubhai battenberg case casper catanzaro chen chrzanowski coates diamos deep speech end-to-end speech recognition english mandarin arxiv preprint arxiv. miao gowayyed metze eesen end-to-end speech recognition using deep models wfst-based decoding ieee workshop automatic speech recognition understanding schuster chen norouzi macherey krikun macherey klingner shah johnson kaiser gouws kato kudo kazawa stevens kurian patil wang young smith riesa rudnick vinyals corrado hughes dean google’s neural machine translation system bridging human machine translation corr vol. abs/. available http//arxiv.org/abs/. bahdanau chorowski serdyuk brakel bengio end-to-end attention-based large vocabulary speech recognition ieee international conference acoustics speech signal processing march gülçehre firat barrault bougares schwenk bengio using monolingual corpora neural machine translation corr vol. abs/. available http//arxiv.org/abs/. pereyra tucker chorowski kaiser hinton regularizing neural networks penalizing conﬁdent output distributions submitted iclr https//openreview.net/forum?id=hkcjniex. povey ghoshal boulianne burget glembek goel hannemann motlicek qian schwarz silovsky stemmer vesely kaldi speech recognition toolkit ieee workshop automatic speech recognition understanding. ieee signal processing society dec. ieee catalog cfpsrw-usb. allauzen riley schalkwyk skut mohri openfst general efﬁcient weighted ﬁnite-state transducer library proceedings ninth international conference implementation application automata ser. lecture notes computer science vol. springer http//www.openfst.org. abadi agarwal barham brevdo chen citro corrado davis dean devin tensorﬂow large-scale machine learning heterogeneous distributed systems arxiv preprint arxiv. mnih badia mirza graves lillicrap harley silver kavukcuoglu asynchronous methods deep reinforcement learning arxiv preprint arxiv. c.-c. chiu jaitly sutskever learning online alignments continuous rewards policy gradient arxiv preprint arxiv. andor alberti weiss severyn presta ganchev petrov collins globally normalized transition-based neural networks corr vol. abs/. available http//arxiv.org/abs/.", "year": 2016}