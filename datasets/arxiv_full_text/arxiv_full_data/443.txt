{"title": "Deep Active Learning for Dialogue Generation", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "We propose an online, end-to-end, neural generative conversational model for open-domain dialogue. It is trained using a unique combination of offline two-phase supervised learning and online human-in-the-loop active learning. While most existing research proposes offline supervision or hand-crafted reward functions for online reinforcement, we devise a novel interactive learning mechanism based on hamming-diverse beam search for response generation and one-character user-feedback at each step. Experiments show that our model inherently promotes the generation of semantically relevant and interesting responses, and can be used to train agents with customized personas, moods and conversational styles.", "text": "propose online end-to-end neural generative conversational model opendomain dialogue. trained using unique combination ofﬂine two-phase supervised learning online human-inthe-loop active learning. existing research proposes ofﬂine supervision hand-crafted reward functions online reinforcement devise novel interactive learning mechanism based hamming-diverse beam search response generation one-character userfeedback step. experiments show model inherently promotes generation semantically relevant interesting responses used train agents customized personas moods conversational styles. several recent works propose neural generative conversational agents open-domain task-oriented dialogue models typically lstm encoderdecoder architectures framework linguistically robust often generate short dull inconsistent responses researchers exploring deep reinforcement learning address hard problems dialogue generation. existing works reward function hand-crafted either speciﬁc task completed based desirable developer-deﬁned work demonstrate online deep active learning integrated standard neural network based dialogue systems enhance open-domain conversational skills. architectural backbone model seqseq framework initially undergoes ofﬂine supervised learning different types conversational datasets. initiate online active learning phase interact human users incremental model improvement unique single-character user-feedback mechanism used form reinforcement turn dialogue. intuition rely all-encompassing human-centric ‘reinforcement’ mechanism instead deﬁning hand-crafted reward functions individually capture many subtle conversational properties. mechanism inherently promotes interesting relevant responses relying humans’ superior conversational prowess. drl-based dialogue generation relatively research paradigm relevant work. task-speciﬁc dialogue reward function usually based task completion rate thus easy deﬁne. much harder problem open-domain dialogue generation handcrafted reward functions used capture desirable conversation properties. propose drl-based diversity-promoting beam search response generation. recently approaches proposed incorporate online human feedback neural conversation models work falls line research distinguished existing approaches following ways. online deep active learning form reinforcement novel eliminates need hand-crafted reward criteria. diversity-promoting decoding heuristic facilitate process. architectural backbone model seqseq framework consisting encoderdecoder layer containing lstm units. end-to-end model training consists ofﬂine supervised learning mini-batches followed online active learning phase cornell movie dialogs consisting message-response pairs. pair treated input target sequence during training joint cross-entropy loss function maximizes likelihood generating target sequence given input. phase phase enables learn language syntax semantics reasonably well difﬁculty carrying short-text conversations remarkably different movie conversations. combat issue curate dataset jabberwacky’s chatlogs available online. network initialized weights obtained ﬁrst phase trained usrmsg io.read; responses hammingdbs; io.write; feedback io.read; botmsg responses feedback; predxntloss model.forwrd; model.backward; model.updateparameters); ofﬂine equipped basic conversational ability responses still short dull. tackle issue initiate online process model interacts real users learns incrementally feedback turn dialogue. heuristic response generation recently proposed diverse beam search algorithm generate responses turn dialogue. shown outperform diverse decoding techniques several tasks including image captioning machine translation visual question generation. incorporates diversity beams maximizing objective consists standard sequence likelihood term dissimilarity metric beams. hamming diversity metric decoding time step penalizes selection words already chosen beams particular weight associated metric tuned aggressively promote diversity ﬁrst tokens generated sequences thereby avoiding similar beams like don’t know really don’t know. refer reader original paper vijayakumar complete algorithm derivation. tunable hyper-parameter; used experiments based observation smaller response usually misses good contender responses become cumbersome user read turn. possible displaying responses decreasing order generation likelihood introduces bias user’s response since users typically prefer pick items located screen. cause concern application problem resolved simply tweaking algorithm responses displayed user random order. experiments assume users unbiased take consideration display order generation likelihood responses. one-shot learning control quickly model learns user feedback tuning parameter ‘initial learning rate’ adam stochastic optimizer appropriately high results oneshot learning user’s feedback immediately becomes model’s likely prediction prompt. scenario depicted figure leads smaller gradient descent steps model requires several ‘nudges’ adapt data point. experiment different values determine suitable value evaluate model qualitative comparison ofﬂine well quantitative evaluation four axes syntactical coherence relevance prompts interestingness user engagement. ﬁrst asked human trainer actively train sl+oal using prompts choice. created test prompts randomly choosing training prompts linguistically rephrasing convey semantics. instance training prompts ‘how’s going?’ hate you’ ‘what favorite pizza toppings?’ altered following test prompts ‘how doing?’ don’t like you’ ‘what like pizza?’. next recorded sl’s sl’s sl+oal’s responses test prompts. finally asked human judges subjectively evaluate responses three models test set. evaluation response done four axes syntactical coherence relevance prompt interestingness user engagement. judge figure shows average percentage success three models sl+oal test prompts four axes syntactical coherence response relevance interestingness engagement. show percentage success sl+oal test prompts four axes adam’s learning rate varies number training interactions changes. model’s syntactical coherence response relevance interestingness user engagement. model’s percentage success test prompts recorded four axes. averaged scores given figure response quality drops signiﬁcantly higher values learning rate. instability parameters induced high learning value associated data causing model forget learned previously. experiments suggest learning rate strikes right finally asked human trainer train sl+oal different number training interactions. results figure conﬁrm model improves slowly continues converse humans. appropriate reﬂection humans learn language gradually effectively. although curves seem plateau training interactions suggest learning stopped case. gradient small nonzero expected behavior reinforcement learning algorithms general. illustrate qualitative differences responses generated sl+oal. table shows results small subset test prompts. generates relevant appropriate responses many cases. illustrates small shorttext conversational dataset useful ﬁne-tuning add-on large generic dialogue dataset ofﬂine seqseq training. also sl+oal generates interesting relevant engaging responses results imply model learns make connections semantically similar prompts syntactically different. slow process effectively emulates humans learn language. table illustrates sl+oal trained adopt wide variety moods conversational styles. here trained three copies separately adopt three different emotional personas cheerful gloomy rude. model underwent training interactions human trainer instructed adopt four conversation styles training sl+oal model. test prompts shown table syntactic variations training prompts before. results illustrate sl+oal able modify mood responses appropriately based trained. similar experiments done create agents customized backgrounds characters akin al.’s persona-based augments seqseq framework online deep active learning overcome known short-comings respect dialogue generation. experiments show model promotes semantically coherent relevant interesting responses trained adopt diverse moods personas conversation styles. future explore context-sensitive active learning encoder-decoder conversation models. also investigate whether existing affective computing techniques leveraged develop emotionally cognizant neural conversational agents.", "year": 2016}