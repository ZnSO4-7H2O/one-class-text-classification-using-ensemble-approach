{"title": "Asymptotic Learnability of Reinforcement Problems with Arbitrary  Dependence", "tag": ["cs.LG", "cs.AI"], "abstract": "We address the problem of reinforcement learning in which observations may exhibit an arbitrary form of stochastic dependence on past observations and actions. The task for an agent is to attain the best possible asymptotic reward where the true generating environment is unknown but belongs to a known countable family of environments. We find some sufficient conditions on the class of environments under which an agent exists which attains the best asymptotic reward for any environment in the class. We analyze how tight these conditions are and how they relate to different probabilistic assumptions known in reinforcement learning and related fields, such as Markov Decision Processes and mixing conditions.", "text": "address problem reinforcement learning observations exhibit arbitrary form stochastic dependence past observations actions. task agent attain best possible asymptotic reward true generating environment unknown belongs known countable family environments. suﬃcient conditions class environments agent exists attains best asymptotic reward environment class. analyze tight conditions relate diﬀerent probabilistic assumptions known reinforcement learning related ﬁelds markov decision processes mixing conditions. many real-world learning problems modelled agent interacts environment rewarded behavior. interested agents perform well sense high long-term reward also called value agent environment known pure computational problem determine optimal agent argmaxπv less clear optimal agent means unknown. reasonable objective single policy high value simultaneously many environments. formalize call criterion self-optimizing later. learning approaches reactive worlds. reinforcement learning sequential decision theory adaptive control theory active expert advice theories dealing problem. overlap diﬀerent core focus reinforcement learning algorithms developed learn directly value. temporal diﬀerence learning computationally eﬃcient slow asymptotic guarantees small observable mdps. others faster guarantee ﬁnite state mdps algorithms optimal ﬁnite connected pomdp apparently largest class environments considered. sequential decision theory bayes-optimal agent maximizes considered mixture environments class environments contains true environment policy self-optimizing arbitrary class provided allows self-optimizingness adaptive control theory considers simple special systems sometimes allow computationally data eﬃcient solutions. action expert advice constructs agent performs nearly well best agent class experts environment important special case passive sequence prediction arbitrary unknown environments actions=predictions aﬀect environment comparably easy diﬃculty active learning problems identiﬁed traps environments. initially agent know asymptotically forgiven taking initial wrong actions. well-studied class ergodic mdps guarantee that action history every state visited need characterize classes environments forgive. instance exact state recovery unnecessarily strong; suﬃcient able recover high rewards whatever states. further many real world problems information available states environment rather trying model environment identify conditions suﬃcient learning. towards propose consider environments which arbitrary ﬁnite sequence actions best value still achievable. performance criterion asymptotic average reward. thus consider environments exists policy whose asymptotic average reward exists upper-bounds asymptotic average reward policy. moreover property hold ﬁnite sequence actions taken property suﬃcient identifying optimal behavior. require that sequence actions possible return optimal level reward steps. environments possess property called value-stable. show countable class value-stable environments exists policy achieves best possible value environments class also show strong value-stability certain sense necessary. also consider examples environments possess strong value-stability. particular ergodic easily shown property. mixing-type condition implies value-stability also demonstrated. finally provide construction allowing build examples value-stable environments isomorphic ﬁnite pomdp thus demonstrating class value-stable environments quite general. important argument class environments seek self-optimizing policy countable although class value-stable environments uncountable. conditions necessary suﬃcient learning rely countability class open problem. however computational perspective countable classes suﬃciently large contents. paper organized follows. section introduces necessary notation agent framework. section deﬁne explain notion value-stability central paper. section presents theorem self-optimizing policies classes value-stable environments illustrates applicability theorem providing examples strongly value-stable environments. section discuss necessity conditions main theorem. section provides discussion results outlook future research. formal proof main theorem given appendix section contains intuitive explanations. strings probabilities. letters iklmn natural numbers denote cardinality sets write ﬁnite strings alphabet inﬁnite sequences. string length write xx...xn abbreviate xkxk+...xn−xn x...xn−. finally deﬁne xk..n +...+xn provided elements added. measure i.e. denote expectations w.r.t. i.e. probabilities expectations respect measures make notation explicit e.g. expectation respect measures called singular exists telligent) system cycle agent performs action yk∈y results observation reward followed cycle assume action space observation space reward space r⊂ir ﬁnite w.l.g. r={...rmax}. abbreviate =ykrkok∈z =y×r×o rkok r×o. agent identiﬁed policy given history probability agent acts cycle thereafter environment provides reward observation i.e. probability agent perceives note policy environment allowed depend complete history. make pomdp assumption here don’t talk states environment observations. pair generates sequence random variable probability first condition means strong large numbers rewards holds uniformly histories z<k; numbers thought expected rewards optimal policy. furthermore sequence actions possible recover reward loss; recover means reach level reward obtained optimal policy beginning taking optimal actions. suppose person made possibly suboptimal actions realized true environment optimally suppose person beginning taking optimal actions. want compare performance ﬁrst steps step environment strongly value stable catch except gain. numbers thought expected rewards catch reward loss probability latter depend past actions observations section present main self-optimizingness result along informal explanation proof illustrate applicability result examples classes value-stable environments. suppose environments deterministic. construct selfoptimizing policy follows ﬁrst environment algorithm assumes true environment tries ε-close optimal value called exploitation part. succeeds exploration follows. picks ﬁrst environment higher average asymptotic value tries ε-close value acting optimally close νe-optimal value true environment next environment picked exploration switches exploitation exploits ε′-close switches time trying ε′-close vνe; happen ﬁnite number times true environment thus exploration either found inconsistent since current history. next environment picked exploration. ﬁrst consistent environment picked exploitation turn happen ﬁnite number times true environment picked this algorithm still continues exploration attempts always keep within optimal probabilistic case somewhat complicated since whether environment consistent current history. instead test environment consistency follows. mixture environments observe together ﬁxed policy environment considered measure moreover shown ratio bounded away zero true environment tends zero singular exploration part algorithm ensures least environments singular current history succession tests used exclude environments consideration. next proposition provides conditions mixing rates sufﬁcient value-stability; intend provide sharp conditions mixing rates rather illustrate relation value-stability mixing conditions. ﬁrst term equals assumption second term shown summable using sequence uniformly bounded zeromean random variables satisfying strong α-mixing conditions following bound holds true integer mdps. applicability theorem proposition illustrated mdps. note self-optimizing policies classes ﬁnite ergodic mdps pomdps known present section show value-stability weaker requirement requirements models also illustrate applicability results. call markov decision process probability perceiving sequence random variables taking values ﬁnite space called state space depends independent given abusing notation sequence called underlying markov chain. pomdp called ergodic exists policy underlying markov chain visits state inﬁnitely often probability proof. denote true environment current history current state environment possible states. observe optimal policy depends current state. moreover policy optimal history. policy. expected reward step min{n xk+n b|xk ergodicity exists examples function constant decays exponentially fast. suggests class value-stable environments stretches beyond ﬁnite mdps. illustrate guess construction follows. example value-stable environment inﬁnitely armed bandit. next present construction environments modelled ﬁnite pomdps value-stable. consider following environment countable family arms sources generating i.i.d. rewards probability reward action space consists three actions ={gud}. next reward current agent action beginning current proof. supi∈in clearly probability policy policy which knowing probabilities achieves a.s. easily constructed. indeed sequence i=ij satisfying limj→∞δij policy carefully exploit arms staying long enough ensure average reward close expected reward probability quickly tends switching arms negligible impact average reward. thus shown explorable. moreover policy sketched made independent rewards. construction also allow action bring agent steps down number current environment according function thus changing function possibly making non-constant close desirable linear. also easy show uniformity convergence dropped. deﬁnition value-stability allow function depend additionally past history theorem hold. shown example constructed proof proposition indeed consider following simple class able easily withdrawn. environments. environment called passive observations rewards independent actions. sequence prediction task well-studied class passive environments task agent gets reward reward otherwise. clearly deterministic passive obviously class deterministic passive environments countable. since every policy environment errs exactly step also shown conditions certain sense tight. class value-stable environments includes ergodic mdps certain class ﬁnite pomdps passive environments environments. concept value-stability allows characterize self-optimizing environment classes proving value-stability typically much easier proving self-optimizingness directly. perspective classes suﬃciently large hand countability excludes continuously parameterized families common statistical practice. perhaps main open problem conditions requirement countability class lifted. ideally would like necessary suﬃcient conditions class environments satisfy condition admits self-optimizing policy. another question concerns uniformity forgetfulness environment. currently deﬁnition value-stability function histories actions histories observations-rewards histories x<k. probably possible diﬀerentiate types forgetfulness actions perceptions. particular countable class passive environments learnable suggesting uniform forgetfulness perceptions necessary. self-optimizing policy constructed follows. step polices exploits explores; policy either takes action according =pt) according =pe) speciﬁed below. policy deﬁned step environment endowed policy considered measure assume meaning environments measures actions-observations. also environment sequence real numbers deﬁne ﬁrst environment index greater case impossible increment deﬁne again. increment deﬁne ﬁrst environment index greater environment exists. otherwise proceed step again. increment consistency. step deﬁne deﬁne increment iterate inﬁnite loop. exploration part iterated inﬁnitely often νtνe observe implies νe-probability breaks greater ϕνe; hence borel-cantelli lemma event breaks inﬁnitely often probability suppose holds almost every time. broken except ﬁnite number times. show probability least −ϕνt +ε/. using borel-cantelli lemma obtain event breaks inﬁnitely often probability thus environments singular respect true environment given described policy current history. denote environment known measures mutually singular since changed exploration phase implies step excluded according consistency condition contradicts assumption. thus exploration part iterated ﬁnite number times bounded away either true environment environment equivalent current history. latter follows fact submartingale bounded expectation hence submartingale convergence theorem converges ν-probability show step always selected consider environment step excluded since optimal sequence equal actions measures singular. point happens suﬃcient number times excluded exploration part algorithm decremented included finally attained policy algorithm consequently excluded", "year": 2006}