{"title": "Towards Understanding Generalization of Deep Learning: Perspective of  Loss Landscapes", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "It is widely observed that deep learning models with learned parameters generalize well, even with much more model parameters than the number of training samples. We systematically investigate the underlying reasons why deep neural networks often generalize well, and reveal the difference between the minima (with the same training error) that generalize well and those they don't. We show that it is the characteristics the landscape of the loss function that explains the good generalization capability. For the landscape of loss function for deep networks, the volume of basin of attraction of good minima dominates over that of poor minima, which guarantees optimization methods with random initialization to converge to good minima. We theoretically justify our findings through analyzing 2-layer neural networks; and show that the low-complexity solutions have a small norm of Hessian matrix with respect to model parameters. For deeper networks, extensive numerical evidence helps to support our arguments.", "text": "widely observed deep learning models learned parameters generalize well even much model parameters number training samples. systematically investigate underlying reasons deep neural networks often generalize well reveal difference minima generalize well don’t. show characteristics landscape loss function explains good generalization capability. landscape loss function deep networks volume basin attraction good minima dominates poor minima guarantees optimization methods random initialization converge good minima. theoretically justify ﬁndings analyzing -layer neural networks; show low-complexity solutions small norm hessian matrix respect model parameters. deeper networks extensive numerical evidence helps support arguments. recently deep learning achieved remarkable success various application areas. spite powerful modeling capability know little deep learning works well theoretical perspective. widely known black-box nature deep learning. observation that deep neural networks learned parameters often generalize well empirically even equipped much effective parameters number training samples i.e. high-capacity. according conventional statistical learning theory rademacher complexity measure over-parameterized non-convex models system easy stuck local minima generalize badly. regularizations required control generalization error. however shown high-capacity neural networks without regularization still obtain complexity solutions generalize well; suitable regularization helps improve test error small margin. thus statistical learning theory cannot explain generalization ability deep learning models. worthy noting call solutions small training error good signiﬁcant difference generalization performance i.e. test accuracy. take task mnist digit classiﬁcation example training accuracy curious striking difference minima achieving test accuracy ones like random guess rather small difference solutions test accuracy. solutions rarely observed normal training procedures intentionally adding attacking data original training set. best knowledge ﬁrst time solutions accessible section directly provides possiblity considering difference good solutions. work answer crucial questions towards understanding generalization deep learning provide reasonable explanation questions. ﬁrst local minima large volume attractors often lead good generalization performance theoretically studied section hessian-based analysis proposed quantifying volume attractor based extensive numerical evidence reveals important relationship generalization performance volume basin attractor. second question several factors investigated systematically might endow neural networks powerful generalization capability including initialization optimization characteristics landscapes objective functions. extensive theoretical empirical justiﬁcations exclude effects optimization illustrated section conjecture characteristics landscapes loss functions automatically guarantees optimization methods random initialization converge good minima almost surely section property irrelevant types optimization methods adopted training. ﬁndings dramatically differ traditional understanding generalization neural network attribute particular optimizers certain regularizing techniques factors explain small difference good solutions rather signiﬁcant difference good minima. conjecture mystery small generalization error special structure neural networks. justiﬁed theoretically analyzing landscape -layer neural network shown section different approaches employed discuss generalization neural networks. works explored implicit regularization property sgd. another perspective relies geometry loss function around global minimum. argues solutions generalize well valleys ones always located sharp ravine. observation dates back early works recently adopts perspective explain small-batch often converges solution generalizing better large-batch sgd; authors proposed controllable noise bias solutions. similar works path also found discrete networks considered. however existing research considered small difference good solutions addressing issues described previously. several numerical experiments authors suggest explicit implicit regularizers well tuned could help reduce generalization error small margin. however away essence interpreting generalization ability. denotes training i.i.d. samples loss function; denotes whole hypothesis space hypothesis often parameterized deep neural networks. according central limit theorem generalization error also related minimum description length theory deep learning remp always trained ignorable threshold complexity candidate solutions determine generalization error according bound intuition this fully connected neural networks different number layers three-order polynomial comparison also conduct experiment using kernel ridge regression model. experiment training consists points. neural networks regularizers removed; regularization parameter result summarized figure easily observe figure ﬁtting results data points using fnns different number layers; overﬁtting solution high complexity intentionally constructed. ﬁtting results kernel regression different orders polynomial kernels. optimizer converge solutions complexity fnns different number layers. especially -layer network still generalizes well even parameters much larger size training set. maybe thinks hypothesis space complicated imagine. however true; many high-complexity solutions shown dash line figure overﬁtting solution ﬁgure found attacking training intentionally section details. hand inevitably produces overﬁtting solutions increasing capacity. control complexity solutions models resort regularization. term complexity measure complexity whole hypothesis class dimension rademacher complexity etc. spite similarity bound emphasize bound universal whole hypothesis space even holds worst solution. therefore surprising complexity leads trivial upper bound however practice really care complexity speciﬁc solutions found optimizers worst case. shown figure section high-capacity deep neural networks solution |remp small enough.} consists many solutions diverse generalization performance even generalize better random guess. surprisingly optimizers comparison traditional random initialization rarely converge solutions basis expansion methods nice property converging low-complexity solutions thus conventional learning theory cannot answer question point understand generalization capability high-capacity deep learning models boils ﬁguring mechanism guides optimizers converge towards low-complexity solution areas rather pursue tighter bounds complexity hypothesis class recalling optimization dynamics exists three factors solution might endow deep learning models good generalization performance stochasticity introduced mini-batch approximation; speciﬁc initialization; special structure landscape remp. systematical investigation three factors central ﬁnding summarized follows geometrical structure loss function deep neural networks guides optimizers converge low-complexity solutions volume basin good minina dominates poor ones stochastic approximation gradient originally proposed overcome computational bottleneck classic gradient descent. seems natural imagine noise introduced stochastic approximation help system escape saddle points local minima. also researchers suggest acts implicit regularizer guiding system converge small-norm/better solutions evaluate observations trained three networks batch gradient descent mnist dataset. ﬁrst images training limited evaluation full batch gradient. architectures networks described supplementary materials. shown table models trained perform better full batch gradient descent similar observed results however improvement limited. thus conclude alone cannot explain good generalization capability deep neural networks since generalization consider focuses signiﬁcant difference good solutions test accuracy poor ones better random guess. describe characteristics loss landscape borrow concept basin attraction dynamical systems region initial point region eventually iterated attractor denoted bgoodbbad basins attractor agood abad w.r.t. optimization dynamics −∇θremp respectively. empirical observation random initialization converges agood indicates choose uniform distribution denotes volume basin attractor. terms lebesgue measure basin minima zero measure set. according random initialization parameters located basin good minima agood overwhelming probability. consequently optimizers converge solutions generalizing well almost surely. conjecture ratio reasonable answer question section empirically demonstrate random initialization result convergence good solutions. numerically tried different strategies random initialization proposed i.e. fanin number inputs node. data augmentation regularization applied experiment. network method random initialization experiments times. results reported table easily observed that strategies random initialization cases converge good solutions. partially supports conjecture random initialization induces starting parameters almost surely located basin good minima. table training/test accuracies models trained various initialization strategies. experiment preprocess data channel-wise normalization train optimizer ﬁxed number epochs lenet resnet- landscape -layer networks section analyze landscape -layer networks show low-complexity solutions indeed located regions large volume attractor basin. hypothesis represented -layer network written number hidden nodes denotes activation function denotes parameters. assume least square loss used ﬁtting hessian matrix remp decomposed terms fisher information ﬁtting residual fisher information matrix w.r.t model parameters cauchy-schwarz inequality obtain following theorem relate complexity hypothesis fisher information matrix w.r.t. model parameters supplementary materials proof. theorem expressed -layer network according theorem establishes relationship hypothesis complexity measured norm expected input gradient fisher information matrix. additionally latter also related hessian remp according thus following characterization landscape remp. corollary expressed -layer network upper bound reveals remarkable properties landscape remp. ignore last term number training samples large enough compared var∇xf relu networks complexity small-norm hypothesis bounded frobenius norm hessian cremp. general activation functions true hypotheses small training error i.e. remp small enough. without loss generality constraint small. since invariant node-scaling i.e. constraint doesn’t shrink hypothesis space. means hypothesis least corresponding bound implies low-complexity solutions areas small hessian. indicates low-complexity solutions located large basins attractor highcomplexity solutions sharp small ones. therefore random initialization tends produce starting parameters located basin good minima high probability giving rise almost sure convergence good minima using gradient-based methods. practice explicitly impose constraint randomly initialize system close zero implicitly results optimizer exploring landscape small vicinity zero. within area high-complexity minima generalizing like random guessing much smaller attractor basins. therefore empirically never observe optimizers converge solutions even though exist area. authors argued property hessian cannot directly applied explain generalization. reason argument although e∇xf invariant node-scaling θremp not. however cases neural networks learned solutions hessian close zero near-zero random initialization thus term therefore cremp reasonable apply property hessian explain generalization ability. theoretical analysis sheds light difference minima generalize well answering question raised section part provides rough analysis hypothesisdependent generalization -layer neural networks. deeper networks elaborated analysis left future work. numerically demonstrate property described bound need large number minima diverse generalization ability particularly including solutions perform nearly random guesses test set. however easy relying training since random initialization always converges solutions generalizing well overcome difﬁculty design extra attack dataset fool networks produce generalization performance. model prepare three datasets strainstestsattack representing training test attack respectively. data points attack intentionally assigned wrong labels. solve following optimization problem instead original high capacity over-parameterized neural networks obtain various solutions achieving test harmed severely. practice tuning hyperparameter size attack series solutions rtrain found generalization error bad. spectral analysis hessian matrices since volume attractor basin global quantity hard estimate directly. fortunately large basin often implies local valley around attractor vice versa. similar ideas also explored however numerical experiments consider solutions agood. investigating difference agood abad understand optimizers random initialization rarely converges abad. figure eigenvalues distribution model test accuracy; top-k eigenvalues three solutions training accuracy model used mlenet dataset mnist. experiment ﬁrst training data selected training rest training data attack set. model initialized figure shows example spectrum hessian small around minima. following observations unique model shared across different models datasets negative eigenvalues since optimizer terminated converges strict minima. eigenvalues concentrate around zero. accordance work good solutions form connected manifold conjecture that large amount zero eigenvalues might imply dimension manifold large eigenvectors zero eigenvalues span tangent space attractor manifold. eigenvectors large eigenvalues correspond directions away attractor manifold. leave justiﬁcation conjecture future work. right plot shows solutions much larger eigenvalues good ones. indicates good solutions wide valley solutions narrow one. consistent analysis -layer networks. based analysis spectrum hessian natural product top-k positive eigenvalues quantify inverse volume attractor. given hessian matrix small neural networks train dozens mlenets datasets mnist svhn. experiment ﬁrst training data used training training data used attack help generate diverse solutions according different optimizers adopted increase diversity solutions. large neural networks resnet- used cifar- dataset ﬁrst samples training selected training data remaining attack set. performance evaluated whole test set. regularization data augmentation used. model million parameters much larger number training prohibitive cost compute spectrum hessian employ statistical estimate frobenius norm hessian although perfect computationally feasible large networks. matrix rq×p frobenius norm estimated therefore practice replacing matrix-vector product ﬁnite difference following estimator figure test accuracies versus volume basin bubbles represent solutions found optimizers without attacking term. solutions initialized trained achieve accuracy training set. good minima located large valley ones small valley. ratio vol/vol estimation exponentially small high dimensionality. evidently supports ﬁndings volume basin good minima dominates generalizing poorly leading optimization methods random initialization converge good solutions almost surely. exists variance relationship generalization error volume basins. also almost impossible distinguish minima equivalent generalization performance. conjecture reason might hessian-based characterization volume basin rough estimate. better non-local quantity necessary. work attempt answer important questions towards understanding generalization deep learning difference minima generalize well poorly; training methods converge good minima overwhelming probability. -layer networks analyzed show low-complexity solutions small norm hessian matrix w.r.t. model parameters. directly reveals difference good minima. also investigate property deeper neural networks various numerical experiments though theoretical justiﬁcation still challenge. property hessian implies volume basin good minima dominates poor ones leading almost sure convergence good solutions demonstrated various empirical results.", "year": 2017}