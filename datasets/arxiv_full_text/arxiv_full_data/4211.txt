{"title": "Non-linear Convolution Filters for CNN-based Learning", "tag": ["cs.CV", "cs.AI"], "abstract": "During the last years, Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance in image classification. Their architectures have largely drawn inspiration by models of the primate visual system. However, while recent research results of neuroscience prove the existence of non-linear operations in the response of complex visual cells, little effort has been devoted to extend the convolution technique to non-linear forms. Typical convolutional layers are linear systems, hence their expressiveness is limited. To overcome this, various non-linearities have been used as activation functions inside CNNs, while also many pooling strategies have been applied. We address the issue of developing a convolution method in the context of a computational model of the visual cortex, exploring quadratic forms through the Volterra kernels. Such forms, constituting a more rich function space, are used as approximations of the response profile of visual cells. Our proposed second-order convolution is tested on CIFAR-10 and CIFAR-100. We show that a network which combines linear and non-linear filters in its convolutional layers, can outperform networks that use standard linear filters with the same architecture, yielding results competitive with the state-of-the-art on these datasets.", "text": "georgios zoumpourlis alexandros doumanoglou nicholas vretos petros daras information technologies institute center research technology hellas greece last years convolutional neural networks achieved state-of-the-art performance image classiﬁcation. architectures largely drawn inspiration models primate visual system. however recent research results neuroscience prove existence non-linear operations response complex visual cells little effort devoted extend convolution technique non-linear forms. typical convolutional layers linear systems hence expressiveness limited. overcome this various non-linearities used activation functions inside cnns also many pooling strategies applied. address issue developing convolution method context computational model visual cortex exploring quadratic forms volterra kernels. forms constituting rich function space used approximations response proﬁle visual cells. proposed second-order convolution tested cifar- cifar-. show network combines linear non-linear ﬁlters convolutional layers outperform networks standard linear ﬁlters architecture yielding results competitive state-of-the-art datasets. convolutional neural networks shown achieve state-of-the-art results various computer vision tasks image classiﬁcation. architectures largely drawn inspiration models primate visual system described hubel wiesel notion convolution used mimic functional aspect neurons visual cortex critical understand success. linear nature lack ability expressing possible non-linearities actually appear response complex cells primary visual cortex hence claim expressiveness limited. overcome this various non-linearities used activation functions inside cnns also many pooling strategies applied. little effort devoted explore computational models extend convolution technique non-linear forms taking advantage research results neuroscience prove existence nonlinear operations response visual cells complexity human visual cortex demonstrates gaps need bridged cnns regarding convolution operations applied. gaps exploration higher-order models. work study possibility adopting alternative convolution scheme increase learning capacity cnns applying volterra’s theory used study non-linear physiological systems adapting spatial domain. considering convolution operation instead summing linear terms compute ﬁlter’s response data patch propose also non-linear terms produced multiplicative interactions pairs elements input data patch. transforming inputs second-order form making separable. convolution ﬁlters rich properties terms selectivity invariance created. section related work outlined. section proposed method described theoretically grounded volterra’s computational method concept training mathematically explained description scheme’s practical implementation given section section experimental results cifar- cifar- datasets drawn ﬁnally section paper concluded. ﬁrst biologically-inspired neural networks fukushima’s neocognitron predecessor introduced lecun convolutional layer core building block cnn. early implementations cnns used predeﬁned gabor ﬁlters convolutional layers. category ﬁlters model quite accurately properties simple cells found primary visual cortex type visual cell response proﬁle characterized spatial summation within receptive ﬁeld. finding optimal spatial stimuli simple cells process based spatial arrangement excitatory inhibitory regions however hold true complex visual cells. also cannot obtain accurate description properties ﬁnding optimal stimulus. fact ignored implementations settled linear type convolution ﬁlters trying apply quantitative rather qualitative changes functionalities. proposed residual networks shortcut connections parallel normal convolutional layers solution problems vanishing/exploding gradient hard optimization increasing model’s parameters zagoruyko komodakis showed wide resnets outperform resnets hundrends layers shifting interest increasing number layer’s ﬁlters. alternatively works focus creating networks convolutional layers ﬁlters evaluate impact using non-linear linear terms approximations convolution kernels boost performance cnns. apart resnets error rates also achieved imagenet challenge methods used convolutional layers ways enhancing representation ability. proposed network network remedy level abstraction typical ﬁlters present. instead conventional convolution ﬁlter generalized linear model build micro neural networks complex structures abstract data within receptive ﬁeld. input data output multilayer perceptrons non-linear function approximator call mlpconv layer. output feature maps obtained sliding micro networks input similar manner cnn. szegedy introduced level organization form inception module uses ﬁlters variable sizes capture different visual patterns different sizes approximates optimal sparse structure. proposed exploit split-transform-merge strategy inception models performing transformations lowdimensional embedding whose outputs aggregated summation. authors based abundancy recurrent synapses brain proposed recurrent neural network image classiﬁcation. proved inserting recurrent connections within convolutional layers gives boosted results compared feed-forward architecture. work biologically plausible incorporation mechanisms originating neuroscience cnns. boltzmann learning algorithm proposed feature interactions used turn hidden units higher-order feature detectors. efﬁcient method apply learning algorithms higher-order boltzmann machines proposed making computationally tractable real problems. bergstra created model neural activation showed improved generalization datasets incorporating second-order interactions using alternative non-linearity activation function. attempt made analyze interpret study quadratic forms receptive ﬁelds. found quadratic forms used model non-linear receptive ﬁelds fact follow properties complex cells primary visual cortex. properties include response edges phase-shift invariance direction selectivity non-orthogonal inhibition end-inhibition side-inhibition. constrast standard linear forms quadratic non-linear forms optimal stimuli provide complete description properties. shown invariances occure optimal stimulus general sub-optimal stimuli exist many invariances could large number lack easy interpretation. although optimal stimulus related ﬁlter’s invariance neighborhood studied loose sense transformation invariance. shown proper quadratic forms demonstrate invariances phase-shift orientation change. previous discussion conclude using non-linear forms convolutional layers reasonable future direction computer vision. weights ﬁlter’s second-order terms. avoid considering twice interaction terms pair input elements adopt uppertriangular form matrix containing weights number trainable parameters secondorder kernel generic type compute total number parameters volterra-based ﬁlter order proposed method earlier stated makes volterra kernel theory provide means exploiting non-linear operations take place receptive ﬁeld. best knowledge non-linearities exploited mainly activation functions pooling operations different layers cnns. nevertheless non-linearities approach code inner processes visual system ones exist receptive ﬁeld’s area. method follows typical workﬂow lining layers different purposes non-linear convolutional layer plugged practically existing architectures. nevertheless augmentation trainable parameters involved care taken complexity overall process. cuda implementation section also provided. volterra series model sequence approximations continuous functions developed represent input-output relationship non-linear dynamical systems using polynomial functional expansion. equations composed terms inﬁnite orders practical implementations based truncated versions retaining terms order similar linear convolution volterra-based convolution uses kernels ﬁlter input data. ﬁrstorder volterra kernel contains coefﬁcients ﬁlter’s linear part. second-order kernel represents coefﬁcients quadratic interactions input elements. general r-th order’s kernel represents weights non-linear interactions input elements response. ﬁeld computer vision volterra kernels previously used face recognition serving effectively approximations non-linear functionals. major difference proposed convolution scheme linear convolution fact case function dependent means that con∂xi trast standard ﬁlters term different every single patch feature resulting extra computational cost error must propagated preceding trainable layers network. cost proportionate height width layer’s output feature respectively. layer’s code available http//vcl.iti.gr/volterra. cifar- cifar- running experiments equipped intel nvidia titan gpu. volterra-based convolutional layer implemented torch. ﬁrst describe experimental setup show quantitative analysis terms parameters classiﬁcation error train loss proposed method. architecture selection explained section using proposed convolution multiple layers extra computational overhead introduced backpropagation. reason restrain testing ﬁlters ﬁrst convolutional layer model. choose modern architecture wide resnet mainly consists convolutional layer followed convolutional groups classiﬁer. network’s depth convolutional group contains convolutional blocks. group number convolutional layer’s ﬁlters controlled widening factor architecture follow rules making three changes insert batch normalization layer start network change number ﬁrst convolutional layer’s output channels change shortcut ﬁrst block ﬁrst group identity mapping consequence second change. ﬁrst change crucial prevent output volterra-based convolution exploding multiplicative interaction terms xixj. experiments parameter afﬁne transformation applied layer settles values second change chosen that volterra-based convolution applied ﬁrst convolutional layer enough non-linear ﬁlters learnt producing feature-rich signal. third derivation equations backward pass volterra-based convolution done adapting classic backpropagation scheme aforementioned inputoutput function train weights volterra kernels need compute gradients layer’s output respect weights propagate error compute gradients layer’s output respect inputs hence terms used opti∂wi mize weight parameters volterra-based convolutional layer minimize network loss. mathematical equations backpropagation follows order experiment non-linear convolution ﬁlters used torch scientiﬁc framework. volterrabased convolution implemented module integrated cuda backend neural network package torch. writing module torch mainly consists implementing module’s forward pass well computation module’s gradients used back-propagation. denote error deﬁned network’s criterion function refer layer’s weight gradient layer’s input gradient. implement forward pass cuda used standard imcol pattern unfold data patches columns followed matrix multiplication volterra-based ﬁlter weights. imcol operation conducted parallel cuda kernel matrix multiplication used well established cuda blas functions. subsequently computing weight gradients extent similar computing forward pass. again imcol operation executed input image cuda kernel output matrix multiplied previous layer’s input gradient resulting expensive operation volterra-based convolutional layer computation input gradients. already mentioned before contrast linear convolution input gradient independent provided input layer’s input gradient input-dependent. thus compute matrix input gradients ﬁrstly compute unfolded matrix containing gradients output respect input. matrix multiplied previous layer’s input gradient using cuda blas loss batch size training network epochs. dropout weight initialization done learning rate weight decay strategy used experiments shown table cifar- change done block’s input output channels equal shortcut identity mapping input added output without need adjust feature channels shortcut using convolutional layer. case signal ﬁrst convolutional layer ﬂows intact shortcuts ﬁrst group’s blocks. model used experiments described table evaluate impact applying volterra-based convolution dataset tested versions general model. ﬁrst version serves baseline model non-linear convolution ﬁlter. version contains non-linear ﬁlters ﬁrst convolutional layer linear ﬁlters convolutional groups network. pre-act resnet wide resnet pyramidnet wide-delugenet orthoreg wide resnet steerable cnns resnext wide resnet singular value bounding oriented response cifar- data-preprocessing operation applied train test set’s data subtracting channel means dividing channel standard deviations computed train set. apply moderate data augmentation using horizontal ﬂipping probability reﬂection-padding pixels image side taking random crop size cifar- cifar- datasets contain images commonly seen object categories train test images. cifar- classes cifar- classes. classes equal number train test samples. cifar- volterrabased wide resnet yields test error shows improvement error using baseline model setting state-of-the-art dataset. cifar- volterra-based wide resnet yields test error shows improvement error using baseline model. results cifar- outperformed huge number parameters model makes features convolutional groups extracted non-linear convolution ﬁlters make network avoid overﬁtting. inferred loss plot models cifar- shown figure baseline wide resnet although constantly lower loss volterra-based wide resnet yields higher test error. volterra-based wide resnets parameters baseline counterparts. summary best methods datasets provided table figure weight values linear convolution ﬁlter weights volterra-based convolution ﬁrst-order weights volterra-based convolution second-order weights manner. linear term process straightforward. second-order term considering weights ﬁlter create weight vectors reshaping vectors volterra-based ﬁlters wide resnet trained cifar partly characterize response proﬁles. given weights ﬁlter compute optimal stimulus optimal stimulus linear term constraint norms equal. then compute four responses described table plot figure comparing various responses infer properties linear ﬁlter weights greatly change it’s extended second-order volterra form adding weight quadratic contributions. response volterra-based ﬁlter quite different response ﬁrst-order terms proving second-order interactions contribute signiﬁcantly functionality quadratic ﬁlter. given weight subset volterra-based ﬁlter optimal stimulus standard pattern. norm takes values inside bounded space varies linear increase intensity values without altering general pattern however hold true quadratic ﬁlters. norm volterra-based second-order ﬁlter’s optimal stimulus takes values inside bounded space rich variety alterations observed elements figure various cases responses. line denotes response volterra-based convolution ﬁlter optimal stimulus dashed line denotes response linear subset volterra-based ﬁlter’s weights blue line denotes response linear subset volterra-based ﬁlter’s weights dashed blue line denotes response volterra-based convolution ﬁlter biologically-inspired computer vision. questions like which ideal ratio linear non-linear ﬁlters convolutional layer? which properties prevail response proﬁles layer’s non-linear ﬁlters? great importance shed light hitherto unexplored category ﬁlters. inference properties present group quadratic ﬁlters risk biased dataset used obtain observe them. happens visual response proﬁles non-linear ﬁlters trained experiments constrained natural statistics dataset happens sensory system primates adapts environment. based research results neuroscience prove existence non-linearities response proﬁles complex visual cells proposed non-linear convolution scheme used architectures. experiments showed network combines linear non-linear ﬁlters convolutional layers outperform networks standard linear ﬁlters architecture. reported error rates state-of-theart cifar- competitive state-of-theart results cifar-. didn’t apply volterrabased convolution layers target demonstrate proof concept proposed method. claim conﬁrmed replacing ﬁrst convolutional layer’s linear ﬁlters non-linear ones achieved lower error rates. testing quadratic convolution ﬁlters certainly interesting direction future work build better computer vision systems.", "year": 2017}