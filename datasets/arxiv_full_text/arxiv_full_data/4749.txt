{"title": "Ensemble Relational Learning based on Selective Propositionalization", "tag": ["cs.LG", "cs.AI"], "abstract": "Dealing with structured data needs the use of expressive representation formalisms that, however, puts the problem to deal with the computational complexity of the machine learning process. Furthermore, real world domains require tools able to manage their typical uncertainty. Many statistical relational learning approaches try to deal with these problems by combining the construction of relevant relational features with a probabilistic tool. When the combination is static (static propositionalization), the constructed features are considered as boolean features and used offline as input to a statistical learner; while, when the combination is dynamic (dynamic propositionalization), the feature construction and probabilistic tool are combined into a single process. In this paper we propose a selective propositionalization method that search the optimal set of relational features to be used by a probabilistic learner in order to minimize a loss function. The new propositionalization approach has been combined with the random subspace ensemble method. Experiments on real-world datasets shows the validity of the proposed method.", "text": "abstract. dealing structured data needs expressive representation formalisms that however puts problem deal computational complexity machine learning process. furthermore real world domains require tools able manage typical uncertainty. many statistical relational learning approaches deal problems combining construction relevant relational features probabilistic tool. combination static constructed features considered boolean features used oﬄine input statistical learner; while combination dynamic feature construction probabilistic tool combined single process. paper propose selective propositionalization method search optimal relational features used probabilistic learner order minimize loss function. propositionalization approach combined random subspace ensemble method. experiments real-world datasets shows validity proposed method. dealing relational domains requires expressive structured representation formalisms graphs ﬁrst-order logic already used area inductive logic programming furthermore real world domains require tools able manage typical uncertainty. frameworks systems able manage relational descriptions reason probabilistic emerged research area probabilistic inductive logic programming statistical relational learning learning approaches build models searching good relational features guided scoring function foil classical setting constructed features assumed strong constraints observations must fulﬁll. typically algorithms search horn clauses must cover subsume positive observations negative ones. order soft assumption many pilp systems feature construction process combined discriminative/generative probabilistic method order deal noisy data uncertainty kfoil markov logic networks combination static dynamic. former case named static propositionalization constructed features usually considered boolean features used oﬄine input propositional statistical learner; latter case named dynamic propositionalization feature construction probabilistic model selection combined single process search optimal relational features used probabilistic learner order minimize loss function. particular ﬁrst feature construction phase relevant features minimizing bayesian classiﬁer’s probability error stochastically searched. here improved formalization proposed approach evaluation area relational propositionalization ﬁrst investigate whether resulting proposed method valuable tool applied classical relational domains. furthermore paper propositionalization approach combined random subspace ensemble method trying improve generalization accuracy single base classiﬁer. experiments real-world datasets compared state systems show validity proposed methods. providing motivations related works next section section describes proposed selective propositionalization approach combination section provides qualitative validation approach. finally section concludes paper. traditional relational learning approaches dynamically generate features providing information observations interleaving feature construction model construction. tackle task inferring predictive discriminant features relational learning reformulate problem attributevalue form apply propositional learner nfoil kfoil examples dynamic propositionalization. differently static propositionalization ﬁrstly features generated parameters statistical learner estimated tightly integrates learning features statistical propositional learner. criterion according features generated statistical learner na¨ıve bayes case nfoil support vector machine kfoil. methods employ adaptation well-known foil algorithm implements separate-and-conquer rule learning algorithm. score well respect observations current hypothesis adds current hypothesis. feature greedily searched using generalto-speciﬁc hill-climbing search strategy. adaption algorithm case nfoil kfoil corresponds evaluate candidate features according probabilistic scoring function. approach however sensitive ordering selected candidate features determine choice following features. furthermore case na¨ıve bayes reported model suﬀer oversensitivity redundant and/or irrelevant attributes. even svms shown perform badly situation many irrelevant examples and/or features. since eﬀectiveness learning algorithms strongly depends used features feature selection task desirable. feature selection optimal subset input features leading high classiﬁcation performance generally carry classiﬁcation task optimally. however search variable subset np-hard problem. therefore optimal solution cannot guaranteed reached except performing exhaustive diﬀerently dynamic propositionalization ﬁrstly construct features adopt wrapper feature selection approach uses stochastic local search procedure embedding na¨ıve bayes classiﬁer select optimal subset features. optimal subset searched using greedy randomized search procedure search guided predictive power selected subset computed using na¨ıve bayes approach. relational examples characterized relational features {fi}m target discrete random variable generating class labels paper subset optimally characterizes variable minimizing classiﬁer’s probability error. section reports components lynx system ensemble extension lynx-rsm implementing probabilistic relational classiﬁer. speciﬁcally start report feature construction capability adopted relational feature-based classiﬁcation model already deﬁned case relational sequence learning approach generalized case relational learning extended case ensemble learning. relational feature construction. ﬁrst step lynx carries feature construction process mining frequent prolog queries adopting approach similar reported algorithm frequent relational query mining based idea generic level-wise search method performing breadth-ﬁrst search lattice queries ordered specialization relation algorithm starts general prolog queries. step tries specialize candidate frequent queries discarding non-frequent ones storing whose length less equal user speciﬁed input parameter. furthermore reﬁned query semantically equivalent patterns detected using θoi-subsumption relation discarded. specialization phase specialization operator basically adds atoms query. relational features need order correctly classify unseen examples. given training ci}n relational examples denotes discrete class random variables taking values goal learn function predicts label unseen observation. features obtained ﬁrst step lynx system example build d-component vector-valued random variable query subsumes example otherwise exactly corresponds propositionalization process. relational observations transformed propositional form classical statistical learner applied. classiﬁer said assign vector class taking maximum discriminant function corresponds maximum posteriori probability. minimum error rate classiﬁcation following discriminant function used feature selection stochastic local search. constructed features presented method features classify unseen sequences problem subset features optimizes prediction accuracy. optimization problem selecting subset features superior classiﬁcation performance formulated follows. constructed original features function scoring selected subset problem feature selection problem would require examining possible subsets feature making impractical even small values |p|. stochastic local search procedure allows obtain good solutions without explore whole solution space. reported feature based representation observation obtained using features initial optimization problem corresponds minimize expectation e=cj characteristic consider combinatorial optimization problem given discrete solutions objective function minimized seek solution method high-quality solutions combinatorial problem consists two-step approach made greedy construction phase followed perturbative local search greedy construction method starts process empty candidate solution construction step adds best ranked component according heuristic selection function. then perturbative local search algorithm searching local neighborhood used improve candidate solution thus obtained. advantages search method much better solution quality fewer perturbative improvement steps needed reach local optimum. grasp solves problem limited number diﬀerent candidate solutions generated greedy construction search method randomizing construction method. grasp iterative process combining iteration construction local search phase. construction phase feasible solution built neighbourhood explored local search. algorithm reports graspfs procedure included lynx system perform feature selection task. iteration computes solution using randomized constructive search procedure applies local search procedure yielding improved solution. main procedure made components constructive phase local search phase. constructive search algorithm used graspfs iteratively adds solution component randomly selecting according uniform distribution named restricted candidate list highly ranked solution components respect greedy function probabilistic component graspfs characterized random choice best candidates rcl. case greedy function corresponds error function errd previously reported particular given errd heuristic function feasible solutions min{errd|s max{errd|s computed. deﬁned including components errd improve solution generated construction phase local search used works iteratively replacing current solution better solution taken neighborhood current solution better solution exists. given patterns order build neighborhood neigh solution following operators exploited particular given solution elements neighborhood neigh solutions obtained applying elementary modiﬁcation local search starts initial solution iteratively generates series improving solutions k-th iteration neigh searched improved solution errd errd. solution found becomes current solution. otherwise search ends local optimum. iteration given solution added ordered solutions algorithm return ensemble learning. combining predictions multiple classiﬁers known ensemble learning standard important technique improving classiﬁcation accuracy machine learning. bagging boosting works sampling training observations techniques investigate performance classiﬁer ensembles trained using attribute subsets selecting optimal subsets relevant features plays important role. popular ensemble method random subspace method whose idea sample feature classiﬁer ensemble. ensemble operates taking majority vote predeﬁned number classiﬁers. assuming observation deﬁned p-dimensional vector described features. randomly selects features p-dimensional data obtaining r-dimensional random subspace original p-dimensional feature space. given random subspaces classiﬁer learned approach used construct ensemble slightly diﬀerent form original one. given constructed relational feature iteration feature selection algorithm gives random subspace considered build classiﬁcation model. particular avoid parameter since change iteration graspfs procedure governed stopping condition. hence ﬁxing size ensemble algorithm reports procedure used build ensemble. ﬁrstly build relational features call graspfs procedure order obtain ordered random subspaces. train classiﬁer subspaces classify observation combining predictions individual classiﬁers combining posterior probabilities classiﬁer. mutagenesis dataset regards problem predict mutagenicity molecules based chemical structure. molecules positive mutagenicity whereas molecules zero negative mutagenicity. molecules positive mutagenicity labeled active remaining labeled inactive. used atom bond information only. alzheimer dataset compare analogues tacrine drug alzheimers disease according four desirable properties inhibit amine re-uptake toxicity high acetyl cholinesterase inhibition good reversal scopolamine-induced memory deﬁciency. examples consist pairs analogues labeled positive ﬁrst rated higher second regard property interest. uw-cse dataset widely used regards department computer science engineering university washington describing relationships among professors students courses publications true ground atoms predicates. task predict relationship advisedby mutagenesis alzheimer datasets compared proposed approach nfoil kfoil uw-cse dataset compared lynx systems learning markov logic networks speciﬁcally mlns important representation formalism combining expressiveness ﬁrst-order logic robustness probabilistic representations. weighted ﬁrst-order formulas learning structure consists learning formulas weights. state mlns structure learning algorithms. experiments maximum length parameter relational features learned lynx feature selection grasp procedure iterated times. table reports experimental results -fold cross validation mutagenesis alzheimer datasets obtained lynx compared nfoil kfoil nfoil kfoil used parameters reported ﬁrst column reports accuracy obtained lynx greater compared obtained nfoil kfoil. second column reports accuracy lynx-rsm obtained ensemble classiﬁers mutagenesis dataset classiﬁers alzheimer dataset always greater obtained lynx single base classiﬁer. ﬁrst results conﬁrms improvements obtained ensemble approach. second experiment compared proposed approach respect classical formalism. table reports area precisionrecall curves lynx ensemble extension. results lynx-rsm obtained using ensemble classiﬁers better obtained single classiﬁer already conﬁrming validity proposed approach. finally table reports results proposed approach compared obtained systems. used parameters reported lynx lynx-rsm outperform thus conﬁrming validity. dealing structured data needs expressive representation formalisms that however puts problem deal computational complexity machine learning process. furthermore real world domains require tools able manage typical uncertainty. paper proposed selective propositionalization method search optimal relational features used probabilistic learner order minimize loss function. propositionalization approach combined random subspace ensemble method. experimental results real-world datasets shows validity proposed method compared approaches.", "year": 2013}