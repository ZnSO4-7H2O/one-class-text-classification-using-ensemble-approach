{"title": "Evaluating approaches for supervised semantic labeling", "tag": ["cs.LG", "cs.AI", "cs.CL"], "abstract": "Relational data sources are still one of the most popular ways to store enterprise or Web data, however, the issue with relational schema is the lack of a well-defined semantic description. A common ontology provides a way to represent the meaning of a relational schema and can facilitate the integration of heterogeneous data sources within a domain. Semantic labeling is achieved by mapping attributes from the data sources to the classes and properties in the ontology. We formulate this problem as a multi-class classification problem where previously labeled data sources are used to learn rules for labeling new data sources. The majority of existing approaches for semantic labeling have focused on data integration challenges such as naming conflicts and semantic heterogeneity. In addition, machine learning approaches typically have issues around class imbalance, lack of labeled instances and relative importance of attributes. To address these issues, we develop a new machine learning model with engineered features as well as two deep learning models which do not require extensive feature engineering. We evaluate our new approaches with the state-of-the-art.", "text": "relational data sources still popular ways store enterprise data however issue relational schema lack well-deﬁned semantic description. common ontology provides represent meaning relational schema facilitate integration heterogeneous data sources within domain. semantic labeling achieved mapping attributes data sources classes properties ontology. formulate problem multi-class classiﬁcation problem previously labeled data sources used learn rules labeling data sources. majority existing approaches semantic labeling focused data integration challenges naming conﬂicts semantic heterogeneity. addition machine learning approaches typically issues around class imbalance lack labeled instances relative importance attributes. address issues develop machine learning model engineered features well deep learning models require extensive feature engineering. evaluate approaches state-of-the-art. important problem database research determining combine multiple data sources described diﬀerent schemata outcome process expected uniform integrated view across data sources. relational data sources still popular ways store enterprise data however relational schema lacks well-deﬁned semantic description. deﬁne semantics data introduce ontology goal attributes relational data sources classes properties ontology. refer problem semantic labeling. semantic labeling plays important role data integration augmenting existing knowledge bases mapping relational sources ontologies various approaches automate semantic labeling developed including typically automated semantic labeling techniques encounter several problems. firstly naming conﬂicts including cases users represent data diﬀerent ways. secondly semantically diﬀerent attributes might syntactically similar content example birth date versus date death. thirdly considerable number attributes corresponding property ontology either accident purpose. majority existing systems focus ﬁrst problems consider third problem evaluation address challenges automated semantic labeling formulate task supervised classiﬁcation problem. semantic labels known classiﬁer speciﬁed training time e.g. provided domain ontology. also introduce special class attributes called unknown. purpose unknown class capture attributes mapped ontology. training data classiﬁer thus consist source attributes semantic labels provided user including unknown labels. since manually assigning labels attributes costly operation lack training data common problem semantic labeling systems. existing systems knowledge transfer techniques overcome issue. instead introduce sampling method similar bagging ensemble models bagging technique allows generate multiple training instances userlabeled attributes thus overcoming lack labeled training data. also allows overcome common issue class imbalance semantic labels support others among attributes. achieve re-balancing training data preferential bagging minority class attributes. introduce bagging approach handle class imbalance lack training data drawing random subsamples values attribute. approach achieve meaningful diversity training data increase number training instances under-represented semantic labels. address issue unwanted attributes i.e. attributes mapped element ontology. cases suﬃcient amount training data models achieve mean reciprocal rank sets data sources benchmark. construct classiﬁcation model dint hand-engineered semantic labeling features implement above. addition design deep learning models simple features normalized character frequencies padded character sequences extracted values data attributes. construct benchmark common evaluation strategy compare diﬀerent approaches supervised semantic labeling. benchmark includes models dint state-of-the-art sets data sources diﬀerent domains. show approach strengths shortcomings choosing particular semantic labeling system depends case. released implementation benchmark open source license benchmark easily extended include models datasets used choose appropriate model given case. illustrate semantic labeling problem using simple domain ontology shown fig. assume three data sources personal-info businessinfo employees whose attributes choose label according example ontology deﬁne semantic label tuple consisting domain class property. example attribute name source personal-info labeled note semantic labels ﬁxed ontology. task semantic labeling deﬁned automatically assigning semantic labels attributes data source. case supervised semantic labeling existing known semantic labels data sources improve performance assigning semantic labels sources. example assume given sources personal-info businessinfo correct semantic labels system automatically assign labels attributes source employees. build system cannot rely names columns. example columns name employee refer property using values columns also problematic. example acronyms used states state names fully written. furthermore values overlap semantically heterogeneous columns like founded birthdate also attributes mapped property ontology. might reasons existence interested content attribute want discard future analysis; might overlooked attribute designing ontology accurately. diﬀerentiate cases mark attributes unknown class example founded presence unknown class makes task semantic labeling complicated. establishing approaches eﬃciently handle attributes crucial since many real-world scenarios relational data sources domain speciﬁc data contain considerable number attributes. machine learning techniques proved eﬃcient building predictive models noisy messy data. apply techniques need represent source attributes feature vectors semantic labels attached vectors. table show representation source employees. explicitly shown possible features simplicity. however actual size feature vector arbitrary long process designing components known feature engineering. next section describe classiﬁers semantic labeling problem used evaluation. also discuss approaches problem unknown attributes lack training data. labeled data sources construct feature vectors attributes mark representatives class corresponding semantic labels. constructed pairs used train classiﬁer. consider several approaches divided major groups dint deep learning state-of-the-art dsl. approach trains multi-class classiﬁcation model produces prediction stage list class probabilities attribute source. class highest predicted probability assigned attribute decision stage. dint ﬁrst approach dint hand-engineer features include characteristics number whitespaces special characters statistics values column many more. complete list features available open source benchmark repository important features characterising information content attribute shannon’s entropy attribute’s concatenated rows. shannon’s entropy finally also -dimensional vector attribute feature vector. addition features directly calculated attribute values compute mean cosine similarity attribute character distribution character distributions class instances. adds many additional scalar features full attribute feature vector classes training data. expect names attributes also contain useful information determine semantic types addition information provided attribute values. extract features attribute names compute string similarity metrics minimum edit distance wordnet based similarity measures k-nearest neighbors using needle-wunsch distance minimum edit distance strings minimum number edit operations insertion deletion substitution required transform string another compute similarity attribute name class instances training data. number thus extracted features depends number semantic labels training data. choose train random forest features. quite robust noisy data works well even correlated features easily captures complex nonlinear relationships features target. additionally classiﬁers require little hyperparameter tuning hence usually work straight makes convenient versatile classiﬁer use. deep learning deep learning gained much popularity tremendous impact areas speech recognition object recognition machine translation biggest advantages deep learning ability process data form discover representation needed classiﬁcation assisting feature engineering step. broadly speaking deep learning overarching term artiﬁcial neural networks word deep refers depth network. basic level neural networks composed perceptrons neural nodes. several layers interconnected neural nodes; ﬁrst layer input layer last output layer. layers called hidden. neural nodes layer take input output nodes previous layer perform computation nonlinear activation function pass result next layer. generally connections nodes layer. overall deep learning models improve performance data trained exact architecture deep learning models i.e. number layers number nodes layer activation functions neurons interconnectedness layers inﬂuence performance trained models. choose diﬀerent architectures deep learning classiﬁers multi-layer perceptron convolutional neural network experimented diﬀerent designs networks varying hyperparameters control number hidden layers numbers nodes/ﬁlters layer dropout probability etc. found designs described brieﬂy below work well datasets benchmark. input layer architecture takes -dimensional feature vector character frequencies shannon entropy. following input layer fully connected hidden layers nodes layer tanh activations. hidden layer introduced stochastic dropout layer dropout probability prevent overﬁtting. finally output layer softmax layer number nodes equal number semantic types model takes input one-hot representation attribute’s concatenated rows character space embeds dense -bit embedding passes embedded \"image\" attribute consecutive convolution layers ﬁlters layers followed max-pooling layer ﬂattening layer dropout layer probability dropout fully connected layer nodes ﬁnally fully connected softmax output layer number nodes equal number semantic types though cannot sure ﬁnal choice architectures optimal seems good trade-oﬀ complexity models required computational resources training overall performance semantic labeling task. implemented models using keras library gpu-based tensorflow backend domain-independent semantic labeler proposed pham feature groups based similarity metrics constructed. metrics measure attribute names values similar characteristics attributes. means given attributes training data distinct semantic labels attribute compared representatives semantic label features calculated total. considered similarity metrics attribute name similarity standard jaccard similarity textual data modiﬁed version numerical data tf-idf cosine similarity distribution histogram similarity. instead building multi-class classiﬁer authors train binary classiﬁers separately semantic label. binary classiﬁer particular semantic label logistic regression model trained similarity metrics representatives label. predicting semantic labels attribute combine predictions classiﬁer produce ﬁnal vector probabilities. distinctive properties approach ability transfer classiﬁcation model trained domain predicting semantic labels attributes another domain. denote enhanced approach dsl+. train classiﬁer semantic labeling need data sources many labeled attributes. however costly operation manually assigning labels attributes relative small number columns compared data size implies lack training data common problem semantic labeling systems. existing systems knowledge transfer techniques overcome issue. introduce method increasing training sample size based machine learning approach known bagging breiman introduced concept bootstrap aggregating also known bagging construct ensembles models improve prediction accuracy. method consists training diﬀerent classiﬁers bootstrapped replicas original dataset. hence diversity obtained resampling procedure usage diﬀerent data subsets. prediction stage individual classiﬁer estimates unknown instance majority weighted vote used infer class. modify idea bagging problem. clear semantics columns table employees change whether rows. create several training instances attribute instance contain random sample content. procedure governed parameters numbags bagsize ﬁrst parameter controls many bags generated attribute latter indicates many rows sampled bag. address issue noise increasing diversity training data well issue insuﬃcient training data. another common problem encountered wide range data mining machine learning initiatives class imbalance. class imbalance occurs class instances dataset equally represented. situation building standard machine learning models lead poor results since favor classes large populations classes small populations. address issue tried several resampling strategies equalize number instances class. mentioned previously attributes mapped property ontology. handle issue introduce class called unknown. example attributes discarded integration process marked unknown. help classiﬁer recognize attributes sources. addition another advantage unknown class deﬁned explicitly. consider attribute unseen semantic label label present training data. instead picking closest match among known semantic labels classiﬁer mark unknown. user need validate attributes classiﬁed unknown. ensure unknown class consists unwanted attributes. introduce another class diﬀerentiate unwanted attributes unseen labels since cannot guarantee overlap them. dint deep learning approaches support unknown class. experiments dell server memory cpus cores each titan geforce gpu. deep learning models optimized gpus using tensorﬂow. benchmark semantic labeling system implemented python available open source license datasets diﬀerent sets data sources evaluation labeled museum city weather soccer weapons corresponds domain speciﬁc semantic labels. descriptive statistics domain shown table museum soccer domains domains unknown attributes. city domain many semantic labels attributes museum domain contains data sources. estimate class imbalance within domain plot class distribution figure museum domain highest imbalance among classes soccer weapons domains also imbalanced classes whereas weather city domains equally represented classes. experimental setting establish common evaluation framework approaches described section performance metric mean reciprocal rank derive comprehensive estimate performance within domains implement cross-validation techniques leave repeated holdout. leave strategy deﬁned using source testing sample rest sources domain training samples. procedure repeated many times sources domain. calculate testing sample report average ﬁnal performance metric iteration. example domain museum obtain models total model trained diﬀerent sources calculated prediction outcome single source. strategy allows estimate performance diﬀerent models given enough instances semantic label. figure distribution attributes according semantic labels including unknown class diﬀerent domains. class imbalance museum soccer weapons domains. x-axis semantic labels sorted number attributes class. y-axis shows number attributes. repeated holdout strategy randomly sample ratio sources place training sample remaining sources testing sample procedure repeated times. ﬁnal score average scores iteration. technique simulate scenario shortage labeled sources. ratio number iterations train models need many training instances bagging parameters numbags= bagsize= increase size initial training set. train semantic labeling system dint diﬀerent sampling strategies. particular report results apply resampling bagging parameters bagsize= numbags=. also experiment various class imbalance resampling strategies including resampling mean maximum instance counts class. brevity without loss generality report results resampling mean strategy denoted resampletomean. design dsl+ resampling. mentioned section dint model built elaborately engineered features. model hand uses chardist entropy. better compare performance dint create model dint base reduce number features chardist entropy. addition create another model dint base+ using chardist entropy feature minimum edit distance. choose feature feature importance scores produced random forest algorithm rank edit distance higher features extracted names. table reports scores leave strategy. surprisingly models built normalized character distributions attribute values perform many cases well. deep learning models often comparable dint models however come usually higher computational cost. times training model shown table dint models bagging sample training instances achieve best results four domains. remarkably also domains higher class imbalance variety among data sources terms number rows number columns. data sources city domains number attributes. also discovered bagging needs performed training prediction stages achieve best performance. observed setting makes noticeable terms computation time best performing model dint museum domain requires time training. computationally expensive features four diﬀerent edit distances minimum edit distance k-nearest neighbors. suggests dint model possible features scale well increasing number attributes training set. considering similarity metrics used approaches like computing tf-idf jaccard’s scores help resolve runtime issue dint all. class imbalance although resampletomean strategy improves performance dint models sampling domains highest class imbalance appears resampletomean strategy leads decreased performance domains less prominent imbalance leads idea class resampling strategy needs improved. potential strategy combining bagging resampling strategies. instead ﬁxing numbags attributes parameter could changed either mean maximum instance counts class. perform resampling strategy produce replicas attributes. apart city weapons domains newly designed models similar performance dsl. however computational complexity models varies. museum domain dint base+ higher dint base+ needs half time less training. appears attributes contain mixture textual numeric bottleneck since data sources city weapons domains multiple mixed data columns. cases labeled instances observe performs well especially dsl+ leverages labeled instances domains. aware scenario many unseen labels makes ill-deﬁned. compare dint models scenario suggests bagging advantageous situations labeled attributes. overall enhancing dint model uses simple features bagging dsl+ knowaddition perform experiments domains museum soccer unmapped attributes cause skewed class distributions. want establish well diﬀerent approaches recognize attributes. tables performance semantic labeling systems changes considerably. dsl+ performance aﬀected inability diﬀerentiate \"unwanted\" attributes. performing bagging attributes training data introduce diversity drawing many samples attribute values. however apply perturbation technique names attributes instead exact replicas. table observe dint base performs better dint base+ bagging used. datasets scarce labeled instances dint models tend overﬁt attribute names present training data. suggests introducing technique similar bagging column headers might lead much better performance. hand results consistent observations work ritze al.. results indicate comparing attribute values crucial task attribute names might introduce additional noise. clearly performance approach dint varies depending chosen bagging parameters numbags bagsize. explore dependence evaluate performance dint chardist entropy features varying bagging parameters ﬁxing one. report results evaluation figure consider unknown attributes choose repeated holdout strategy analyze behavior bagging shortage training data. interestingly increasing values bagging parameters always lead improved performance though computational time required training prediction stages increases. city domain sensitive bagging parameters. assume city domain domain equal distribution semantic labels equal numbers columns rows across data sources. appears domains bagging makes models robust towards variance characteristics. problem semantic labeling addressed work regarded problem schema matching ﬁeld data integration schema matching problem match elements source target schemata. case elements source schema attributes want attributes properties ontology. semantic labeling problem also known literature attribute-to-property matching indicating semantic correspondences manually might appropriate data sources need integrated however becomes tedious growing number heterogeneous schemata. hence automatic semi-automatic approaches schema matching actively developed. machine learning perspective categorize approaches unsupervised techniques compute various similarity metrics supervised techniques build multi-class classiﬁcation model. unsupervised approaches used semantictyper extended version approaches authors design similarity metrics attribute names attribute values substantial diﬀerence whether additional knowledge used computation. example authors leverage contextual information dbpedia. among supervised approaches probabilistic graphical models used work limaye annotate tables entities cell values types attributes relationships binary combinations attributes. mulwad extend approach leveraging information wikitology knowledge base problem probabilistic graphical models though scale number semantic labels domain. also mulwad well venetis used database extract additional data knowledge bases assign semantic label attribute. hence approaches limited domains well represented knowledge bases. approach hand domain speciﬁc allows model trained data. however cannot apply model learnt domain another possible approach best knowledge introduced pham among semantic labeling systems. pham compare previous approach semantictyper system achieve higher scores variety datasets. therefore state-of-the model benchmark evaluate approaches. ritze pham mention problem unknown class. ﬁrst work authors discuss \"unwanted\" attributes second work authors reﬂect handle \"unseen\" attributes. work diﬀerentiate cases show successfully identify attributes suﬃcient training data available. paper studied problem supervised semantic labeling conducted experiments evaluate diﬀerent approaches perform task. main ﬁnding bagging sampling technique provide meaningful diversity training data improve performance. additionally technique overcome lack labeled attributes domain increase number instances under-represented semantic labels. given scarce training data bagging leads noticeable improvement performance though state-of-the-art system achieves better precision leveraging information labeled instances domains. however consider unwanted attributes unseen semantic labels system dint demonstrates best performance. among semantic labeling systems benchmark observed performance results highly dependent case. also shown deep learning models also applied solve problem. though models excel performance majority cases advantage simplicity features extracted attributes. example built sequences attribute values. surprisingly discovered even random forests constructed character distributions values entropy attributes provide remarkable results many cases. supports observations literature attribute values crucial semantic labeling task future work involve exploring combination bagging class imbalance resampling strategies. observed domain data high imbalance among representatives diﬀerent semantic labels resampling lead improved performance sophisticated approach required domains exhibit characteristics. another possible direction improvement introduce equivalent bagging attribute names. addition experiments indicate performance systems often aﬀected variance sizes data sources well semantic label represented training data. consider including tkmatch benchmark well domain sets rodi benchmark", "year": 2018}