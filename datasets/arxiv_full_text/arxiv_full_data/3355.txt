{"title": "Adversarial examples in the physical world", "tag": ["cs.CV", "cs.CR", "cs.LG", "stat.ML"], "abstract": "Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.", "text": "existing machine learning classiﬁers highly vulnerable adversarial examples. adversarial example sample input data modiﬁed slightly intended cause machine learning classiﬁer misclassify many cases modiﬁcations subtle human observer even notice modiﬁcation classiﬁer still makes mistake. adversarial examples pose security concerns could used perform attack machine learning systems even adversary access underlying model. previous work assumed threat model adversary feed data directly machine learning classiﬁer. always case systems operating physical world example using signals cameras sensors input. paper shows even physical world scenarios machine learning systems vulnerable adversarial examples. demonstrate feeding adversarial images obtained cell-phone camera imagenet inception classiﬁer measuring classiﬁcation accuracy system. large fraction adversarial examples classiﬁed incorrectly even perceived camera. figure demonstration black attack phone image classiﬁcation using physical adversarial examples. took clean image dataset used generate adversarial images various sizes adversarial perturbation printed clean adversarial images used tensorflow camera demo classify them. clean image recognized correctly washer perceived camera adversarial images misclassiﬁed. video full demo https//youtu.be/zq_umenobck. recent advances machine learning deep neural networks enabled researchers solve multiple important practical problems like image video text classiﬁcation others however machine learning models often vulnerable adversarial manipulation input intended cause incorrect classiﬁcation particular neural networks many categories machine learning models highly vulnerable attacks based small modiﬁcations input model test time problem summarized follows. let’s machine learning system input sample call clean example. let’s assume sample correctly classiﬁed machine learning system i.e. ytrue. it’s possible construct adversarial example perceptually indistinguishable classiﬁed incorrectly i.e. ytrue. adversarial examples misclassiﬁed often examples perturbed noise even magnitude noise much larger magnitude adversarial perturbation adversarial examples pose potential security threats practical machine learning applications. particular szegedy showed adversarial example designed misclassiﬁed model often also misclassiﬁed model adversarial example transferability property means possible generate adversarial examples perform misclassiﬁcation attack machine learning system without access underlying model. papernot papernot demonstrated attacks realistic scenarios. however prior work adversarial examples neural networks made threat model attacker supply input directly machine learning model. prior work known whether adversarial examples would remain misclassiﬁed examples constructed physical world observed camera. threat model describe scenarios attacks take place entirely within computer evading spam ﬁlters malware detectors however many practical machine learning systems operate physical world. possible examples include limited robots perceiving world cameras sensors video surveillance systems mobile applications image sound classiﬁcation. scenarios adversary cannot rely ability ﬁne-grained per-pixel modiﬁcations input data. following question thus arises still possible craft adversarial examples perform adversarial attacks machine learning systems operating physical world perceiving data various sensors rather digital representation? prior work addressed problem physical attacks machine learning systems context fooling neural networks making small perturbations input. example carlini demonstrate attack create audio inputs mobile phones recognize containing intelligible voice commands humans hear unintelligible voice. face recognition systems based photos vulnerable replay attacks previously captured image authorized user’s face presented camera instead actual face adversarial examples could principle applied either physical domains. adversarial example voice command domain would consist recording seems innocuous human observer contains voice commands recognized machine learning algorithm. adversarial example face recognition domain might consist subtle markings applied person’s face human observer would recognize identity correctly machine learning system would recognize different person. similar work paper sharif appeared publicly work submitted conference earlier. sharif also print images adversarial examples paper demonstrated printed images fool image recognition systems photographed. main differences work that cheap closed-form attack experiments sharif expensive attack based optimization algorithm make particular effort modify adversarial examples improve chances surviving printing photography process. simply make scientiﬁc observation many adversarial examples survive process without intervention. sharif introduce extra features make attacks work best possible practical attacks face recognition systems. sharif restricted number pixels modify modify pixels large amount; restricted amount modify pixel free modify them. investigate extent adversarial examples survive physical world conducted experiment pre-trained imagenet inception classiﬁer generated adversarial examples model examples classiﬁer cellphone camera measured classiﬁcation accuracy. scenario simple physical world system perceives data camera runs image classiﬁcation. found large fraction adversarial examples generated original model remain misclassiﬁed even perceived camera. surprisingly attack methodology required modiﬁcation account presence camera—the simplest possible attack using adversarial examples crafted inception model resulted adversarial examples successfully transferred union camera inception. results thus provide lower bound attack success rate could achieved specialized attacks explicitly model camera crafting adversarial example. limitation results assumed threat model attacker full knowledge model architecture parameter values. primarily single inception model experiments without devise train different high-performing model. adversarial example transfer property implies results could extended trivially scenario attacker access model description haven’t detailed experiments study transferability physical adversarial examples able build simple phone application demonstrate potential adversarial black attack physical world better understand non-trivial image transformations caused camera affect adversarial example transferability conducted series additional experiments studied adversarial examples transfer across several speciﬁc kinds synthetic image transformations. rest paper structured follows section review different methods used generate adversarial examples. followed section details physical world experimental set-up results. finally section describes experiments various artiﬁcial image transformations affect adversarial examples. section describes different methods generate adversarial examples used experiments. important note none described methods guarantees generated image misclassiﬁed. nevertheless call generated images adversarial images. ytrue true class image cross-entropy cost function neural network given image class intentionally omit network weights cost function assume ﬁxed context paper. neural networks softmax output layer cross-entropy cost function applied integer class labels equals negative dileep george noticed another kind adversarially constructed input designed true class categorized belonging speciﬁc class fooled convolutional networks photographed less systematic experiments. august mentioned figure http//www. evolvingai.org/fooling simplest methods generate adversarial images described motivated linearizing cost function solving perturbation maximizes cost subject constraint. accomplished closed form cost call back-propagation introduce straightforward extend fast method—we apply multiple times small step size clip pixel values intermediate results step ensure \u0001-neighbourhood original image experiments used i.e. changed value pixel step. selected number iterations min. amount iterations chosen heuristically; sufﬁcient adversarial example reach edge max-norm ball restricted enough keep computational cost experiments manageable. methods described simply increase cost correct class without specifying incorrect classes model select. methods sufﬁcient application datasets mnist cifar- number classes small classes highly distinct other. imagenet much larger number classes varying degrees signiﬁcance difference classes methods result uninteresting misclassiﬁcations mistaking breed sled another breed sled dog. order create interesting mistakes introduce iterative least-likely class method. iterative method tries make adversarial image classiﬁed speciﬁc desired target class. desired class chose least-likely class according prediction trained network image well-trained classiﬁer least-likely class usually highly dissimilar true class attack method results interesting mistakes mistaking airplane. make adversarial image classiﬁed maximize makfigure top- top- accuracy inception attack different adversarial methods different compared clean images unmodiﬁed images dataset. accuracy computed validation images imagenet dataset. experiments varies mentioned above guaranteed adversarial image actually misclassiﬁed— sometimes attacker wins sometimes machine learning model wins. experimental comparison adversarial methods understand actual classiﬁcation accuracy generated images well types perturbations exploited methods. experiments performed validation samples imagenet dataset using pre-trained inception classiﬁer validation image generated adversarial examples using different methods different values pair method computed classiﬁcation accuracy images. also computed accuracy clean images used baseline. top- top- classiﬁcation accuracy clean adversarial images various adversarial methods summarized figure examples generated adversarial images could found appendix figures shown figure fast method decreases top- accuracy factor top- accuracy even smallest values increase accuracy adversarial images generated fast method stays approximately level slowly decreases almost grows could explained fact fast method adds \u0001-scaled noise image thus higher values essentially destroys content image makes unrecognisable even humans figure hand iterative methods exploit much ﬁner perturbations destroy image even higher time confuse classiﬁer higher rate. basic iterative method able produce better adversarial images however increase unable improve. least likely class method destroys correct classiﬁcation images even relatively small. limit experiments perturbations perceived small noise adversarial methods able produce signiﬁcant number misclassiﬁed examples \u0001-neighbourhood clean images. study inﬂuence arbitrary transformations adversarial images introduce notion destruction rate. described fraction adversarial images longer misclassiﬁed transformations. formal deﬁnition following number images used comput destruction rate image dataset corresponding adversarial image. function arbitrary image transformation—in article study variety transformations including printing image taking photo result. function indicator function returns whether image classiﬁed correctly figure experimental setup generated printout contains pairs clean adversarial images well codes help automatic cropping; photo printout made cellphone camera; automatically cropped image photo. explore possibility physical adversarial examples series experiments photos adversarial examples. printed clean adversarial images took photos printed pages cropped printed images photos full page. think black transformation refer photo transformation. print image figure order reduce amount manual work printed multiple pairs clean adversarial examples sheet paper. also codes corners printout facilitate automatic cropping. generated pictures printouts saved lossless format. batches printouts converted multi-page using convert tool imagemagick suite default settings convert *.png output.pdf generated ﬁles printed using ricoh ofﬁce printer. page automatically scaled entire sheet paper using default printer scaling. printer resolution dpi. squares size source images figure detect values locations four codes corners photo. codes encode batch validation examples shown photo. detection corners failed entire photo discarded images photo used calculate accuracy. observed images discarded experiment typically number discarded images procedure involves manually taking photos printed pages without careful control lighting camera angle distance page etc. intentional; introduces nuisance variability potential destroy adversarial perturbations depend subtle co-adaptation exact pixel values. said intentionally seek extreme camera angles lighting conditions. photos taken normal indoor lighting camera pointed approximately straight page. average case. measure average case performance randomly selected images experiment given adversarial method. experiment estimates often adversary would succeed randomly chosen photos—the world chooses image randomly adversary attempts cause misclassiﬁed. preﬁltered case. study aggressive attack performed experiments images preﬁltered. speciﬁcally selected images clean images classiﬁed correctly adversarial images classiﬁed incorrectly addition used conﬁdence threshold prediction ypredicted class predicted network image experiment measures often adversary would succeed adversary choose original image attack. threat model adversary access model parameters architecture attacker always inference determine whether attack succeed absence photo transformation. attacker might expect best choosing make attacks succeed initial condition. victim takes photo physical object attacker chooses display photo transformation either preserve attack destroy found fast adversarial images robust photo transformation compared iterative methods. could explained fact iterative methods exploit subtle kind perturbations subtle perturbations likely destroyed photo transformation. success rate lower preﬁltered images rather randomly selected images. suggests that obtain high conﬁdence iterative methods often make subtle co-adaptations able survive photo transformation. overall results show fraction adversarial examples stays misclassiﬁed even non-trivial transformation photo transformation. demonstrates possibility physical adversarial examples. example adversary using fast method could expect images would top- misclassiﬁed images would top- misclassiﬁed. thus generating enough adversarial images adversary could expect cause misclassiﬁcation would occur natural inputs. experiments described study physical adversarial examples assumption adversary full access model however black scenario attacker access model realistic model many security threats. adversarial examples often transfer model another used black attacks szegedy papernot black attack demonstrated physical adversarial examples fool different model used construct them. speciﬁcally showed fool open source tensorflow camera demo mobile phones performs image classiﬁcation on-device. showed several printed clean adversarial images observed change classiﬁcation true label incorrect label. video demo available https//youtu.be/zq_umenobck. also demonstrated effect live geekpwn transformations applied images process printing them photographing them cropping could considered combination much simpler image transformations. thus better understand going conducted series experiments measure adversarial destruction rate artiﬁcial image transformations. explored following transformations change contrast brightness gaussian blur gaussian noise jpeg encoding. experiments used subset images randomly selected validation set. subset images selected once thus experiments section used subset images. performed experiments multiple pairs adversarial method transformation. given pair transformation adversarial method computed adversarial examples applied transformation adversarial examples computed destruction rate according equation adversarial examples generated fast method robust transformations adversarial examples generated iterative least-likely class method least robust. coincides results photo transformation. top- destruction rate typically higher top- destruction rate. explained fact order destroy top- adversarial examples transformation push correct class labels top- predictions. however order destroy top- adversarial examples push correct label top- prediction strictly stronger requirement. changing brightness contrast affect adversarial examples much. destruction rate fast basic iterative adversarial examples less iterative least-likely class method less blur noise jpeg encoding higher destruction rate changes brightness contrast. particular destruction rate iterative methods could reach however none transformations destroy adversarial examples coincides photo transformation experiment. paper explored possibility creating adversarial examples machine learning systems operate physical world. used images taken cell-phone camera input inception image classiﬁcation neural network. showed set-up signiﬁcant fraction adversarial images crafted using original network misclassiﬁed even classiﬁer camera. ﬁnding demonstrates possibility adversarial examples machine learning systems physical world. future work expect possible demonstrate attacks using kinds physical objects besides images printed paper attacks different kinds machine learning systems sophisticated reinforcement learning agents attacks performed without access model’s parameters architecture physical attacks achieve higher success rate explicitly modeling phyiscal transformation adversarial example construction process. also hope future work develop effective methods defending attacks. battista biggio igino corona davide maiorca blaine nelson nedim ˇsrndi´c pavel laskov giorjoint giacinto fabio roli. evasion attacks machine learning test time. european conference machine learning knowledge discovery databases springer nicholas carlini pratyush mishra tavish vaidya yuankai zhang micah sherr clay shields david wagner wenchao zhou. usenix security symposium austin august usenix association. https//www.usenix.org/conference/usenixsecurity/ technical-sessions/presentation/carlini. nilesh dalvi pedro domingos sumit sanghai deepak verma adversarial classiﬁcation. proceedings tenth sigkdd international conference knowledge discovery data mining geoffrey hinton deng dong george dahl abdel rahman mohamed navdeep jaitly andrew senior vincent vanhoucke patrick nguyen tara sainath brian kingsbury. deep neural networks acoustic modeling speech recognition. signal processing magazine alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems blaine nelson marco barreno fuching jack anthony joseph benjamin rubinstein udam saini charles sutton doug tygar xia. exploiting machine learning subvert spam ﬁlter. papernot mcdaniel goodfellow. transferability machine learning phenomena black-box attacks using adversarial samples. arxiv e-prints http//arxiv.org/abs/.. nicolas papernot patrick drew mcdaniel goodfellow somesh berkay celik ananthram swami. practical black-box attacks deep learning systems using adversarial examples. corr abs/. http//arxiv.org/abs/.. olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein imagenet large scale visual recognition challenge. arxiv preprint arxiv. mahmood sharif sruti bhagavatula lujo bauer michael reiter. accessorize crime real stealthy attacks state-of-the-art face recognition. proceedings sigsac conference computer communications security october appear. christian szegedy wojciech zaremba ilya sutskever joan bruna dumitru erhan goodfellow fergus. intriguing properties neural networks. iclr abs/. http//arxiv.org/abs/.. christian szegedy vincent vanhoucke sergey ioffe jonathon shlens zbigniew wojna. rethinking inception architecture computer vision. corr abs/. http//arxiv.org/abs/.. figure examples adversarial images produced different adversarial methods. figure examples adversarial images various values figure contain plots adversarial destruction rates various image transformations. figure comparison different adversarial methods perturbations generated iterative methods ﬁner compared fast method. also iterative methods always select point border \u0001-neighbourhood adversarial image. figure comparison images resulting adversarial pertubation using fast method different size perturbation image washer bottom hamster. cases clean images classiﬁed correctly adversarial images misclassiﬁed considered", "year": 2016}