{"title": "Towards Neural Network-based Reasoning", "tag": ["cs.AI", "cs.CL", "cs.LG", "cs.NE"], "abstract": "We propose Neural Reasoner, a framework for neural network-based reasoning over natural language sentences. Given a question, Neural Reasoner can infer over multiple supporting facts and find an answer to the question in specific forms. Neural Reasoner has 1) a specific interaction-pooling mechanism, allowing it to examine multiple facts, and 2) a deep architecture, allowing it to model the complicated logical relations in reasoning tasks. Assuming no particular structure exists in the question and facts, Neural Reasoner is able to accommodate different types of reasoning and different forms of language expressions. Despite the model complexity, Neural Reasoner can still be trained effectively in an end-to-end manner. Our empirical studies show that Neural Reasoner can outperform existing neural reasoning systems with remarkable margins on two difficult artificial tasks (Positional Reasoning and Path Finding) proposed in [8]. For example, it improves the accuracy on Path Finding(10K) from 33.4% [6] to over 98%.", "text": "propose neural reasoner framework neural network-based reasoning natural language sentences. given question neural reasoner infer multiple supporting facts answer question speciﬁc forms. neural reasoner speciﬁc interaction-pooling mechanism allowing examine multiple facts deep architecture allowing model complicated logical relations reasoning tasks. assuming particular structure exists question facts neural reasoner able accommodate diﬀerent types reasoning different forms language expressions. despite model complexity neural reasoner still trained eﬀectively end-to-end manner. empirical studies show neural reasoner outperform existing neural reasoning systems remarkable margins diﬃcult artiﬁcial tasks proposed example improves accuracy path finding reasoning essential natural language processing tasks obviously examples like document summarization question-answering dialogue. previous eﬀorts direction built rule-based models requiring ﬁrst mapping natural languages logic forms inference them. mapping inference means easy given variability ﬂexibility natural language variety reasoning tasks brittleness rule-based system. recently eﬀort mainly represented memory network dynamic variants trying build purely neural network-based reasoning system fully distributed semantics infer multiple facts answer simple questions natural language e.g. paper give systematic treatment problem propose ﬂexible neural reasoning system named neural reasoner. purely neural network based trained end-to-end using supervision ﬁnal answer. contributions mainly two-folds method eﬀectively ﬁtting model relatively small amount data yielding signiﬁcantly better results existing neural models artiﬁcial reasoning task; neural reasoner layered architecture deal complicated logical relations reasoning illustrated figure consists encoding layer multiple reasoning layers. encoder layer ﬁrst converts question facts natural language sentences vectorial representations. speciﬁcally interaction question representation fact representations. intuitively interaction models reasoning including examination facts comparison facts questions. finally layer-l resulted question representation answerer layer classiﬁer choosing number pre-determined section give instantiation neural reasoner described section illustrated figure nutshell question facts symbol sequences ﬁrst converted vectorial representations encoding layer recurrent neural networks vectorial representations reasoning layers question facts updated nonlinear transformation jointly controlled deep neural networks pooling. finally answering layer resulted question representation used generate ﬁnal answer question. speciﬁcally reasoning layer deep neural network model pairwise interaction question representation fact representation previous layer yields updated fact representation summarizes word sequence vector ﬁxed length. diﬀerent modeling choices purpose e.g. paper variant encoding module. shown able alleviate gradient vanishing issue similar performance complicated lstm pooling aims fuse understanding question right interaction facts form current status question enable comparison diﬀerent facts. several strategies pooling simplicity focus reasoning tasks formulated classiﬁcation predetermined classes. speciﬁcally apply neural reasoner deal following types questions another type prediction classiﬁcation eﬀective classes dynamically change instances e.g. single-supporting-fact task tasks cannot directly solved neural reasoner. simple circumvent deﬁne following score function end-to-end training training stepby-step labels supporting facts instance addition answer. described extra labels brings much stronger supervision answer end-to-end learning setting typically yield signiﬁcantly better result relatively complicated tasks. auxiliary training facilitate learning representations question facts. basically addition using learned representations question facts reasoning process also representations reconstruct original questions abstract forms variables whereereasoning cross entropy loss describing discrepancy model prediction correct answer erecovering negative log-likelihood sequences recovered. speciﬁcally likelihood estimated encoder-decoder framework proposed encoding layer another decoding layer trained sequentially predict words original sentence. instead recovering original sentence question facts also study eﬀect producing abstract form auxiliary training task. speciﬁcally decoding recover sentence entities replaced variables e.g. babi synthetic question answering dataset. contains tasks composed facts question followed answer mostly single word. time subset facts relevant given question. versions data available training instances task instances task testing versions. select challenging tasks positional reasoning path finding test reasoning ability neural reasoner. positional reasoning task tests model’s spatial reasoning ability path finding task ﬁrst proposed tests ability reason correct path objects based natural language instructions. table give instance task. table samples tasks path ﬁnding positional reasoning facts questions given answers panel ﬁrst list facts question needs answer based given facts. task answer ﬁrst question south east standing going south ﬁrst east obtained based fact model trained standard back-propagation aiming maximize likelihood correct answers. parameters including word-embeddings initialized randomly sampling uniform distribution momentum weight decay used. trained tasks epochs stochastic gradient descent gradients norm larger clipped learning rate controlled adadelta multi-task learning diﬀerent mixture ratios tried compare neural reasoner following three neural reasoning models )memory network including step-by-step supervision end-to-end version dynamic memory network proposed also step-by-step supervision. table report performance particular case neural reasoner reasoning layers -layer dnns interaction modules reasoning layer auxiliary task recovering original question facts. results compared three neural competitors. following observations. please note results neural reasoner reported table based architectures speciﬁcally tuned tasks. matter fact complicated models achieve even better results large datasets however leave discussion diﬀerent architectural variants next section. section devoted study architectural variants neural reasoner. speciﬁcally consider variations )the number reasoning layers depth interaction auxiliary tasks results summarized table following observations auxiliary tasks essential eﬃcacy neural reasoner without performances neural reasoner drop dramatically. reason conjecture section reasoning task alone cannot give enough supervision learning accurate word vectors parameters encoder. note neural reasoner still outperform memory data tasks. -layer apparently beneﬁt auxiliary learning recovering abstract forms small datasets however deeper architectures training data improvement recovering original sentences become smaller despite extra information utilizes. proposed neural reasoner framework neural network-based reasoning natural language sentences. neural reasoner ﬂexible powerful language indepedent. empirical studies show neural reasoner dramatically improve upon existing neural reasoning systems diﬃcult artiﬁcial tasks proposed future work explore tasks higher diﬃculty reasoning depth e.g. tasks require large number supporting facts facts complex intrinsic structures common structure diﬀerent similar reasoning tasks automatic selection reasoning architecture example determining stop reasoning based data.", "year": 2015}