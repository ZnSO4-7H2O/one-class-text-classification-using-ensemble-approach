{"title": "An Optimal Online Method of Selecting Source Policies for Reinforcement  Learning", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Transfer learning significantly accelerates the reinforcement learning process by exploiting relevant knowledge from previous experiences. The problem of optimally selecting source policies during the learning process is of great importance yet challenging. There has been little theoretical analysis of this problem. In this paper, we develop an optimal online method to select source policies for reinforcement learning. This method formulates online source policy selection as a multi-armed bandit problem and augments Q-learning with policy reuse. We provide theoretical guarantees of the optimal selection process and convergence to the optimal policy. In addition, we conduct experiments on a grid-based robot navigation domain to demonstrate its efficiency and robustness by comparing to the state-of-the-art transfer learning method.", "text": "ammar general circumstance agent source task selection itself. selecting appropriate source task usually demands quite little extra knowledge concerning domain. example need prior experience target task. well-estimated known model assumed always available practice. policy reuse q-learning requires prior knowledge target task environment converge suboptimal policy. address limitations propose optimal method select reuse source policies online automatically without extra prior knowledge. contributions paper claimed follows. first formulate source policy selection problem multi-armed bandit problem different source policies regarded bandits enable optimal online source policy selection. second augment q-learning policy reuse maintaining theoretical guarantee convergence traditional q-learning. finally empirical experiments conducted grid-based robot navigation domain verify approach accomplishes optimal source policy selection process; transfers useful knowledge target task signiﬁcantly speeds learning process; achieves virtually empirical performance traditional q-learning equivalent situations source knowledge useful. remainder paper start reviewing related work. then background knowledge problem formulation described. that present approach theoretical results followed empirical results comparing approach state-of-the-art algorithm. finally conclude outline directions future work. transfer received much attention recently discuss greater detail relationship algorithm others. treated previously learned policies experts mediated experts intelligently. since mixing time experts known episodic domains method effective standard algorithms. contrast approach works transfer learning signiﬁcantly accelerates reinforcement learning process exploiting relevant knowledge previous experiences. problem optimally selecting source policies learning process great importance challenging. little theoretical analysis problem. paper develop optimal online method select source policies reinforcement learning. method formulates online source policy selection multi-armed bandit problem augments q-learning policy reuse. provide theoretical guarantees optimal selection process convergence optimal policy. addition conduct experiments grid-based robot navigation domain demonstrate efﬁciency robustness comparing state-of-the-art transfer learning method. reinforcement learning widely-used framework learn optimal control decision-making policy. however high sample complexity since agent gains data repeated interactions environment. transferring past knowledge target task greatly accelerate reinforcement learning. ﬁrst step transfer select useful knowledge reuse process. irrelevant source task chosen learning performance could worse learning scratch called negative transfer problem challenging practical situation environment mostly unknown agent prior knowledge source task useful. agent online source task selection. transfer learning recognized important direction long time works leverage source task knowledge without automatically identifying related source tasks however methods suffer negative transfer. others require humans deﬁne relationships between tasks relevant source tasks association advancement artiﬁcial intelligence rights reserved. reward time step episode convergence speed value indicate learning performance. approach applies transfer learning qlearning off-policy learning method. since exploration strategy affect average reward learning greatly evaluation following fully greedy strategy learning episode obtain learning curve average reward. rewards reusing source policies stochastic therefore dilemma exploiting policy yields high current rewards exploring policy produce future rewards. adapt method problem. approach accomplishes online source policy selection evaluating utility source policy learning target task. exploration process guided intelligently selected policy q-learning off-policy learner. situation source knowledge useful \u0001-greedy strategy approach play major role maintain learning performance traditional q-learning. section ﬁrstly present optimal online source policy selection method. that introduces reuse source knowledge q-learning. finally provide theoretical optimality analysis algorithm. figure provides overview algorithm. firstly execute \u0001-greedy exploration strategy probability select source policy library using method probability decreases time. beginning learning exploit source knowledge mostly. learning goes past knowledge becomes less useful exploit \u0001-greedy strategy learning. order exploit past policy algorithm combines past policy random policy. policy current episode determined algorithm execute policy update q-function. therefore optimally select source policy reuse selected policy transfer learning. episodic tasks. prql selected source tasks library probabilistically using softmax method. however stops exploration soon greedy policy’s reward exceeds reuse reward guarantee convergence optimal policy. learned transferability source-target task pair using meta-data. method efﬁciency expensive generate large data using every source-target pair. contrary approach adapts method select source policies accomplishes optimal selection process. proposed bayesian method policy reuse policy library. mainly solves problem short-lived sequential policy selection therefore method learn full policy. leveraged prior knowledge lateral connections previously learned features neural networks. although showed positive transfer even orthogonal adversarial tasks theoretical foundation algorithm. related works focus multi-task learning similar transfer learning. assumes mdps drawn distribution learning parallel several tasks contrast make assumption regarding distribution mdps concentrate transfer learning problem. previous work represented distribution mdps hierarchical bayesian model. continuously updated distribution served prior rapid learning environments. wilson mentioned work algorithm computationally efﬁcient. recent works proposed technique involves phases learning reduce sample complexity determined similar source tasks based compliance interpreted sort distance metric tasks. section brieﬂy reviews background describes problem formulation. dominant framework solve control decision-making problems mapping situations actions. learning environment deﬁned tuple denotes discrete state space. time step agent speciﬁc state performs action discrete action space based transition function agent switches next state gets reward according reward function agent begins interact environment start state sampled initial belief keeps taking actions reaches ﬁnal state absorbing state. policy directs agent action take given particular state. agent’s goal learn optimal policy maximizes expected value cumulative reward training. discount factor reduce impact future rewards learning policy. q-learning model-free method able optimal policy ﬁnite mdp. q-learning agent learns expected utility action πpast random policy probabilistically policy reuse strategy demonstrated algorithm time step take action based πpast probability take random action probability algorithm shares similar ideas prql. take action probabilistically episode. however greedy action reuse episodes maintain ﬁxed expected value reuse reward. addition source policy uncorrelated stochastic assumption satisﬁed. choose \u0001-greedy strategy probability outside reuse episodes instead. \u0001-greedy strategy crucial useful source policy library. decreases time algorithm reuse source policy less converge \u0001-greedy strategy. previous rewards obtain larger cumulative reward. different source policies regarded bandits stochastic rewards mab. source policy selection exploitation versus exploration tradeoff essence. pseudo code source policy selection method shown algorithm order solve target task method chooses \u0001-greedy policy probability chooses policy source policy library using probability past policy chosen execute combining method algorithm chosen past policy episode need keep policy average gain number selected times previous episodes controls reuse degree algorithm executed episodes. simple efﬁcient algorithm achieves optimal logarithmic regret deﬁnes collection independent bandits different expected reward µn}. agent sequentially selects bandits make cumulative regret minimum. regret difference expected reward selected bandit maximum reward expectation family algorithms maintains number steps machine selected steps empirical expected reward ˆµi. machine collection played initially selects machine follows every time step afterwards source policy reuse take full advantage selected policy random policy indispensable interacting unexplored states. without random actions past policies lead agent original goals rather goal target task. exploiting useful source policy regarded directed exploration. therefore combine source policy theoretical analysis present theoretical results provide foundation approach below. theorem given source policy library selects source policy according reward episode algorithm expected regret logarithmically bounded. proof. greedy action reuse episode source policies correlated. addition policy reward every episode independent sample distribution ﬁxed expectation. stochastic assumption satisﬁed. achieve logarithmic regret bound asymptotically proved minimum robbins classical paper optimal allocation strategy preliminary knowledge reward distribution. result method selecting source policies library theoretically optimal. proof. since controlling exploration rate decreases time algorithm execute \u0001-greedy policy frequently. \u0001-greedy policy less algorithm keep exploration inﬁnite episodes. q-learning proper learning rate converges optimal q-function ﬁnite probability executing random actions algorithm never equal state-action pairs visited inﬁnitely often. result algorithm converge optimal q-function traditional q-learning. demonstrate algorithm empirically sound robust carry experiments grid-based robot navigation domain multiple rooms compare results state-of-the-art algorithm prql experimental settings navigation domain used prql paper. recent works transfer learning also conduct experiments similar grid-world domain navigation domain composed states denote free positions goal area wall. state plotted cell. agent’s position represented two-dimensional coordinates continuously. afterwards take integer part determine discrete state agent. agent problem take four actions respectively down left right. arbitrary action makes agent’s position move corresponding direction step size make problem practical design stochastic adding uniformly distributed random variant within respectively action. agent reaches wall state wall keep agent state taking actions. agent reaches goal area obtain reward episode ends. arriving states except goal state generates reward. agent high-level view observes current state. figure represent goals target task numbers figure denote goals source task library source policies library optimal respective tasks. obviously similar goals room tasks dissimilar problem large number states initial belief uniform distribution. therefore learning scratch rather slow transfer learning signiﬁcantly accelerate learning process. experiment q-function update. enough approach learn policy high reward. avoid agent entering dead loop goals different tasks differentan agent takes actions according past policy larger probability beginning episode. afterwards takes random actions. addition decreases time. approach converges \u0001-greedy last. make parameters simple \u0001-greedy policy conduct experiments prql q-learning comparison. q-learning utilizes \u0001-greedy parameter \u0001-greedy approach. parameters prql consistent greedy policy episode since three learning methods off-policy. learning process figure executed times. average value shown error bars represent standard deviations. figure axis represents number episodes axis represents average evaluation reward. figure three observations. first algorithm uses less time reach threshold average reward prql. average reward approach greater episodes end. second asymptotic performance approach better prql. although overwhelmingly comparable convergence value cumulative reward since reward experiment sparse exceedingly large. third reward q-learning increases much slowly approach knowledge transfer approach intensely efﬁcient negative transfer occurs. addition standard deviations approach smallest among three curves demonstrates approach extremely stable performance executions. upper bound variance bernoulli random ucb-tuned outperforms multitude experiments. although proved theoretically optimal another algorithm ucb-v also considers variance bandit rewards already proved optimal theory variance expectation reward experiment much smaller algorithm lager lead higher exploration rate suitable circumstance larger variance. experimental results order manifest approach achieves optimal source policy selection ﬁrstly show learning curve evaluation episode frequency selecting source policies. next compare expected reward among prql approach traditional q-learning. then reward functions target tasks randomly conduct experiment demonstrate robustness approach. afterwards conduct experiment indicate method equally applicable circumstance similar task exists source task library. finally present scenario prql converge greedy policy. however case approach still converges \u0001-greedy strategy. since approach selects source policies deterministically cannot compare probability selecting source policy. therefore show frequency selectindicate method applied circumstance similar task source task library conduct experiment solve target task figure using source task library. goal totally different room compared goals source tasks. source policies library useless performance comparison prql approach q-learning shown figure shown figure ﬁrst episodes three curves similar growth trend algorithms exploration beginning. afterwards curve prql starts ﬂatten converged greedy policy random actions taken. contrast approach almost performance q-learning curves keep rising since methods exploration \u0001-greedy strategy. \u0001-greedy strategy approach plays paramount role learning. position marked figure goal task present case prql converge greedy policy. show probability executing policy prql solve figure goal goal tasks especially similar. prql ends reusing similar source task solve rather greedy policy times. source task especially similar target task reward reusing similar policy exceed reward greedy policy prql converges relevant source task. contrast algorithm controls exploration rate tuning choose \u0001-greedy strategy probability decreases time algorithm invariably converges \u0001-greedy policy matter similar target task source tasks. source policies approach prql figure approach almost select irrelevant tasks episodes. contrast frequency choosing dissimilar tasks prql still around episodes. costs shorter time method detect source task proper transfer from. curve drops slowly method keep using past policy explore environment reuse method different prql. satisfy stochastic assumption split greedy actions reuse process. frequency different probability select \u0001-greedy policy high probability large. since initial belief experiment uniform distribution expected reward denoted maxa figure shows expected reward figure curve approach rises fastest demonstrates agent reaches goal times using time. obtain rewards learning rewards back states. expected reward converges slowly reaching goal state generates reward. therefore shown convergent part curves seem polynomial. demonstrate robustness algorithm randomly select goals target tasks guaranteed goals room source tasks figure similar source task transfer from. next experiment discuss situation source knowledge useful. choose different goals experiment shown figure numbers. figure shows average evaluation reward algorithm prql q-learning solving tasks denoted figure reward algorithm faster increment prql cases. moreover convergence value reward algorithm sometimes little larger. prql converges greedy policy explore more soon reward greedy policy exceeds source policies. thus converge suboptimal policy end. however approach keeps exploration every state-action pair visited inﬁnitely often. policies. method formulates online source policy selection problem. contrast previous works work provides theoretical ground achieve optimal source policy selection process. addition augment q-learning policy reuse maintain theoretical guarantee convergence tradional q-learning. furthermore present empirical validation algorithm outperforms state-of-the-art transfer learning method promotes transfer successfully practice. reuse. able deal general circumstance different state action spaces. second intend formulate source task selection problem setting select source tasks based current state thus moment transfer determined automatically. finally it’s importance extend proposed algorithm deep test benchmark problems.", "year": 2017}