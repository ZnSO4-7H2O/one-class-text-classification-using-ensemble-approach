{"title": "Distance and Similarity Measures Effect on the Performance of K-Nearest  Neighbor Classifier - A Review", "tag": ["cs.LG", "cs.AI"], "abstract": "The K-nearest neighbor (KNN) classifier is one of the simplest and most common classifiers, yet its performance competes with the most complex classifiers in the literature. The core of this classifier depends mainly on measuring the distance or similarity between the tested example and the training examples. This raises a major question about which distance measures to be used for the KNN classifier among a large number of distance and similarity measures? This review attempts to answer the previous question through evaluating the performance (measured by accuracy, precision and recall) of the KNN using a large number of distance measures, tested on a number of real world datasets, with and without adding different levels of noise. The experimental results show that the performance of KNN classifier depends significantly on the distance used, the results showed large gaps between the performances of different distances. We found that a recently proposed non-convex distance performed the best when applied on most datasets comparing to the other tested distances. In addition, the performance of the KNN degraded only about $20\\%$ while the noise level reaches $90\\%$, this is true for all the distances used. This means that the KNN classifier using any of the top $10$ distances tolerate noise to a certain degree. Moreover, the results show that some distances are less affected by the added noise comparing to other distances.", "text": "k-nearest neighbor classiﬁer simplest common classiﬁers performance competes complex classiﬁers literature. core classiﬁer depends mainly measuring distance similarity tested example training examples. raises major question distance measures used classiﬁer among large number distance similarity measures? review attempts answer previous question evaluating performance using large number distance measures tested number real world datasets without adding diﬀerent levels noise. experimental results show performance classiﬁer depends signiﬁcantly distance used results showed large gaps performances diﬀerent distances. found recently proposed non-convex distance performed best applied datasets comparing tested distances. addition performance degraded noise level reaches true distances used. means classiﬁer using distances tolerate noise certain degree. moreover results show distances less aﬀected added noise comparing distances. oldest simplest accurate algorithms patterns classiﬁcation regression models. proposed hodges modiﬁed cover hart identiﬁed methods data mining consequently studied past decades widely applied many ﬁelds thus comprises baseline classiﬁer many pattern classiﬁcation problems pattern recognition text categorization ranking models object recognition event recognition applications. non-parametric algorithm kataria singh non-parametric means either parameters ﬁxed number parameters irrespective size data. instead parameters would determined size training dataset. assumptions need made underlying data distribution. thus could best choice classiﬁcation study involves little prior knowledge distribution data. addition lazy learning methods. implies storing training data waits test data produced without create learning model wettschereck john several studies conducted analyze performance classiﬁer using diﬀerent distance measures. study applied various kinds datasets diﬀerent distributions types data using diﬀerent numbers distance similarity measures. chomboon co-workers analyzed performance classiﬁer using distance measures. include euclidean mahalanobis manhattan minkowski chebychev cosine correlation hamming jaccard standardized euclidean spearman distances. experiment applied eight binary synthetic datasets various kinds distributions generated using matlab. divided dataset training testing set. results showed manhattan minkowski chebychev euclidean mahalanobis standardized euclidean distance measures achieved similar accuracy results outperformed tested distances. punam nitin evaluated performance classiﬁer using chebychev euclidean manhattan distance measures dataset dataset contains features classes type data numeric. dataset normalized conducting experiment. evaluate performance accuracy sensitivity speciﬁcity measures calculated distance. reported results indicate manhattan distance outperform tested distances accuracy rate sensitivity rate speciﬁcity rate. analyzed eﬀect distance measures classiﬁer medical domain datasets. experiments based three diﬀerent types medical datasets containing categorical numerical mixed types data chosen machine learning repository four distance metrics including euclidean cosine square minkowsky distances. divided dataset data training testing values ranging experimental results showed square distance function best choice three different types datasets. however using cosine euclidean minkowsky distance metrics performed ‘worst’ mixed type datasets. ‘worst’ performance means method lowest accuracy. todeschini ballabio consonni todeschini analyzed eﬀect eighteen diﬀerent distance measures performance classiﬁer using eight benchmark datasets. investigated distance measures included manhattan euclidean soergel lance–williams contracted jaccard– tanimoto jaccard–tanimoto bhattacharyya lagrange mahalanobis canberra wave-edge clark cosine correlation four locally centered mahalanobis distances. evaluating performance distances non-error rate average rank calculated distance. result indicated ‘best’ performance manhattan euclidean soergel contracted jaccard–tanimoto lance–williams distance measures. ‘best’ performance means method highest accuracy. lopes ribeiro analyzed impact distance metrics namely euclidean manhattan canberra chebychev minkowsky instance-based learning algorithms. particularly classiﬁer incremental hypersphere classiﬁer classiﬁer reported results empirical evaluation ﬁfteen datasets diﬀerent sizes showing euclidean manhattan metrics signiﬁcantly yield good results comparing tested distances. alkasassbeh altarawneh hassanat investigated eﬀect euclidean manhattan hassanat distance metrics performance classiﬁer ranging square root size training considering k’s. addition experimenting classiﬁers ensemble nearest neighbor classiﬁer inverted indexes neighbors classiﬁer experiments conducted datasets taken machine learning repository reported results show hassanat distance outperformed manhattan euclidean distances tested datasets using three investigated classiﬁers. lindi investigated three distance metrics best performer among classiﬁer employed matcher face recognition system proposed robot. tested distances chi-square euclidean hassanat distances. experiments showed hassanat distance outperformed distances terms precision slower distances. table provides summary previous works evaluating various distances within classiﬁer along best distance assesed them. seen literature review related works previous works investigated either small number distance hassanat hassanat hassanat euclideanandmanhattan lance–williams contractedjaccard–tanimoto manhattaneuclideansoergel chisquare manhattan standardizedeuclidean euclideanmahalanobis chebychev manhattanminkowski currentworkcomparesthehighestnumberofdistancemeasuresonvarietyofdatasets. tablecomparisonbetweenpreviousstudiesfordistancemeasuresinknnclassiﬁeralongwith‘best’performingdistance.comparativelyour classiﬁer distances test sample training data samples identiﬁed diﬀerent measures tools. therefore distance measures play vital role determining ﬁnal classiﬁcation output euclidean distance widely used distance metric classiﬁcations however studies examined eﬀect diﬀerent distance metrics performance used small number distances small number datasets both. shortage experiments prove distance best used classiﬁer. therefore review attempts bridge testing large number distance metrics large number diﬀerent datasets addition investigate distance metrics least aﬀected added noise. classiﬁer deal noisy data therefore need investigate impact choosing diﬀerent distance measures performance classifying large number real datasets addition investigate distance lowest noise implications. main research questions addressed review organized review follows. first section provide introductory overview classiﬁcation method present history characteristics advantages disadvantages. review deﬁnitions various distance measures used conjunction knn. section explains datasets used classiﬁcation experiments structure experiments model performance evaluations measures. present discuss results produced experimental framework. finally section provide conclusions possible future directions. algorithm classiﬁes unlabelled test sample based majority similar samples among k-nearest neighbors closest test sample. distances test sample training data samples determined speciﬁc distance measure. figure shows example contains training samples classes ﬁrst class ‘blue square’ second class ‘red triangle’. test sample represented green circle. samples placed dimensional feature spaces dimension feature. classify test sample belongs class ‘blue square’ class ‘red triangle’; adopts distance function nearest neighbors test sample. finding majority classes among nearest neighbors predicts class test sample. case test sample classiﬁed ﬁrst class ‘red triangle’ triangles blue square inside inner circle classiﬁed blue square class triangles blue squares. optimum value algorithm? high computational time cost need compute distance test sample training samples test example need time complexity number examples training data number features example. ﬁrst problem solved either using examples taking inverted indexes using ensemble learning second third problems many studies proposed diﬀerent solutions depending reducing size training dataset include limited wilson martinez using approximate classiﬁcation although previous studies literature investigated fourth problem attempt investigate fourth problem much larger scale i.e. investigating large number distance metrics tested large problems. addition investigate eﬀect noise choosing suitable distance metric used classiﬁer. repeated class neighbors assigned test sample. words test sample assigned class frequent class label among nearest training samples. class nearest neighbor assigned test sample. algorithm described algorithm existence noise data mainly related applied acquire preprocess data environment noisy data corrupted form data leads partial alteration data values. main sources noise identiﬁed first implicit errors caused measurement tools using diﬀerent types sensors. second random errors caused batch processes experts collecting data example errors process document digitization. based sources errors types noise classiﬁed given dataset magnitude generated noise values extent noise aﬀects data relative data value attribute relative standard deviation minimum maximum attribute. ﬁrst appearance word distance found writings aristoteles argued word distance means extremities distance greatest things something them certain distance. addition distance sense dimension euclid important mathematicians ancient history used word distance third postulate principia every circle described centre distance. distance numerical description apart entities are. data mining distance means concrete describing means elements space close away other. synonyms distance include farness dissimilarity diversity synonyms similarity include proximity nearness distance function vectors function deﬁnes distance vectors non-negative real number. function considered metric satisfy certain number properties include following consider eight major distance families consist ﬁfty four total distance measures. categorized distance measures following similar categorization done follows give mathematical deﬁnitions distances measure closeness vectors numeric attributes. example show computed distance value vectors theoretical analysis diﬀerent minkowski distance measures family distances includes three distance metrics special cases minkowski distance corresponding diﬀerent values power distance. minkowski distance also known norm generalized metric. deﬁned distance variant minkowski distance value vector value vector manhattan manhattan distance also known norm taxicab norm rectilinear distance city block distance considered hermann minkowski th-century germany. distance represents absolute diﬀerences opposite values vectors. chebyshev chebyshev distance also known maximum value distance lagrange chessboard distance distance appropriate cases objects deﬁned diﬀerent diﬀerent dimension metric deﬁned vector space distance vectors greatest diﬀerence along coordinate dimension. distance measures distance family depends mainly ﬁnding absolute diﬀerence family include lorentzian canberra sorensen soergel kulczynski mean character intersection distances. lorentzian distance lorentzian distance represented natural absolute diﬀerence vectors. distance sensitive small changes since scale expands lower range compresses higher range. canberra distance canberra distance introduced williams lance modiﬁed lance williams weighted version manhattan distance absolute diﬀerence attribute values vectors divided absolute attribute values prior summing distance mainly used positive values. sensitive small changes near zero sensitive proportional absolute diﬀerences. therefore characteristic becomes apparent higher dimensional space respectively increasing number variables. canberra distance often used data scattered around origin. sorensen distance sorensen distance also known bray–curtis commonly applied measurements express relationships ecology environmental sciences related ﬁelds. modiﬁed manhattan metric summed diﬀerences attributes values vectors standardized summed attributes values also known ruzicka distance. binary variables only distance identical complement tanimoto similarity coeﬃcient distance obeys four metric properties provided attributes nonnegative values inner product distance measures distance measures belonging family calculated products pair wise values vectors type distances includes jaccard cosine dice chord distances. jaccard distance jaccard distance measures dissimilarity sample sets complementary jaccard similarity coeﬃcient obtained subtracting jaccard coeﬃcient one. distance metric cosine distance cosine distance also called angular distance derived cosine similarity measures angle vectors cosine distance obtained subtracting cosine similarity one. dice distance dice distance derived dice similarity complementary dice similarity obtained subtracting dice similarity one. sensitive values near zero. distance metric particular property triangle inequality hold. distance widely used information retrieval documents biological taxonomy. chord distance modiﬁcation euclidean distance introduced orloci used analyzing community composition data deﬁned length chord joining normalized points within hypersphere radius one. distance distance measures commonly used clustering continuous data squared chord distance measures distances belong family obtained calculating geometrics. geometric mean values square root product. distances family cannot used features vector negative values family includes bhattachayya squared chord matusita hellinger distances. squared chord distance squared chord distance mostly used paleontologists studies pollen. distance square square root diﬀerence point taken along vectors increases diﬀerence dissimilar feature. hellinger distance hellinger distance also called jeﬀries matusita distance introduced hellinger metric used measure similarity probability distributions. distance closely related bhattacharyya distance. squared distance measures distance measure family square diﬀerence point long vectors considered total distance family includes squared euclidean clark neyman pearson squared probabilistic symmetric divergence additive symmetric average mean censored euclidean squared chi-squared distances. average distance average distance also known average euclidean modiﬁed version euclidean distance euclidean distance following drawback data vectors attribute values common smaller distance pair data vectors containing attribute values that distance adopted. mean censored euclidean distance distance squared diﬀerences values calculated mean value summed value divided total number values pairs values equal zero. that square root mean computed ﬁnal distance. shannon entropy distance measures distance measures belonging family related shannon entropy distances include kullback-leibler jeﬀreys divergence topsoe jensen-shannon jensen diﬀerence distances. kullback-leibler distance kullback-leibler distance introduced kullback leibler also known divergence relative entropy information deviation measures diﬀerence probability distributions. distance metric measure symmetric. furthermore satisfy triangular inequality property therefore called quasi-distance. kullback-leibler divergence used several natural language applications query expansion language models categorization topsoe distance topsoe distance also called information statistics symmetric version kullbackleibler distance. topsoe distance twice jensen-shannon divergence. distance metric square root metric. vicissitude distance measures vicissitude distance family consists four distances vicis-wave hedges vicis symmetric symmetric symmetric distances. distances generated syntactic relationship aforementioned distance measures. vicis-wave hedges distance so-called wave-hedges distance applied compressed image retrieval content based video retrieval time series classiﬁcation image ﬁdelity ﬁnger print recognition etc.. interestingly source wave-hedges metric correctly cited previously mentioned resources allude incorrectly hedges source distance measures metrics exhibits distance measures utilizing multiple ideas measures previous distance measures include limited average kumar-johnson taneja pearson correlation squared pearson hamming hausdorﬀ statistic whittaker’s index association meehl motyka hassanat distances. pearson distance pearson distance derived pearsons correlation coeﬃcient measures linear relationship vectors distance obtained subtracting pearsons correlation coeﬃcient one. hamming distance hamming distance distance metric measures number mismatches vectors. mostly used nominal data string bitwise analyses also useful numerical data. satisfying metric properties distance proved metric hassanat metric matter diﬀerence values distance range maximum distance approaches dimension tested vectors therefore increases dimensions increases distance linearly worst case. experiments done twenty eight datasets represent real life classiﬁcation problems obtained machine learning repository machine learning repository collection databases domain theories data generators used machine learning community empirical analysis machine learning algorithms. database created david fellow graduate students irvine. since time widely used students educators researchers world primary source machine learning data sets. dataset consist examples. example deﬁned number attributes examples inside data represented number attributes. attributes called class attribute contains class value data whose values predicted test examples. short description datasets used provided table real& integer positive integer positive integer positive integer real& integer positive integer positive integer positive integer positive integer positive integer positive integer positive real positive real positive real positive real positive real positive real positive real real& integer binary real real real real real real real real name heart balance cancer german liver vehicle vote haberman letter rec. wholesale australian glass sonar wine parkinson iris diabetes monkey ionosphere phoneme segmen vowel wave wave banknote qsar dataset divided data sets training testing. purpose data used testing data dedicated training. value simplicity. data used test sample chosen randomly experiment data repeated times obtain random examples testing training. overall experimental framework shown figure experiments divided major parts second part experiments aims best distance measure used classiﬁer case noisy data. work deﬁne ‘best’ method method performs highest accuracy. added noise dataset various levels noise. experiments second part conducted using distances achieved best results ﬁrst part experiments. therefore order create noisy dataset original level noise selected range level noise means number examples need noisy amount noise selected randomly minimum maximum values attribute attributes examples corrupted random noise number noisy examples selected randomly. algorithm describes process corrupting data random noise used experiments purposes work. diﬀerent measures available evaluating performance classiﬁers. study three measures used accuracy precision recall. accuracy calculated evaluate overall classiﬁer performance. deﬁned ratio test samples correctly classiﬁed number tested examples order assess performance respect every class dataset compute precision recall measures. precision fraction retrieved instances relevant recall fraction relevant instances retrieved. measures constructed computing following accuracy precision recall calculated classiﬁer using similarity measures distance metrics discussed section datasets described table compare asses performance classiﬁer using diﬀerent distance metrics similarity measures. purposes review sets experiments conducted. ﬁrst compare performance classiﬁers used distances similarity measures reviewed section without noise. second experiments designed robust distance aﬀected least diﬀerent noise levels. number diﬀerent predeﬁned distance families used experiments. accuracy distance dataset averaged runs. technique followed distance families report accuracy recall precision classiﬁer distance dataset. average values distances considered paper summarized table hasd obtained highest overall average. ilies datasets. achieved highest accuracy datasets namely vehicle vowel average accuracies respectively. hand cand achieved highest accuracy datasets australian wine datasets average accuracies respectively. achieved highest accuracy segmen dataset average accuracy among minkowski distance families achieved similar performance overall accuracies datasets; similarity distances. tances letter rec. dataset average accuracy among minkowski distance families jacd dicd outperform tested distances banknote dataset average accuracy similar performance overall accuracy datasets similarity distances. distance measures family outperform distance families datasets namely ascsd achieved highest accuracy german dataset average accuracy clad divd achieved highest accuracy vote dataset average accuracy among minkowski squared distance measures family dataset australian cand balance banknote cancer diabetes glass haberman heart ionosphere hasd liver vsdf monkey wiad vsdf parkinson vsdf phoneme hasd qsar segmen sodsd miscsd sonar vehicle claddivd vote vowel wholesale avgd cand wine ascsd german iris chodspeadcordcosd edadmcedsed wave chodspeadcordcosd edadmcedsed wave letter rec. jacddicd achieved similar performance datasets; similarity three distances. also distances mced outperform tested distances datasets wave wave average accuracies respectively. among distance squared families mced achieved highest accuracy glass dataset average accuracy similar performance overall accuracies datasets similarity distances topd twice jsd. outperforms tested distances haberman dataset average accuracy families datasets namely vsdf achieved highest accuracy three datasets liver parkinson phoneme accuracies respectively. mscsd achieved highest accuracy diabetes dataset average accuracy miscsd also achieved highest accuracy sonar dataset average accuracy lies datasets. wiad achieved highest accuracy monkey dataset average accuracy avgd also achieved highest accuracy wholesale dataset average accuracy hasd also achieved highest accuracy four datasets namely cancer ionosphere qsar average accuracies respectively. finally hamd achieved highest accuracy heart dataset average accuracy among inner product distance measures families spead cord chod cosd outperform tested distances three datasets namely balance iris average accuracies respectively. datasets example cand achieved highest recalls datasets australian wine average recalls respectively. also achieved highest recalls four datasets glass ionosphere vehicle vowel average recalls respectively. achieved highest recall segmen dataset average recall. among minkowski distance families achieved similar performance expected similarity. dataset australian cand balance banknote cancer diabetes glass haberman dicdjacd heart ionosphere liver vsdf monkey wiad vsdf parkinson vsdf phoneme hasd qsar segmen sodsd miscsd sonar vehicle claddivd vote vowel pcsd wholesale cand wine ascsd german chodspeadcordcosd iris edadmcedsed wave chodspeadcordcosd wave edadmcedsed letter rec. vsdf families datasets namely clad divd outperform tested distances vote dataset average recall. pcsd outperforms tested distances wholesale dataset average recall. ascsd also outperforms tested distances german dataset average recall. among minkowski squared distance measures families achieved similar performance datasets; equations similarity clariﬁed previously. distances mced distance outperform tested distances datasets namely wave wave average recalls respectively. families datasets. vsdf achieved highest recall three datasets liver parkinson phoneme datasets average recalls respectively. mscsd achieved highest recall diabetes dataset average recall. miscsd also achieved highest recall sonar dataset average recall. vsdf achieved highest recall letter rec. dataset average recall. families datasets. particularly hamd achieved highest recall heart dataset average recall. wiad also achieved highest average recall monkey dataset average recall. hasd also achieved highest average recall three datasets namely cancer qsar average recalls respectively. among inner product distance measures families spead cord chod cosd outperform tested distances three datasets namely balance iris average recalls respectively. dataset australian cand balance banknote cancer diabetes glass haberman heart ionosphere hasd liver vsdf monkey wiad vsdf parkinson vsdf phoneme hasd qsar segmen sodsd miscsd sonar vehicle claddivd vote vowel wholesale dicdjacd wine german iris wave wave letter rec. edadsed namely australian wine average precisions respectively. achieved highest precision segmen dataset average precision. addition achieved highest precision three datasets namely glass vehicle vowel average precisions respectively. among minkowski distance families achieved similar performance datasets; equations similarity clariﬁed previously. also jacd dicd outperform tested measures wholesale dataset average precision. among minkowski distance families jacd dicd tested distances banknote dataset average precision. performance; equations similarity clariﬁed previously. distance measures family outperform distance families three datasets namely ascsd achieved highest average precisions datasets diabetes german average precisions respectively. clad dicd also achieved highest precision vote dataset average precision. among minkowski squared distance measures families achieved similar performance expected similarity. distances mced outperform tested measures datasets namely wave wave average precisions respectively. also outperform tested measures letter rec. average precision. ilies four datasets. vsdf achieved highest average precisions three datasets liver parkinson phoneme average precisions respectively. miscsd also achieved highest precision sonar dataset average precision. families datasets. particular hamd achieved highest precision heart dataset average precision. also wiad achieved highest precision monkey dataset average precision. moreover hasd yield highest precision four datasets namely cancer ionosphere average precisions respectively. among inner product distance measures families spead cord chod cosd outperform tested distances three datasets namely balance iris average precisions respectively. also cosd spead cord achieved highest precision heberman dataset average precision. table shows distances respect overall average accuracy recall precision datasets. hasd outperforms tested distances performance measures followed cand scsd. moreover closer look data average well highest accuracies precisions recalls hasd outperform distance measures datasets namely cancer ionosphere qsar true accuracy precision recall distance metric least datasets noise-free experiment set. note performance following group members topd squd pscsd matd within close similarity deﬁning corresponding distances. attribute success hassanat distance experimental part characteristics discussed section dimension tested vectors contributes maximally ﬁnal distance lowers neutralizes eﬀects outliers diﬀerent datasets. analyze performance hassanat distance comparing distances used wilcoxon’s rank-sum test non-parametric pairwise test aims detect signiﬁcant diﬀerences sample means judge null hypothesis true not. null hypothesis hypothesis used statistics assumes signiﬁcant diﬀerence diﬀerent results observations. test conducted hassanat distance distances tested datasets. therefore null hypothesis there signiﬁcant diﬀerence performance hassanat distance compared distance datasets used. according wilcoxon test result test showed p-value less signiﬁcance level reject null hypothesis conclude signiﬁcant diﬀerence tested samples; otherwise cannot conclude anything signiﬁcant diﬀerence accuracies recalls precisions hassanat distance datasets used experiment compared distance measures corresponding p-values given table p-values less signiﬁcance level highlighted bold. seen table p-values accuracy results less signiﬁcance level eight times reject null hypothesis conclude signiﬁcant diﬀerence performance hassanat distance compared cand cosd clad scsd wiad cord divd since average performance hassanat distance better distance measures previous tables conclude accuracy yielded hassanat distance better distance measures tested. similar analysis applies recall precision columns comparing hassanat results distances. next experiments identify impact noisy data performance classiﬁer regarding accuracy recall precision using different distance measures. accordingly nine diﬀerent levels noise added dataset using algorithm simplicity experiments figure shows experimental results classiﬁer clarify impact noise accuracy performance measure using distances. x-axis denotes noise level y-axis represents classiﬁcation accuracy. column noise level represents overall average accuracy distance datasets used. error bars represent average standard deviation values distance datasets. figure shows recall results classiﬁer clarify impact noise performance using distance measures. figure shows precision results classiﬁer clarify impact noise performance using distance measures. seen figures performance degraded noise level reaches true distances used. means classiﬁer using distances tolerate noise certain degree. moreover distances less aﬀected added noise comparing distances. therefore ordered distances according overall average accuracy recall precision results level noise. distance highest performance ranked ﬁrst position distance lowest performance ranked last position order. tables show ranking structure terms accuracy precision recall noise level high empty cells occur sharing rank distance. following points summarize observations terms accuracy precision recall values tained hasd achieved ﬁrst rank majority noise levels. distance succeeds ﬁrst rank noise levels however level outperformed hasd. also outperformed hasd noise level %.the cand achieved second rank noise levels moreover distance achieved third rank rest noise levels except noise levels scsd achieved fourth rank noise levels third rank level noise distance equal noise level clard achieved third rank noise levels based results tables observe ranking distances terms accuracy recall precision without presence noise diﬀerent ranking adding ﬁrst level noise become variants signiﬁcantly increased level noise progressively. means distances aﬀected noise. however crucial question distances least aﬀected noise? results conclude hasd least aﬀected followed cand scsd. order justify distances aﬀected either less noise following examples designed. illustrate eﬀect noise ﬁnal decision classiﬁer using hassanat standard euclidean distances. examples assume training vectors three attributes each addition test vector usual calculate distances using euclidean hassanat distances. example example shows feature vectors example corrupting features added noise. make previous calculations using noisy data instead clean data; ﬁrst attribute corrupted added noise based minimum distance approach using euclidian distance test vector assigned class instead however test vector assigned class using hassanat distance makes distance accurate existence noise. although simple examples showed euclidean distance aﬀected noise consequently aﬀected classiﬁcation ability. although performance classiﬁer decreased noise increased distances less aﬀected noise distances. example using change attribute contributes highly ﬁnal distance even vectors similar feature noise distance becomes unpredictable. contrast hassanat distance found distance consecutive attributes bounded range thus regardless value added noise feature contributes maximally ﬁnal distance proportional value added noise. therefore impact noise ﬁnal classiﬁcation mitigated. review performance classiﬁer evaluated using large number distance measures clean noisy datasets attempting appropriate distance measure used general. addition tried ﬁnding appropriate robust distance used case noisy data. large number experiments conducted purposes review results analysis experiments show following performance classiﬁer depends signiﬁcantly distance used results showed large gaps performances diﬀerent distances. example found hassanat distance performed best applied datasets comparing tested distances. similar classiﬁcation results distances family almost equation distances similar example twice other square another. cases since compares examples using distance nearest neighbors distances multiplied divided constant. distances less aﬀected added noise comparing distances example found hassanat distance performed best applied datasets diﬀerent levels heavy noise. although tested large number distance measures still many distances similarity measures available machine learning area need tested evaluated performance. datasets though higher previously tested still might enough draw signiﬁcant conclusions terms eﬀectiveness certain distance measures therefore need larger number datasets varied data types.", "year": 2017}