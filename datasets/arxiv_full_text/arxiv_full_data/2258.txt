{"title": "Continuous DR-submodular Maximization: Structure and Algorithms", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "DR-submodular continuous functions are important objectives with wide real-world applications spanning MAP inference in determinantal point processes (DPPs), and mean-field inference for probabilistic submodular models, amongst others. DR-submodularity captures a subclass of non-convex functions that enables both exact minimization and approximate maximization in polynomial time.  In this work we study the problem of maximizing non-monotone DR-submodular continuous functions under general down-closed convex constraints. We start by investigating geometric properties that underlie such objectives, e.g., a strong relation between (approximately) stationary points and global optimum is proved. These properties are then used to devise two optimization algorithms with provable guarantees. Concretely, we first devise a \"two-phase\" algorithm with $1/4$ approximation guarantee. This algorithm allows the use of existing methods for finding (approximately) stationary points as a subroutine, thus, harnessing recent progress in non-convex optimization. Then we present a non-monotone Frank-Wolfe variant with $1/e$ approximation guarantee and sublinear convergence rate. Finally, we extend our approach to a broader class of generalized DR-submodular continuous functions, which captures a wider spectrum of applications. Our theoretical findings are validated on synthetic and real-world problem instances.", "text": "dr-submodular continuous functions important objectives wide real-world applications spanning inference determinantal point processes mean-ﬁeld inference probabilistic submodular models amongst others. dr-submodularity captures subclass non-convex functions enables exact minimization approximate maximization polynomial time. work study problem maximizing non-monotone continuous drsubmodular functions general down-closed convex constraints. start investigating geometric properties underlie objectives e.g. strong relation stationary points global optimum proved. properties used devise optimization algorithms provable guarantees. concretely ﬁrst devise two-phase algorithm approximation guarantee. algorithm allows existing methods ﬁnding stationary points subroutine thus harnessing recent progress non-convex optimization. present non-monotone frank-wolfe variant approximation guarantee sublinear convergence rate. finally extend approach broader class generalized dr-submodular continuous functions captures wider spectrum applications. theoretical ﬁndings validated synthetic real-world problem instances. submodularity classically well known function optimization enables efﬁcient minimization approximate maximization polynomial time. submodularity recently studied integer lattice continuous domains signiﬁcant theoretical results practical applications. functions well known submodularity equivalent diminishing returns property. however hold integer-lattice functions continuous functions property deﬁnes subclass submodular functions called dr-submodular functions. continuous domains applying convex optimization techniques enables efﬁcient minimization submodular continuous functions shown continuous submodularity enables constant-factor approximation schemes constrained monotone dr-submodular maximization constrained non-monotone submodular maximization problems. many real-world non-convex problems maximizing softmax extension dpps require maximizing non-monotone dr-submodular function general down-closed convex constraint. current theory apply general problem setting motivates develop guaranteed efﬁcient algorithms problems. exploring structure underlies dr-submodularity crucial deriving guaranteed algorithms. combined notion non-stationarity constrained optimization problems notion strong dr-submodularity rich structure problem continuous dr-submodular maximization. turn gives rise approximation algorithms provable guarantees. speciﬁcally make following contributions based geometric properties present algorithms two-phase frankwolfe-style algorithm approximation guarantee converges rate; non-monotone frank-wolfe variant exhibits approximation guarantee converges sublinearly. even though worst-case guarantee ﬁrst worse second yields several practical advantages discuss section investigate generalized class submodular functions conic lattices. allows model larger class non-trivial applications. include logistic regression non-convex separable regularizer non-negative etc. optimize them provide reduction enables invoke algorithms continuous submodular optimization problems. experimentally demonstrate applicability methods synthetic realnotation. boldface letters e.g. represent vector boldface capital letters e.g. denote matrix. entry entry denote standard basis vector. used denote continuous function represent function. integer means euclidean norm default. given vectors means yi∀i. denote coordinate-wise maximum coordinate-wise minimum respectively. general setup constrained non-monotone dr-submodular maximization interval wlog assume lower bound i.e. assumed down-closed convex down-closedness means implies diameter maxxy∈p holds denote global maximum assume non-negative since otherwise needs lower bound minimum function value continuous domains dr-submodular function submodular function diminishing returns property deﬁnition function dr-submodular s.t. still holds differentiable show deﬁnition equivalent antitone mapping furthermore twice-differentiable property equivalent entries hessian non-positive i.e. function dr-supermodular dr-submodular. also assume lipschitz gradients many continuous objectives practice turn dr-submodular. list several them. found appendix softmax extension. determinantal point processes probabilistic models repulsion used model diversity machine learning constrained inference problem np-hard combinatorial problem general. currently methods best approximation guarantees based either maximizing multilinear extension softmax extension dr-submodular functions multilinear extension given expectation original function values thus evaluating objective extension requires expensive sampling. constast softmax extension closed form expression much appealing computational perspective. positive semideﬁnite kernel matrix softmax extension identity matrix diag diagonal matrix diagonal elements problem inference dpps corresponds problem maxx∈p down-closed convex constraint e.g. matroid polytope matching polytope. mean-ﬁeld inference log-submodular models. log-submodular models class probabilistic models subsets ground log-densities submodular functions exp) typically hard evaluate. mean-ﬁeld inference approximate factorized minimizing distance measured submodular continuous functions already model many scenarios. several interesting cases general submodular still captured generalized notion. generalization enables develop polynomial algorithms guarantees using ideas continuous submodular optimization. present representative objective appendix show technical details covered class submodular continuous functions conic lattices. consider logistic regression model non-convex separable regularizer. ﬂexibility result better statistical performance compared classical models convex regularizers. training points corresponding binary labels {±}m. assume following mild assumption satisﬁed ﬁxed task solve following non-convex optimization problem deﬁne vector {±}n sign. show dr-submodular dr-supermodular. appendix show kα-dr-supermodular latter generalizes dr-supermodularity. usually assume optimal solution lies problem instance constrained non-monotone kα-dr-submodular maximization. section present several properties arising dr-submodular function maximization. first show properties related concavity objective along certain directions establish relation locally stationary points global optimum properties used derive guarantees algorithms section omitted proofs appendix dr-submodular function concave along non-negative/non-positive direction notice dr-submodularity stronger condition concavity along directions instance concave function concave along direction dr-submodular function. dr-submodular function l-lipschitz gradients following quadratic lower bound using standard techniques combing concavity lipschitz gradients quadratic lower bound. dr-submodular l-lipschitz gradient used section analyzing non-monotone frank-wolfe variant strong dr-submodularity. dr-submodular objectives strongly concave along directions e.g. dr-submodular quadratic functions. show additional structure exploited obtain stronger guarantees local-global relation. deﬁnition function µ-strongly dr-submodular first present following lemma motivate consider non-stationarity measure general constrained optimization problems. lemma µ-strongly dr-submodular points holds lemma implies stationary gives implicit relation practice ﬁnding exact stationary point easy usually non-convex solvers arrive approximately stationary point thus requiring proper measure non-stationarity constrained optimization problem. non-stationarity measure. looking naturally suggests maxy∈p non-stationarity measure happens coincide measure proposed recent work calculated free frank-wolfe-style algorithms order adapt local-global relation give slightly general deﬁnition here constraint non-stationarity point always holds stationary point natural generalization non-stationarity measure unconstrained optimization. next statement shows plays important role characterizing local-global relation. proposition point non-stationarity point non-stationarity holds that note propose similar relation special cases multilinear/softmax extensions mainly proving conclusion claim relation incorporate properties non-stationarity strong dr-submodularity. proof idea constructing complicated auxiliary function tailored speciﬁc dr-submodular functions. present different proof method directly utilizing property carefully constructed auxiliary points proof claim arguably succint straightforward based properties present algorithms solving ﬁrst based localglobal relation second frank-wolfe variant adapted non-monotone setting. omitted proofs deferred appendix summarize two-phase algorithm algorithm generalized two-phase method invokes non-convex solver pseudocode included algorithm appendix approximately stationary points respectively returns solution larger function value. though non-convex frank-wolfe subroutine here worth noting algorithm guaranteed approximately stationary point plugged algorithm subroutine. give improved approximation bound considering properties dr-submodular functions. borrowing results non-convex frank-wolfe subroutine following theorem output algorithm satisﬁes rate. however theorem indicates algorithm approximation guarantee good empirical performance demonstrated experiments section informally partially explained term term augment bound; close smoothness close optimal. algorithm summarizes non-monotone frank-wolfe variant inspired uniﬁed continuous greedy algorithm maximizing multilinear extension submodular function. initializes solution maintains cumulative step size. iteration maximizes linearization shrunken constraint {v|v different classical frank-wolfe-style algorithms employs update step direction chosen uniform step size cumulative step size used ensure overall step sizes thus output solution convex combination outputs hence also lies shrunken difference compared monotone frank-wolfe variant extra constraint added prevent aggressive growth solution since non-monotone setting aggressive growth hurt overall performance. next theorem states guarantees algorithm theorem consider algorithm uniform step size holds that observing applying theorem following corollary corollary output algorithm satisﬁes corollary shows algorithm enjoys sublinear convergence rate towards point inside approximation guarantee. proof sketch theorem proof induction. prepare building blocks ﬁrst show growth indeed bounded lemma assume holds remarks algorithms. notice though two-phase algorithm worse guarantee non-monotone frank-wolfe variant still interest allows ﬂexibility using wide range existing solvers ﬁnding stationary point. guarantees present rely worst-case analysis. empirical performance twophase algorithm often comparable better frank-wolfe variant. suggests explore properties concrete problems favor two-phase algorithm leave future work. test performance analyzed algorithms considering following baselines quadprogip global solver non-convex quadratic programming; projected gradient ascent diminishing step sizes algorithms iterations. subroutine two-phase frank-wolfe synthetic results average repeated experiments. experiments implemented using matlab. source code found https//github.com/bianan/non-monotone-dr-submodular. state-of-the-art global solver quadprogip global optimum used calculate approximation ratios. problem instances synthetic dr-submodular quadratic objectives down-closed polytope constraints i.e. objective rm×n constraints randomly generated following manners uniform distribution. rn×n symmetric matrix uniformly distributed entries rm×n uniformly distributed entries small positive constant order make entries strictly positive. exponential distribution. entries sampled exponential distributions speciﬁcally entry sampled matrix made symmetric. entry sampled small positive constant. cases tightest upper bound order make non-monotone mini∈ using make sure non-negative ﬁrst solve problem minx∈p quadprogip solution approximation ratios w.r.t. dimensionalities plotted figures manners data generation. number constraints figures respectively. two-phase frank-wolfe usually performs best projgrad follows non-monotone frank-wolfe variant last. good performance two-phase frankwolfe partially explained strong dr-submodularity quadratic functions according theorem performance analyzed algorithms consistent theoretical bounds approximation ratios frank-wolfe variant always much higher maximizing softmax extensions derivation derivative softmax extension i)−i)∀i denotes matrix obtained zeroing entries except gives efﬁcient calculate gradient results synthetic data. generate softmax objectives following ﬁrst generate eigenvalues randomly distributed diag. generating random unitary matrix udu. verify positive semideﬁnite eigenvalues entries generate down-closed polytope constraints form drsubmodular quadratic functions except setting function values returned different solvers w.r.t. shown figure random polytope constraints generated uniform distribution number constraints figures respectively. observe two-phase frank-wolfe still best performance non-monotone frank-wolfe variant follows projgrad worst performance. real-world results matched summarization. task matched summarization select document pairs corpus documents documents within pair similar overall pairs diverse possible. motivation task practical could example compare opinions various politicians range representative topics. experiments used similar setting experimented republican debates data consists candidates bachman gingrich huntsman paul perry romney santorum. task involves pair candidates total tasks. figure plots averaged function values returned three solvers tasks w.r.t. different values hyperparameter reﬂecting matching quality figure traces objectives w.r.t. iterations speciﬁc candidate pair two-phase frank-wolfe objectives selected phase plotted. twophase frank-wolfe also achieves best performance performance nonmonotone frank-wolfe variant projgrad comparable. submodular optimization broadly non-convex optimization extensively studied literature renders difﬁcult comprehensively surveying previous work. brieﬂy summarize related papers. submodular optimization integer-lattice continuous domains. many results submodular function optimization generalized integer-lattice case particular interest reduction integer-lattice dr-submodular maximization problem submodular function maximization problem. submodular optimization continuous domains attracted considerable attention recently classes functions covered continuous submodularity lovasz extensions multilinear extensions submodular functions. particularly multilinear extensions submodular functions also continuous dr-submodular special property coordinate-wise linear. combined rounding technique contention resolution maximizing multilinear extensions become state-of-the-art method submodular function maximization. techniques maximizing multilinear extensions inspired work. however ﬁrst explore rich properties devise algorithms general constrained dr-submodular maximization problem continuous domains. non-convex optimization. non-convex optimization receives surge attention past years. active research topic reach stationary point unconstrained optimization constrained optimization however without proper assumptions stationary point lead global approximation guarantee. local-global relation provides strong relation stationary points global optimum thus making ﬂexible incorporate progress area. studied problem constrained non-monotone dr-submodular continuous maximization. explored structural properties problems established local-global relation. based properties presented two-phase algorithm approximation guarantee non-monotone frank-wolfe variant approximation guarantee. generalized submodular continuous function conic lattices enabled model larger class applications. lastly theoretical ﬁndings veriﬁed synthetic real-world experiments. antoniadis anestis gijbels irène nikolova mila. penalized likelihood regression generalized linear models non-quadratic penalties. annals institute statistical mathematics bian andrew mirzasoleiman baharan buhmann joachim krause andreas. guaranteed non-convex optimization submodular maximization continuous domains. international conference artiﬁcial intelligence statistics boyd stephen vandenberghe lieven. convex optimization. cambridge university press calinescu gruia chekuri chandra martin vondrák jan. maximizing submodular function subject matroid constraint. integer programming combinatorial optimization springer chekuri chandra vondrák zenklusen rico. submodular function maximization multilinear relaxation contention resolution schemes. siam journal computing chekuri chandra jayram vondrák jan. multiplicative weight updates concave submodular function maximization. proceedings conference innovations theoretical computer science eghbali reza fazel maryam. designing smoothing functions improved worst-case competitive ratio online optimization. advances neural information processing systems feldman moran naor joseph schwartz roy. uniﬁed continuous greedy algorithm submodular maximization. foundations computer science ieee annual symposium ieee fuchssteiner benno lusky wolfgang. convex cones volume elsevier fujishige satoru. submodular functions optimization volume elsevier garg vijay introduction lattice theory computer science applications. john wiley ghadimi saeed guanghui zhang hongchao. mini-batch stochastic approximation methods nonconvex stochastic composite optimization. mathematical programming gharan shayan oveis vondrák jan. submodular maximization simulated annealing. proceedings twenty-second annual acm-siam symposium discrete algorithms society industrial applied mathematics gillenwater jennifer kulesza alex taskar ben. near-optimal inference determinantal point processes. advances neural information processing systems montanari andrea richard emile. non-negative principal component analysis message passing algorithms sharp asymptotics. ieee transactions information theory reddi sashank suvrit poczos barnabas smola alexander proximal stochastic methods nonsmooth nonconvex ﬁnite-sum optimization. advances neural information processing systems soma tasuku yoshida yuichi. generalization submodular cover diminishing return property integer lattice. advances neural information processing systems soma tasuku kakimura naonori inaba kazuhiro kawarabayashi ken-ichi. optimal budget allocation theoretical guarantee efﬁcient algorithm. international conference machine learning motivated objectives modeled continuous submodular functions consider general submodular continuous functions lattices induced conic inequalities. furthermore provide reduction original submodular optimization problem. deﬁnitions properties look proper cone used deﬁne conic inequality ﬁrstly. cone proper cone convex closed solid pointed proper cone used deﬁne conic inequality also deﬁnes partial ordering since binary relation reﬂexive antisymmetric transitive. easy partial ordered elements least upper bound denoted join lattice poset contains join meet pair elements lattice cone proper cone used deﬁne lattice. note conic inequalities used deﬁne lattice. example positive semideﬁne cone kpsd rn×n|a symmetric proper cone induced ordering used deﬁne lattice. simple counter example show appendix speciﬁcally name lattice deﬁned conic inequality conic lattice since particular interest modeling real-world applications paper. deﬁnition given poset induced conic inequality exist joint meet operations every pair elements s.t. still call conic lattice. word conic lattice lattice induced conic inequality following introduce class conic lattices model applications work. provide general characterization submodularity conic lattice. orthant conic lattice. given sign vector {±}n orthant cone deﬁned xiαi proper cone. points deﬁne join meet operations max{αiai αibi} min{αiai αibi} show poset conic lattice. function submodular lattice holds that establish characterizations submodularity orthant conic lattice similarly proposition function submodular lattice following equivalent characterizations s.t. s.t. s.t. still holds that proposition proved directly generalizing proof proposition proof omitted high similarity. next generalize deﬁnition dr-submodularity conic lattice correspondence relation dr-submodularity submodularity continuous domains easily similar relation bellow proposition function kα-dr-submodular kα-submodular coordinate-wise concave. combining proposition show twice differentiable kα-drsubmodular holds that similarly function kα-dr-supermodular kα-dr-submodular. remark consider orthant conic lattice here since already model applications paper. however noteworthy framework generalized arbitrary conic lattices interest model complicated applications. left future exploration. reduction optimizing submodular functions continuous domains succint section discuss reduction kα-dr-submodular maximization problems. however easy reduction works kinds kα-submodular optimization problems e.g. kα-submodular minimization problem. suppose kα-dr-submodular function kα-dr-submodular maximization problem maxy∈p rn|hi bi∀i down-closed w.r.t. conic inequality down-closedness means well. diag function kα-dr-submodular dr-submodular assume wlog. twice differentiable a∇ga afﬁne transformation transform kα-dr-submodular maximization problem dr-submodular maximization problem maxx∈p rn|hi bi∀i down-closed w.r.t. ordinary component-wise inequality verify down-closedness w.r.t. ordinary inequality here suppose point s.t. down-closedness know thus looking equivalent thus establish down-closedness given reduction reuse algorithms original dr-submodular maximization problem claim kα-dr-supermodular. proof claim show kα-dr-supermodular check second-order conpql easily dition whether holds αpαq∇ still equivalent characterizations instance characterization applications present applications fall submodular kα-submodular optimization problems. class notable examples objectives studied online setting. objectives captured dr-submodular property continuous domains. also refer section examples. dr-submodular quadratic functions. price optimization continuous prices drsubmodular quadratic optimization problem another representative class dr-submodular quadratic objectives arises computing stability number graph minx∈∆ adjacency matrix graph standard simplex. instance convex-constrained dr-submodular maximization problem. non-negative nn-pca widely used alternative models dimension reduction since projection involves non-negative weights—a required property ﬁelds like economics bioinformatics computer vision. given data points nn-pca aims solve following non-convex optimization problem make following weak assumption dimension/feature data points not). choosing sign vector {±}n sign. notice holds αpαq∇ thus kα-dr-submodular according thus treat constrained kα-dr-submodular minimization problem. submodular spectral functions. discussed submodular spectral functions following form dr-submodular algorithm taken difference lies output output solution minimum non-stationarity needed apply local-global relation. outputs solution last step. since generally hard evaluate used classical frank-wolfe step size dr-submodularity established directly applying lemma immediately implies entries non-positive dr-submodular. multilinear extension. dr-submodularity multilinear extension directly recognized considering conclusion appendix fact multilinear extension coordinate-wise linear. ﬁrst term negative multilinear extension dr-supermodular. second term separable coordinate-wise convex affect off-diagonal entries contribute diagonal entries. entries non-negative dr-supermodular w.r.t. counter example show cone lattice positive semideﬁne cone kpsd rn×n|a symmetric proper cone lattice cone. used deﬁne lattice space symmetric matrices. consider dimensional symmetric matrix space speciﬁcally following symmetric matrices generate down-closed polytope constraints form dr-submodular quadratic functions. figure shows function values returned different solvers w.r.t. random polytope constraints generated exponential distribution. speciﬁcally random polytope form entry sampled small positive constant. tightest upper bound mini∈ two-phase frank-wolfe best performance non-monotone frank-wolfe projgrad comparable performance.", "year": 2017}