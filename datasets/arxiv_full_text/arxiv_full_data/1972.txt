{"title": "Memory Aware Synapses: Learning what (not) to forget", "tag": ["cs.CV", "cs.AI", "stat.ML"], "abstract": "Humans can learn in a continuous manner. Old rarely utilized knowledge can be overwritten by new incoming information while important, frequently used knowledge is prevented from being erased. In artificial learning systems, lifelong learning so far has focused mainly on accumulating knowledge over tasks and overcoming catastrophic forgetting. In this paper, we argue that, given the limited model capacity and the unlimited new information to be learned, knowledge has to be preserved or erased selectively. Inspired by neuroplasticity, we propose an online method to compute the importance of the parameters of a neural network, based on the data that the network is actively applied to, in an unsupervised manner. After learning a task, whenever a sample is fed to the network, we accumulate an importance measure for each parameter of the network, based on how sensitive the predicted output is to a change in this parameter. When learning a new task, changes to important parameters are penalized. We show that a local version of our method is a direct application of Hebb's rule in identifying the important connections between neurons. We test our method on a sequence of object recognition tasks and on the challenging problem of learning an embedding in a continuous manner. We show state of the art performance and the ability to adapt the importance of the parameters towards what the network needs (not) to forget, which may be different for different test conditions.", "text": "humans learn continuous manner. rarely utilized knowledge overwritten incoming information important frequently used knowledge prevented erased. artiﬁcial learning systems lifelong learning focused mainly accumulating knowledge tasks overcoming catastrophic forgetting. paper argue that given limited model capacity unlimited information learned knowledge preserved erased selectively. inspired neuroplasticity propose online method compute importance parameters neural network based data network actively applied unsupervised manner. learning task whenever sample network accumulate importance measure parameter network based sensitive predicted output change parameter. learning task changes important parameters penalized. show local version method direct application hebb’s rule identifying important connections neurons. test method sequence object recognition tasks challenging problem learning embedding continuous manner. show state-of-the-art performance ability adapt importance parameters towards network needs forget different different test conditions. world around evolves continuously. millions images tags appear social media. every minute hundreds hours video uploaded youtube. content contains topics trends different seen think e.g. emerging news topics fashion trends social media hypes technical evolutions. consequently keep speed learning systems dominating paradigm date using supervised learning ignores issue. traditional supervised learning learns given task using existing training examples. training ﬁnished trained model frozen switch test mode. incoming data processed without capability adapting customizing model. soon model becomes outdated. case training process repeated using previous data extended category labels. world like ours practice becomes impractical even impossible moving real scenarios mentioned earlier data streaming might disappearing given period time even can’t stored storage constraints privacy issues. setting lifelong learning comes natural solution. studies continual learning across tasks data without storing data. goal accumulate knowledge across tasks resulting single model performs well learned tasks. question overcome catastrophic forgetting knowledge starting learning process using model. additionally techniques need scalable easy setup cope constraints real environments. unfortunately current setup methods developed evaluated rather artiﬁcial supposes sequence disjoint tasks learned other. training task ﬁnished moves next task. tasks never revisited assumed data tasks longer available. makes sense labeled data used initial training assumption also applied unlabeled data fact easily collected anytime. instead would like methods natural adaptive. figure continuous learning setup. common literature tasks learned sequence other. here assume that learning tasks agent active performs learned tasks. phase sees unlabeled samples previous tasks. information used update importance weights model parameters. classes appear frequently bigger contribution. agent learns classes important forgotten. result knowledge classes preserved erased learning task. need method makes best possible compromise different tasks. actually forget different different agents depending context deployed. words would like model adapts speciﬁc conditions system active. ideally adaptation uses unlabelled data model adapt actual test environment continuous basis. adaptation memory organization also observe biological neurosystems. ability preserve learned largely dependent frequent make skills practice often appear unforgettable unlike used long time. remarkably ﬂexibility adaptation occur absence form supervision. according hebbian theory process basis phenomenon strengthening synapses connecting neurons synchronously compared connecting neurons unrelated ﬁring behavior. work propose method coined memory aware synapses short inspired hebbian learning biological systems. unlike previous works method learn important using unlabelled data. allows personalisation continuous updating importance weights figure contributions summarized follows first method proposed. based function approximation rather focusing loss avoids need labels learning importance weights. allows adaptation unlabeled data e.gin actual test environment. second show method linked hebbian learning scheme seen local variant method. third achieve better performance state-of-the-art context object recognition context fact learning embedding used following discuss related work section then give background information section section describes method. section sheds light connection hebbian learning. test method different settings section conclude section lifelong learning studied since long time different domains machine learning touches upon broader ﬁelds metalearning learning learn focus section recent work context computer vision only. main challenge make learned model adapt data similar different environment tasks dealt sequential manner absence data previous tasks introduces risk catastrophic forgetting previously acquired knowledge. avoid issue main approaches studied data-based model-based approaches. data-based approaches data task approximate performance previous tasks. works best data distribution mismatch between tasks limited. overall need data-based approaches preprocessing step task record targets previous tasks limits applicability practical continual learning scenarios. model-based approaches like method focus parameters network instead depending task data. similar work elastic weight consolidation uses approximation argued companion paper fact learning embedding natural setup lifelong learning full model shared different tasks. challenging object recognition setup shared representation only specialized ﬁnal classiﬁcation layer task separately relying oracle activate right classiﬁcation layer depending input image. learned other. tasks correspond different datasets different splits dataset without overlap category labels different splits. crucial setup that training task data related task accessible. guarantee scalability data older tasks cannot stored models grow linearly number tasks. ideally newer tasks beneﬁt representations learned older tasks practice biggest challenge avoid catastrophic forgetting tasks’ knowledge. challenging setup joint learning tasks trained simultaneously. synaptic intelligence indicated before work involves preprocessing step task learning sequence. limits applicability approaches real scenarios. intelligent synapses approach differentiates establishing online computing importance parameters. training network estimate importance network parameter evaluating extent changing value affects loss minimized. contribution particular parameter total change loss point learning started convergence given task obtained summing contributions along training trajectory. procedure allows compute importance parameters online fashion. however based dynamics learning process suffers drawbacks mentioned earlier related work actually need online method computing adapting importance parameters network actively tested input data. diagonal term fisher information matrix identify important parameters task. training task regularizer used prevent important weights overwritten task. fisher information matrix needs computed separate phase task also needs stored task later learning task. thus stores large number parameters grows number seen tasks. avoid this improved multitask learning synaptic intelligence adopts online computing importance network parameters. showed method works equally well better training parameter update approximation much loss would change parameter value change. method described detail section ﬁrst suggest online computing accumulating importance network parameters across tasks without need preprocessing step store full importance matrix task sequence. however also drawbacks relying weight changes batch gradient descent might overestimate importance weights noted authors. also unclear method respond different learning rates layer dropout used fully connected layers. method assumes starting randomly initialized network. starting pretrained network practical computer vision applications weights might used without much changes. result importance underestimated. computation importance done training ﬁxed later. contrast believe importance weights adapt test data system actually applied work propose model-based method computes importance network parameters online manner also adaptive test unsupervised manner. goal build continual system able adapt importance weights system actually needs remember. imagine agent equipped image recognition module. module trained large images classes however user real environment subset skills actually useful. introducing task using method agent protect used skills erased less conservative others. introducing method brieﬂy remind reader standard setup summarize work synaptic intelligence build. setup standard setup focuses image classiﬁcation. consists sequence disjoint tasks figure difference loss based function based approach. training ﬁrst task measures parameter importance sensitivity loss function parameter changes contrast estimate parameter importance training based sensitivity output function using unlabeled data learning task changing important parameters penalized dimensional output function case neural network would need compute gradients output would need many backward passes size output. alternative propose gradients norm learned function output i.e. importance parameters measured sensitivity norm function output changes. regions input space sampled densely function preserved catastrophic forgetting avoided. however parameters affecting regions given importance weights used optimize function approximation tasks affecting function regions input space. learning task. task needs learned addition task loss lnew regularizer penalizes changes parameters considered important previous tasks. similarly weight regularization methods hyperparameter regularizer network parameters. allow task change parameters important previous task important parameters reused penalty changing them. finally importance matrix updated task training. since don’t make loss function computed available data. experiment section show allows method adapt specialize training test. consider deep model composed multiple layers. sake clarity slightly different notation before. parameters {θij} model weights connections pairs neurons consecutive layers goal design method computes importance value parameter indicating importance respect previous tasks. estimating parameter importance learning sequence ﬁrst receive task learned along training data input data corresponding output data. train model minimize task loss training procedure converges local minimum model learned approximation true function maps input output mapping target want preserve. instead measuring sensitivity loss function network parameters measure sensitive function output changes network parameters. given data point output network small change parameters results change function output approximated gradient learned function respect parameter evaluated data point goal preserve prediction network data point prevent changes parameters crucial prediction. based equation measure importance parameter magnitude gradient i.e. much small change parameter value change output learned function. accumulate gradients given data points given parameter obtain importance weights postulates cells together wire together synapses neurons synchronously given input strengthened time maintain possibly improve corresponding outputs. parameter importance based hebb’s rule. reconsider theory perspective artiﬁcial neural network trained successfully backpropagation. sample network predicted class corresponds last layer neuron highest activation. ﬁring neuron caused neurons previous layers highly activated given input sample. following hebb’s rule parameters connecting neurons often together learning sequence illustrated above training achieved importance weights computed follows outk output activation function neuron show application hebb’s rule ﬁnding importance network parameters above seen local version proposed approach. local version method. instead considering function learned network whole decompose sequence functions corresponding layer network i.e. fl))) total number layers. locally preserving output layer preserve global function similar procedure followed previously consider norm layer activation function following previous derivation inﬁnitesimal change parameters connecting consecutive layers results change norm local function layer output given input conclude applying hebb’s rule measure importance parameters neural network seen local variant method considers layer time instead global function learned network. since relative importance weights really matter scale factor ignored. discussion global local method advantage computing importance parameters given data point without need access labels condition computed training model. global version needs compute gradients output function local variant computed locally multiplying input output connecting neurons. proposed method resembles implicit memory included parameter network. therefore refer memory aware synapses short. keeps updating value based activations network applied data points. adapt specialize given subset data points rather preserving every functionality network. further method doesn’t need network trained. applied pretrained network compute importance data without need labels. important criterion differentiates work methods rely loss function compute importance parameters. finetuning learning ﬁrst task receiving task learn method uses previous tasks network initialization task ﬁnetunes parameters network task data. baseline expected suffer forgetting tasks advantageous task. follow standard setup commonly used computer vision evaluate lll. consists sequence supervised classiﬁcation tasks particular dataset. note arguably somewhat relaxed setup supposes different classiﬁcation layers task cannot changed remain unshared. moreover oracle used test time decide task different literature also evaluate methods g-mas l-mas using unlabeled test data learn forget. experimental setup alexnet architecture pretrained imagenet consider sequence tasks based three datasets scenes indoor scene classiﬁcation caltech-ucsd birds ﬁne-grained bird classiﬁcation oxford flowers ﬁnegrained ﬂower classiﬁcation consider scene birds birds→ scenes flower→scenes flower→ birds used previously didn’t consider imagenet task sequence would require retraining network scratch importance weights int. synapses performance measured terms classiﬁcation accuracy. warmup phase learning second task ﬁrst freeze parameters network train last layer convergence. then free parameters continue learning process. procedure used training different tasks done epochs batch size using learning rate compared methods. results shown table finetuning performs comparably methods task clearly falls behind looking previous task catastrophic forgetting. int. synapses reduces forgetting previous task allowing unused parameters adjust values towards task. similar performance obtained hebbian based approach l-mas. global method g-mas preserves previous task performance among competitors performing equally well better task. table compares training test adapting importance parameters. l-mas g-mas independent used computing importance weights preservation previous task performance current task quite next move realistic challenging setup layers network shared including last layer. instead learning classiﬁer learn embedding space. goal learn facts natural images suppose data streaming facts learn time learning tasks agent goes active phases model applied data previous tasks unlabeled data used estimate importance weights. modeled either processing training data again processing test data. argued companion paper believe natural continual learning. facts structured units subject object predicate example fact could person eating pizza. design different experimental settings show ability method learn forget. base model build model introduced recently model based vgg- architecture pretrained imagenet. architecture composed convolutional layers followed branches treating separately subject modiﬁers branch consists additional convolutional layers followed fully connected layers. lastly modiﬁers branch forks enabling model three separated structured outputs subject predicate object. loss minimizes pairwise distance visual language embedding. language embedding wordvec representation fact units used. dataset scale dataset presented consists images divided training samples test samples belonging unique facts constructed merging object recognition datasets adapting annotations hence name study fact learning lifelong perspective divided whole dataset batches belonging different groups facts. experimental setup. task learning different batch dataset. slightly different previous setup tasks different datasets thus refer tasks following batch. training different tasks done epochs using learning rate compared methods learning rates layer suggested optimizer used. evaluation report fact image retrieval scenario. follow evalmethod computed birds scenes scenes birds flower bird flower scenes l-mas train l-mas test l-mas train test g-mas train g-mas test g-mas train test uation protocol proposed report mean average precision. batch consider retrieving images belonging facts batch only. also report mean average precision dataset differs average performance achieved batch. please refer supplementary materials details. tasks experiments. start randomly splitting facts groups resulting batches data consider tasks learned table shows performance task sequence. variants method show performance achieved using training data only training test data estimating importance weights ωij. note always unlabeled data estimate importance weights ωij. clear much harder task ﬁnetuning suffers badly forgetting. methods manage control forgetting performance second task lower ﬁnetuning. finetuning cares current task easier achieve better performance subset facts ignoring rest. l-mas achieves ﬁrst task slightly lower obtained int. synpases. g-mas scores best using training g-mas l-mas sets results better importance estimation translated better performance preservation l-mas g-mas similar results note synapses cannot exploit unlabeled data uses gradient back propagation. demonstrate method capture general importance weights really adapt unsupervised fashion particular test conditions split test ﬁrst batch random subsets facts learning ﬁrst batch importance parameters computed using subset table adaptation test condition. learning importance weights subsets test split mean average precision fact learning tasks scenario random split dataset. results reference. compare within column. forgetting subset used estimating importance parameters less subset considered. example g-mas learning importance parameters ﬁrst subset preserves performance compared computing importance batch stand empirical proof method’s capability learning importance parameters based network actively tested method finetune int.synap train train g-mas train&test g-mas table mean average precision fact learning disjoint batch/tasks scenario dataset. reported experiments training last task sequence. sequence tasks/batches composed dataset. tasks resemble grouping different facts disjoint concepts explained companion paper. table presents achieved performance disjoint tasks learned sequence. spite preserving previous knowledge int. synapses ﬁnetuning clear performance methods decreased quite severely ﬁnetuning int. synapses sequence. fact challenging situation faced continual learning agent. however method still manages save reasonable amount knowledge previous groups facts achieving average performance sequence. looking scores different batches notice consistency good performance among subsets method quite good learning important preserve. adaptation test. finally want test ability method learning forget speciﬁc subset task. explained earlier sometimes agent specializes makes speciﬁc capabilities others remain unused. learning task care performance speciﬁc rest. reason selected specialized subset namely facts person playing sports. method importance parameters computed examples along tasks sequence. figure shows achieved performance sport subset method step learning sequence. joint training shown reference. violates setting trains data jointly. note int. synapses learn importance weights figure mean average precision sport subset dataset task task/batch sequence. g-mas managed learn sport subset important preserve prevents signiﬁcantly forgetting subset. training therefore cannot adapt particular subset. g-mas succeeds learn important preserve achieves performance sequence performance ﬁnetuning int. synapses close paper argued that given limited model capacity unlimited evolving data possible preserve previous knowledge. instead agents learn forget. forgetting relate rate speciﬁc piece knowledge used. similar biological systems learning. absence error signals synapses connecting biological neurons strengthen weaken based concurrence connected neurons activations. work inspired synaptic plasticity proposed method able learn importance network parameters input data system active unsupervised manner. showed local variant method seen application hebb’s rule learning importance parameters. ﬁrst tested method sequence object recognition problems traditional setting. moved challenging test case learn facts images continuous manner. showed ability method better learn importance parameters using training data test data both; ability method adapt importance parameters towards frequent data. believe step forward developing systems always learn adapt ﬂexible manner. figure visualization clarify training test data splits adaptation test condition experiment shown table main paper lines blue horizontal lines indicate training data model parameters green diagonal lines indicate data used computing importance weights ωij. note batches split according facts evaluation performed different splits test data following start explaining details experimental settings followed main paper move section analyzing statistics importance parameters obtained proposed method mas. later section focus comparing importance values computed method based different sets. section looks projections obtained sport subset along tasks learning sequence concerning adaptation test main paper. finally section explain differences companion paper. data split visualization figure ﬁgure illustrate designed setup different splits used tasks experiments fact learning setting main paper. histogram parameters importance shown empirically main paper proposed method able identify important parameters penalize changing learning task. analyze importance values spread among different parameters plotted histogram ideally good importance measure would give importance values unused parameters high values crucial task hand. part ﬁgure shows histogram last shared convolutional layer computed training data ﬁrst task. based tasks experiments fact learning setting. notice histogram peak value close zero goes ﬂat. part ﬁgure shows histogram magniﬁed area covering important parameters. long tail distribution values sparser move higher importance assignment. indicates method allow changes parameters unused ﬁrst task penalizing changes crucial parameters carry meaningful information learned task. main paper conducted several experiments examine method’s ability preserving previous task’s performance computing importance parameters different sets e.g. train test subset thereof. shown method able adequately compute importance parameters using figure histogram parameters importance last shared convolutional layer based fact learning tasks experiments computed training data. magniﬁed look histogram important values. important parameters high value crucial speciﬁc task parameters penalty adapt tasks. figure important parameters computed training data. x-axis represents values computed training data y-axis represents values computed test data. object recognition experiment birds→scenes figure important parameters computed test data. x-axis represents values computed test data y-axis represents values computed training data. based object recognition experiment birds→scenes figure important parameters computed x-axis represents values computed y-axis represents values computed fact learning setting tasks experiments. importance shown last convolutional layer branch. figure important parameters computed x-axis represents values computed y-axis represents values computed fact learning setting tasks experiments. importance shown last convolutional layer branch. figure important parameters computed x-axis represents values computed y-axis represents values computed fact learning setting tasks experiments. importance shown last convolutional layer branch. figure important parameters computed x-axis represents values computed y-axis represents values computed fact learning setting tasks experiments. importance shown last convolutional layer branch. either training data test data unsupervised manner. also shown method able adapt subset preserve mostly performance subset rest task. want shed light correlation difference between importance assigned parameters computed different sets. first compare estimated parameters importance using training data computed using test data. that used model object recognition experiment namely birds→scenes results shown table main paper. figure shows scatter plot important parameters according computed training data x-axis represents values computed training data y-axis represents values computed test data. figure shows similar scatter plot important parameters according computed test data here x-axis represents values computed test data y-axis represents values computed training data. plot points closely lying around straight line indicates parameters similar importance values. plot points spread line scattered among plotted area indicates lower correlation seen similar importance values computed test data computed training data form tight grouping points around straight line values would identical. demonstrates method’s ability correctly identify important parameters unsupervised manner regardless used purpose long covers different classes concepts task hand. using different subsets cover partial classes concepts task? main paper conducted experiment fact learning setting split data ﬁrst task disjoint groups facts showed computing importance subset results better preservation performance subset used computing importance suggests importance parameters differs using different subsets. investigate claim plotted values important parameters estimated subset training data ﬁrst task along parameters importance computed using subset figures show last convolutional layer branch. branch learns subjects highly shared between subsets. figures show plot last convolutional layer branch forks projects sample features. clear branch strongly correlated sets branch differs subsets. suggests method identiﬁes important parameters needed subset parameters shared parameters importance correlated subsets different different parameters receive different importance values based used subset. finally main paper showed method tries preserve performance speciﬁc subset case encounters subset frequently test time along learning sequence. done picking subset ﬁrst task tasks fact learning sequence. subset mainly composed sports facts. showed method reduces forgetting subset among competitors specialization capabilities eager know happens learned embedding space i.e. projections samples belong subset change along sequence compared right training ﬁrst task. purpose extract projection learned embedding task sequence. done method adapting sport subset method preserving performance facts ﬁrst task also show projections points embedding learned ﬁnetuning baseline point reference also show projections originally learned representation ﬁrst task figure shows projections different variants learning second task compared original projections. seen adaptive adaptive variants method preserve projections subset. adaptive projections closer original look closely finetuning projections starts drifting away were. third task shown ﬁgure adaptive projections closer original ones adaptive considers subset part task preserved tries prevent forgetting well. finetuning started destroying learned topology subset lies apart. however comes fourth task main paper quite challenging hard task forgetting appears severe preservation projections become even harder. nevertheless adaptive adaptive still preserve topology learned projections. adaptive projections closer look similar originals adaptive mas. finetune forgets completely subset samples projected point becomes quite hard recognize corresponding facts.", "year": 2017}