{"title": "A Neural Network Approach to Context-Sensitive Generation of  Conversational Responses", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "abstract": "We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations. A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.", "text": "present novel response generation system trained large quantities unstructured twitter conversations. neural network architecture used address sparsity issues arise integrating contextual information classic statistical models allowing system take account previous dialog utterances. dynamic-context generative models show consistent gains context-sensitive non-context-sensitive machine translation information retrieval baselines. recently goal training open-domain conversational systems emulate human conversation seemed elusive. however vast quantities conversational exchanges available social media websites twitter reddit raise prospect building data-driven models begin communicate conversationally. work ritter example demonstrates response generation system constructed twitter conversations using statistical machine translation techniques status post twitter user translated plausible looking response. ∗this paper appeared proceedings naacl-hlt †the entirety work conducted microsoft however approach presented ritter address challenge generating responses sensitive context conversation. broadly speaking context linguistic involve grounding physical virtual world focus linguistic context. ability take account previous utterances building dialog systems keep conversations active engaging. figure illustrates typical twitter dialog contextual information crucial phrase good luck plainly motivated reference your game ﬁrst utterance. model contextual sensitivity difﬁcult capture; moreover naive injection context information would entail unmanageable growth phrase table cost increased sparsity skew towards rarely-seen context pairs. statistical approaches machine translation phrase pairs share statistical weights regardless intrinsic semantic commonality. compactly encode semantic syntactic similarity. argue embedding-based models afford ﬂexibility model transitions consecutive utterances capture long-span dependencies domain traditional word phrase alignment difﬁcult present simple context-sensitive response-generation models utilizing recurrent neural network language model architecture models ﬁrst encode past information hidden continuous representation decoded promote plausible responses simultaneously ﬂuent contextually relevant. unlike typical complex task-oriented multi-modular dialog systems architecture completely data-driven easily trained end-to-end using unstructured data without requiring human annotation scripting automatic parsing. paper makes following contributions. present neural network architecture response generation context-sensitive datadriven. such trained massive amounts social media data. knowledge ﬁrst application neural-network model open-domain response generation believe present work groundwork complex models come. additionally introduce novel multi-reference extraction technique shows promise automated evaluation. work naturally lies path opened ritter generalize approach exploiting information larger context. ritter work represent radical paradigm shift work dialog. traditional dialog systems typically tease apart dialog management response generation holistic approach considered ﬁrst attempt accomplish tasks jointly. previous uses machine learning response generation dialog state tracking user modeling many components typical dialog systems remain hand-coded particular labels attributes deﬁning dialog states. contrast dialog state neural network model completely latent directly optimized towards end-to-end performance. sense believe framework paper significant milestone towards data-driven less hand-coded dialog processing. continuous representations words phrases estimated neural network models applied variety tasks ranging information retrieval online recommendation machine translation language modeling successfully embedding model reﬁne estimation rare phrase-translation probabilities traditionally affected sparsity problems. robustness sparsity crucial property method allows capture context information avoiding unmanageable growth model parameters. work extends recurrent neural network language model uses continuous representations estimate probability function natural language sentences. propose conditional rlms contextual information encoded continuous context vector help generate response. models differ previous work context vector constructed. example mikolov zweig auli pre-trained topic model. models context vector learned along conditional generates response. additionally learned context encodings exclusively capture contentful words. indeed even stop words carry discriminative power task; example words utterance you? commonly characterized stop words contentful dialog utterance. distinguish three linguistic entities conversation users context message response context represents sequence past dialog exchanges length; emits message reacts formulating response tripled language model ﬁrst model dubbed rlmt straightforwardly concatenate utterance single sentence train minimize given compute probability response follows perform forward propagation known utterances obtain hidden state encoding useful information previous utterances. subsequently compute likelihood response hidden state. issue simple approach concatenated sentence long average especially context comprises multiple utterances. modelling long-range dependencies difﬁcult still considered open problem consider work context purely linguistic future work might integrate contextual information e.g. geographical location time information forms grounding. model architecture parameterized three weight matrices θrnn wout input matrix recurrent matrix output matrix wout usually initialized randomly. rows input matrix contain k-dimensional embeddings word language vocabulary size denote vocabulary token one-hot representation i.e. zero vector dimensionality corresponding index token. embedding obtained win. recurrent matrix rk×k keeps history subsequence already processed. output matrix wout rk×v projects hidden state output layer entry word vocabulary value used generate probability distribution next word sequence. speciﬁcally forward pass proceeds following recurrence non-linear function applied elementwise case logistic sigmoid. recurrence seeded setting zero vector. probability distribution next word given previous history obtained applying softmax activation function cabulary. different employed play crucial role promoting specialization context encoder distinct task. hidden layer decoder takes following form model conditions previous utterances biasing hidden layer state context representation note context representation change time. useful because forces context encoder produce representation general enough useful generating words response helps decoder remember context information generating long responses. dcgm-i distinguish model propensity underestimate strong dependency holds third model addresses issue concatenating linear mappings bag-ofwords representations input layer feed-forward network representing concatenating continuous representations prior deep architectures common strategy obtain order-sensitive representations notice ﬁrst layer encoder network linear. found helps learning embedding matrix reduces vanishing gradient effect partially stacking squashing non-linearities figure compact representations dcgm-i dcgm-ii decoder receives bias context encoder. dcgm-i encode bag-ofwords representation single vector bcm. dcgm-ii concatenate representations ﬁrst layer preserve order information. limitation rlmt addressed strengthening context bias. second model context message encoded ﬁxed-length vector representation used decode response. illustrated figure first consider single sentence compute single bag-of-words representation then provided input multilayered non-linear forward architecture produces ﬁxed-length representation used bias recurrent state decoder rlm. training time context encoder decoder learned minimize negative log-probability generated response. parameters model θdcgm-i wout{w weights layers feed-forward context networks. ﬁxed-length context vector obtained forward propagation network dataset construction computational efﬁciency alleviate burden human evaluators restrict context sequence single sentence. hence dataset composed triples consisting three sentences. mined context-messageresponse triples twitter firehose covering -month period june august triples context response generated user extracted. minimize noise selected triples contained least frequent bigram appeared times corpus. produced corpus twitter triples. additionally hired crowdsourced raters evaluate approximately candidate triples. judgments -point scale obtained raters apiece. yielded triples mean score better randomly binned tuning triples test triples. mean length responses sets approximately tokens cleanup including punctuation. automatic evaluation evaluate systems using bleu meteor supplement results targeted human pairwise comparisons section major challenge using automated metrics response generation reasonable responses task potentially vast extremely diverse. dataset construction method described yields single reference status. accordingly extend references using approach mine potential responses human judges rate appropriateness. section turns optimizing systems towards bleu using mined multi-references bleu rankings align well human judgments. lays groundwork interesting future correlation studies. goal mine responses {r˜τ} context message pair ﬁrst select candidate triples {˜τ} using system. system calibrated order select candidate triples message response similar original message response formally score candidate triple bag-of-words similarity function controls impact similarity responses smoothing factor avoids zero scores candidate responses share words reference response. found simple formula provided references diverse plausible. given candidate triples {˜τ} human evaluators asked rate quality response within triples after human evaluation retain references score better point scale resulting references example average average lengths responses multi-reference tuning test sets tokens respectively. response generation systems evaluated paper parameterized log-linear models framework typical statistical machine translation log-linear models comprise following feature sets features derived large response generation system built along lines ritter based phrase-based decoder similar moses feature includes following features common moses forward backward maximum likelihood translation probabilities word phrase penalties linear distortion modiﬁed kneser-ney language model trained twitter responses. translation probabilities built large phrase table million entries ﬁrst ﬁltering twitterisms selecting candidate phrase pairs using fisher’s exact test also included decoder features speciﬁcally motivated response generation task jaccard distance source target phrase fisher’s exact probability score relating lengths source target phrases. also feature built index triples whose implementation roughly matches irstatus approach described ritter test triple choose candidate response max˜τ neither traditionally take account contextual information. therefore take consideration context message matches i.e. exact matches deﬁne features -gram matches candidate reply -gram matches candidate reply exact matches help capture promote contextual information replies. rlmt dcgm-i dcgm-ii consider trained concatenated triples denoted rlmt context-sensitive baseline. neural network model contributes additional feature corresponding likelihood candidate response given context message. model training proposed models trained subset triple data. vocabulary consists frequent words. order speed training noise-contrastive estimation loss avoids repeated summations approximating probability target word parameter optimization done using adagrad mini-batch size learning rate found work well held-out data. order stabilize learning clip gradients ﬁxed range suggested mikolov parameters neural models sampled normal distribution recurrent weight initialized random orthogonal matrix scaled prevent over-ﬁtting evaluate performance held-out training stop objective increases. size hidden layer context encoder multilayer network. bottleneck middle compresses context information leads similar responses thus achieves better generalization. last layer embeds context vector hidden space decoder rlm. rescoring setup evaluate proposed models rescoring n-best candidate responses obtained using phrase-based decoder system. contrast candidate responses provided created humans less affected ﬂuency issues. different n-best lists provide comprehensive testbed experiments. first augment n-best list tuning scores model interest. then iteration mert estimate log-linear weights features. test time rescore test n-best list weights. lower upper bounds table shows expected upper lower bounds task suggested bleu scores human responses random response baseline. random system comprises responses randomly extracted triples corpus. human computed choosing reference amongst multi-reference context-status pair. although scores human score compute corpus-level bleu sampling scheme randomly leaves reference human sentence score reference set. sampling scheme also applied table context-sensitive ranking results n-best lists subscript feat. indicates number features models. log-linear weights estimated running iteration mert. mark relative improvements respect reference system bleu meteor results automatic evaluation using bleu meteor presented table broad patterns emerge. first metrics indicate phrase-based decoder outperforms purely approach. second adding features baseline systems helps. third neural network models contribute measurably improvement rlmt dcgm models outperform baselines dcgm models provide consistent gains rlmt. bleu meteor scores indicate phrase-based decoder outperforms purely approach despite fact proposes ﬂuent human generated responses. because model loosely captures important patterns message response ranks candidate responses solely similarity message message test triple result ranked response likely drift purpose original conversation. approach contrast directly models statistical patterns message response. mt+cmm totaling features improves bleu points relative improvement baseline model. ir+cmm features beneﬁts even more attaining bleu points meteor points baseline. figure plots magnitude learned feature weights mt+cmm ir+cmm. features help hypothesis spaces especially n-best list. figure supports hypothesis formulated previous paragraph since solely captures intermessage similarities matches message response important context matches help providing additional gains. phrase-based statistical patterns captured system good explaining away -gram -gram message matches performance gain mainly comes context matches. hand observe -gram matches important selecting appropriate responses. inspection tuning reveals instances responses contain long subsequences corresponding messages e.g. good night best friend love love good night best friend. although infrequent higher-order n-gram matches occur provide robust signal quality response -gram matches given highly conversational nature dataset. rlmt dcgm rlmt dcgm models outperform respective baselines. models also exhibit similar performance show improvements mt+cmm models albeit using lower dimensional feature space. believe similar performance limited diversity n-best list together gains ﬂuency stemming strong language model provided rlm. case models hand headroom improvement ﬂuency already guaranteed. evidence exact matches dcgm semantic matches interact positively ﬁnding comports show semantic relationships mined phrase embeddings correlate positively classic co-occurrencebased estimations. analysis feature weights figure suggests -gram matches explained away dcgm model higher order matches important. appears dcgm models might improved preserving word-order information context message encodings. human evaluation human evaluation conducted using crowdsourced annotators. annotators asked compare quality system output responses pairwise relation context message strings item test set. identical strings held annotators outputs differed. paired responses presented random order annotators pair responses judged annotators. table summarizes results human evaluation giving difference mean scores systems conﬁdence intervals generated using welch’s t-test. identical strings shown raters incorporated automatically assigned score pattern results clear consistent context-sensitive systems outperform non-context-sensitive systems preference gains high approximately case dcgm-ii+cmm versus case dcgm-ii+cmm versus similarly context-sensitive dcgm systems outperform non-dcgm context-sensitive systems results consistent automated bleu rankings conﬁrm best performing dcgm models outperform baseline context-sensitive baseline using features. discussion table provides examples responses generated tuning corpus mt-based dcgm-ii+cmm system best system terms bleu human evaluation. responses system average shorter original human gains must come context message matches. hence rlmt underperforms respect dcgm ir+cmm. dcgm models appear better capacity retain contextual information thus achieve similar performance ir+cmm despite lack exact n-gram match information. present experimental setting striking performance difference observed versions dcgm architecture. multiple sequences used context expect dcgm-ii model would likely beneﬁt owing separate encoding message context. dcgm+cmm also investigated whether mixing exact n-gram overlap semantic information encoded dcgm models bring additional gains. dcgm-{i-ii}+cmm systems totaling features show increases bleu points mt+cmm bleu model based ritter meteor improvements similarly align bleu improvements lists. take responses tuning overall outputs tend generic commonplace often reasonably plausible context examples especially context message contain common conversational elements. example illustrates impact context-sensitivity word book response found message. nonetheless longer generated responses degrade syntactically terms content. notice longer responses likely present information conﬂicts either internally within response itself odds context examples unsurprising since model lacks mechanisms reﬂecting agent intent response maintaining consistency respect sentiment polarity. longer context message components also result responses wander off-topic lapse incoherence especially relatively frequency unigrams echoed response. general expect larger datasets incorporation extensive contexts model help yield coherent results cases. consistent representation agent intent outside scope work likely remain signiﬁcant challenge. formulated neural network architecture data-driven response generation trained social media conversations generation responses conditioned past dialog utterances provide contextual information. proposed novel multi-reference extraction technique allowing robust automated evaluation using standard metrics bleu meteor. context-sensitive models consistently outperform context-independent context-sensitive baselines relative improvement bleu setting setting albeit using minimal number features. models completely data-driven self-contained hold potential improve ﬂuency contextual relevance types dialog systems. work suggests several directions future research. anticipate much room improvement employ complex neural network models take account word order within message context utterances. direct generation neural network models interesting potentially promising next step. future progress area also greatly beneﬁt thorough study automated evaluation metrics. thank alan ritter mooney chris quirk lucy vanderwende susan hendrich mouni reddy helpful discussions well three anonymous reviewers comments.", "year": 2015}