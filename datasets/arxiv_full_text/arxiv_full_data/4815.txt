{"title": "Increasing the Action Gap: New Operators for Reinforcement Learning", "tag": ["cs.AI", "cs.LG"], "abstract": "This paper introduces new optimality-preserving operators on Q-functions. We first describe an operator for tabular representations, the consistent Bellman operator, which incorporates a notion of local policy consistency. We show that this local consistency leads to an increase in the action gap at each state; increasing this gap, we argue, mitigates the undesirable effects of approximation and estimation errors on the induced greedy policies. This operator can also be applied to discretized continuous space and time problems, and we provide empirical results evidencing superior performance in this context. Extending the idea of a locally consistent operator, we then derive sufficient conditions for an operator to preserve optimality, leading to a family of operators which includes our consistent Bellman operator. As corollaries we provide a proof of optimality for Baird's advantage learning algorithm and derive other gap-increasing operators with interesting properties. We conclude with an empirical study on 60 Atari 2600 games illustrating the strong potential of these new operators.", "text": "timal state bellman’s equation describes value nonstationary policy upon returning policy selects rather preserving global consistency appears impractical propose simple modiﬁcation bellman operator provides ﬁrst-order solution inconsistency problem. accordingly call operator consistent bellman operator. show consistent bellman operator generally devalues suboptimal actions preserves optimal policies. result action value difference optimal second best actions increases. increasing action advantageous presence approximation estimation error crucial systems operating time scale video games real-time markets robotic platforms fact idea devaluating suboptimal actions underpins baird’s advantage learning designed continuous time control occurs naturally considering discretized solution continuous time space mdps whose limit hamilton-jacobi-bellman equation empirical results bicycle domain show marked increase performance using consistent bellman operator. second half paper derive novel sufﬁcient conditions operator preserve optimality. relative weakness conditions reveal possible deviate signiﬁcantly bellman operator withsacriﬁcing optimality optimality-preserving operator needs contractive even guarantee convergence q-values suboptimal actions. numerous alternatives bellman operator forward believe work ﬁrst propose major departure canonical ﬁxed-point condition required optimality-preserving operator. proof richness operator family describe practical instantiations unique properties. paper introduces optimality-preserving operators q-functions. ﬁrst describe operator tabular representations consistent bellman operator incorporates notion local policy consistency. show local consistency leads increase action state; increasing argue mitigates undesirable effects approximation estimation errors induced greedy policies. operator also applied discretized continuous space time problems provide empirical results evidencing superior performance context. extending idea locally consistent operator derive sufﬁcient conditions operator preserve optimality leading family operators includes consistent bellman operator. corollaries provide proof optimality baird’s advantage learning algorithm derive gap-increasing operators interesting properties. conclude empirical study atari games illustrating strong potential operators. value-based reinforcement learning attractive solution planning problems environments unknown unstructured dynamics. canonical form value-based reinforcement learning produces successive reﬁnements initial value function repeated application convergent operator. particular value iteration directly computes value function iterated evaluation bellman’s equation either exactly samples simplest form value iteration begins initial value function successively computes bellman operator. environment dynamics unknown typically replaced state-action value function approximated empirical bellman operator. ﬁxed point bellman operator optimal state-action value function optimal q-function optimal policy recovered. figure two-state illustrating non-stationary aspect bellman operator. here indicate transition probabilities rewards respectively. state agent either cake receive reward transition probability abstain reward. state low-value absorbing state well known optimal policy stationary deterministic. looking therefore restrict search space stationary deterministic policies. interestingly show bellman operator sense restricted begin consider two-state depicted figure abstracts faustian situation agent repeatedly chooses immediately rewarding ultimately harmful option unrewarding alternative concreteness imagine agent faced endless supply delicious cake call cake cake actions. results arcade learning environment consider deep q-network architecture mnih replacing learning rule operators. remarkably one-line change produces agents signiﬁcantly outperform original dqn. work believe demonstrates potential impact rethinking core components value-based reinforcement learning. bounded subset discount factor. denote space bounded real-valued functions respectively. write maxa follow convention related quantities etc.) whenever convenient unambiguous. context speciﬁc write ex∼p mean expectation respect convention always denotes deterministic policy induces q-function emphasize focus bellman operator results easily extend variations sarsa policy evaluation ﬁtted q-iteration particular operators sample-based form i.e. analogue q-learning rule watkins aggregation methods ﬁrst glance indicator function seem limiting zero close zero everywhere state described features preclude meaningful identity test however important family value functions tabular-like properties aggregation schemes show consistent bellman operator well-deﬁned aggregation schemes. aggregation scheme tuple aggregate states mapping distributions mapping distributions ex∼d ez∼a) assign speciﬁc roles deﬁne aggregation bellman operator ﬁnite subset corresponds identity transition function i.e. recover class averagers kernel-based methods also corresponds identity ﬁnite reduces bellman operator recover familiar tabular representation light original deﬁnition differ interpretation transition kernel. thus consistent bellman operator remains relevant cases deterministic transition kernel example applying multilinear barycentric interpolation continuous space mdps considered q-functions given stationary policies argued nonstationary. make similar statement bellman operator nongreedy components generally describe expected return stationary policies. hence bellman operator restricted interest solved exactly nonstationarity non-issue since q-values optimal actions matter. presence estimation approximation error however small perturbations q-function result erroneously identifying optimal action. example illustrates effect estimate induce pessimal greedy policy deﬁnition action optimal pol−γ/ unfortunately visibly yield useful operator practical approximation propose consistent bellman operator preserves local form stationarity effectively operator redeﬁnes meaning q-values state action taken next state taken. example q-value describes expected return repeatedly eating cake transition unpleasant state since optimal policy stationary intuit iterated application operator also yields fact show consistent bellman operator optimality-preserving presence direct loops corresponding transition graph gap-increasing optimality-preserving experiments bicycle domain study behaviour operators bicycle domain domain agent must simultaneously balance simulated bicycle drive goal north initial position. time step consists hundredth second successful episode typically lasting steps. driving aspect problem particularly challenging value-based methods since step contributes little eventual success curse dimensionality precludes representation state-space. setting consistent operator provides signiﬁcantly improved performance stability. approximated value functions using multilinear interpolation uniform ×···× grid -dimensional feature vector ﬁrst four components describe relevant angles angular velocities polar coordinates describing bicycle’s position relative goal. approximated q-functions using q-value interpolation grid since typical setting access forward model. interested quality value functions produced different operators. thus computed q-functions using value iteration rather trajectorybased method q-learning. precisely iteration simultaneously apply operator grid points expected next state values estimated samples. interested reader full experimental details videos appendix. general. especially relevant relatively highdimensional bicycle domain discretization state space practical trajectories take place grid points. example consider relative angle goal grid cell covers single time step typically changes less figure summarizes results. policies derived consistent operator safely balance bicycle earlier also reach goal earlier policies derived bellman operator. note particular striking difference trajectories followed resulting policies. effect even pronounced using grid effectively decreasing suboptimal q-values grid points produce much better policies within grid cells. phenomenon consistent theoretical results farahmand relating size action gaps quality derived greedy policies. thus second beneﬁt increasing action improves policies derived q-value interpolation. rather optimal particular implies initially devalue optimal action favour greedy action. theorem shows cannot undervalued inﬁnitely often fact must ultimately reach proof perhaps surprising result found appendix. best knowledge theorem ﬁrst result show convergence iterates dynamic programming-like operators without resorting contraction argument. indeed conditions theorem contraction ticularly weak require assume existence ﬁxed point fact conditions laid function space theorem characterize optimality-preserving operators following sense remark exists single-state operator cases exists limk→∞ maxa equality leave open problem existence divergent example corollary consistent bellman operator consistent q-value interpolation bellman operator tcqvi optimality-preserving. fact hard show consistent bellman operator contraction thus enjoys even stronger convergence guarantees provided theorem informally whenever condition theorem strengthened inequality also expect operators gap-increasing; fact case consistent operators. conclude section describe operators satisfy conditions theorem thus optimality-preserving gap-increasing. critically none operators contractions; them lazy operator also possesses multiple ﬁxed points. baird’s advantage learning method advantage learning proposed baird means increasing optimal suboptimal actions context residual algorithms applied continuous time problems. corresponding operator figure top. falling goal-reaching frequency greedy policies derived value iteration. bottom. sample bicycle trajectories iterations. coarse-resolution regime bellman operator initially yields policies circle goal forever consistent operator quickly yields successful trajectories. whether possible extend consistent bellman operator q-value approximation schemes lack probabilistic interpretation linear approximation locally weighted regression neural networks even informationtheoretic methods section answer afﬁrmative. family operators describe applicable arbitrary q-value approximation schemes. operators general longer contractions gap-increasing optimality-preserving qfunction represented exactly. theorem main result; corollary convergence proof baird’s advantage learning incidentally taking minimum fact accident rather simple application theorem. theorem bellman operator deﬁned operator property exists letting maxb optimality-preserving gap-increasing. thus operator satisﬁes conditions theorem eventually yield optimal greedy policy assuming exact representation q-function. condition particular states subtract maxb iteration. exactly action note that operators motivated principle share ﬁxed point isomorphic. believe version stable practice avoids multiplication term. corollary advantage learning operator unique limit persistent advantage learning domains high temporal resolution advantageous encourage greedy policies infrequently switch actions encourage form persistence. deﬁne operator favours repeated actions α-lazy operator updates q-values would affect greedy policy. theorem applies hence optimality-preserving gap-increasing even though possess multitude ﬁxed points note theorem apply -lazy operator latter also optimality-preserving; case however guaranteed optimal action remain optimal. evaluated operators arcade learning environment reinforcement learning interface atari games. frame lasts second actions typically selected every four frames. intuitively setting related continuous domains bicycle domain studied above sense individual action little effect game. evaluation trained agents based deep q-network architecture mnih acts according \u0001-greedy policy learned neuralnetwork q-function. uses experience replay mechanism train q-function performing gradient descent sample squared error parametrized weight given persistent advantage learning ﬁrst experiment used standard versions call stochastic minimal setting. setting includes stochasticity applied atari controls death information per-game minimal action set. speciﬁcally frame environment accepts agent’s action probability rejects probability action rejected previous frame’s action repeated. setting agent selects action every four frames stochastic controls therefore approximate form reaction delay. evidenced lower performance stochastic minimal challenging previous settings. trained agent million frames using either regular bellman updates advantage learning persistent advantage learning optimized parameters training games tested algorithms games using independent trials each. game performed paired t-test post-training evaluation scores obtained algorithms dqn. a.l. p.a.l. statistically better games respectively; perform worse p.a.l. often achieves higher scores a.l. statistically better games worse results especially remarkable given difference operators simple modiﬁcation update rule. comparison also trained agents using original setting particular using longer million frames training. figure depicts learning curves games asterix space invaders. curves representative results rather exceptional games advantage learning outperforms bellman updates persistent advantage learning improves result. across games median score improvement a.l. p.a.l. average score improvement respectively full experimental details provided appendix. weaker conditions optimality. core results lies redeﬁnition q-values order facilitate approximate value estimation. theorem empirical results indicate many practical operators preserve suboptimal q-values. naturally preserving optimal value function unnecessary long iterates converge q-function well even weaker maxa conditions optimality exist required theorem present however proof technique appear extend case. statistical efﬁciency operators. advantage learning viewed generalization consistent bellman operator unknown irrelevant. light probabilistic interpretation advantage learning? further wonder statistical efﬁciency consistent bellman operator ever less efﬁcient usual bellman operator considering probability misclassifying optimal action? answers might shed light differences performance observed experiments. maximally efﬁcient operator. revealed existence broad family optimality-preserving operators wonder operators preferred bellman operator. clearly trivial mdps optimality-preserving operator performs equally well. however whether given maximally efﬁcient optimalitypreserving operator; whether learning agent beneﬁt simultaneously searching operator estimating value function. presented paper family optimality-preserving operators consistent bellman operator distinguished member. center pursuits desire increase action gap; showed experiments plays central role performance greedy policies approximate value functions signiﬁcantly increased performance could obtained simple modiﬁcation bellman operator. believe work highlights inadequacy classical q-function producing reliable policies practice calls question traditional policy-value relationship value-based reinforcement learning illustrates revisiting concept value fruitful. authors thank michael bowling csaba szepesv´ari craig boutilier dale schuurmans marty zinkevich lihong thomas degris joseph modayil useful discussions well anonymous reviewers excellent feedback. stems instability q-functions learned bellman updates provided conclusive empirical evidence effect. spirit work compared learned q-functions single trajectory generated trained agent playing space invaders original setting. q-function state along trajectory computed well action value functions action gaps resulting experiment depicted figure expected action gaps signiﬁcantly greater operators comparison action gaps produced dqn. furthermore value estimates lower correspond realistic estimates true value function. experiments hasselt observed similar effect value estimates replacing bellman updates double q-learning updates many solutions recently proposed mitigate negative impact statistical bias value function estimation bias positive consequence term bellman operator. hypothesize lower value estimates observed figure also consequence bias reduction. speciﬁcally increased action gaps consistent bias reduction easily shown value estimation bias strongest q-values close other. hypothesis holds true third beneﬁt increasing action thus mitigate statistical bias q-value estimates.", "year": 2015}