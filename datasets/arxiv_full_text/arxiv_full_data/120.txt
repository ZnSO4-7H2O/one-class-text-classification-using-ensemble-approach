{"title": "Avoiding Your Teacher's Mistakes: Training Neural Networks with  Controlled Weak Supervision", "tag": ["cs.LG", "cs.CL", "cs.NE", "stat.ML"], "abstract": "Training deep neural networks requires massive amounts of training data, but for many tasks only limited labeled data is available. This makes weak supervision attractive, using weak or noisy signals like the output of heuristic methods or user click-through data for training. In a semi-supervised setting, we can use a large set of data with weak labels to pretrain a neural network and then fine-tune the parameters with a small amount of data with true labels. This feels intuitively sub-optimal as these two independent stages leave the model unaware about the varying label quality. What if we could somehow inform the model about the label quality? In this paper, we propose a semi-supervised learning method where we train two neural networks in a multi-task fashion: a \"target network\" and a \"confidence network\". The target network is optimized to perform a given task and is trained using a large set of unlabeled data that are weakly annotated. We propose to weight the gradient updates to the target network using the scores provided by the second confidence network, which is trained on a small amount of supervised data. Thus we avoid that the weight updates computed from noisy labels harm the quality of the target network model. We evaluate our learning strategy on two different tasks: document ranking and sentiment classification. The results demonstrate that our approach not only enhances the performance compared to the baselines but also speeds up the learning process from weak labels.", "text": "paper propose semi-supervised learning method train neural networks multi-task fashion target network conﬁdence network. target network optimized perform given task trained using large unlabeled data weakly annotated. propose weight gradient updates target network using scores provided second conﬁdence network trained small amount supervised data. thus avoid weight updates computed noisy labels harm quality target network model. evaluate learning strategy different tasks document ranking sentiment classiﬁcation. results demonstrate approach enhances performance compared baselines also speeds learning process weak labels. deep neural networks shown impressive results tasks computer vision natural language processing information retrieval. however success conditioned availability exhaustive amounts labeled data many tasks data available. hence unsupervised semi-supervised methods becoming increasingly attractive. using weak noisy supervision straightforward approach increase size training data. instance search task ranking ideal training data would rankings documents ordered relevance large queries. however practical collect data large scale small judged query-document pairs available. however usually done pre-training network weak data ﬁne-tuning true labels however independent stages leverage full capacity information true labels. instance pet-raining stage handle control extent data weak labels contribute learning process different quality. paper propose semi-supervised method leverages small amount data true labels along large amount data weak labels. proposed method three main components weak annotator heuristic model weak classiﬁer even human crowdsourcing employed annotate massive amount unlabeled data target network uses large weakly annotated instances weak annotator learn main task conﬁdence network trained small human-labeled estimate conﬁdence scores instances annotated weak annotator. train target network conﬁdence network multi-task fashion. joint learning process target network conﬁdence network learn suitable representation data layer shared two-way communication channel. target network tries learn predict label given input supervision weak annotator. time output conﬁdence network conﬁdence scores deﬁne magnitude weight updates target network respect loss computed based labels weak annotator backproposed method effective leveraging large amounts weakly labeled data compared traditional ﬁne-tuning tasks. also show explicitly controlling weight updates target network conﬁdence network leads faster convergence since ﬁltered supervision signals solid less noisy. following section introduce general architecture model explain training process. then describe details applications apply model section section present experimental setups tasks along results analysis. review related works conclude paper. following describe recipe semi-supervised learning neural networks scenario along small human-labeled training large weakly labeled instances leveraged. formally given unlabeled training instances weak annotator generate weak labels. gives training consists tuples training instances small training instances true labels also apply weak annotator generate weak labels. creates training consisting triplets training instances weak labels true labels general architecture proposed framework train multi-task neural network jointly learns conﬁdence score weak training instances main task using controlled supervised signals. high-level representation model shown figure comprises weak annotator neural networks namely conﬁdence network target network. propagation phase target network. conﬁdence network helps target network avoid mistakes teacher i.e. weak annotator down-weighting weight updates weak labels look reliable conﬁdence network. meta-learning perspective goal conﬁdence network trained jointly target network calibrate learning rate instance batch. i.e. ularization term. thus effectively control contribution parameter updates target network weakly labeled instances based reliable labels according conﬁdence network setup requires running weak annotator label large amount unlabeled data done pre-processing time. many tasks possible simple heuristic implicit human feedback generate weak labels. used train target network. contrast small expert-labeled used train conﬁdence network estimates good weak annotations i.e. controls effect weak labels updating parameters target network. method allows learning different types neural architectures different tasks meaningful weak annotator available. paper study performance proposed model focusing applications information retrieval natural language processing document ranking sentiment classiﬁcation. whilst applications differ considerably exact operationalization model cases also clear similarities. first cases human gold standard data based cognitively complex subjective judgments causing high interrater variation increasing cost obtaining labels need larger sets labels. second also cases weak supervision signal systemic objective facilitates learning data representation. figure learning controlled weak supervision proposed multi-task network learning target task semi-supervised fashion using large amount weakly labeled data small amount data true labels. faded parts network disabled training corresponding mode. red-dotted arrows show gradient propagation. parameters parts network frames updated backward pass parameters network blue frames ﬁxed training. goal conﬁdence network estimate conﬁdence score training instances. learned triplets training input weak label true label score used control effect weakly annotated training instances updating parameters target network backward pass backpropagation. target network charge handling main task want learn words approximating underlying function predicts correct labels. given data instance weak label training target network aims predict label ˆyi. target network parameter updates based noisy labels assigned weak annotator magnitude gradient update based output conﬁdence network. networks trained multi-task fashion alternating full supervision weak supervision mode. full supervision mode parameters conﬁdence network updated using batches instances training depicted figure training instance passed representation layer mapping inputs vectors. vectors concatenated corresponding weak labels generated weak annotator. conﬁdence network estimates probability taking data instance account training target network. layer processed supervision layer part target network predicting label main task. also pass learned representation training instance along corresponding label generated weak annotator conﬁdence network estimate conﬁdence score training instance i.e. ˜ci. conﬁdence score computed instance conﬁdence scores used weight gradient updating target network parameters words step size back-propagation. noteworthy representation layer shared networks besides regularization effect layer sharing leads better generalization sharing layer lays ground conﬁdence network beneﬁt largeness target network utilize quality networks trained alternating weak supervision full supervision mode. full supervision mode parameters conﬁdence network updated using training instance drawn training cross-entropy loss function conﬁdence network capture difference predicted conﬁdence score instance i.e. stance conﬁdence score weakly annotated instance estimated conﬁdence network. note treated constant weak supervision mode gradient propagation conﬁdence network backward pass minimize loss functions jointly randomly alternating full weak supervision modes training based chosen supervision mode sample batch training instances replacement without replacement point main task conﬁdence scoring task always deﬁned close tasks sharing representation beneﬁt conﬁdence network implicit data augmentation compensate small amount data true labels. besides noticed updating representation layer respect loss network acts regularization networks helps generalization target conﬁdence network since capture tasks less chance overﬁtting. also investigated possible setups training scenarios. instance tried updating parameters supervision layer target network using also data true labels. instead using alternating sampling tried training target network using controlled weak supervision signals conﬁdence network fully trained. shown experiments architecture training strategy described provide best performance. section apply semi-supervised method different tasks document ranking sentiment classiﬁcation. task start introduction task followed setup target network i.e. description representation learning layer supervision layer. document ranking task core information retrieval problem challenging needs capture notion relevance query documents. employ state-of-the-art pairwise neural ranker architecture target network setting training instance consists sentiment classiﬁcation task aims identify sentiment underlying individual sentence. target network convolutional model similar training instance consists sentence sentiment label architecture target network illustrated figure representation learning layer learns representation input sentence shared target network conﬁdence network. bedding word corresponding position sentence. matrix passed convolution layer. layer ﬁlters applied sliding window length generate feature matrix feature given ﬁlter generated oi=∑kj skjfkj denotes concatenation word vectors position i+h. concatenation produces feature vector o∈r\u0000s\u0000−h+. feature matrix o∈rf×. also bias vector result convolution. convolutional layer followed non-linear activation function applied element-wise. afterward output passed pooling layer operates columns feature matrix returning largest denote term query respectively document embedding function maps term dense mdimensional real value vector learned training phase. weighting function assigns weight term vocabulary. shown global term weighting function along embedding function improves performance ranking simulates effect inverse document frequency important feature information retrieval experiments initialize embedding function wordvec embeddings pre-trained google news weighting function idf. supervision layer receives vector representation inputs processed representation learning layer outputs prediction simple fully connected feed-forward network hidden layers followed softmax. hidden layer network computes weak supervision supervision layer fine-tuning i.e. target network trained weakly labeled data supervision layer ﬁne-tuned true labeled data representation learning layer kept ﬁxed. label inference similar proposed neural architecture inspired teacher-student paradigm instead conﬁdence network predict conﬁdence score training instance label generator network trained weak labels instances labels. labels used target training target network. controlled weak supervision joint training proposed neural architecture jointly train target network conﬁdence network alternating batches drawn sets controlled weak supervision full supervision joint training cwsjt except parameters supervision layer target network also updated using batches regards true labels. separate training i.e. consider conﬁdence network separate network withsharing representation learning layer train train target network controlled weak supervision signals. circular training i.e. train target network conﬁdence network trained data true labels target network trained controlled weak supervision signals. network similar supervision layer ranking task softmax instead sigmoid output layer returns probability distribution three classes. employ weighted cross batch instances conﬁdence score weakly annotated instance classes. weak annotator sentiment classiﬁcation task simple unsupervised lexicon-based method sentiwordnet assign probabilities token sentencelevel distribution derived simply averaging distributions terms yielding noisy label weak annotator works better assigning single hard label. target label conﬁdence network calculated using mean absolute difference true label weak progressive training mixture previous baselines. inspired transfer learned information converged target network conﬁdence network using progressive training. train target network controlled weak supervision signals. proposed architectures implemented tensorflow adam optimizer back-propagation algorithm. furthermore prevent feature co-adaptation dropout regularization technique models. setup conﬁdence network predict fully connected feed forward network. given conﬁdence network learned small true labels speed training initialize representation learning layer pre-trained parameters i.e. pre-trained word embeddings. relu non-linear activation function target network conﬁdence network. following describe task-speciﬁc setups experimental results. collections. standard trec collections task ad-hoc retrieval ﬁrst collection consists news articles different news agencies homogeneous second collection collection. clueweb category large-scale collection million english documents considered heterogeneous collection. spam documents ﬁltered using waterloo spam scorer default threshold data true labels. take query sets contain human-labeled judgments queries robust collection queries experiments clueweb collection. query take documents judged relevant plus number documents judged non-relevant form pairwise combinations among them. data weak labels. create query using unique queries appearing query logs query contains queries initiated real users search engine sampled three-month period march applied standard pre-processing queries. ﬁltered large volume navigational queries containing substrings also removed non-alphanumeric characters queries. dataset took queries least hits target corpus using weak annotator method. applying steps collect million queries train robust million queries clueweb. prepare weakly labeled training take retrieved documents using query training query training/validation split fold. hyperparameters models baselines tuned individually validation using batched bandits expected improvement acquisition function size number hidden layers ranker conﬁdence network separately selected parameters network optimized employing adam optimizer using computed gradient loss perform back-propagation algorithm. inference time query take retrieved documents using candidate documents re-rank using trained models. indri implementation default results discussions. evaluate report standard evaluation metrics mean average precision top-ranked documents normalized discounted cumulative gain calculated retrieved documents statistical signiﬁcant differences ndcg values determined using table performance proposed method baseline models different datasets. improvements degradations statistically signiﬁcant level using paired two-tailed t-test. model improvement/degradations respect weak supervision only baseline cwsjt improvement baselines considered bonferroni correction applied signiﬁcant tests.) table performance variants proposed method different datasets. cwsjt improvement baselines considered bonferroni correction applied signiﬁcant tests.) interesting points want highlight. first among ﬁne-tuning experiments updating parameters target network best tuning strategy. updating parameters representation layer based true labels works better updating parameters supervision layer. supports designed choice shared embedding layer gets updated second seems reasonable make true labels updating parameters achieves better results cwsjt. also performs mostly even worse ws+ft. training direction parameter optimization highly affected type supervision signal control magnitude gradients change directions alternating sets different label qualities confuses supervision layer target network. tinning problem since optimize parameters respect supervision sets separate stages. ranking task target network designed particular trained weak annotations hence training network weak supervision performs better fso. fact ranking complex task requiring many training instances relatively true labels available. performance worse cwsjt learning mapping imperfect labels accurate labels training target network labels essentially harder learning ﬁlter noisy labels hence needs supervised data. reason ranking training instances regards task complexity fails generate better labels hence directly misleads target network completely fails improve performance. table shows performance different training strategies. shown cwsjt cwsct perform better strategies. cwsct conﬁdence network trained separately still able enjoy shared learned information target network. however less efﬁcient need rounds training weakly labeled data. cwsst performs poorly since training data small train high-quality conﬁdence network without taking advantage vast amount weakly annotated data also noticed strategy leads slow convergence compared wso. also transferring learned information target network conﬁdence network progressive training i.e. cwspt performs better full sharing representation learning layer. table performance baseline models well proposed method different datasets. cwsjt improvement baselines considered bonferroni correction applied signiﬁcant tests.) semevalth table performance variants proposed method sentiment classiﬁcation task different datasets. cwsjt improvement baselines considered bonferroni correction applied signiﬁcant tests.) collections. test model twitter message-level sentiment classiﬁcation semeval- task datasets semeval- subsume test sets previous editions semeval i.e. semeval- semeval-. tweet preprocessed urls usernames masked. data true labels. train development data semeval training semeval--test validation. make results comparable ofﬁcial runs semeval semeval- semeval- test sets data weak labels. large corpus containing tweets collected months both training word embeddings creating weakly annotated using lexicon-based method explained section parameters settings. similar document ranking task tuned hyper-parameters model including baselines separately respect true labels validation using batched bandits expected improvement acquisition function size number hidden layers classiﬁer conﬁdence network separately selected from{} and{} respectively. ﬁlter width selected from{ and{ respectively. initial learning and{...} respectively. considered embedding sizes of{} results discussion. report performance model baseline models terms ofﬁcial semeval metric macro-f table also report statistical signiﬁcance improvements using two-tailed paired t-test unlike ranking task training network data true labels i.e. performs rather good. sentiment classiﬁcation task learning representation input sentence simpler ranking task learn representation query long documents. consequently need fewer data able learn suitable representation amount available data true labels already capture rather good representation without helps weak data impossible ranking task. however results suggest still gain improvement using ﬁne-tuning. task behaviors different ﬁne-tuning experiments similar ranking task. furthermore updating parameters supervision layer respect model perform better cwsjt supports choice updating representation learning layer respect signals data true labels. sentiment classiﬁcation task performance acceptable compared ranking task. ﬁrst generating classiﬁcation labels essentially simpler. secondly task need learn represent simpler input learn simpler function predict labels relatively bigger supervised data helps generate labels. however performance still lower cwsjt. argue cwsjt conservative approach. fact equipped soft ﬁlter decreases effect noisy training examples parameter updates training. smoother action down-weight gradient might change direction gradient generating completely label consequently prone errors especially enough high-quality training data learn generate better labels. sentiment classiﬁcation task besides general baselines also report best performing systems also convolution-based models semeval-; deriu semeval-). proposed model outperforms best system datasets. table also presents results different training strategies sentiment classiﬁcation task. shown similar ranking task cwsjt cwsct perform better strategies. although cwsct slightly better terms effectiveness compared cwsjt efﬁcient cwsjt training. compared ranking task sentiment classiﬁcation easier estimate conﬁdence score instances respect amount available supervised data. therefore cwsst able improve performance signiﬁcantly. moreover cwspt fails compared strategies representation learning layer shared target network conﬁdence network. faster learning pace controlling effect supervision train neural networks improves performance also provides network solid signals speeds learning process. figure illustrates training/validation loss networks compared loss training target network weak supervision along performance test sets respect different amounts training data sentiment classiﬁcation task. shown training loss target observed similar speed-up learning process ranking task however skip bringing plots space limit since nested cross-validation ranking task plots fold. losses calculated respect weak labels training loss indication overﬁtting imperfection weak labels. words regardless general problem lack generalization overﬁtting setup learning weak labels predicting labels similar train labels necessarily desirable incident. overﬁts imperfection weak labels setup helps target network escape imperfection good validation set. terms performance compared performance test sets increases quickly able pass performance weak annotator seeing much fewer instances annotated weak annotator. semi-supervised learning. semisupervised learning algorithms developed utilize weakly even unlabeled data. self-training pseudo-labeling tries predict labels unlabeled data. unlabeled data provided additionally. particular neural networks methods greedy layer-wise pre-training weights using unlabeled data alone followed supervised ﬁne-tuning methods learn unsupervised encodings multiple levels architecture jointly supervised signal meta-learning. meta-learning perspective approach similar andrychowicz separate recurrent neural network called optimizer learns predict optimal update rule updating parameters target network. optimizer receives gradient target network outputs adjusted gradient matrix. number parameters modern neural networks typically order millions gradient matrix becomes large feed optimizer approach andrychowicz applied small models. contrast approach leverages additional weakly labeled data conﬁdence network predict per-instance scores calibrate gradient updates target network. direct learning weak/noisy labels. many studies tried address learning condition imperfect labels. noise cleansing methods proposed remove correct mislabeled instances studies showed weak noisy labels leveraged employing particular architecture deﬁning proper loss function avoid overﬁtting training data imperfection modeling imperfection. also research trying model pattern noise weakness labels. methods leverage generative models denoise weak supervision sources discriminative model learn methods capture pattern noise inserting extra layer separated module infer better labels noisy labels supervise training network. inspired teacher-student paradigm teacher generates label given training instance corresponding weak noisy label. however show experiments approach sufﬁcient amount supervised data enough generate better labels. training neural networks using large amounts weakly annotated data attractive approach scenarios adequate amount data true labels available. paper propose multi-task neural network architecture uniﬁes learning estimate conﬁdence score weak annotations training neural networks learn target task controlled weak supervision i.e. using weak labels updating parameters taking estimated conﬁdence scores account. helps alleviate updates instances unreliable labels harm performance. applied model tasks document ranking sentiment classiﬁcation empirically veriﬁed proposed model speeds training process obtains accurate results. promising future direction going understand extent using weak annotations potential training high-quality models neural networks understand exact conditions proposed method works. references mart´ın abadi tensorflow large-scale machine learning heterogeneous systems. software available tensorﬂow.org. http//tensorﬂow.org/. marcin andrychowicz misha denil sergio gomez matthew hoffman david pfau schaul nando freitas. learning learn gradient descent gradient descent. advances neural information processing systems. pages stefano baccianella andrea esuli fabrizio sebastiani. sentiwordnet enhanced lexical resource sentiment analysis opinion mining. lrec. volume pages eyal beigman beata beigman klebanov. proceedings learning annotation noise. joint conference annual meeting international joint conference natural language processing afnlp volume -volume association computational linguistics pages mostafa dehghani sascha rothe enrique alfonseca pascal fleury. learning attend copy generate session-based query suggestion. proceedings international conference information knowledge management mostafa dehghani hamed zamani aliaksei severyn jaap kamps bruce croft. neural ranking models weak supervision. proceedings international sigir conference research development information retrieval deriu maurice gonzenbach fatih uzdilli aurelien lucchi valeria luca martin jaggi. swisscheese semeval- task sentiment classiﬁcation using ensemble convolutional neural networks distant supervision. proceedings semeval pages deriu aurelien lucchi valeria luca aliaksei severyn simon m¨uller mark cieliebak thomas hofmann martin jaggi. leveraging large amounts weakly supervised data multilanguage sentiment classiﬁcation. proceedings international international world wide conference pages thomas desautels andreas krause joel burdick. parallelizing exploration-exploitation tradeoffs gaussian process bandit optimization. journal machine learning research hussam hamdan frederic b´echet patrice bellot. experiments dbpedia wordnet sentiwordnet resources sentiment analysis second joint conference micro-blogging. lexical computational semantics volume pages thorsten joachims. optimizing search engines proceedings using clickthrough data. eighth sigkdd international conference knowledge discovery data mining. pages dong-hyun lee. pseudo-label simple efﬁcient semi-supervised learning method deep workshop challenges neural networks. representation learning icml. volume page vinod nair geoffrey hinton. rectiﬁed linear units improve restricted boltzmann machines. proceedings international conference machine learning pages preslav nakov alan ritter sara rosenthal fabrizio sebastiani veselin stoyanov. semeval- task sentiment analysis twitter. proceedings semeval pages alexander ororbia giles david reitter. learning deep hybrid model semi-supervised text classiﬁcation. proceedings conference empirical methods natural language processing giorgio patrini alessandro rozza aditya menon richard nock lizhen making neural networks robust label noise loss correction approach. arxiv preprint arxiv. paroma varma bryan iter peng rose christopher christopher r´e. socratic learning correcting misspeciﬁed generative models using discriminative models. arxiv preprint arxiv. andreas veit neil alldrin chechik ivan krasin abhinav gupta serge belongie. learning noisy large-scale datasets minimal supervision. conference computer vision pattern recognition. jason weston fr´ed´eric ratle hossein mobahi ronan collobert. deep learning semisupervised embedding. neural networks tricks trade springer pages tong xiao tian yang chang huang xiaogang wang. learning massive noisy labeled data image classiﬁcation. proceedings ieee conference computer vision pattern recognition. pages alexander ratner christopher daniel selsam christopher r´e. data programming creating large training sets quickly. advances neural information processing systems. pages adriana romero nicolas ballas samira ebrahimi kahou antoine chassang carlo gatta yoshua bengio. fitnets hints thin deep nets. arxiv preprint arxiv. chuck rosenberg martial hebert henry schneiderman. semi-supervised self-training object detection models. seventh ieee workshop applications computer vision. sara rosenthal preslav nakov svetlana kiritchenko saif mohammad alan ritter veselin stoyanov. semeval- task sentiment proceedings interanalysis twitter. national workshop semantic evaluation pages andrei rusu neil rabinowitz guillaume desjardins hubert soyer james kirkpatrick koray kavukcuoglu razvan pascanu raia hadsell. progressive neural networks. arxiv preprint arxiv. aliaksei severyn alessandro moschitti. twitter sentiment analysis deep convolutional proceedings interneural networks. national sigir conference research development information retrieval. pages aliaksei severyn alessandro moschitti. unitn training deep convolutional neural network twitter sentiment classiﬁcation. proceedings international workshop semantic evaluation association computational linguistics denver colorado. pages nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. mach. learn. res.", "year": 2017}