{"title": "Exploiting Low-dimensional Structures to Enhance DNN Based Acoustic  Modeling in Speech Recognition", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "We propose to model the acoustic space of deep neural network (DNN) class-conditional posterior probabilities as a union of low-dimensional subspaces. To that end, the training posteriors are used for dictionary learning and sparse coding. Sparse representation of the test posteriors using this dictionary enables projection to the space of training data. Relying on the fact that the intrinsic dimensions of the posterior subspaces are indeed very small and the matrix of all posteriors belonging to a class has a very low rank, we demonstrate how low-dimensional structures enable further enhancement of the posteriors and rectify the spurious errors due to mismatch conditions. The enhanced acoustic modeling method leads to improvements in continuous speech recognition task using hybrid DNN-HMM (hidden Markov model) framework in both clean and noisy conditions, where upto 15.4% relative reduction in word error rate (WER) is achieved.", "text": "propose model acoustic space deep neural network class-conditional posterior probabilities union lowdimensional subspaces. training posteriors used dictionary learning sparse coding. sparse representation test posteriors using dictionary enables projection space training data. relying fact intrinsic dimensions posterior subspaces indeed small matrix posteriors belonging class rank demonstrate low-dimensional structures enable enhancement posteriors rectify spurious errors mismatch conditions. enhanced acoustic modeling method leads improvements continuous speech recognition task using hybrid dnn-hmm framework clean noisy conditions upto relative reduction word error rate achieved. need sparse representations better acoustic modeling speech advocated consistently better characterization underlying low-dimensional parsimonious structure speech major emerging trends namely deep neural networks exemplar-based sparse modeling different approaches exploiting sparsity speech representations achieve invariance discrimination noise separation hand speech utterances formed union words turn consist phonetic components subphonetic attributes. linguistic component produced activation highly constrained articulatory mechanisms leading generation speech data union low-dimensional subspaces however existing speech classiﬁcation acoustic modeling methods explicitly take account multi-subspace structure data. present study focuses exploiting multi-subspace lowdimensional structure speech learned training data enhance based acoustic modeling unseen test data. hence also potential enable domain adaptation handling mismatch framework based acoustic modeling. high-dimensional space underlying structures disentangled. structures form union low-dimensional subspaces models non-linear manifold speech data resides. prior work sparse representation includes exemplar-based methods sparse representation learned using spectral features achieve promising performance automatic speech recognition specially robustness handling noise corruption. recent advancement based acoustic modeling relies estimation highly sparse sub-word class-conditional posterior probabilities. conventional gaussian mixture models statistically inefﬁcient modeling data lying near non-linear manifolds dnns achieve accurate sparse acoustic modeling multiple layers non-linear transformations hidden layers successively learn underlying structures different levels express highly invariant discriminative representations towards deeper layers. enforcing sparsity constraints training mostly employed purpose regularization prevent overﬁtting various studies shown sparsity architectures directly contributes towards simpler networks superior performance asr. successful application sparse activity sparse connectivity well better performance sparsity inducing techniques like dropout neural network training conﬁrm belief ‘sparser’ better acoustic modeling asr. previous studies found sparse activations dnns showing individual neurons hidden layers learn selectively active different ways towards distinct phone patterns. since sparsiﬁcation learned hidden layers explicitly hand-crafted upto extent union low-dimensional subspaces structure speech actually exploited dnns despite effective seen conditions dnns found highly sensitive unseen variations data mismatch condition causes erroneous estimates posterior probabilities exhibited spurious noises output posterior probabilities. correct errors low-dimensional model improve acoustic modeling noisy conditions rigorous proof property requires certain conditions assumptions disjoint subspaces. since train binary senone target outputs intersection senone subspaces expected rare event suggests disjointedness subspaces. although theoretical analysis beyond scope present work experiments conducted section empirically conﬁrm property indeed holds subspace-sparse modeling senones. senone posterior probabilities speech features posterior feature vector consisting class-conditional probabilities output layer dnn. contrast spectral features posterior features proven highly effective sparse modeling inherently sparse invariant speaker/environmental conditions presented training data. although choose work posterior probabilities context-dependent senone levels theoretical underpinning proposed approach applicable type speech units. dictionary learning sparse coding algorithms building previous work dictionary learning sparse modeling posterior features online dictionary learning algorithm solving sparse coding problem expressed denotes atom dictionary. class-speciﬁc data senone posterior features obtained gmm-hmm based forced alignment training data used learn individual over-complete basis senone subspace using dictionary learning algorithm. class-speciﬁc dictionaries concatenated larger dictionary subspace-sparse acoustic modeling. since posterior feature obtained lies union subspaces test posterior feature reconstructed using atoms dictionary according property atoms associated correct class used sparse representation. noted dictionary learning approach fundamentally different dictionary construction using random subset training features since training data compute over-complete basis sparse representation smaller actual collection size effective sparse representation enhanced acoustic modeling group sparsity based hierarchical lasso algorithm sparse coding enforce group sparsity based internal partitioning dictionary senone-speciﬁc sub-dictionaries high dimensional group sparse representation computed output posterior feature sparse recovery projection test posterior feature training data space given computing paper address issues explicit modeling underlying structures speech using prior knowledge speech data lives union low-dimensional subspaces. implement idea using principled dictionary learning sparse coding algorithms posterior probabilities recover sparse representations non-zero values correspond class-speciﬁc subspaces. subspace sparse representations used enhance original posterior probabilities dictionary based reconstruction. build upon compressive sensing subspace sparse recovery theory provide theoretical support validity approach. also elaborate choice features algorithms dictionary learning structured sparse coding essentially distinguish approach previous exemplar based sparse representation methods demonstrate improvements performance achieved proposed enhanced acoustic modeling hybrid dnn-hmm continuous system using numbers’ database show increased robustness noisy conditions. rest paper proposed subspace sparse acoustic modeling method elaborated section experimental analysis carried section section provides concluding remarks directions future work. section model space class-conditional posterior probabilities union low-dimensional subspaces. relying subspace sparse representation show posteriors enhanced accurate class-speciﬁc representations. subspace sparse representation speech features reside near non-linear manifolds best characterized union low-dimensional subspaces. proposed approach relies fact data point union subspaces efﬁciently reconstructed using sparse combination data points subspace data points subspaces thus resulting subspace-sparse representation linear disjoint subspaces associated classes dimensions individual subspaces {r}l smaller dimension actual space i.e. speech features union low-dimensional subspaces. rm×n class-speciﬁc over-complete dictionary subspace number atoms data point represented sparse linear combination atoms deﬁning -norm vector absolute values components subspace sparse recovery property union disjoint subspaces asserts norm sparse representation data point collection classspeciﬁc dictionaries {d}l lead separation classspeciﬁc subspaces selecting atoms underlying class data point reconstruction. thus obtained sparse representations activations atoms corresponding actual subspace lives. considering speech utterance union words phones sub-phonetic components subspaces modeled different levels corresponding speech units. consequently dictionary constructed learning basis sets individual classes. present study focus context-dependent senones superior quality dnn-hmm framework. nevertheless theoretfig. output senone posteriors projected space training posteriors using resulting projected posteriors used typical decoding dnn-hmm framework. note approximation posterior feature based -norm sparse reconstruction using atoms consequently dimension forced probability simplex normalization. figure summarizes procedure. section provide empirical analysis theoretical results established section experiments conﬁrm information bearing components class-conditional probabilities indeed live low-dimensional space. exploiting structure enables enhancement based acoustic models removes effect high-dimensional noise leading improvement dnn-hmm speech recognition performance. numbers’ database study utterances consisting digits considered phoneset includes phones accordingly context dependent tied states referred senones learned forced alignment training data using kaldi speech recognition toolkit trained using sequence discriminative training hidden layers nodes. every speech frame input vector mfcc+∆+∆∆ features context frames output vector posterior probabilities corresponding senone classes. posteriors features dictionary learning sparse coding explained sections posteriors used learn senone-speciﬁc dictionaries training data. number atoms senone dictionary approximately value optimized development data used sparse coding sparse representations subsequently projected posterior probabilities computed test data. sparsity leads selection subspaces training data resulting test posteriors live low-dimensions projected onto subspace training posteriors separated subspaces senone classes. investigate properties analysis. provide insight dimension senone subspaces construct matrices class-speciﬁc senone posteriors compute number singular values required preserve variability data. skewed distribution posteriors take prior singular value decomposition. refer number required singular values roughly rank senone matrices. ideal posterior feature maximum component support indicating associated class. hence group posteriors correct maximum component corresponds correct class incorrect maximum component corresponds incorrect class. table shows average number required singular values senones projected posteriors. another approach referred robust based posteriors discussed subsequent section. correct posteriors live space lower dimension space incorrect posteriors. words information bearing components correct senone posteriors fewer resulting matrices lower rank compared incorrect posteriors. given ranks nevertheless incorrect posterior exposed high-dimensional spurious noise. therefore enhance posterior probabilities investigate subspaces selected sparse recovery values sparse representation class summed form α-sum vectors rank senone-speciﬁc α-sum matrices computed. according property expected sparse recovery select subspaces underlying classes rank α-sum matrices fact found empirical results averaged whole test conformed theoretical insight indicating class-speciﬁc dictionary learning sparse coding enables model non-linear manifold training data union low-dimensional subspaces. posterior test data manifold presence high-dimensional noise embedded components. important extract low-dimensional structure separating effect noise. sparse coding exactly ﬁnding true underlying subspaces sparse representation enables projecting class-speciﬁc subspace training data manifold reconstruction. study true underlying dimension senone-speciﬁc subspaces consider robust principle component analysis based decomposition senone posteriors idea rpca decompose data matrix matrix low-rank matrix sparse building upon observations section low-rank component corresponds enhanced posteriors high dimensional erroneous estimates separated sparse matrix collect posterior features senone training data using ground truth based gmm-hmm forced alignment. rpca decomposition applied data senone-class reveal true underlying dimension class-speciﬁc senone subspaces. rank senone posteriors obtained rpca decomposition correct incorrect classes listed table true dimension class-speciﬁc subspaces senone posteriors indeed lower posteriors lower projected posteriors exploiting multi low-rank structure speech lead posterior enhancement low-rank representation utterance level low-rank bottleneck layer based studied shows low-dimensional structuring architecture yields smaller footprint faster training. contrast proposed method suggests added layer sparse coding structuring outputs relying generic sparse low-rank structures. since generic structures characterized training data approach enables handle mismatches train test conditions. enhanced dnn-hmm speech recognition continuous speech recognition performed using posteriors well projected posteriors framework conventional hybrid dnn-hmm. topology learned training hybrid dnn-hmm used decoding word transcription cases. hence parameters different systems shown difference terms senone posterior probabilities frame results different best paths decoded viterbi algorithm. demonstrate increased robustness projected posteriors compared posteriors also compared performance noisy conditions artiﬁcial white gaussian noise added signal level test utterances signal-to-noise ratios trained clean speech used computing posteriors noisy test spectral features artiﬁcially added noise acts unseen variation data dnn. comparison performance shown table terms word error rate percentage. projected posteriors outperform posteriors cases suggesting projection based provides enhanced acoustic models dnn-hmm decoding. note experiments consistent decrease insertion substitution errors observed using projected posteriors place posteriors. implies fewer wrong hypotheses made case projected posteriors word level compared posteriors. similar insight comes comparing gmm-hmm based forced senone alignment senone alignments achieved best viterbi paths projected posterior posterior systems. senone classiﬁcation error case posteriors reduced case projected posteriors. improvement senone alignments subsequent reduction proves superior quality projected posteriors posteriors supports hypothesis projection moves test features closer subspace correct classes. table comparison performance using posteriors projected posteriors clean noisy conditions numbers’ database. rpca posteriors indicate ideal enhancement low-dimensional posterior reconstruction. breakdown terms insertions deletions substitutions also shown total words test utterances. finally rpca posteriors ranks close true underlying dimensions senone subspaces perform extremely well using posteriors reduces using projected posteriors i.e. relative improvement rpca posteriors used reduced mere since rpca based low-rank reconstruction posteriors done using ground truth senone alignment performance case best case scenario demonstrates scope improvement possible even based acoustic modeling. paper demonstrated explicit modeling low-dimensional structures speech using dictionary learning sparse coding class conditional probabilities. showed albeit power representation learning based acoustic modeling still room improvement exploiting union low-dimensional subspaces structure underlying speech data acoustic modeling noisy conditions. using dictionary learning sparse coding posteriors transformed projected posteriors shown suitable acoustic models. sparse reconstruction moves test posteriors closer correct underlying class data exploiting fact true information embedded low-dimensional subspace thus separating high dimensional erroneous estimates. improvements performance shown clean noisy conditions paving towards effective robust framework using unseen conditions. importance low-dimension structures conﬁrmed rpca analysis. proposed method improved discriminative dictionary learning better class-speciﬁc subspace modeling. furthermore study low-rank clustering techniques enhance posterior probabilities exploiting low-dimensional multisubspace structure. moreover consider analysis challenging databases particular case accented nonnative speech recognition. projection accented speech posteriors dictionaries trained native language speech result transformation accented phonetic space native phonetic space lead improvements accented speech recognition task. jeff bilmes what hmms ieice transactions information systems vol. yoshua bengio learning deep architectures foundations trends machine learning vol. tara sainath bhuvana ramabhadran michael picheny david nahamoo dimitri kanevsky exemplar-based sparse representation features timit lvcsr ieee transactions audio speech language processing vol. saon jen-tzung chien jort gemmeke bert cranen noise robust digit recognition using sparse representations proceedings isca itrw speech analysis processing knowledge discovery simon king frankel karen livescu erik mcdermott korin richmond mirjam wester speech production knowledge automatic speech recognition journal acoustical society america vol. jort gemmeke tuomas virtanen antti hurmalainen exemplar-based sparse representations noise robust auieee transactions audio tomatic speech recognition speech language processing vol. geoffrey hinton deng dong george dahl abdelrahman mohamed navdeep jaitly andrew senior vincent vanhoucke patrick nguyen tara sainath deep neural networks acoustic modeling speech recognition shared views four research groups ieee signal processing magazine vol. dong michael seltzer jinyu jui-ting huang frank seide feature learning deep neural networks-studies speech recognition tasks arxiv preprint arxiv. jian kang cheng meng wei-qiang zhang neuron sparseness versus connection sparseness deep neural network large vocabulary speech recognition icassp april nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov dropout simple prevent neural networks overﬁtting journal machine learning research vol. julien mairal francis bach jean ponce guillermo sapiro online learning matrix factorization sparse coding journal machine learning research vol. pablo sprechmann ignacio ramirez guillermo sapiro yonina eldar c-hilasso collaborative hierarchical sparse modeling framework signal processing ieee transactions vol. ehsan elhamifar rene vidal sparse subspace clustering algorithm theory applications ieee transactions pattern analysis machine intelligence vol. pranay dighe afsaneh asaei herv´e bourlard sparse modeling neural network posterior probabilities exemplar-based speech recognition speech communication afsaneh asaei benjamin picart herv´e bourlard analysis phone posterior feature space exploiting class-speciﬁc sparsity mlp-based similarity measure icassp dong deng automatic speech recognition deep daniel povey arnab ghoshal gilles boulianne luk´aˇs burget ondˇrej glembek nagendra goel mirko hannemann petr motl´ıˇcek yanmin qian petr schwarz kaldi speech recognition toolkit guangcan zhouchen shuicheng yong robust recovery subspace structures low-rank representation ieee transactions pattern analysis machine intelligence tara sainath brian kingsbury vikas sindhwani ebru arisoy bhuvana ramabhadran low-rank matrix factorization deep neural network training high-dimensional output targets icassp. ieee", "year": 2016}