{"title": "On the Compressive Power of Deep Rectifier Networks for High Resolution  Representation of Class Boundaries", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "This paper provides a theoretical justification of the superior classification performance of deep rectifier networks over shallow rectifier networks from the geometrical perspective of piecewise linear (PWL) classifier boundaries. We show that, for a given threshold on the approximation error, the required number of boundary facets to approximate a general smooth boundary grows exponentially with the dimension of the data, and thus the number of boundary facets, referred to as boundary resolution, of a PWL classifier is an important quality measure that can be used to estimate a lower bound on the classification errors. However, learning naively an exponentially large number of boundary facets requires the determination of an exponentially large number of parameters and also requires an exponentially large number of training patterns. To overcome this issue of \"curse of dimensionality\", compressive representations of high resolution classifier boundaries are required. To show the superior compressive power of deep rectifier networks over shallow rectifier networks, we prove that the maximum boundary resolution of a single hidden layer rectifier network classifier grows exponentially with the number of units when this number is smaller than the dimension of the patterns. When the number of units is larger than the dimension of the patterns, the growth rate is reduced to a polynomial order. Consequently, the capacity of generating a high resolution boundary will increase if the same large number of units are arranged in multiple layers instead of a single hidden layer. Taking high dimensional spherical boundaries as examples, we show how deep rectifier networks can utilize geometric symmetries to approximate a boundary with the same accuracy but with a significantly fewer number of parameters than single hidden layer nets.", "text": "paper provides theoretical justiﬁcation superior classiﬁcation performance deep rectiﬁer networks shallow rectiﬁer networks geometrical perspective piecewise linear classiﬁer boundaries. show that given threshold approximation error required number boundary facets approximate general smooth boundary grows exponentially dimension data thus number boundary facets referred boundary resolution classiﬁer important quality measure used estimate lower bound classiﬁcation errors. however learning naively exponentially large number boundary facets requires determination exponentially large number parameters also requires exponentially large number training patterns. overcome issue curse dimensionality compressive representations high resolution classiﬁer boundaries required. show superior compressive power deep rectiﬁer networks shallow rectiﬁer networks prove maximum boundary resolution single hidden layer rectiﬁer network classiﬁer grows exponentially number units number smaller dimension patterns. number units larger dimension patterns growth rate reduced polynomial order. consequently capacity generating high resolution boundary increase large number units arranged multiple layers instead single hidden layer. taking high dimensional spherical boundaries examples show deep rectiﬁer networks utilize geometric symmetries approximate boundary accuracy signiﬁcantly fewer number parameters single hidden layer nets. networks wide range applications handwritten digit recognition object detection image classiﬁcation beneﬁts neural networks’ depth investigated extensively recent years superior power deep networks function approximation superior capacity deep networks separating input space large number regions linearity theoretical analyses conclude that functions represented approximated deep neural networks single hidden layer networks representation provided deep networks much compact thus generalises better. however functions considered limited certain families polynomial functions hand-coded functions chosen demonstrate expressive power deep neural networks. unlikely practically trained neural networks fall analysed categories functions. good understanding deep neural networks achieve empirical successes thus still missing paper aims theoretically justify superior classiﬁcation performances deep rectiﬁer networks shallow rectiﬁer networks geometrical perspective classiﬁer boundaries. given dataset comprising several classes according learning theory simpler learning model smaller sample complexity usually generalises better. however approximation models simple geometric surface quite complex large difference exist complexity approximation model original model. instance shows exponentially large number facets required approximate spherical surface dimensional space. despite fact spherical surface simple geometric model represented parameters show required number units least polynomial function dimension patterns single hidden layer rectiﬁer networks used approximate spherical boundary present upper bound number facets respect number units single hidden layer rectiﬁer network. upper bound shows capacity single hidden layer rectiﬁer nets generate facets increases exponentially number units smaller dimension input. however growth rate reduced polynomial order number units larger dimension patterns consequently capacity generating facets increase large number units arranged multiple layers instead single hidden layer. spherical surfaces examples show deep rectiﬁer nets exponentially efﬁcient single hidden layer nets. main contributions paper include introduction boundary resolution classiﬁers resolution classiﬁer boundaries measure classiﬁer quality used estimate lower bound classiﬁcation errors. introduction concept provides approach analyse beneﬁts rectiﬁer networks’ depth. ﬁrst investigation efﬁciency deep rectiﬁer networks approximating class boundaries given ultimate goal deep learning classiﬁcation tasks learn class boundaries rather classiﬁer functions critical investigate properties deep neural networks generating class boundaries order understand beneﬁts networks’ depth classiﬁcation tasks. required number facets threshold approximation error. weights need linear units required linear units learnt least approximations general convex boundaries iii) explicit upper bound provided number facets single hidden layer rectiﬁer generate given number units upper bound shows limitations single hidden layer networks implies deep nets potential improve efﬁciency. approximations general convex boundaries show parameters learnt deep rectiﬁer networks advantage signiﬁcantly smaller number parameters model classiﬁer boundaries usually thus generalize better single hidden layer rectiﬁer neural networks. rest paper organised follows. section addresses related work. section deﬁnes resolution classiﬁer boundaries relates resolution convex classiﬁer boundaries number linear units maxout representation convex classiﬁers. section addresses required resolution general convex classiﬁer boundaries show exponentially large number facets required approximate convex boundary even spherical surfaces section presents limit single hidden layer rectiﬁer networks reducing number weights learnt high resolution boundary representation. section solution deep rectiﬁer networks approximation euclidean balls presented show superior efﬁciency single hidden layer nets. concluding remarks provided section depth neural networks investigated extensively recent years show superior expressive power deep neural networks shallow networks. delalleau bengio showed deep network representation certain family polynomials much compact provided shallow network. similarly number hidden units deep networks able split input space many regions linearity shallow counterparts eldan shamir presented example function expressible small -layer neural networks cannot approximated -layer network certain constant accuracy unless width exponential dimension data. cohen proved that except negligible functions implemented deep network polynomial size require exponential size order realized shallow network. mhaskar demonstrated deep networks approximate class compositional functions accuracy shallow networks exponentially lower number training parameters. superior expressive power deep residual nets analysed showed residual nets viewed collection many paths differing lengths enabling deep networks activating short paths training. related work paper analysis maximum number regions split rectiﬁer neural network given number units. hand-coded construction deep rectiﬁer nets mont´ufar showed deep nets exponentially efﬁcient splitting space large number regions raghu presented upper bound number split regions single hidden layer network given number linear units. results conclude complexity split regions rectiﬁer networks could grow exponentially depth. however understood growth complexity split regions linearity improves generalization performance. related works focus properties functions represented neural networks. however ultimate goal classiﬁcation class boundaries whose function representations however unique. furthermore complexity different function representations identical class boundary arbitrarily large. example boundary consequently complexity analysis functions represented neural networks directly complexity class boundaries. fortunately rich history approximations euclidean balls general convex bodies shown closely related rectiﬁer/maxout networks. paper investigate approximations convex class boundaries show superior power deep rectiﬁer networks single hidden layer networks. section ﬁrst deﬁne resolution classiﬁer boundary number exposed facets boundary consider resolution convex classiﬁer boundaries particular. since general class boundary consists convex concave subsets required resolution approximation convex boundaries provides lower bound required resolution approximation general class boundaries. since continuous function approximated function boundary classiﬁer represented continuous function also approximated sufﬁciently larger number facets around boundary. high accuracy approximation large number facets required general. resolution classiﬁer boundaries similar resolution digital images later represents quality digital images approximation natural scenes former represents quality classiﬁers approximation class boundaries. paper focus approximations convex boundaries convex classiﬁers. general smooth class boundary viewed union number convex/concave surfaces. next address resolution convex classiﬁer boundaries. next section consider required resolutions convex classiﬁers approximation general convex boundaries. required resolution used section estimate number required units single hidden layer rectiﬁer network approximate general smooth convex boundaries high dimensional spaces. section starts spherical boundaries moves general smooth convex boundaries. surprisingly given threshold approximation errors required resolution general convex surfaces higher required resolution spherical surfaces given boundary convex body convex convex combinations points convex boundary form convex body approximation convex boundaries convex classiﬁers generates polytopic boundaries equivalent volume approximation convex bodies polytopes. next results polytopic approximations euclidean balls derive required number facets approximate spherical boundaries given error thresholds. approximation general convex boundaries addressed section using results polytopic approximations general convex bodies corollary provides required number facets approximate euclidean balls. next section generalize result general convex bodies show required resolution higher approximation eulidean balls. theorem shows representation general convex boundary requires exponentially high resolution matter shape boundary simple complex. learning parameters linear units maxout network approximate spherical boundary prone overﬁtting searching center radius directly. overcome overﬁtting problem compact representations required. thanks symmetry euclidean balls group facets represented compactly small number independent parameters regular polytope used approximate spherical boundary. next section show limitations single hidden layer nets compressing representation high resolution class boundaries. superior compressive power deep rectiﬁer networks considered section number facets generated single hidden layer rectiﬁer network closely related number regions space partitioned linear units network. section ﬁrst give brief review results relating partition space hyperplanes. show results relate compressive power single hidden layer rectiﬁer networks. family distinct hyperplanes d-dimensional space denote number regions space partitioned hyperplanes denote maximum across possible sets namely hyperplanes regions sign invariant thus linear function constrained within regions. therefore boundary facet regions maximum number facets across functions lower bound consider conditions exactly facets. first choose linear units associated hyperplanes general position. then according proposition hyperplanes space regions unbounded. since linear constrained within regions bounded maximum bounded closed regions. furthermore unbounded regions region within linear units negative therefore unbounded least unbounded regions. hence elements positive sufﬁciently small negative bounded regions facet unbounded regions except within linear units negative. thus maximum number facets across lower bounded ﬁrst construct deep rectiﬁer network dimensional data section construct deep rectiﬁer network section efﬁcient approximation high dimensional spherical boundaries. high dimensional cases ﬁrst constructed rectiﬁer network approximate input output previous network another input apply network approximate norm shows that similar approximation accuracy single hidden layer nets require much larger number units constructed deep rectiﬁer nets. particular small number required facets dominated approximation accuracy constructed deep exponentially efﬁcient single hidden layer nets. note node constructed deep rectiﬁer network connected nodes unit parameters determined unit single hidden layer parameters learn. large dimension constructed deep rectiﬁer network least efﬁcient single hidden layer neural networks. introducing boundary resolution classiﬁers paper shown superior compressive power deep rectiﬁer networks single hidden layer rectiﬁer networks high resolution representation class boundaries. requirement universal approximation capacity non-polynomial activation functions rectiﬁers used neural networks cost exponentially increased model complexity approximation geometrically-simple class boundaries spherical boundaries boundaries represented small number parameters. learn geometricallysimple boundaries deep neural nets required learn compact models purpose good generalization exploiting symmetric properties class boundaries.", "year": 2017}