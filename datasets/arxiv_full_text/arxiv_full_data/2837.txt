{"title": "Sim-To-Real Optimization Of Complex Real World Mobile Network with  Imperfect Information via Deep Reinforcement Learning from Self-play", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Mobile network that millions of people use every day is one of the most complex systems in real world. Optimization of mobile network to meet exploding customer demand and reduce CAPEX/OPEX poses greater challenges than in prior works. Learning to solve complex problems in real world to benefit everyone and make the world better has long been ultimate goal of AI. However, it still remains an unsolved problem for deep reinforcement learning (DRL), given imperfect information in real world, huge state/action space, lots of data needed for training, associated time/cost, multi-agent interactions, potential negative impact to real world, etc. To bridge this reality gap, we proposed a DRL framework to direct transfer optimal policy learned from multi-tasks in source domain to unseen similar tasks in target domain without any further training in both domains. First, we distilled temporal-spatial relationships between cells and mobile users to scalable 3D image-like tensor to best characterize partially observed mobile network. Second, inspired by AlphaGo, we used a novel self-play mechanism to empower DRL agent to gradually improve its intelligence by competing for best record on multiple tasks. Third, a decentralized DRL method is proposed to coordinate multi-agents to compete and cooperate as a team to maximize global reward and minimize potential negative impact. Using 7693 unseen test tasks over 160 unseen simulated mobile networks and 6 field trials over 4 commercial mobile networks in real world, we demonstrated the capability of our approach to direct transfer the learning from one simulator to another simulator, and from simulation to real world. This is the first time that a DRL agent successfully transfers its learning directly from simulation to very complex real world problems with incomplete and imperfect information, huge state/action space and multi-agent interactions.", "text": "yongxi tanjin yangxin chenqitao songyunjun chenzhangxiang yezhenqiang futurewei tech. inc. research center {yongxi.tanjin.yangzhenqiang.su}huawei.com huawei technologies p.r.china {chenxinsongqitaochenyunjunyezhangxiang}huawei.com mobile network millions people every complex systems real world. optimization mobile network meet exploding customer demand reduce capex/opex poses greater challenges prior works. actually learning solve complex problems real world benefit everyone make world better long ultimate goal however still remains unsolved problem deep reinforcement learning given incomplete/imperfect information real world huge state/action space lots data needed training associated time/cost interactions among multi-agents potential negative impact real world etc. bridge reality proposed framework direct transfer optimal policy learned multi-tasks source domain unseen similar tasks target domain without training domains. first distilled temporal-spatial relationships cells mobile users scalable image-like tensor best characterize partially observed mobile network. second inspired alphago used novel self-play mechanism empower agent gradually improve intelligence competing best record multiple tasks. third decentralized method proposed coordinate multi-agents compete cooperate team maximize global reward minimize potential negative impact. using unseen test tasks unseen simulated mobile networks field trials commercial mobile networks real world demonstrated capability approach direct transfer learning simulator another simulator simulation real world. first time agent successfully transfers learning directly simulation complex real world problems incomplete imperfect information huge state/action space multi-agent interactions. using deep neural network rich representation high-dimensional visual input universal function approximator deep reinforcement learning achieved unprecedented success challenging domains atari game ultimate goal creating agent learn like human also make world better solving complex problems real world. however application complex real world problems still remains unsolved problem imperfect information huge state/action space simulation real world multi-agent interactions time/cost etc. work one-shot optimization real world mobile network millions people every day. coverage capacity optimization mobile network crucial mobile carrier meet exploding customer demand reduce capex/opex e.g. capex verizon cisco acquired intucell however poses much difficult challenges prior works. first mobile network complex systems real world since multiusers multi-cells multi-technologies heterogeneous network figure framework figure distill tensor figure self play figure cooperate mobile services consumed billions devices many resource management decisions need made provide seamless services. second critical take actions since involves time-consuming costly site visits adjust vertical horizontal angle cell antennas. third important state information typically missing erroneous fourth action space huge possible actions cells. last coordinating actions multiple cells crucial since cell action significant impact coverage interference neighbor cells. figure given discrepancy source target domain perception input agent domains projecting observations source target domain second tasks source target domain similar source target task distributions thought drawn task population direct transfer policy treated generalization problem. therefore design generate sufficient amount diversified tasks source domain minimize difference source task distribution target task distribution view agent. ideally want learn optimal policy target domain transit initial state optimal state one-shot. practice instead learn optimal policy source domain approximate approximate neural network weight since partially observable. given complex temporal-spatial relationships cells mobile users discrepancy simulator real world capability convolutional neural network exploit spatially local pattern distill local observations agent/cell scalable tensor field view agent. figure cell rank neighbor cells based relationships e.g. inter-site distance overall interference. select important neighbors center arrange around axis tensor based rank. last channel along z-axis extract relevant information temporal-spatial relationships pair cells axis topology performance indicator measured signal etc. inspired alphago novel self-play mechanism encourage competition best record multi-tasks like athletes compete world record decathlon. figure initial state task drawn distribution actions achieve better immediate global reward cells rnew best record rbest history threshold ∆rg=rnew-rbest>=thge encourage backpropagating gradient ge=te)*dθ function absolute value function) expected total reward baseline reinforce gradient w.r.t. weights ∆rg<=thgp penalize gradient gp=tp)*dθ function -*abs. thgp<∆rg <thge simulated annealing decide accepting comparing uniform random number acceptance probability pg=/) global temperature annealed according certain cooling schedule proposed decentralized self-play competitive/cooperative method coordinate multiagents compete team best global reward self-play cooperate minimize negative impact. figure cell/agent takes action local view pnn. actions accepted global level local reward cell larger threshold rc>=thce accept action gradient rc<=thcp reject gradient thcp<rc<thce acceptance probability pc=/) cell level temperature. first generated tasks netlab simulator random tilt settings initial states simulated mobile networks designed agent optimize training task steps generate labels supervised learning depth- residual network input output tilt using training data achieved accuracy degree degree validation data. weights sl-dnn initialize agents double training tasks mobile networks train agents one-shot epochs threads validation tasks epoch. figure achieved better result terms immediate global reward averaged validation tasks ratio validation tasks positive global reward also tested policy unseen tasks mobile networks netlab without retraining. figure achieved average global reward ratio test tasks positive gain. verified cross-domain generalization power testing policy unseen tasks unseen mobile networks another simulator unet without training simulators. figure also achieved good results average global reward ratio test tasks positive gain. verify generalization capability approach direct transfer learning simulation unseen tasks unseen complex real world mobile network without training domains performed field trials commercial mobile networks never simulated simulators different simulated mobile networks e.g. multi-frequency carrier aggregations never simulated before user distribution/number real world mobile network temporal-spatial dynamic different static distribution/number simulators different cell/building layouts radio propagation. separated commercial mobile network neighboring clusters performed trial rsrp rsrq improvement rsrp rsrq improvement trial done whole mobile network significant improvement observed since gain achieved first trials. trial achieved rsrp rsrq improvement mobile network trial mobile network significant improvement observed either little room optimization significant difference mobile network task distributions simulation. trial achieved rsrp rsrq improvement commercial mobile network konstantinos bousmalis sergey levine. closing simulation-to-reality deep robotic learning. google research blog https//research.googleblog.com///closing-simulation-to-reality-gap-for.html shaoshuai tian cigdem sengul. self-optimization coverage capacity based fuzzy neural network cooperative reinforcement learning. eurasip journal wireless communications networking hado hasselt arthur guez david silver. deep reinforcement learning double q-learning. aaai' proceedings thirtieth aaai conference artificial intelligence marketwired. cisco announces intent acquire intucell. http//www.marketwired.com/press-release/ciscoannounces-intent-to-acquire-intucell-nasdaq-csco-.htm volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski stig petersen charles beattie amir sadik ioannis antonoglou helen king dharshan kumaran daan wierstra shane legg demis hassabis. human-level control deep reinforcement learning. nature volodymyr mnih adrià puigdomènech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. preprint https//arxiv.org/abs/. matej moravčík martin schmid neil burch viliam lisý dustin morrill nolan bard trevor davis kevin waugh michael johanson michael bowling. deepstack expert-level artificial intelligence no-limit poker. science josh tobin rachel fong alex jonas schneider wojciech zaremba pieter abbeel. domain randomization transferring deep neural networks simulation real world. iros andrei rusu matej vecerik thomas rothörl nicolas heess razvan pascanu raia hadsell. sim-to-real robot learning pixels progressive nets. preprint https//arxiv.org/abs/. david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot sander dieleman dominik grewe john nham kalchbrenner ilya sutskever timothy lillicrap madeleine leach koray kavukcuoglu thore graepel demis hassabis. mastering game deep neural networks tree search. nature david silver julian schrittwieser karen simonyan ioannis antonoglou huang arthur guez thomas hubert lucas baker matthew adrian bolton yutian chen timothy lillicrap laurent sifre george driessche thore graepel demis hassabis. mastering game without human knowledge. nature oriol vinyals timo ewalds sergey bartunov petko georgiev alexander sasha vezhnevets michelle alireza makhzani heinrich küttler john agapiou julian schrittwieser john quan stephen gaffney stig petersen karen simonyan schaul hado hasselt david silver timothy lillicrap kevin calderone paul keet anthony brunasso david lawrence anders ekermo jacob repp rodney tsing. starcraft challenge reinforcement learning. preprint https//arxiv.org/abs/.", "year": 2018}