{"title": "Fast Bayesian Optimization of Machine Learning Hyperparameters on Large  Datasets", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Bayesian optimization has become a successful tool for hyperparameter optimization of machine learning algorithms, such as support vector machines or deep neural networks. Despite its success, for large datasets, training and validating a single configuration often takes hours, days, or even weeks, which limits the achievable performance. To accelerate hyperparameter optimization, we propose a generative model for the validation error as a function of training set size, which is learned during the optimization process and allows exploration of preliminary configurations on small subsets, by extrapolating to the full dataset. We construct a Bayesian optimization procedure, dubbed Fabolas, which models loss and training time as a function of dataset size and automatically trades off high information gain about the global optimum against computational cost. Experiments optimizing support vector machines and deep neural networks show that Fabolas often finds high-quality solutions 10 to 100 times faster than other state-of-the-art Bayesian optimization methods or the recently proposed bandit strategy Hyperband.", "text": "bayesian optimization become successful tool hyperparameter optimization machine learning algorithms support vector machines deep neural networks. despite success large datasets training validating single conﬁguration often takes hours days even weeks limits achievable performance. accelerate hyperparameter optimization propose generative model validation error function training size learned optimization process allows exploration preliminary conﬁgurations small subsets extrapolating full dataset. construct bayesian optimization procedure dubbed fabolas models loss training time function dataset size automatically trades high information gain global optimum computational cost. experiments optimizing support vector machines deep neural networks show fabolas often ﬁnds high-quality solutions times faster state-of-the-art bayesian optimization methods recently proposed bandit strategy hyperband. performance many machine learning algorithms hinges certain hyperparameters. example prediction error non-linear support vector machines depends regularization kernel hyperparameters modern neural networks sensitive wide range hyperparameters including learning rates momentum terms number units layer dropout rates weight decay etc. poor scaling naïve methods like grid search dimensionality driven interest sophisticated hyperparameter optimization methods past years bayesian optimization emerged efﬁcient framework achieving impressive successes. example several studies found better instantiations convolutional network hyperparameters domain experts repeatedly improving score cifar- benchmark without data augmentation traditional setting bayesian hyperparameter optimization loss machine learning algorithm hyperparameters treated black-box problem ﬁnding minx∈x mode interaction objective evaluate inputs individual evaluations entire dataset require days weeks evaluations possible limiting quality best found value. human experts instead often study performance subsets data ﬁrst become familiar characteristics gradually increasing subset size approach still outperform contemporary bayesian optimization methods. motivated experts’ strategy leverage dataset size additional degree freedom enriching representation optimization problem. treat size randomly subsampled dataset nsub additional input blackbox function allow optimizer actively choose function evaluation. allows bayesian optimization mimic improve upon human experts exploring hyperparameter space. nsub hyperparameter itself goal remains good performance full dataset i.e. nsub multi-task bayesian optimization swersky knowledge transferred ﬁnite number correlated tasks. tasks represent manually-chosen subset-sizes method also tries best conﬁguration full dataset evaluating smaller cheaper subsets. however discrete nature tasks approach requires evaluations entire dataset learn necessary correlations. instead approach exploits regularity performance across dataset size enabling generalization full dataset without evaluating directly. approaches hyperparameter optimization large datasets include work nickson estimated conﬁguration’s performance large dataset evaluating several training runs small random subsets ﬁxed manually-chosen sizes. krueger showed that practical applications small subsets sufﬁce estimate conﬁguration’s quality proposed cross-validation scheme sequentially tests ﬁxed conﬁgurations growing subset data discarding poorly-performing conﬁgurations early. parallel work proposed multi-arm bandit strategy called hyperband dynamically allocates resources randomly sampled conﬁgurations based performance subsets data. hyperband assures well-performing conﬁgurations trained full dataset discarding ones early. despite simplicity experiments method able outperform well-established bayesian optimization algorithms. review bayesian optimization particular entropy search algorithm related method multitask bayesian optimization. introduce bayesian optimization method fabolas hyperparameter optimization large datasets. iteration fabolas chooses conﬁguration dataset size nsub predicted yield information loss-minimizing conﬁguration full dataset unit time spent. broad range experiments support vector machines various deep neural networks show fabolas often identiﬁes good hyperparameter settings times faster state-of-the-art bayesian optimization methods acting full dataset well hyperband. bayesian optimization given black-box function bayesian optimization aims input minx∈x globally minimizes requires prior function acquisition function quantifying hyperband ﬁrst described arxiv paper fabolas ﬁrst described nips workshop paper utility evaluation ingredients following three steps iterated promising numerical optimization; evaluate expensive often noisy function resulting data point observations j=...n; update typically evaluations acquisition function cheap compared evaluations optimization effort negligible. gaussian processes prominent choice thanks descriptive power analytic tractability formally collection random variables every ﬁnite subset follows multivariate normal distribution. identiﬁed mean function positive deﬁnite covariance function given observations j=...n joint gaussian likelihood posterior follows another mean covariance functions tractable analytic form. covariance function determines observations inﬂuence prediction. hyperparameters wish optimize adopt matérn kernel automatic relevance determination form stationary twice-differentiable model constitutes relatively standard choice bayesian optimization literature. contrast gaussian kernel popular elsewhere makes less restrictive smoothness assumptions helpful optimization setting here free parameters—hyperparameters surrogate model—and diag) mahalanobis distance. dataset size dependent performance cost construct custom kernel additional hyperparameter model overall noise covariance needed handle noisy observations. clarity hyperparameters internal hyperparameters bayesian optimizer opposed target machine learning algorithm tuned. section shows handle them. recent acquisition function selects evaluation points based predicted information gain optimum rather aiming evaluate near optimum. heart lies probability distribution pmin belief function’s minimum given prior observations information gain measured expected kullback-leibler divergence pmin}) uniform distribution expectations taken measurement obtained kernel represented implicitly cholesky decomposition whose entries sampled mcmc together hyperparameters considering distribution optimum target task computing information w.r.t. swersky information gain unit cost acquisition function expectation represents information gain target task averaged possible outcomes based current model. cost conﬁguration task known priori modelled objective function. model supports machine learning hyperparameter optimization large datasets using discrete dataset sizes tasks. swersky indeed studied approach special case representing small large dataset; baseline experiments. primary numerical challenge framework computation pmin}) integral above. intractability several approximations made. refer hennig schuler details well supplemental material also provide pseudocode implementation. despite conceptual computational complexity offers well-deﬁned concept information gained function evaluations meaningfully traded quantities evaluations’ cost. multi-task bayesian optimization method swersky refers general framework optimizing presents different correlated tasks. given tasks objective function corresponds evaluating given tasks relation points modeled using product kernel here introduce approach fast bayesian optimization large data sets traditional bayesian hyperparameter optimizers model loss machine learning algorithms given dataset blackbox function minimized fabolas models loss computational cost across dataset size uses models carry bayesian optimization extra degree freedom. blackbox function takes another input representing data subset size; relative sizes nsub/n representing entire dataset. eventual goal minimize loss entire dataset evaluating smaller usually cheaper function values obtained correlate across unfortunately correlation structure initially unknown challenge design strategy trades cost function evaluations beneﬁt learning scaling behavior ultimately conﬁgurations work best full dataset. following nomenclature fact swersky deviated slightly formula considering difference information gains min}). stated work better practice evidence experiments thus consistency variant presented throughout. propose principled rule automatic selection next pair evaluate. nutshell standard bayesian optimization would always conﬁgurations full dataset reason about much learned performance full dataset evaluation fabolas automatically determines amount data necessary extrapolate full dataset. initial intuition performance changes dataset size evaluated grid conﬁgurations support vector machine subsets mnist dataset mnist data points evaluated relative subset sizes figure visualizes validation error conﬁgurations evidently dataset quite representative sufﬁcient locate reasonable conﬁguration. additionally deceiving local optima smaller subsets. based observations expect relatively small fractions dataset yield representative performances therefore vary relative size parameter logarithmic scale. transfer insights illustrative example formal model loss cost across subset sizes extend model additional input dimension namely allows surrogate extrapolate full data without necessarily evaluating there. chose factorized kernel consisting standard stationary kernel hyperparameters multiplied ﬁnite-rank covariance function since choice basis function yields positive semi-deﬁnite covariance function provides ﬂexible language prior knowledge relating form kernel model loss cost respectively different basis functions supplemental material visualize scaling loss cost example show kernels indeed well. also evaluate possibility modelling heteroscedastic noise introduced subsampling data fabolas starts initial design described detail section afterwards beginning iteration loss computational cost across dataset sizes using kernel then capturing selects maximizer following acquisition function trade information gain versus cost models data choose maximizing acquisition function equation evaluate also measuring cost augment data choose incumbent based predicted loss xt}. loss machine learning algorithms usually decreases training data. incorporate behavior choosing enforce monotonic predictions extremum kernel choice equivalent bayesian linear regression basis functions gaussian priors weights. proposed acquisition function resembles used mtbo differences first mtbo’s discrete tasks replaced continuous dataset size second prediction computational cost augmented overhead bayesian optimization method. inclusion reasoning overhead important appropriately reﬂect information gain unit time spent figure validation error grid conﬁgurations subsets mnist dataset various sizes nsub. small subsets quite representative validation error conﬁguration remains constant around whereas region good conﬁgurations change drastically matter whether time spent function evaluation reasoning evaluation perform. practice cubic scaling number data points computational complexity approximating additional overhead fabolas within orps= minutes differences computational cost order seconds become negligible comparison. anytime algorithm fabolas keeps track incumbent time step. select conﬁguration performs well full dataset predicts loss evaluated conﬁgurations using model picks minimizer. found work robustly globally minimizing posterior mean similar approaches. common bayesian optimization start initial design points chosen random latin hypercube design allow reasonable models starting points. fully leverage speedups obtain evaluating small datasets bias selection towards points small datasets order improve prediction dependencies draw random points evaluate different subsets data provides information scaling behavior assuming costs increase linearly superlinearly function evaluations function evaluations cost less full dataset. important cost initial design course counts towards fabolas’ runtime. true standard mtbo never exploited emphasis total wall clock time spent hyperparameter optimization. want emphasize express budgets terms wall clock time since natural practical applications. presentation fabolas omits details impact performance method. become standard bayesian optimization markov-chain monte carlo integration marginalize hyperparameters accelerate optimization hyper-priors emphasize meaningful values parameters chieﬂy adopting choices spearmint toolbox uniform prior length scales space lognormal prior covariance amplitude horseshoe prior length scale noise variance used original formulation hennig schuler rather recent reformulation hernández-lobato main reason latter prohibits non-stationary kernels bochner’s theorem spectral approximation. could principle extended work particular choice kernels since would complicate making modiﬁcations kernel leave avenue future work note case improve method. maximize acquisition function used blackbox optimizer direct cmaes empirical evaluation fabolas compared standard bayesian optimization mtbo hyperband. method tracked wall clock time storing incumbent returned every iteration. ofﬂine validation step trained models incumbents full dataset measured figure evaluation grid mnist. baseline comparison test performance methods’ selected incumbents time. test performance time variants mtbo different dataset sizes auxiliary task. dataset size fabolas mtbo pick iteration trade small cost high information gain; unlike elsewhere paper right plot shows mean stddev runs test error. plot test errors throughout. obtain error bars performed independent runs method different seeds plot medians along percentiles experiments. details hyperparameter ranges used every experiment given supplemental material implemented hyperband following using recommended setting parameter controls intermediate subset sizes. experiment adjusted budget allocated hyperband iteration allow minimum dataset size fabolas times number classes support vector machine benchmarks maximum batch size neural network benchmarks. also followed prescribed incumbent estimation iteration conﬁguration best performance full dataset size. first considered benchmark allowing comparison various bayesian optimization methods ground truth grid mnist performed function evaluations beforehand measuring loss cost times conﬁguration subset size account performance variations. ants single auxiliary task relative size respectively. auxiliary tasks either mtbo improved quickly converged slowly optimum; believe small correlations tasks cause this. figure shows dataset sizes chosen different algorithms optimization; methods slowly increased average subset size used time. auxiliary task worked best used mtbo remaining experiments. ﬁrst glance might expect many tasks work best quite opposite true. preliminary experiments evaluated mtbo auxiliary tasks found performance strongly degrade kernel parameters learned discrete task kernel tasks main reason. mcmc sampling short correlations appropriately reﬂected especially early iterations adjusted sampling creates large computational overhead dominates wall-clock time. therefore obtained best performance auxiliary task. figure shows results using random search mtbo fabolas benchmark. perform equally well best conﬁguration around seconds roughly times faster random search. mtbo achieves good performance faster requiring around seconds global optimum. fabolas roughly another order magnitude faster mtbo ﬁnding good conﬁgurations ﬁnds global optimum time. figure hyperparameter optimization datasets covertype vehicle mnist. time plots show test performance methods’ respective incumbents. fabolas ﬁnds good conﬁguration times faster methods. realistic scenario optimized hyperparameters without grid constraint mnist prominent datasets vehicle registration forest cover types data points also comparing hyperband. training svms datasets take several hours figure shows fabolas found good conﬁgurations times faster methods. hyperband required relatively long time recommended ﬁrst hyperparameter setting ﬁrst recommendation already good making hyperband substantially faster good settings standard bayesian optimization running full dataset. however fabolas typically returned conﬁgurations quality another order magnitude faster. convolutional neural networks shown superior performance variety computer vision speech recognition benchmarks ﬁnding good hyperparameter settings remains challenging almost theoretical guarantees exist. tuning cnns modern large datasets often infeasible standard bayesian optimization; fact motivated development fabolas. experimented hyperparameter optimization cnns well-established object recognition datasets namely cifar svhn used setup datasets layer optimized using adam considered total hyperparameters initial learning rate batch size number units layer. cifar used images training estimate validation error standard hold-out images estimate presented fabolas bayesian optimization method based entropy search mimics human experts evaluating algorithms subsets data quickly gather information good hyperparameter settings. fabolas extends standard modelling objective function treating dataset size additional continuous input variable. allows incorporation strong prior information. models time takes evaluate conﬁguration aims evaluate points yield—per time spent—the information globally best hyperparameters full dataset. various hyperparameter optimization experiments using support vector machines deep neural networks fabolas often found good conﬁgurations times faster related approach multi-task bayesian optimization hyperband standard bayesian optimization. opensource code available https//github.com/automl/robo along scripts reproducing experiments. future work plan expand algorithm model environmental variables resolution size images number classes number epochs expect yield additional speedups. since method reduces cost individual function evaluations requires cheaper evaluations expect cubic complexity gaussian processes become limiting factor many practical applications. therefore plan extend work model classes bayesian neural networks lower computational overhead similar predictive quality. results figure show that—compared tasks—fabolas’ speedup smaller cnns scale linearly number datapoints. nevertheless found good conﬁgurations times faster vanilla bayesian optimization. reason linear scaling hyperband substantially slower vanilla bayesian optimization make recommendation good hyperparameter settings given enough time. ﬁnal experiment evaluated performance method expensive benchmark optimizing validation performance deep residual network cifar dataset using original architecture hyperparameters exposed learning rate regularization momentum factor learning rate multiplied epochs. figure shows fabolas found conﬁgurations reasonable performance roughly times faster mtbo. note limited computational capacities unable hyperband benchmark single iteration took longer making prohibitively expensive. want emphasize runtime could improved adapting hyperband’s parameters benchmark decided keep methods’ parameters ﬁxed throughout experiments also show robustness. hansen. evolution strategy comparing review. j.a. lozano larranaga inza bengoetxea editors towards evolutionary computation. advances estimation distribution algorithms. springer blackard dean. comparative accuracies artiﬁcial neural networks discriminant analysis predicting forest cover types cartographic variables. comput. electron. agric. netzer wang coates bissacco reading digits natural images unsupervised feature learning. nips workshop deep learning unsupervised feature learning ioffe szegedy. batch normalization accelerating deep network training reducing internal covariate shift. proceedings international conference machine learningicml lille france july brochu cora freitas. tutorial bayesian optimization expensive cost functions application active user modeling hierarchical reinforcement learning. corr williams santner notz. sequential design computer experiments minimize integrated response functions. statistica sinica lecun bottou bengio haffner. gradient-based learning applied document recognition. haykin kosko editors intelligent signal processing. ieee press", "year": 2016}