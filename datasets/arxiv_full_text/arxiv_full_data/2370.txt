{"title": "An Algorithm for Training Polynomial Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We consider deep neural networks, in which the output of each node is a quadratic function of its inputs. Similar to other deep architectures, these networks can compactly represent any function on a finite training set. The main goal of this paper is the derivation of an efficient layer-by-layer algorithm for training such networks, which we denote as the \\emph{Basis Learner}. The algorithm is a universal learner in the sense that the training error is guaranteed to decrease at every iteration, and can eventually reach zero under mild conditions. We present practical implementations of this algorithm, as well as preliminary experimental results. We also compare our deep architecture to other shallow architectures for learning polynomials, in particular kernel learning.", "text": "consider deep neural networks output node quadratic function inputs. similar deep architectures networks compactly represent function ﬁnite training set. main goal paper derivation efﬁcient layer-by-layer algorithm training networks denote basis learner. algorithm universal learner sense training error guaranteed decrease every iteration eventually reach zero mild conditions. present practical implementations algorithm well preliminary experimental results. also compare deep architecture shallow architectures learning polynomials particular kernel learning. signiﬁcant recent developments machine learning resurgence deep learning usually form artiﬁcial neural networks. systems based multi-layered architecture input goes several transformations higher-level concepts derived lower-level ones. thus systems considered particularly suitable hard tasks computer vision language processing. history multi-layered systems long uneven. extensively studied early mixed success eventually displaced large extent shallow architectures support vector machine boosting algorithms. shallow architectures worked well practice also came provably correct computationally efﬁcient training algorithms requiring tuning small number parameters thus allowing incorporated standard software packages. however recent years combination algorithmic advancements well increasing computational power data size breakthrough effectiveness neural networks deep learning systems shown impressive practical performance variety domains well references therein). resurgence interest learning systems. nevertheless major caveat deep learning always strong reliance heuristic methods. despite decades research clear-cut guidance choose architecture size network type computations performs. even chosen training networks involves non-convex optimization problems often quite difﬁcult. worst-case guarantees possible pulling successfully still much black requiring specialized expertise much manual work. basic idealized form algorithm parameter-free. network grown incrementally added layer decreases bias increasing variance. process stopped satisfactory performance obtained. architectural details network automatically determined theory. describe efﬁcient variant algorithm requires specifying maximal width network advance. optionally additional ﬁne-tuning experimental results indicate even rough tuning already sufﬁcient promising results. algorithm present trains particular type deep learning system computational node computes linear quadratic function inputs. thus predictors learn polynomial functions input space networks learn also related sum-product networks introduced context efﬁcient representations partition functions derivation algorithm inspired ideas used different purpose. core method attempts build network provides good approximate basis values attained polynomials bounded degree training instances. similar well-known principle modern deep learning layers network built one-by-one creating higher-and-higher level representations data. representation built ﬁnal output layer constructed solving simple convex optimization problem. rest paper structured follows. sec. introduce notation. heart paper sec. present algorithm analyze properties. sec. discuss sample complexity issues. sec. compare deep architecture learning polynomials shallow architecture obtained kernel learning. sec. present preliminary experimental results. bold-face letters denote vectors. particular denotes all-ones vector. vectors denote hadamard product namely vector refers euclidean norm. refers indicator function. matrices number rows denote matrix formed concatenating columns matrix refers entry column refers j-th column; refers number columns. assume given labeled training data scalar label/target value. denote matrix vector simplicity presentation assume note results easily relaxed. turn develop basis learner algorithm well accompanying analysis. three stages first derive generic idealized version algorithm runs polynomial time practical; second analyze properties terms time complexity training error etc.; third discuss analyze realistic variant algorithm also enjoys theoretical guarantees generalizes better ﬂexible practice. recall goal learn polynomial predictors using deep architecture based training instances however ignore learning aspect focus representation problem build network capable representing values polynomial instances? ﬁrst glance seem like tall order since space polynomials speciﬁed bounded number parameters. however ﬁrst crucial observation care values training instances. represent values m-dimensional vectors since space polynomials attain values ﬁnite distinct points polynomials span linear projection. standard result linear algebra immediately implies polynomials pi)}m form basis write values linear combination these. formally following nice able express target values function input instances expressive machine likely lead overﬁtting. generic algorithm builds deep network nodes ﬁrst layers form basis values attained degree-r polynomials. therefore start simple network might large bias tend overﬁt make network deeper deeper gradually decrease bias increasing variance. thus principle algorithm used train natural curve solutions used control bias-variance tradeoff. remains describe build network. first show construct basis spans values attained degree- polyonomials show enlarge basis values attained degree- polynomials enlargement degree corresponds another layer network. later prove step calculated polynomial time whole process terminates polynomial number iterations. -dimensional linear subspace thus construct basis need vectors vectors )}d+ linearly independent. done many ways. example construct orthogonal basis using gram-schmidt matrix orthogonal columns). stage focus present approach full generality avoid ﬁxing speciﬁc basis-construction method. this essentially ﬁrst step algorithm moreover similar performing principal component analysis data often standard ﬁrst step learning. differs done augmented matrix rather centered version signiﬁcant here since columns centered data matrix cannot express vector hence cannot express constant polynomial data. whatever basis-construction method linear transformation maps constructed basis. columns specify linear functions forming ﬁrst layer network j’th node ﬁrst layer function one-layer network whose outputs span values attained linear functions training instances. principle trick basis degree- polynomials degree polynomial consider space values attained polynomials training data spanning basis. however quickly computational problem since space degree polynomials increases exponentially requiring consider exponentially many vectors. instead utilize deep architecture compact representation required basis using following simple important observation proof. polynomial degree written weighted monomials degree plus polynomial degree moreover monomial degree written product monomial degree monomial degree since -degree monomials particular -degree polynomials result follows. lemma implies degree- polynomial written products degree- polynomials plus degree- polynomial. since nodes ﬁrst layer network span degree- polynomials particular span polynomials follows degree- polynomial scalars. words vector values attainable degree- polynomial span vector values attained nodes ﬁrst layer products outputs every nodes ﬁrst layer. switch back algebraic representation. recall constructing ﬁrst layer formed matrix whose columns span values attainable degree- polynomials. implies matrix spans possible values attainable degree- polynomials. thus basis values attained degree- polynomials enough column subset columns linearly independent basis columns. again basis construction done several ways using standard linear algebra columns specify layer network column corresponds corresponds turn node layer computes ﬁrst layer. redeﬁne augmented matrix product nodes column subset columns form basis columns redeﬁne assured columns span values polynomials degree data. adding newly constructed layer network whose outputs form basis values attained polynomials degree training instances. process described figure resulting network architecture shown figure note resulting network feedforward architecture. connections though adjacent layers unlike many common deep learning architectures. moreover although refrain ﬁxing basis creation methods stage provide possible implementation figure emphasize though variants possible basis construction method different different layers. constructing output layer iterations matrix whose columns form basis values attained polynomials degree training data. moreover column exactly values attained node network training instances. figure basis learner algorithm. main algorithm constructs network bottom output layer construction procedure. stage buildbasis buildbasist ﬁxed provide possible implementation figure figure schematic diagram network’s architecture polynomials degree element represents layer nodes speciﬁed figure represent layer nodes compute functions wizi layers consist nodes compute functions form network train simple linear predictor miniming convex loss function done using convex optimization procedure. assured polynomial degree linear predictor attains value polynomial data. linear predictor forms output layer network. mentioned earlier inspiration approach based present incremental method efﬁciently build basis polynomial functions. particular basic ideas order ensure iterations resulting basis spans polynomials degree ideas also emphasize differences first emphasis generators ideal polynomials vanishing training set. second goal nothing deep learning result algorithm basis rather deep network. third build basis different seem effective context fourth practical variant algorithm described different methods used continuing analysis make several important remarks remark iteration algorithm corresponds another layer network. however note need specify number iterations. instead simply create layers one-by-one time attempting construct output layer existing nodes. check performance resulting network validation stop reach satisfactory performance. figure details. remark compared algorithm many standard deep learning algorithms constrained terms loss function especially directly attempt minimize training error. since algorithms solve hard non-convex problems important loss nice smooth possible often focus squared loss tailored loss). contrast algorithm easily work convex loss. buildbasis buildbasist procedures. figure buildbasis implemented return orthogonal basis columns buildbasist uses gram-schmidt procedure appropriate columns subset together forms basis columns. pseudo-code tolerance parameter remark intermediate layers proposed constructing basis columns using columns column subset however construct basis. example full linear transformation columns form orthogonal basis however approach combines important advantages. hand creates network connections nodes depend inputs nodes. makes network fast test-time well better-generalizing theory practice hand still sufﬁciently expressive compactly represent high-dimensional polynomials product-of-sums form whose expansion explicit monomials would prohibitively large. particular network computes functions involve exponentially many monomials. ability compactly represent complex concepts major principle deep learning also chose linear transformation ﬁrst layer non-output layers compute product outputs previous layers resulting predictor limited computing polynomials small number monomials. remark algorithm deep connections algebraic geometry interpolation theory. particular problem ﬁnding basis polynomial functions given well studied areas many years. however methods aware construction newton basis polynomials multivariate extensions standard polynomial interpolation methods computationally efﬁcient i.e. polynomial dimension polynomial degree based explicit handling monomials algorithm ﬁnding generators ideal polynomials vanishing given references therein). sense deep architecture orthogonal approach since focus constructing bsis polynomials vanish points. enables efﬁcient compact representation using deep architecture getting arbitrary values training set. describing generic algorithm derivation turn prove formal properties. particular show runtime polynomial training size dimension drive training error zero. next section discuss make algorithm practical computational statistical point view. theorem given training distinct points suppose algorithm figure constructing network total depth then maxt maxt algorithm terminates min{m iterations loop. assuming algorithm implemented using memory time plus polynomial time required solve convex optimization problem computing output layer. item algorithm always augments breaks whenever since columns follows algorithm cannot iterations. algorithm also terminates iterations deﬁnition. item memory bound follows bounds sizes associated sizes constructed network. note require much memory don’t need store explicitly entry speciﬁed product entries found computed on-the-ﬂy time. time bound iteration algorithm involves computations polynomial dominant factors buildbasist buildbasis. time bounds follow implementations proposed figure using upper bounds sizes relevant matrices assumption item follows fact iteration create layer nodes min{m− iterations/layers excluding input output layers. moreover node network corresponds column nodes plus output nodes. finally network computes linear transformation nodes perform products each ﬁnal output node computes weighted linear combination output nodes number operations item follows immediately derivation presented earlier. finally need show item recall derivation output layer linear weights minimize increase depth constructed network happens augment linearly independent columns initial columns exactly same. thus size prediction vectors r|f|} increases training error down. algorithm till columns span since columns linearly independent. hence r|f|} implies always training error zero. case left treat algorithm stops however claim can’t happen. happen basis construction process namely columns already span columns however would imply span values degree-t polynomials training instances using polynomials degree using lemma would imply could write values every degree-t polynomial using linear combination polynomials degree repeating this values polynomials degree spanned polynomials degree however values polynomials degree distinct points must span must remark algorithm universal algorithm sense iterations training error provably decreases eventually hitting zero. thus curve solutions trading-off training error hand size resulting network hand. algorithm presented runs provable polynomial time important limitations. particular always control depth network early stopping control width worst case large number training instances drawbacks tackle this propose simple modiﬁcation scheme network width explicitly constrained iteration. recall width layer constructed iteration equal number columns till columns span column space large might large well resulting wide layer many nodes. however give exactly spanning instead seek approximately span using smaller partial basis bounded size resulting layer width next natural question choose partial basis. several possible criterions supervised unsupervised. focus following choice found quite effective practice next layers standard orthogonal least squares procedure greedily pick columns seem relevant prediction. intuition wish quickly decrease training error using small number nodes computationally cheap way. specifically binary classiﬁcation regression consider vector training labels/target values iteratively pick column whose residual correlated residual column added existing basis process repeats itself. simple extension idea applied multiclass case. finally speed-up computation process columns mini-batches time correlated vectors iterating. procedures implemented subroutines buildbasis buildbasist whereas main algorithm remains unaffected. precise pseudo-code appears figure note practical implementation pseudo-code need explicitly compute potentially large figure practical width-limited implementations buildbasis buildbasist procedures. buildbasis implemented return orthogonal partial basis spans largest singular vectors data. buildbasist uses supervised procedure order pick partial basis useful prediction. code represents vector training labels binary classiﬁcation regression indicator matrix multiclass prediction. simplicity assume batch size divides layer width turn discuss theoretical properties width-constrained variant algorithm. recall idealized version basis learner guaranteed eventually decrease training error zero cases. however width constraint adversarial cases algorithm stuck terminate training error gets zero. happen columns spanned linearly independent vectors added zero algorithm terminate however never witnessed happen experiments prove indeed case long input instances general position thus completely analogous result thm. practical variant basis learner. intuitively general position condition require implies take columns product vector linearly independent columns intuitively plausible since entry-wise product highly non-linear operation general reason happen exactly subspace spanned columns. formally following deﬁnition distinct points m-general position every monomials matrix deﬁned rank following theorem analogous thm. difference item m-general position assumption. theorem given training distinct points suppose algorithm figure subroutines implemented figure using uniform value width batch size constructing network depth then maxt maxt assume case regression classiﬁcation constant number classes. algorithm implemented using memory time plus polynomial time required solve convex optimization problem computing output layer createbasis rows matrix returned width-limited variant buildbasis m-general position unconstrained exists vector prediction values iterations training error proof. proof theorem except part simple adaptation proof thm. using construction remarks made earlier. left prove part algorithm terminate driving error zero iteration columns spanned construction implies monomials apply rows obtain linearly dependent vectors. contradicts assumption rows m-general position concludes proof. note item mentioned matrix requires time perform exactly. however randomized approximate procedures perform computation time. exact approximate methods known perform well practice experiments observed signiﬁcant degradation using lieu exact svd. overall ﬁxed allows basis learner algorithm construct network time linear data size. overall compared thm. practical variant signiﬁcantly lowers memory time requirements still property training error decreases monotonically network depth reduces zero mild conditions likely hold natural datasets. continuing emphasize approach quite generic criterions presented section pick partial basis iteration means ones possible. example greedy selection procedures pick best columns buildbasist well unsupervised methods. similarly supervised methods construct ﬁrst layer. also width different layers differ. however goal propose sophisticated bestperforming method rather demonstrate using approach even simple regularization greedy construction methods good theoretical guarantees work well experimentally. course much work remains trying methods. focused network build reduces training error. however learning context actually interested getting good generalization error namely good prediction expectation distribution training data sampled. view algorithm procedure given training data picks network width depth network binary classiﬁcation relevant measure generalization performance vc-dimension class networks. luckily vc-dimension neural networks well-studied topic. particular theorem know binary function class euclidean space parameterized parameters function speciﬁed using addition multiplication comparison operations dimension network speciﬁed manner using operations parameters immediately implies dimension bound ensures generalization training data size sufﬁciently large compared network size. note bound generic rather coarse suspect substantially improved case. however qualitatively speaking tells reducing number parameters network reduces overﬁtting. principle used network architecture node intermediate layers connected nodes rather nodes previous layer. also possible prove bounds scale-sensitive measures generalization example wellknown expected squared loss related empirical squared loss training data given bound fat-shattering dimension class functions learning combining theorems known class networks learning fat-shattering dimension upper-bounded dimension slightly larger class networks additional real input additional output node computing linear threshold function class networks similar dimension original class hence effectively bound fat-shattering dimension well. kernel learning enjoyed immense popularity past years efﬁcient training instances kernel function efﬁciently computes inner product high inﬁnite-dimensional hilbert space data mapped implicitly feature mapping section discuss interesting relationships work kernel learning. kernel learning common kernel choice polynomial kernel x)∆. easy predictors deﬁned polynomial kernel correspond polynomial functions degree moreover gram matrix full-rank values training data realized kernel predictor desired vector values simply coefﬁcient vector thus algorithm completion polynomial network represent predictor class kernel predictors polynomial kernel. however important differences make system potentially better polynomial kernels always manipulate matrix requires memory runtime scaling least quadratically expensive large hinders application kernel learning large-scale data. quadratic dependence also true test time need explicitly training examples prediction. contrast size network controlled memory runtime requirements algorithm linear good results moderately-sized network train predict much faster kernels. words potential expressiveness polynomial kernel predictors ability control training prediction complexity potentially requiring much less time memory. kernels specify degree polynomial kernel advance training. contrast network degree resulting polynomial predictor speciﬁed advance iteration algorithm increases effective degree stop satisfactory performance obtained. learning polynomial kernels corresponds learning linear combination polynomials {xi·)∆}m contrast network learns linear combination different polynomials constructed different data-dependent way. thus algorithm uses different incomparable hypothesis class compared polynomial kernel learning. learning polynomial kernels viewed network shallow architecture follows node ﬁrst layer corresponds support vector applies function x)∆. then second layer linear combination outputs ﬁrst layer. contrast section present preliminary experimental results demonstrate feasibility approach. focus show superiority existing learning approaches rather illustrate approach match performance benchmarks using couple parameters manual tuning. study approach used benchmarks protocol described benchmark datasets designed test deep learning systems require highly non-linear predictors. consist datasets instance -dimensional vector representing normalized intensity values pixel image. datasets follows dataset algorithm last examples training split used validation sets parameter tuning algorithm trained entire training using parameters classiﬁcation error test reported. algorithms used involved several deep learning systems deep belief algorithms stacked autoencode algorithm standard single-hidden-layer feedforward neural network also experiments support vector machines using kernel polynomial kernel experimented practical variant basis learner algorithm using simple publicly-available implementation matlab. mentioned earlier text avoided storing instead computing parts need arose. followed experimental protocol above using split training using validation parameter tuning. output layer used stochastic gradient descent train linear classiﬁer using standard l-regularized hinge loss intermediate layer construction procedure ﬁxed batch size tuned following parameters importantly need train network every combination values. instead every value simply built network layer time time training output layer layers checking results validation set. deviated protocol case mnist-basic dataset allowed check additional architectures width ﬁrst layer constrained layers width reason mnist known work well preprocessing since ﬁrst layer also performs similar type processing seems narrow ﬁrst layer would work well dataset indeed we’ve observed practice. without trying additional architectures test classiﬁcation error mnist-basic worse reported below. report test error results table below. dataset number corresponds numbering dataset descriptions above. dataset report test error parenthesis indicate depth/width network comparison also include test error results reported algorithms. note mnist-related datasets correspond multiclass classiﬁcation classes result achieving less error non-trivial. results algorithm performs quite well building deep networks modest size competitive previous reported results. exception rectangles dataset artiﬁcial small found hard avoid overﬁtting however compared deep learning approaches training networks required minimal human intervention modest computational resources. results also quite favorable compared kernel predictors predictors constructed algorithm stored evaluated much faster. recall kernel generally requires time memory proportional entire training order compute single prediction test time. contrast memory time requirements predictors produced algorithm generally least orders magnitudes smaller. also illustrative consider training/generalization error curves algorithm seeing bias/variance trade-off plays different parameter choices. present results mnist-rotated dataset based data gathered parameter tuning stage results datasets qualitatively similar. investigate quantities behave function network depth width ﬁrst quantity shows well generalize function network size third quantity shows expressive predictor class. second quantity hybrid showing expressive predictor class output layer regularized avoid much overﬁtting. behavior quantities presented graphically figure first it’s clear dataset requires non-linear predictor network depth resulting predictor linear classiﬁer whose train test errors around dramatically better results obtained deeper networks correspond non-linear predictors. lowest attainable training error shrinks quickly attaining error virtually larger depths/widths. accords claim basis learner algorithm essentially universal learning algorithm able monotonically decrease training error. similar decreasing trend also occurs training error tuned based validation effect important training errors small. contrast validation error classical unimodal behavior error decreases initially network continues increase size overﬁtting starts kick choosing intermediate layer’s connections sparse crucial effect. example experimented variants similar spirit algorithm columns forced orthogonal. translates adding general linear transformation layers. however variants tried tended perform worse suffer overﬁtting. surprising since linear transformations large number additional parameters greatly increasing complexity network risk overﬁtting. similarly performing linear transformation data ﬁrst layer seems important. example experimented alternative algorithm builds ﬁrst layer intermediate layers results quite inferior. experiments required explain this note without linear transformation ﬁrst layer resulting predictor represent polynomials modest number monomials moreover monomials tend sparse sparse data. mentioned earlier algorithm still performed well exact computation ﬁrst layer construction replaced approximate randomized computation useful handling large datasets exact computationally expensive.", "year": 2013}