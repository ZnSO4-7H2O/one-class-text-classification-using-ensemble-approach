{"title": "Identification of Probabilities", "tag": ["cs.LG", "cs.AI"], "abstract": "Within psychology, neuroscience and artificial intelligence, there has been increasing interest in the proposal that the brain builds probabilistic models of sensory and linguistic input: that is, to infer a probabilistic model from a sample. The practical problems of such inference are substantial: the brain has limited data and restricted computational resources. But there is a more fundamental question: is the problem of inferring a probabilistic model from a sample possible even in principle? We explore this question and find some surprisingly positive and general results. First, for a broad class of probability distributions characterised by computability restrictions, we specify a learning algorithm that will almost surely identify a probability distribution in the limit given a finite i.i.d. sample of sufficient but unknown length. This is similarly shown to hold for sequences generated by a broad class of Markov chains, subject to computability assumptions. The technical tool is the strong law of large numbers. Second, for a large class of dependent sequences, we specify an algorithm which identifies in the limit a computable measure for which the sequence is typical, in the sense of Martin-Lof (there may be more than one such measure). The technical tool is the theory of Kolmogorov complexity. We analyse the associated predictions in both cases. We also briefly consider special cases, including language learning, and wider theoretical implications for psychology.", "text": "vit´anyi national research institute mathematics computer science netherlands university amsterdam. address science park amsterdam netherlands. email paulvcwi.nl. chater behavioural science group. address warwick business school university warwick coventry email nick.chaterwbs.ac.uk. chater supported grant -rationality esrc network integrated behavioural science leverhulme trust research councils grant ep/k/. stochastic phrase structure grammar conventional phrase structure grammar probabilities associated rewrite rules. example noun phrase might sometimes expand give determiner followed noun sometimes expanding give single proper noun; individual grammatical categories proper nouns probabilistically speciﬁc proper nouns. definition associated probability mass function function satisfying px∈l markov chain extension deﬁnition measure function satisfying measure equalities appendix outcomes ﬁnite countable state space discrete time-homogeneous markov chain every ordered pair states quantity called transition probability state state appendix data sequence typical supj assumption exists measure data sequence typical. measure. since halting algorithms occur inﬁnitely often list halting algorithm list means exists measure data sequence theorem theorem incomparable although tempting think latter corollary former. inﬁnite sequences considered theorem typical computable measure. restricted i.i.d. measures sequences proper subset resulting i.i.d. draws corresponding probability mass function. reason result theorem certain result theorem almost surely. deﬁne time-bounded conditional preﬁx kolmogorov complexity minp{|p| steps}. obtain unconditional versions preﬁx kolmogorov complexities empty word shown incomputable clearly computable moreover every limt→∞ assumption exists measure data sequence typical. measure since halting algorithms occur inﬁnitely often enumeration halting algorithm enumeration therefore exists measure data sequence typical least. algorithm determine", "year": 2017}