{"title": "Episodic Exploration for Deep Deterministic Policies: An Application to  StarCraft Micromanagement Tasks", "tag": ["cs.AI", "cs.LG", "I.2.1; I.2.6"], "abstract": "We consider scenarios from the real-time strategy game StarCraft as new benchmarks for reinforcement learning algorithms. We propose micromanagement tasks, which present the problem of the short-term, low-level control of army members during a battle. From a reinforcement learning point of view, these scenarios are challenging because the state-action space is very large, and because there is no obvious feature representation for the state-action evaluation function. We describe our approach to tackle the micromanagement scenarios with deep neural network controllers from raw state features given by the game engine. In addition, we present a heuristic reinforcement learning algorithm which combines direct exploration in the policy space and backpropagation. This algorithm allows for the collection of traces for learning using deterministic policies, which appears much more efficient than, for example, {\\epsilon}-greedy exploration. Experiments show that with this algorithm, we successfully learn non-trivial strategies for scenarios with armies of up to 15 agents, where both Q-learning and REINFORCE struggle.", "text": "consider scenarios real-time strategy game starcraft benchmarks reinforcement learning algorithms. propose micromanagement tasks present problem short-term low-level control army members battle. reinforcement learning point view scenarios challenging stateaction space large obvious feature representation state-action evaluation function. describe approach tackle micromanagement scenarios deep neural network controllers state features given game engine. addition present heuristic reinforcement learning algorithm combines direct exploration policy space backpropagation. algorithm allows collection traces learning using deterministic policies appears much eﬃcient than example \u0001-greedy exploration. experiments show algorithm successfully learn non-trivial strategies scenarios armies agents q-learning reinforce struggle. starcraft real-time strategy game player must build army control individual units destroy opponent’s army. today starcraft considered diﬃcult games computers best bots reach level high amateur human players. main diﬃculty comes need control large number units wide partially observable environment. implies particular extremely large state action spaces typical game least possible states joint action space θunits) peak number units machine learning point view starcraft provides ideal environment study control multiple agents large scale also opportunity deﬁne tasks increasing diﬃculty micromanagement concerns short-term low-level control ﬁghting units battles long-term strategic hierarchical planning uncertainty. building controller full game based authors contributed equally work. starcraft expansion starcraft brood trademarks blizzard entertainmenttm http//webdocs.cs.ualberta.ca/~cdavid/starcraftaicomp/report.shtmlmvm work atari games recent minecraft scenarios studied researchers focus control single agent ﬁxed limited actions. coherently controlling multiple agents main challenge reinforcement learning micromanagement tasks. comes main diﬃculties. ﬁrst diﬃculty eﬃciently explore large action space. implementation coherent strategy requires units take actions depend other also implies small alteration strategy must maintained suﬃciently long time properly evaluate long-term eﬀect change. contrast requirement consistency exploration reinforcement learning algorithms successful training deep neural network policies q-learning reinforce perform exploration randomizing actions. case micromanagement randomizing actions mainly disorganizes units rapidly lose battle without collecting relevant feedback. second major diﬃculty micromanagement scenarios obvious parameterize policy given state actions actions describe relation entities state e.g. restricted constant symbols move left move right. approach learning directly pixels pixel input multi-class convolutional neural network successful atari games however pixels capture spatial relationships units. parts relationships interest generally kind multi-class architecture cannot evaluate actions parameterized entity state. contribution paper twofold. first propose several micromanagement tasks starcraft describe approach tackle evaluate well known reinforcement learning algorithms tasks q-learning reinforce particular present approach greedy inference break complexity taking actions step also describe features used jointly represent states actions well deep neural network model policy second propose heuristic reinforcement learning algorithm address diﬃculty exploration tasks avoid pitfalls exploration taking randomized actions step algorithm explores directly policy space randomizing small part deep network parameters beginning episode running altered deterministic policy thoughout whole episode. parameter updates performed using heuristic approach combining gradient-free optimization randomized parameters plain backpropagation others. compared algorithms eﬃcient direct exploration parameter space novelty algorithm exploration parameter randomization plain gradient descent. parameter randomization eﬃcient exploration learns slowly large number parameters whereas gradient descent take part exploration rapidly learn models millions parameters. multi-agent reinforcement learning active area research focus learning agents competitive environments adaptive adversaries work looked learning control policies individual agents collaborative setting communication constraints applications soccer robot control methods hierarchical reinforcement learning communicating high-level goals learning eﬃcient communication protocol decentralized control framework likely relevant playing full games starcraft avoid diﬃculty imperfect information therefore multi-agent structure means structure action space. approach reinforcement learning structured output prediction greedy sequential inference scheme time frame unit decides action based solely state combined actions units came sequence. algorithms used train deep neural network controllers reinforcement learning include q-learning method temporal diﬀerences policy gradient variants actor/critic architectures except deterministic policy gradient algorithms rely randomizing actions step exploration. collects traces following deterministic policies remain constant throughout episode applied action space continuous. work closely related works explore parameter space policies rather action space. several approaches proposed randomize parameters policy beginning episode deterministic policy throughout entire episode borrowing ideas gradient-free optimization however algorithms rely gradient-free optimization parameters scale well number parameters. osband describe another type algorithm parameters deterministic policy randomized beginning episode learn posterior distribution parameters thomson sampling algorithm particularly suitable problems depth-ﬁrst search exploration eﬃcient motivation similar ours. approach proved eﬃcient applies linear functions scales quadratically number parameters. bootstrapped deep q-networks practical implementation ideas deep neural networks. however bdqn still performs exploration action space beginning training randomization parameters. instead several versions last layer deep neural network controller maintainted used alternatively entire episode generate diverse traces perform q-learning updates. contrast randomize parameters last layer beginning episode contrarily q-learning algorithm rely estimation state-action value function. context starcraft micromanagement large spectrum approaches studied. work bayesian fusion hand-designed inﬂuence maps fast heuristic search even evolutionary optimization closer work successfully applied tabular q-learning sarsa without experience replay reward similar used several experiments. however action space reduced pre-computed meta-actions ﬁght retreat features hand-crafted. none approaches used existing starcraft bots mainly lack robustness micromanagement scenarios happen full game lack completeness lack computational eﬃciency detailed overview research starcraft reader consult starcraft real-time strategy game actions durative approximately frames second. take action unit every frames consider actions executed time frame move directions holding current position attack action existing enemy units. tasks control units side opponent attacking task control marines opponent marines. good strategy focus whatever means. example attack weakest opponent unit breaking attack closest group. dragoons_zealots symmetric armies types units zealots dragoons strategy requires focus possible spend much time zealots walk instead ﬁght focus dragoons control wraiths opponent flying units collision multiple units occupy tile. anywhere important overkill wraiths points damage frame cooldown. collision moving easier. describe notation deﬁnition underlying algorithms q-learning policy gradient used baselines here. reformulate joint inference potential actions diﬀerent units greedy inference reduces usual states fewer actions state. show normalize cumulative rewards state order keep rewards full interval entire episode even units disappear. preliminaries q-learning reinforce notation environment approximated ﬁnite states denoted state units policy issue command them. commands ﬁnite. action represented sequence pairs u|s|} denotes number units state c)|s| actions state denote transition probability probability distribution initial states. transition state state agent receives reward reward function. assume commands received executed concurrently order commands action alter transition probabilities. finally consider episodic reinforcement learning scenario ﬁnite horizon undiscounted rewards. learner learn policy deﬁnes probability distribution actions every objective maximize expected undiscounted cumulative reward expectation taken respect training usually carried collecting traces t=...t− using \u0001-greedy exploration state stage action argmaxa∈a chosen probability action chosen uniformly random probability practice stationary functions neural networks described section training carried using standard online update rule learning function approximation apply mini-batches training phase distinct test phase record average cumulative reward deterministic policy argmaxa∈a reinforce algorithm reinforce belongs family policy gradient algorithms given stochastic policy parameterized learning carried generating traces t=...t− following current policy. then stochastic gradient updates performed using gradient estimate greedy inference break complexity jointly infering commands individual unit perform greedy inference step state units choose command knowing commands previously taken units. learning greedy policy boils learning policy another fewer actions state exponentially states additional states correspond intermediate steps greedy inference. reduction previously proposed context structured prediction maes proved optimal policy cumulative reward optimal policy original mdp. natural deﬁne associated greedy inference hereafter called greedy deﬁne atomic actions greedy policy possible pairs units whose command still decided. would lead inference quadratic complexity respect number units undesirable. another possibility ﬁrst choose unit command apply unit yields algorithm steps state since commands executed concurrently environment commands decided cumulative reward depend order choose units. going further environment greedy choose next unit instance uniformly random among remaining units. resulting inference complexity linear number units. formally using notation a..k denote ﬁrst pairs action state space greedy deﬁned shown optimal policy greedy chooses actions optimal original immediate reward original depend order actions taken. result applies family policies enough capacity. practice ordering easier learn others investigate issue gain terms computation time random ordering critical experiments. normalized cumulative rewards immediate rewards necessary provide feedback guides exploration. case micromanagement natural reward signal diﬀerence damage inﬂicted incurred states. cumulative reward episode total damage inﬂicted minus total damage incurred along episode. however scale quantity heavily depends number units present state quantity signiﬁcantly decreases along episode. without proper normalization respect number units current state learning artiﬁcially biased towards large immediate rewards beginning episode. present simple method normalize immediate rewards per-state basis assuming scale factor available learner simple number units. then instead considering cumulative rewards starting state deﬁne normalized cumulative rewards ¯nt..t following recursive computation episode features models intended test ability algorithms learn strategies given little prior knowledge possible. voluntarily restrict features extracted state description given game engine without encoding prior knwoledge game dynamics. contrast prior work q-learning micromanagement features expected inﬂicted damage. allow encode eﬀect attack action points attacked unit; either construct cross-features provide relevant discretization features transformation features perform computation distances units targets commands. represent state sequence feature vectors feature vector unit state. remind state greedy tuple action corresponds command shall execute. frame unit commands consider attack given enemy unit move speciﬁc position. order reduce number possible move commands consider move commands either correspond move basic directions staying position. attack commands non-trivial featurize model needs able solve reference identiﬁers units attack attacked corresponding attributes. order solve issue construct joint state/action feature representation unit positions indirectly used refer units. detail feature representation neural network model. unit attributes unit type coordinates remaining number points shield corresponds additional points recovered unit attacked ﬁnally weapon cooldown additional enemy used distinguish units enemy units. attributes describe command currently executed unit. first target attribute which empty identiﬁer enemy unit currently attack. identiﬁer integer attributed arbitrarily game engine convey semantics. encode directly identiﬁer model rather position unit second attribute target_pos gives coordinates position target ﬁelds available ally enemy units; infer current command cur_cmd unit currently performs. order assign score tuple candidate command joint representation deﬁned sequence feature vectors unit feature vector unit denoted joint representation together next command next_cmd already decided command evaluated uk+. commands ﬁeld act_type ﬁeld target_pos. want featurize command available given unit next command enemy unit act_type speciﬁc command value target_pos unit position. given vector contains features described below. non-positional features u.enemy u.type u.hp u.shield u.cd u.cur_cmd.act_type u.next_cmd.act_type .type. stage encode type command c.act_type another input network relative distance features {u.pos u.cur_cmd.target_pos u.next_cmd.target_pos} {uk+.pos uk+.cur_cmd.target_pos c.target_pos}. features particular encode unit distance positions also encode unit target command units target. encoding unambiguous long units cannot position true ﬂying units practice however confusion units seem major issue since units rarely exactly position. finally full tuple greedy represented matrix j-th model describe below deals variable-size input global pooling operations. deep neural network model shall section consider state-action scoring functions form argmaxc∈cw vector ψθc) deep network parameters embeds state command greedy embedding network takes input matrix describe below cross featurization pooling step goes -layer neural network layer width nonlinearity ﬁrst layer hyperbolic tangeants ﬁnal activation functions. resulting matrix aggreated diﬀerent vectors size ﬁrst taking mean value column second taking maximum vectors concatenated yield -dimensional vector next step. note ﬁnal ﬁxed-length representation invariant ordering rows original matrix. scoring respect action type -dimensional vector concatenated type action c.act_type concatenation goes -layer network activation units layer. ﬁrst non-linearity second rectiﬁer linear unit rationale behind model represent answer variety question regarding relationship candidate command state type unit command’s target? many damages shall inﬂicted? many units already target? many units attacking uk+? order answer questions learner must perform appropriate crossfeatures paramter updtaes reinforcement signal alone learning task non-trivial even fairly simple strategies. present algorithm exploring deterministic policies discrete action spaces based policies parameterized deep neural network. algorithm inspired ﬁnitediﬀerence methods stochastic gradient-free optimization well exploration strategies parameter space algorithm viewed heuristic. present within general formulation section simplicity although experiments apply greedy section described section consider case pairs embedded parametric function deterministic policy parameterized additional vector action taken state deﬁned overall algorithm described algorithm order explore policy space consistent manner episode uniformly sample vector unit sphere policy πw+δuθ whole episode hyper-parameter. expectation taken vector sampled unit sphere constant absorbed learning rates ignore following. thus given pair observed cumulative reward estimator gradient expected cumulative reward respect motivation update network parameters following given function v)w. denoting term-by-term division vectors term-by-term multiplication operator obtain update algorithm corresponds taking above using estimated gradient cumulative reward respect before. since need make sure ratios avoid numerical issues. estimated gradient backpropagated network. preliminary experiments suggested taking sign eﬀective e.g. clipping simpler since parameter heuristic experiments. reasoning partial justiﬁcation update rule algorithm neglected dependency parameters argmax operation chooses actions. nonetheless considering crude approximation real estimator gradient seems work well practice shall experiments. finally adagrad update parameters diﬀerent layers. found adagrad’s update scheme fairly important practice compared approaches rmsprop even though rmsprop tended work slightly better q-learning reinforce experiments. setup torch experiments. connect torch code models starcraft socket server. experiments deep networks policy gradient zero order extensive hyper-parameters search particular learning rates optimization methods algorithms variants potential annealings. appendix details. baseline heuristics results report built-in compare rates ones baseline heuristics. heuristics often perform micromanagement full-ﬂedged starcraft bots basis heuristic search baselines following noop literally send action something forbidden models case built-in control units exhibit symmetry given scenario. always defensive position enemy commanded walk towards things considered equal easier defending built-in attacking one. closest units targets enemy unit closest heuristic enemy units formation always make several units opponent unit closest unit also quite robust melee units means spend less time moving time attacking. overkill change weakest closest heuristic register number units target opponent unit choosing another target focus becomes overkill keep targeting given unit. units keep ﬁring target without changing note results ﬁrst thing looked sliding average rates training built-in various models. figure much dependent initialization variable zero order unlearn reach suboptimal plateaux overall need exploration start learning results present tables models test mode making deterministic. remove epsilon-greedy exploration sample gibbs policy instead take value-maximizing action noise last layer. table advantage player’s side whereas hard looking results heuristics overkill problem attack closest approximatively good nok_nc spreading damage thus better lots collisions overall zero order optimization outperforms maps. perform well seems easier learn focus ﬁring heuristic identifying locking feature also learn overkill. studied well model trained previous maps performs maps diﬀerent number units test generalization. table contains results experiment. observe performs best trained learned simpler heuristic. noop attack closest quite good large marines generate less moves overall consistently signiﬁcantly better algorithms generalization tasks even though reach optimal strategy. table rates games out-of-training-domain maps methods. method trained indicated left. best result bold best result reinforcement learning methods italics. visually inspected model’s performance large battles. larger marines learned focus ﬁre. many units focus ﬁring leads units bumping focus single unit. player seemed policy attacks closest marine though doesn’t good switching targets. marines range often bump other. zero order optimization learns hybrid focus ﬁring attacking closest unit. units would switch units range possible still focus speciﬁc targets. leads marines attacking constantly well focus ﬁring can. however learned strategy perfected since marines would still split occasionally left units. wraiths player’s strategy hard decipher. likely explanation tried attack closest target though likely algorithm converge speciﬁc strategy. player learned focus ﬁre. however takes wraiths kill another actions \"wasted\" zero order player learns focusing enemy good learn many attacks necessary. leads much higher rate player still assigns wraiths enemy target occasionally focus wraiths remaining. similar zero order player learned marines scenario. paper presents main contributions. first establishes starcraft micromanagement scenarios complex benchmarks reinforcement learning durative actions delayed rewards large action spaces making random exploration infeasible. second introduces reinforcement learning algorithm performs better prior work discrete action spaces micromanagement scenarios robust training episodically consistent exploration work leaves several doors open calls future work. simpler embedding models state actions variants model presented here tried none produced eﬃcient units movement ongoing work convolutional networks based models conserve geometry game zero order optimization technique presented studied depth empirically evaluated domains starcraft starcraft scenarios speciﬁcally subsequent experiments include self-play multi-map training complex scenarios include several types advanced units actions move attack. finally goal playing full games starcraft lost future scenarios would also include actions recruiting units well make them. thank y-lan boureau antoine bordes florent perronnin dave churchill léon bottou alexander miller helpful discussions feedback work earlier versions paper. thank timothée lacroix alex auvolat technical contributions starcraft/torch bridge. thank davide cavalca support windows virtual machines cluster environment. references abel agarwal diaz krishnamurthy schapire exploratory gradient boosting reinforcement learning complex domains. arxiv preprint arxiv. busoniu babuska schutter comprehensive survey multiagent reinforcement learning. ieee transactions systems cybernetics-part applications reviews gelly wang exploration exploitation monte-carlo nips neural information processing systems conference on-line trading exploration exploitation workshop louis ballinger evolving eﬀective micro behaviors game. computational intelligence games ieee conference ieee maes denoyer gallinari structured prediction reinforcement learning. ontanón synnaeve uriarte richoux churchill preuss survey real-time strategy game research competition starcraft. computational intelligence games ieee transactions sehnke osendorfer rückstieß graves peters schmidhuber policy gradients parameter-based exploration control. artiﬁcial neural networksicann springer silver huang maddison guez sifre driessche schrittwieser antonoglou panneershelvam lanctot mastering game deep neural networks tree search. nature watkins dayan q-learning. machine learning wender watson applying reinforcement learning small scale combat real-time strategy game starcraft broodwar. computational intelligence games ieee conference ieee starcraft speciﬁcs advocate using existing video games experiments interesting simulators oftentimes complex control source code simulator. games like starcraft access simulator cannot tree search directly even less setting full games paper consider problem micromanagement scenarios subset full play. micromanagement making good given units game. units diﬀerent features like range cooldown points attack power move speed collision etc. numerous features dynamics game advantage player take right actions right times. speciﬁcally game starcraft professional players good competitive players professional players perform actions minute intense battles. experiments simple scenarios battles game starcraft broodwar. scenarios considered small scale starcraft already deem challenging existing approaches. example scenario units enemy units even reducing action space \"atomic\" actions obtain possible discrete actions unit controller choose beginning battle. battles last tens seconds durative actions simultaneous moves frames second. strategies need learn consist coordinated sets actions need repeated e.g. focus ﬁring without overkill. featurization gives access state game pre-process state make easier learn given strategy thus keeping problem elegant unbiased. tasks number units agent consider changes episode number actions. fact playing speciﬁc adversarial environment units follow coherent strategy suﬃcient amount time suﬀer unrecoverable loss game state game units rapidly make little damage independently play state mostly useless learning. tasks represent battles homogeneous types units little diversity instance unit type marine soldier points average move speed average range frames cooldown attack power normal damage type symmetric and/or monotyped maps strategies required focus ﬁring without overkill perfect rates maps require moves units focus ﬁring opponent. units moving. take actions units synchronously frame even skip_frames frames. tried several values hyper-parameter smooth changes performance. following experiments skip_frames also report strongest numbers baselines skip frames. optimize models battle rmsprop except zero-order optimized adagrad case learning rate chosen among methods tried experience replay either episodes batches additionally random batches quintuplets case q-learning seem help compared batching last battle. consistency present results training batches consisted last episode \u0001a.t respectively optimization batch target/double network used optimizations thus battles following experiments. according initial runs/sweep seems slightly help cases over-estimation value.", "year": 2016}