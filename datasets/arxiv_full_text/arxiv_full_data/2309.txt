{"title": "Understanding the Loss Surface of Neural Networks for Binary  Classification", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "It is widely conjectured that the reason that training algorithms for neural networks are successful because all local minima lead to similar performance, for example, see (LeCun et al., 2015, Choromanska et al., 2015, Dauphin et al., 2014). Performance is typically measured in terms of two metrics: training performance and generalization performance. Here we focus on the training performance of single-layered neural networks for binary classification, and provide conditions under which the training error is zero at all local minima of a smooth hinge loss function. Our conditions are roughly in the following form: the neurons have to be strictly convex and the surrogate loss function should be a smooth version of hinge loss. We also provide counterexamples to show that when the loss function is replaced with quadratic loss or logistic loss, the result may not hold.", "text": "widely conjectured reason training algorithms neural networks successful local minima lead similar performance; example performance typically measured terms metrics training performance generalization performance. focus training performance neural networks binary classiﬁcation provide conditions training error zero local minima appropriately chosen surrogate loss functions. conditions roughly following form neurons increasing strictly convex neural network either single-layered multi-layered shortcut-like connection surrogate loss function smooth version hinge loss. also provide counterexamples show that conditions relaxed result hold. local search algorithms like stochastic gradient descent variants gained huge success training deep neural networks example). despite spurious saddle points local minima loss surface widely conjectured local minima empirical loss lead similar training performance example empirically showed neural networks identical architectures diﬀerent initialization points converge local minima similar classiﬁcation performance. however still remains challenge characterize theoretical properties loss surface neural networks. setting regression problems theoretical justiﬁcations established support conjecture local minima lead similar training performance. shallow models provide conditions local search algorithms guaranteed converge globally optimal solution regression problem. deep linear networks shown every local minimum empirical loss global minimum order characterize loss surface general deep networks regression tasks proposed interesting approach. based certain constructions network models additional assumptions relate loss function spin glass model show almost local minima similar empirical loss number local minima decreases quickly distance global optimum. despite interesting results remains concern properly justify assumptions. recently shown that dataset satisﬁes certain conditions layer multilayer network neurons number training samples subset local minima global minima. although loss surfaces regression tasks well studied theoretical understanding loss surfaces classiﬁcation tasks still limited. treat classiﬁcation problem regression problem using quadratic loss show local minima global minima. however global minimum quadratic loss necessarily zero misclassiﬁcation error even simplest cases issue mentioned diﬀerent loss function used result studied linearly separable case subset critical points. siﬁcation error local minima overparameterizing neural network. reason over-parameterization quadratic loss function tries match output neural network label training sample. labels. possible achieve zero misclassiﬁcation error without over-parametrization. provide conditions misclassiﬁcation error neural networks zero local minima hinge-loss functions. convex neural network either single-layered multi-layered shortcut-like connection surrogate loss function smooth version hinge loss function. positively negatively labeled samples located diﬀerent subspaces. whether assumption necessary open problem except case certain special neurons. outline paper follows. section present necessary deﬁnitions. section present main results discuss condition section conclusions presented section proofs provided appendix. denote neuron activation function rml−×ml denote weight matrix connecting layer l-th layer denote bias vector neurons l-th layer. therefore output network expressed data distribution. paper consider binary classiﬁcation tasks sample drawn underlying data distribution px×y deﬁned sample considered positive negative otherwise. denote orthonormal basis space denote subsets positive negative samples located linear span respectively i.e. px|y px|y denote size denote size denote size respectively. loss error. denote dataset samples independently drawn distribution px×y given neural network parameterized loss function binary classiﬁcation tasks deﬁne empirical loss average loss network sample dataset i.e. furthermore neural network deﬁne binary classiﬁer form sign function otherwise. deﬁne training error misclassiﬁcation rate neural network dataset i.e. section present main results. ﬁrst introduce several important conditions order derive main results provide discussions conditions next section. fully specify problem need specify assumptions several components model including loss function data distribution network architecture neuron activation function. assumption denote loss function satisfying following conditions surrogate loss function i.e. denotes indicator function; continuous derivatives order non-decreasing ﬁrst condition assumption ensures training error always upper bounded empirical loss i.e. ˆln. guarantees neural network correctly classify samples dataset neural network achieves zero empirical loss second condition ensures empirical loss continuous derivatives respect parameters suﬃciently high order. third condition ensures loss assumption states support conditional distribution px|y suﬃciently rich samples drawn linearly independent. words stating assumption avoiding trivial cases positively labeled points located small subset assumption assumes positive negative samples located linear subspace. previous works observed classes natural images reconstructed lower-dimensional representations. example using dimensionality reduction methods approximately reconstruct original image small number principal components here assumption states positively negatively labeled samples lower-dimensional representations exist lower-dimensional subspace. provide additional analysis section showing main results generalize data distributions. assumption assume neural network single-layered neural network generally shortcut-like connections shown single layer network feedforward network. shortcut connections widely used modern network architectures resnet densenet etc.) skip connections allow deep layers direct access outputs shallow layers. instance residual network residual block identity shortcut connection shown output residual block vector input output network vector denotes weight vector matrix denotes weight matrix vector denotes vector containing parameters next output network network addition output whole network i.e. vector denote vector containing parameters network whole network respectively. note that paper restrict number layers neurons network means network feedforward network introduced section single layer network even constant. fact network single layer network constant whole network becomes single layer network. furthermore note that section show remove connection replace shortcut-like connection identity shortcut connection main result hold. assumption assume neurons network inﬁnitely diﬀerentiable positive second order derivatives neurons network real functions. make assumptions ensure loss function partially diﬀerentiable w.r.t. parameters network suﬃciently high order allow taylor expansion analysis. here list neurons used network softplus neuron i.e. quadratic neuron etc. note neurons network need type means general class neurons used network e.g. threshold neuron i.e. rectiﬁed linear unit max{z sigmoid neuron +e−z etc. discussion eﬀects neurons main results provided section present following theorem show assumptions satisﬁed every local minimum empirical loss function zero training error number neurons network chosen appropriately. remark setting network constant directly follows theorem single layer network consisting neurons satisfying assumption conditions theorem satisﬁed every local minimum empirical loss zero training error. positiveness guaranteed assumption worst case number neurons needs least greater number samples i.e. proposition assume assumptions satisﬁed. assume samples dataset independently drawn distribution px×y assume neurons network satisfy number neurons network satisﬁes remark proposition shows number neuron greater dimension subspace i.e. every local minimum empirical loss function zero training error. note although result stronger quadratic neurons imply quadratic neuron advantages types neurons fact neuron positive derivatives result theorem holds dataset positive negative samples linearly separable. provide formal statement result theorem however neuron quadratic activation function result theorem hold linearly separable dataset illustrate providing counterexample next section. shown theorem data distribution satisﬁes assumption every local minimum empirical loss zero training error. however easily distributions satisfying assumptions linearly separable. therefore provide complementary result theorem consider case data distribution linearly separable. presenting result ﬁrst present following assumption data distribution. theorem show samples drawn data distribution linearly separable network shortcut-like connection shown figure local minima empirical loss function zero training errors type neuron network chosen appropriately. theorem suppose loss function satisﬁes assumption network architecture satisﬁes assumption assume samples dataset independently drawn distribution satisfying assumption assume single layer network neurons neurons network twice diﬀerentiable satisfy local minimum loss function ˆrn. provide discussions eﬀects neurons next section. provided results showing certain constraints neuron activation function network architecture loss function data distribution every local minimum empirical loss function zero training error. next section discuss implications conditions main results. section discuss eﬀects neuron activation shortcut-like connections loss function data distribution main results respectively. show result hold assumptions relaxed. begin with discuss whether results theorem still hold vary neuron activation function single layer network speciﬁcally consider following classes figure five types neuron activations including softplus neuron relu leaky-relu sigmoid neuron quadratic neuron. four types surrogate loss functions including binary loss neurons softplus class rectiﬁed linear unit class leaky rectiﬁed linear unit class quadratic class sigmoid class. following class neurons show whether main results hold provide counterexamples certain conditions main results violated. summarize ﬁndings table visualize neurons activation functions classes fig. relu class contains neurons piece-wise continuous commonly adopted neurons class include threshold units i.e. rectiﬁed linear units i.e. max{z rectiﬁed quadratic units i.e. neurons class satisfy neither assumptions theorem proposition show single layer network consists neurons relu class even conditions theorem satisﬁed empirical loss function local minimum non-zero training error. satisfy piece-wise continuous exists network architecture distribution satisfying assumptions theorem probability empirical loss local minima satisfying min{n+n−} number positive negative samples respectively. remark note result holds over-parametrized case number neurons network larger number samples dataset. addition counterexamples shown section hold over-parametrized case. note applying analysis generalize result larger class neurons satisfying following condition exists scalar constant piece-wise continuous note training error strictly non-zero dataset positive negative samples happen probability least e−ω. leaky-relu class contains neurons piece-wise continuous commonly used neurons class include relu i.e. max{z leaky rectiﬁed linear unit i.e. constant exponential linear unit i.e. constant neurons class satisfy assumptions theorem neurons class satisfy condition theorem neurons proposition provided counterexample showing theorem hold neurons class next present following proposition show network consists neurons leaky-relu class even conditions theorem satisﬁed empirical loss function likely local minimum non-zero training error high probability. satisfy piece-wise continuous exists network architecture distribution satisfying assumptions theorem that probability least empirical loss local minima sigmoid class contains neurons constant list commonly adopted neurons family sigmoid neuron i.e. +e−z hyperbolic tangent neuron i.e. +|z| note real functions satisfy conditions sigmoid class. none neurons satisfy assumptions theorem since neurons class satisfy either showing theorem hold neurons class next present following proposition showing network consists neurons sigmoid class always exists data distribution satisfying assumptions theorem that positive probability empirical loss local minima non-zero training error. proposition suppose assumptions satisfed. assume exists constant neurons network satisfy assume dataset samples. exists network architecture distribution satisfying assumptions theorem that positive probability empirical loss function local minimum real analytic strongly convex global minimum point simple example neuron family quadratic neuron i.e. easy check neurons class satisfy conditions theorem theorem theorem present counterexample show that network consists neurons quadratic class even positive negative samples linearly separable empirical loss local minimum non-zero training error. proposition suppose assumption satisﬁed. assume neurons satisfy strongly convex twice diﬀerentiable global minimum exists network architecture distribution satisfying assumptions theorem satisfying denote number positive negative samples subsection discuss whether main results still hold remove shortcut-like connections replace identity shortcut connections used residual network specifically provide counterexamples show main results hold shortcut-like connections removed replaced identity shortcut connections. feed-forward networks. shortcut-like connections removed network architecture viewed standard feedforward neural network. provide counterexample show that feedforward network relu neurons even conditions theorem satisﬁed empirical loss functions likely local minimum non-zero training error. words neither theorem holds shortcut-like connections removed. identity shortcut connections. stated earlier adding shortcut-like connections network improve loss surface. however shortcut-like connections shown diﬀerent popular shortcut connections used real-world applications e.g. identity shortcut connections residual network. thus natural question arises main results still hold identity shortcut connections? address question provide following counterexample show that replace shortcut-like connections identity shortcut connections even conditions theorem satisﬁed empirical loss function likely local minimum non-zero training error. words theorem hold identity shortcut connections. proposition assume feedforward neural network parameterized neurons relus. deﬁne network identity shortcut connections exists distribution px×y satisfying assumptions theorem probability least empirical loss subsection discuss whether main results still hold change loss function. mainly focus following types surrogate loss functions quadratic loss logistic loss. show loss function replaced quadratic loss logistic loss neither theorem holds. addition show loss function logistic loss network feedforward neural network local minima zero training error real parameter space. fig. visualize surrogate loss functions discussed subsection. quadratic loss. quadratic loss well-studied prior works. shown loss function quadratic certain assumptions local minima empirical loss global minima. however global minimum quadratic loss necessarily zero misclassiﬁcation error even realizable case illustrate this provide simple example network simpliﬁed linear network data distribution linearly separable. example distribution px×y satisfy px|y uniform distribution interval linear model ax+b every global minimum population loss ex×y satisﬁes px×y sgn)] remark proof result appendix straightforward. provided since unable reference explicitly states result surprised result known others. example shows every global minimum quadratic loss non-zero misclassiﬁcation error although linear model able achieve zero misclassiﬁcation error data distribution. similarly easily datasets global minima quadratic loss non-zero training error. addition provide examples appendix show that loss function replaced quadratic loss even conditions theorem satisﬁed every global minimum empirical loss training error larger positive probability. words main results hold quadratic loss. following observation independent interest. diﬀerent quadratic loss loss functions conditioned assumption following properties minimum empirical loss zero exists parameters achieving zero training error; every global minimum empirical loss zero training error realizable case. proposition denote feedforward network parameterized dataset samples. loss function satisﬁes assumption minθ minθ furthermore minθ every global minimum empirical loss zero training error i.e. remark note network need feedforward network. fact results hold large class network architectures including architectures shown provide additional analysis appendix logistic loss. logistic loss diﬀerent loss functions conditioned assumption since logistic loss global minimum here logistic loss function show even remaining assumptions theorem hold every critical point saddle point. words theorem hold logistic loss. additional analysis theorem provided appendix proposition assume loss function logistic loss i.e. log. assume assumptions satisﬁed. assume samples dataset independently drawn distribution px×y assume number neurons max{r+ r−}. denotes critical network satisﬁes max{ point empirical loss saddle point. particular local minima. remark note result generalized every loss function real analytic positive derivative furthermore provide following result show dataset contains positive negative samples loss logistic loss every critical point empirical loss function non-zero training error. proposition assume dataset consists positive negative samples. assume feedforward network parameterized assume loss function logistic i.e. real parameters denote critical point empirical loss remark provide proof appendix proposition implies every critical point either local minimum non-zero training error saddle point note that similar proposition result generalized every loss function diﬀerentiable positive derivative paper mainly considered class non-linearly separable distribution positive negative samples located diﬀerent subspaces. show samples drawn distribution certain additional conditions local minima empirical loss zero training errors. however well result generalize non-linearly separable distributions datasets? here partially answer question presenting following necessary condition dataset theorem hold. remark proposition implies dataset meet necessary condition exists feedforward architecture empirical loss function local minimum non-zero training error. implication prove counterexamples provided appendix assumption dataset satisﬁed. therefore theorem longer holds assumption removed. note necessary condition shown equivalent assumption present following result show suﬃcient necessary condition dataset satisfy proposition hold. proposition suppose loss function satisﬁes assumption neurons network satisfy assumption assume single layer network neurons assume neurons quadratic neurons i.e. network architecture every local minimum empirical loss function satremark suﬃcient necessary condition implies network architecture exists parameters network correctly classify samples dataset. also indicates existence parameters achieving zero training error regardless network architecture provide proof appendix note proposition holds quadratic neuron. problem ﬁnding suﬃcient necessary conditions types neurons open. paper studied surface smooth version hinge loss function binary classiﬁcation problems. provided conditions neural network zero misclassiﬁcation error local minima also provide counterexamples show assumptions relaxed result hold. work involves exploiting results design eﬃcient training algorithms classiﬁcation tasks using neural networks.", "year": 2018}