{"title": "Agnostic System Identification for Model-Based Reinforcement Learning", "tag": ["cs.LG", "cs.AI", "cs.SY", "stat.ML"], "abstract": "A fundamental problem in control is to learn a model of a system from observations that is useful for controller synthesis. To provide good performance guarantees, existing methods must assume that the real system is in the class of models considered during learning. We present an iterative method with strong guarantees even in the agnostic case where the system is not in the class. In particular, we show that any no-regret online learning algorithm can be used to obtain a near-optimal policy, provided some model achieves low training error and access to a good exploration distribution. Our approach applies to both discrete and continuous domains. We demonstrate its efficacy and scalability on a challenging helicopter domain from the literature.", "text": "figure example train-test mismatch helicopter domain. train model based samples near desired trajectory e.g. watching expert. test learned policy ends regions model leading poor control performance. problem fully appreciated system identiﬁcation literature attacked considering open loop identiﬁcation procedures persistent excitation attempt sufﬁciently cover state-action space. unfortunately methods rely strong assumption true system lies class models considered e.g. continuous systems require true system modeled class linear models. assumption ensure eventually correct model learned– e.g. learning every discrete state-action pair modes linear system– provide guarantees. work provide algorithms system identiﬁcation controller synthesis strong performance guarantees weaker agnostic assumption system identiﬁcation achieves statistically good prediction. adopt reduction-based analysis relates learned policy’s performance prediction error training. begin providing agnostic bounds simple generic batch algorithm represent many learning methods used practice much control rely system identiﬁcation building model system observations useful controller synthesis. often treated typical statistical learning problem system identiﬁcation presents different fundamental challenges executed controller data generating process inextricably intertwined. naively attempting estimate controlled system lead model makes small error training exhibits poor controller performance. problem arises policy resulting controller synthesis often different exploration policy used collect data. might expect model make good predictions states frequented exploration policy learned watching expert running base policy want improve upon). mismatch train/test distributions uniform exploration often best option approach. unfortunately makes sample complexity performance bounds scale size markov decision process next propose simple iterative approach closely related online learning stronger guarantees scale size given good exploration distribution. approach simple implement iterates collecting data system executing good policy current model well sampling given exploration distribution updating model data. approach inspired recent reduction imitation learning no-regret online learning addresses mismatch train/test distributions. results interpreted reduction mbrl noregret online learning optimal control show no-regret algorithm used learn policy strong agnostic guarantees. enables mbrl methods match strongest existing agnostic guarantees model-free methods ﬁrst introduce notation related work. present batch method online learning approach agnostic guarantees finally demonstrate efﬁcacy approach challenging domain literature learning perform aerobatic maneuvers simulated helicopter assume real system behaves according unknown represented states actions transition function denotes next state distribution action state initial state distribution time assume cost function known seek minimize expected discounted costs inﬁnite horizon discount policy action distribution performed state state-action distribution time started state distribution time followed state-action distribution inﬁnite horizon follow starting time ea∼πss∼tsa γvπ)] value function γes∼tsa action-value function small. achieved indirectly learning model system solving optimal policy e.g. using dynamic programming approximate methods continuous systems important special case linear models quadratic cost functions potentially additive gaussian noise known linear quadratic regulators solved exactly efﬁciently. non-linear systems non-quadratic cost functions also solved approximately using efﬁcient iterative linearization techniques ilqr. related work contrast textbook system identiﬁcation methods practice control engineers often proceed iteratively build good models controller synthesis. ﬁrst batch data collected model obtain controller tested real system. performance unsatisfactory data collection repeated different sampling distributions improve model needed control performance satisfactory. engineers feedback policies found training decide collect data improve performance. methods commonly used practice demonstrated good performance work atkeson schaal abbeel works authors proceed ﬁtting ﬁrst model state transitions observed expert demonstrations task following iterations using optimal policy current model collect data model data seen far. abbeel show approach good guarantees non-agnostic settings must policy performs well expert providing initial demonstrations. method seen making algorithmic engineering practice extending generalizing previous methods atkeson schaal abbeel suggesting slight modiﬁcations provide good guarantees even agnostic settings. similarly dataset aggregation algorithm ross uses similar data aggregation procedure iterations obtain policies mimic expert well imitation learning. authors show deﬁned matrices abqr s.t. state action time gaussian white noise cost optimal policy linear value function quadratic solved dynamic programming procedure interpreted online learning algorithm speciﬁcally follow-the--leader using no-regret online algorithm ensures good performance. approach seen extension dagger mbrl settings. approach leverages agnostic model-free algorithms perform exploration. methods conservative policy iteration policy-search dynamic programming learn policy directly updating policy parameters iteratively. exploration assume access state exploration distribution restart system guarantee ﬁnding policy performing nearly well policies inducing state distribution close similarly approach uses state-action exploration distribution sample transitions allows guarantee small regret policy state-action distribution close exploration distribution. exploration distribution close near-optimal policy approach guarantees near-optimal performance provided good model data exists. allows model-based method match strongest agnostic guarantees existing model-free methods. good exploration distributions often obtained practice; e.g. human expert demonstrations domain knowledge desired trajectory would like system follow. additionally base policy want improve used generate exploration distribution potentially additional random exploration actions. describe simple algorithm refered batch used analyze many common approaches literature e.g. learning generative model open loop excitation watching expert denote class transition models considered state-action exploration distribution sample system from. batch ﬁrst executes real system state-action pairs sampled i.i.d. obtain sampled transitions. ﬁnds best model observed transitions solves optimal control problem known cost function return policy test execution. data solves problem well guarantees provide control performance results illustrate drawbacks purely batch method mismatch train-test distribution. measure quality problem’s solution follows. policy denote much better compared model value functions learned model respectively). \u0001-optimal poloc within class policies natural measure model error arises analysis terms distance predicted true next state’s distributions. predictive deﬁne error measured distance training distribution however distance cannot evaluated optimized sampled transitions training therefore also provide bounds terms losses minimize samples. directly relates control performance model’s training loss. convenient loss divergence ˆtsa e∼νs∼tsa log))]. mini\u0001kl mizing corresponds maximizing likelihood sampled transitions. convenient common model classes linear models amounts linear regression. particular cases deterministic models real system ﬁnitely many states predictive error measured classiﬁcation loss predicting next state e∼νs∼tsa loss \u0001cls whether predicts upper bound loss e.g. multi-class hinge loss svms. case model ﬁtting supervised classiﬁcation problem guarantee directly related training classiﬁcation loss. related follows general loss minimizable samples upper bounds models class. bounds also related mismatch exploration distribution distribution induced executing andµπ policy starting denoted assume costs crng cmax cmin γcrng scaling factor relates model error error total cost predictions. theorem policy s.t. policy data collected. minimize supπ∈π best batch often uniform distribution possible. introduces dependency number states actions multiplying modeling error. sampling uniform distribution often requires access generative model. access reset model base policy inducing executed system could arbitrarily large arbitrarily worse next section show iterative learning methods leverage feedback learned policies obtain bounds depend leads better guarantees good exploration distribution collect data reset model. also leads better performance practice shown experiments. extension dagger mbrl setting proceeds follows. starting initial model solve problem obtain policy iteration collect data system sampling state-action pairs distribution sample transition occurring exploratory state-action pair drawn dataset otherwise sample state transition occurring running current policy starting stopping trajectory w.p. step adding last transition dataset contains transitions observed iterations. data collected best model minimizes appropriate loss solve problem obtain next policy πn+. iterated iterations. test time could either policy lowest expected total cost sequence uniform mixture policy guarantee good performance both. last policy often performs equally well trained data. experimental results conﬁrm intuition. theory good guarantees distributions dµπi converge small region space distributions guarantee always occurs. implementation off-the-shelf online learner dagger described interpreted using followthe--leader online algorithm pick sequence models iteration pick bound indicates batch solves problem well small enough error training distribution must good policy. importantly bound tight i.e. construct examples holds equality interestingly happens collect data. ﬁtting procedure consistent relate guarantee capacity model class achieve error training distribution denote modeling error measured distance sa||]. similarly deﬁne e∼νs∼tsa sa))] e∼νs∼tsa s)]. \u0001cls realizable settings generally non-zero agnostic settings. sampling transitions generalizagen bounds high probability tion error quantity denote generalization error classiﬁcation loss respectively. \u0001cls related dimension ﬁnite mdps. corollary observing transitions probability least policy generalization error typically scales complexity class goes rate ideal conditions). given enough samples dominating factor limiting performance becomes modeling error mdl) quantiﬁes performance degrades agnostic settings. drawback batch factors qualitatively different. measures well explores stateactions visited policy compare factor inevitable cannot hope compete policies spend time rarely explore. measures mismatch train-test distribution. presence major drawback batch. cannot considering known advance bound policies could learn supπ∈π worst case likely realized practice rarely explores state-action regions model could signiﬁcantly underestimate cost. learned policy thus encouraged visit low-cost regions solve problem well guarantees dagger provide control performance? results show sampling data learned policies dagger provides guarantees train-test mismatch factor leading improved performance. es∼µ policy deﬁne respectively value function model measures well solved problem average iterations. instance iteration found \u0001i-optimal policy within class policies learned model batch av\u0001π erage predictive error models measured terms distance predicted true next state distribution tsa||]. however discussed distance observed samples makes hard minimize. instead deﬁne measures upper bounds distance minimized samples i.e. divergence classiﬁcation loss sa))] e∼ρis∼tsa \u0001cls given argminπ∈πn best policy sequence uniform mixture policy sequence. lemma policies s.t. policy note \u0001cls using no-regret algorithm sequence losses average reinf gret algorithm iterations s.t. relates modeling error class e∼ρs∼tsa similarly e∼ρs∼tsa deﬁne \u0001cls \u0001cls \u0001cls using no-regret algorithm lcls cases even distance cannot \u0001cls estimated samples statistical estimators still no-regret high probability sequence loss e∼ρi sa||]. case nite mdps empirical estimator based data seen deﬁne sa||] implies theorem policies s.t. policy best model hindsight samples seen far. general dagger also implemented using no-regret online algorithm provide good guarantees. done follows. minimizing negative likelihood loss function online learning problem iteration e∼ρis∼tsa ))]. estimated sampled state transitions iteration evaluated model online algorithm applied obtain sequence models quence loss iterations. before model solved obtain next policy online algorithm effectively runs mini-batches data collected iteration update model mini-batch comes different distribution changes update policy. similarly ﬁnite deterministic model class minimize loss instead loss e∼ρis∼tsa iteration lcls particular classiﬁcation loss. corresponds online classiﬁcation problem. many model classes negative likelihood convex upper bounds loss lead convex online learning problems no-regret algorithms exist shown below sequence models no-regret performance related minimum divergence achievable model class overall training distribution batch). input exploration distribution number iterations number samples iteration cost function online learning procedure onlinelearner optimal control procedure ocsolver. initial guess model onlinelearner. ocsolver. near-optimal policy improves upon state-of-the-art mbrl algorithms rmax recent modiﬁcation rmax here dependency |s||a| complexity class simpler classes dependency size mdp. supplementary material analyze scenario kernel rkhs norm bounded guark. choosing ˆ\u0001cls multi-class hinge loss training iterations best hindsight. thus good exploration distribution exists good model predicting observed data obtain nearoptimal policy sample complexity depends complexity size mdp. emphasize provide reduction-style guarantees. dagger sometimes fail good policies e.g. model class achieves error training data. however dagger guarantees following occur either good policies models error aggregate dataset exist. latter occurs need better model class. contrast batch models training error still fail obtaining policy good control performance train/test mismatch. occurs even scenarios dagger ﬁnds good policies shown experiments. dagger needs solve many problems. computationally expensive e.g. non-linear highdimensional models. many approximate methods used e.g. policy gradient ﬁtted value iteration ilqr models often change slightly iteration next often iterations dynamic programming/policy gradient last value function/policy obtain good policy current model. long good solutions average remains small hinder performance. dagger generalizes approach atkeson schaal abbeel noregret algorithm update model well exploration distribution. difference dagger keeps even balance exploration data data running learned policies. crucial avoid setthm. illustrates reduce original mbrl problem no-regret online learning problem particular sequence loss functions. general no-regret algorithms average regret ideal cases) regret term goes similar rate generalization error term batch cor. here given enough iterations term determines performance degrades agnostic setting no-regret algorithm sequence classiﬁcation loss respectively). unlike batch dependence thus error model exists training distribution no-regret methods guaranteed learn policies performs well compared policy small. hence ideally near-optimal policy finite sample analysis remaining issue current guarantees apply evaluate expected loss exactly. requires inﬁnite samples iteration. no-regret algorithm estimates loss functions i.e. loss sampled transitions still obtain good guarantees using martingale inequalities online-to-batch techniques. extra generalization error term typically guarantees w.p. policy tling suboptimal performance agnostic settings exploration data could ignored occupies small fraction dataset favor models lower error data learned policies. modiﬁcation main contribution showing methods good guarantees even agnostic settings. demonstrate efﬁcacy dagger challenging problem learning perform aerobatic maneuvers simulated helicopter using simulator abbeel continuous -dimensional state -dimensional control space. consider learning hover perform nose-in funnel maneuver. compare dagger batch several choices adding small white gaussian noise state action along desired trajectory expert controller expert controller additional white gaussian noise controls expert. expert controller obtained linearizing true model desired trajectory solving also compare abbeel’s algorithm expert used ﬁrst iteration. hover approaches begin initial model ∆xt+ a∆xt b∆ut difference current hover state time delta controls time identity adds delta controls actual controls ∆xt. seek learn offset matrices minimizes ||∆xt+ )∆xt )∆ut]|| observed data. attempt learn hover presence noise delay delay introduces high-order dynamics cannot modeled current state. methods sample transitions iteration iterations delay iterations delay figure shows test performance method iteration. cases choice dagger outperforms batch signiﬁcantly converges good policy faster. dagger robust choice always obtains good performance given enough iterations whereas batch obtains good performance choice covariance states actions. covariance also frobenius norm regularizer )∆xi )∆ui]|| minab ||b|| number samples transition dataset. training stop trajectory becomes hover |||| represents event state i.e. would recovered from. testing trajectory completion case. also dagger eventually learns policy outperforms expert policy expert policy inevitably visiting states hover state large noise delay linearized model good states leading slightly suboptimal performance. thus dagger learning better linear model states visited learned policy leads better performance. abbeel’s algorithm improves initial policy reaches plateau. lack exploration ﬁrst iteration. objective show dagger outperforms model-based approaches also compared model-free policy gradient method similar cpi. however samples iteration insufﬁcient good gradient estimates lead small improvement. even samples iteration could reach avg. total cost iterations. nose-in funnel maneuver consists rotating ﬁxed speed distance around axis normal ground helicopter’s nose pointing towards axis rotation attempt learn perform complete rotations radius presence noise delay. start approach linearized model hover state learn time-varying linear model. methods collect samples iteration iterations. figure shows test performance iteration. initial model controller fails produce maneuver performance quite poor. again choice dagger outperforms batch unlike batch performs well choices video comparing qualitatively learned maneuver dagger batch available youtube abbeel’s method improves performance slightly suffers lack expert demonstrations ﬁrst iteration. presented no-regret online learning approach mbrl strong performance theory practice even agnostic settings. simple implement formalizes makes algorithmic engineering practice iterating controller synthesis system identiﬁcation applied control problem approximately solving problem feasible. additionally sample complexity scales model same except gradient descent done directly deterministic linear controller. solve linear system estimate gradient sample cost perturbed parameters. class complexity size mdp. knowledge ﬁrst practical mbrl algorithm agnostic guarantees. agnostic mbrl approach aware recent agnostic extension rmax largely theoretical requires unknown quantities algorithm sample complexity exponential class complexity. todorov iterative linear quadratic regulator design nonlinear biological movement systems. icinco ljung system identiﬁcation theory user. prentice figure average total cost test trajectories function data collected averaged repetitions experiments starting different random seed bottom hover delay hover delay nose-in funnel. denotes dagger using exploration distribution respectively similarly batch algorithm abbeel’s algorithm linearized model’s optimal controller.", "year": 2012}