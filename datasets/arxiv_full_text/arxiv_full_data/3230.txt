{"title": "Harmonic Networks: Deep Translation and Rotation Equivariance", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Translating or rotating an input image should not affect the results of many computer vision tasks. Convolutional neural networks (CNNs) are already translation equivariant: input image translations produce proportionate feature map translations. This is not the case for rotations. Global rotation equivariance is typically sought through data augmentation, but patch-wise equivariance is more difficult. We present Harmonic Networks or H-Nets, a CNN exhibiting equivariance to patch-wise translation and 360-rotation. We achieve this by replacing regular CNN filters with circular harmonics, returning a maximal response and orientation for every receptive field patch.  H-Nets use a rich, parameter-efficient and low computational complexity representation, and we show that deep feature maps within the network encode complicated rotational invariants. We demonstrate that our layers are general enough to be used in conjunction with the latest architectures and techniques, such as deep supervision and batch normalization. We also achieve state-of-the-art classification on rotated-MNIST, and competitive results on other benchmark challenges.", "text": "figure patch-wise translation equivariance cnns arises translational weight tying translation input image leads corresponding translation feature maps general pooling effects. however rotations cnns feature space transformation ‘hard-baked’ structure complicated discover exists all. harmonic networks hard-baked representation allows easier interpretation feature maps—see figure consider detecting deformable object butterfly. pose wings limited range certain poses detector normally see. transformation invariant detector good detecting wings would detect whether bigger apart rotated etc. would encode cases representation. would fail notice nonsense situations however butterfly wings rotated past usual range thrown extra pose information away. equivariant detector hand dispose local pose information hands richer useful representation downstream processes. equivariance conveys information input downstream processes also constrains space possible learned models valid rules natural image formation makes learning reliable helps generalization. instance consider cnns. insight statistics natural images embodied correlations pixels invariant translation highly localized. thus features every layer computed local receptive fields weights shared translating rotating input image affect results many computer vision tasks. convolutional neural networks already translation equivariant input image translations produce proportionate feature translations. case rotations. global rotation equivariance typically sought data augmentation patch-wise equivariance difficult. present harmonic networks h-nets exhibiting equivariance patch-wise translation -rotation. achieve replacing regular filters circular harmonics returning maximal response orientation every receptive field patch. h-nets rich parameter-efficient fixed computational complexity representation show deep feature maps within network encode complicated rotational invariants. demonstrate layers general enough used conjunction latest architectures techniques deep supervision batch normalization. also achieve state-of-the-art classification rotated-mnist competitive results benchmark challenges. tackle challenge representing ◦-rotations convolutional neural networks currently convolutional layers constrained design image feature vector translated versions image proportionally-translated versions feature vector —see figure however rotates input feature vectors necessarily rotate meaningful easy predict manner. sought-after property directly relating input transformations feature vector transformations called equivariance. special case equivariance invariance feature vectors remain constant transformations input. desirable property globally model classifier careful restrict intermediate levels processing transformation invariant. example across translated receptive fields. weight-tying serves constraint translational structure image statistics effective technique reduce number learnable parameters—see figure essence translational equivariance ‘baked’ architecture existing models. rotation refer hard-baking. current widely accepted practice cope rotation train aggressive data augmentation certainly improves generalization exact fails capture local equivariances ensure equivariance every layer within network. maintain richness local rotation information present paper. another disadvantage data augmentation leads so-called black-box problem lack feature indeed close inspection first-layer interpretability. weights reveals many rotated scaled translated copies another waste computation learning redundant weights? paper present harmonic networks h-nets. design patch-wise ◦-rotational equivariance deep image representations constraining filters family circular harmonics. circular harmonics steerable filters means represent rotated versions filter using finite linear combination steering bases. overcomes issue learning multiple filter copies cnns guarantees rotational equivariance produces feature maps transform predictably input rotation. multiple existing approaches seek encode rotational equivariance cnns. many follow broad approach introducing filter feature copies different rotations. none dominated standard practice. steerable filters root h-nets lies property filter steerability filters exhibiting steerability constructed rotation finite linear combination base filters. removes need learn multiple filters different rotations bonus constant memory requirements. such h-nets could thought using infinite bank rotated filter copies. work combines steerable filters learning build shallow features steerable filters kernel object detection rigid pose regression. h-nets filters added rotation offset term filters different layers orientation-selectivity relative another. hard-baked transformations cnns h-nets hard-bake patch-wise ◦-rotation feature representation numerous related works encoded equivariance discrete rotations. following works grouped those encode global equivariance versus patch-wise equivariance rotate filters versus feature maps. introduce equivariance ◦-rotations dihedral flips cnns copying transformed filters different rotation–flip combinations. recently generalized theory group-structured transformations demonstrated applications finite groups—an extension continuous transformations would require treatment anti-aliasing bandlimiting. larger number rotations texture classification also many rotated handcrafted filter copies opting learn filters. achieve equivariance greater number rotations methods would need infinite amount computation. h-nets achieve equivariance rotations finite computation. feed multiple rotated copies input fuse output predictions. broader class global image transformations propose novel per-pixel pooling technique output fusion. discussed techniques lead global equivariances produce interpretable feature maps. step copy feature four ◦-rotations. propose different equivariance preserving feature transformations. similar terms computed rotating feature maps instead filters. downside inputs feature maps square; whereas sized input. learning generalized transformations others tried learn transformations directly data. appealing idea said certain transformations makes sense hard-bake interpretability reliability. construct higher-order boltzmann machine learns tuples transformed linear filters input–output pairs. although powerful shown work shallow architectures. introduced capsules units neurons designed mimic action cortical columns. capsules designed invariant complicated transformations input. outputs merged deepest layer invariant global transformation. present method regress equivariant feature detectors using objective penalizes representations equivariant manifold. again encourages global equivariance; although work could adapted encourage equivariance every layer deep pipeline. many computer vision systems strive view independent object recognition invariant affine transformations boundary detection equivariant non-rigid deformations. h-nets hard-bake ◦-rotation equivariance feature representation constraining convolutional filters family circular harmonics. below outline formal definition equivariance circular harmonics exhibit rotational equivariance properties circular harmonics must heed successful integration framework figure real imaginary parts complex gaussian filter wm;e−r eimφ rotation orders. simple example r=e−r general learn quantities. cross-correlation feature rotation order filters rotation order results feature rotation order m+n. note negative rotation order filters flipped imaginary parts compared positive orders. feature maps live discrete domain. shall instead continuous spaces analysis easier. later section shall demonstrate convert back discrete domain practical implementation work entirely continuous euclidean space. equivariance equivariance useful property transformations input produce predictable transformations features interpretable make learning easier. formally feature mapping equivariant group transformations associate every transformation input transformation features; means order apply feature mapping transformation unimportant—they commute. example depicted figure shows cnns order application integer pixel-translations feature interchangeable. important point note general seek rotations image domain require find looks like rotation feature space rather searching exists equivalent class transformations feature space. special case equivariance invariance ψ={i} identity. complex circular harmonics data augmentation cnns learn rotation equivariance difficult quantify h-nets take simpler approach hard-baking structure feature mapping standard convolutional layer ◦-rotational equivariance hard-baked restricting filters circular harmonic family figure down cross-correlation input patch yields scalar complex-valued response. across-then-down crosscorrelation θ-rotated image yields another complex-valued response. bottom transform unrotated response rotated response multiplication eimθ. spatial coordinates image/feature maps expressed polar form known rotation order r+→r function called radial profile controls overall shape filter phase offset term gives filter orientation-selectivity. training learn radial profile phase offset terms. examples real component ‘gaussian envelope’ different rotation orders shown figure since dealing complex-valued filters filter responses complex-valued assume reader understands feature maps complex-valued unless otherwise specified. note works complex filters treatment differs complex phase response explicitly tied rotation angle. rotational equivariance circular harmonics deep learning libraries implement cross-correlation rather convolution since understanding slightly easier follow consider correlation. strictly cross-correlation complex functions requires arguments conjugated model/implementation consider correlating circular harmonic order rotated image patch. assume image patch able rotate locally origin filter. means cross-correlation response scalar function input image patch rotation using notation equation recalling working polar coordinates figure example hidden layer h-net output input–output left-to-right. horizontal stream represents series feature maps constant rotation order. edges represent cross-correlations numbered rotation order corresponding filter. rotation orders along path consecutive edges network must equal maintain disentanglement rotation orders. considered ◦-rotational equivariance feature maps arising cross-correlation circular harmonics determined rotation orders chained cross-correlations sum. next results construct deep architecture leverage equivariance properties circular harmonics. harmonic networks rotation order feature maps filters upon crosscorrelation achieve given output rotation order must obey equivariance condition. fact every feature equivariance condition must otherwise possible arrive feature along different paths different summed rotation orders. problem combining complex features phases rotate different frequencies leads entanglement responses. resultant feature longer equivariant single rotation order making difficult work with. resolve enforcing equivariance condition every feature map. solution create separate streams constant rotation order responses running network—see figure streams contain multiple layers feature maps separated rotation order zero cross-correlations nonlinearities. moving streams cross-correlations rotation order equal difference streams. easy check equivariance condition holds networks. multiple responses converge feature multiple choices combine them. could stack them could pool across them could save memory chose responses rotation order written place brevity. response θ-rotated image circular harmonic order equivalent cross-correlation unrotated image harmonic followed multiplication eimθ. rotation done input space multiplication eimθ performed feature space eimθ using notation equation process shown figure note included subscript feature space transformation. important kind feature space transformation apply dependent rotation order harmonic. phase response rotates input frequency response m-equivariant feature map. thinking input image complex-valued feature zero imaginary part could think -equivariant. rotation order filter defines response properties input rotation. particular rotation order defines invariance defines linear equivariance. be=ei·θ·fm cause denoting ei·θfm—as independent input rotates eiθfm complex-valued number constant magnitude spinning round phase equal naturally constrained using rotation orders only make higher negative orders work. arithmetic equivariance condition important properties circular harmonics proven supplementary material chained crosscorrelation rotation orders lead response rotation order point-wise nonlinearities hc→c acting solely magnitudes maintain rotational equivariance interleave cross-correlations typical nonlinearities adapted complex domain. summation responses order remains order thus construct output m-equivariant input rotation require rotation orders along path equals cross-correlation interchangeable either correlate continuous space downsample downsample correlate discrete space. since point-wise nonlinearities sampling also commute entire h-net seen deep feature-mapping commutes sampling. could allow implement h-net non-regular grids; although explore this. viewing cross-correlation discrete domains sheds insight equivariance properties behave. figure sampling strategy introduces multiple origins feature patch. call these centers equivariance feature exhibit local rotation equivariance points. move using exotic sampling strategies strided crosscorrelation average pooling centers equivariance ablated shifted. max-pooling center equivariance would complicated nonlinear function input image harmonic weights. reason used max-pooling experiments. circular harmonics implemented current deep learning frameworks minor engineering. implement jgiw e−ri−xj polar representation mapped components stack polar filter samples matrix write point outer product radial tensor trigonometric angular tensor phase offset separated noting complex exponential trigonometric terms element-wise identity matrix. reweighting ring elements. full generality could also per-radius phase would allow spiral-like leftright-handed features investigate this. figure h-nets operate continuous spatial domain implement pixel-domain data sampling crosscorrelation commute. schematic shows example layer h-net solid arrows follow path implementation dashed arrows follow possible alternative easier analyze computationally infeasible. introduction sampling defines centers equivariance pixel centers feature rotationally equivariant. above structure harmonic network simple. replaced regular filters radially reweighted phase shifted circular harmonics. causes filter response equivariant input rotations order prevent responses different rotation order entangling upon summation separated filter responses streams equal rotation order. complex nonlinearities cross-correlations complex nonlinearities magnitudes complex feature maps only preserve rotational equivariance. example complex version relu provide similar analogues nonlinearities batch normalization experiments. thus presented harmonic network. layer collection feature maps different rotation orders transform predictably rotation input network ◦-rotation equivariance achieved finite computation. next show implement practice. operated domain continuous spatial dimensions r×r×{k}. however h-net needs operate real-world images sampled d-grid thus need anti-alias input discretized layer. simple gaussian blur. regular architecture without problems. works fact order bandlimited sampling compare h-nets classification boundary detection. classification typical rotation invariant task suit h-nets well. contrast boundary detection rotation equivariant task. success h-net achieve global equivariance without sacrificing local equivariance features. mnist course small dataset simple visual structures good indication introducing right equivariances cnns inference. investigate classification rotated mnist dataset baseline. training images validation images test images. ◦-rotations small training size make difficult task classical cnns. compare collection previous state-of-the-art papers build deep filter copies ◦-rotations. mimic network architecture h-nets best using rotation order streams deepest layer complex-valued versions relu nonlinearities batch normalization also replace max-pooling mean-pooling layers shown figure perform stochastic gradient descent cross-entropy loss using adam adaptive learning rate divide improvement validation accuracy last epochs. train multiple models randomly chosen hyperparameters report test error model performed best validation training combined training validation table lists results. model actually parameters larger standard uses convolutions instead interestingly overfit small dataset still outperforms standard trained rotation augmentations use. state-ofthe-art improvement previous best model. deep boundary detection boundary detection equivariant non-rigid transformations; although edge presence locally invariant orientation. current state-of-the-art figure networks used experiments. left mnist networks right deeply-supervised networks boundary segmentation boxes denote feature maps. blue boxes pooling green boxes side feature maps connected dashed lines ease viewing. main cross-correlations unless otherwise stated experiments section. increased computational cost crosscorrelation return continuous rotational equivariance. analyze computational cost terms number multiplications. standard cross-correlation input size h·w·iz filters size k·k·oz number multiplications form feature size input m=hwkizoz. h-net rotation orders input rotation orders output perform complex cross-correlations. complex cross-correlation formed real cross-correlations number multiplications number input output channels respectively. thus similar computational cost equate yield m=mfr. rearranging; setting taking square root sides arrive simple rule thumb network design =fih. example want build h-net similar computational cost regular channels layer rotation orders m∈{} number h-net channels validate rotation equivariant formulation below performing introspective investigations measuring relevant baselines classification rotatedmnist dataset boundary detection berkeley segmentation dataset selected baselines representative examples current state-of-the-art demonstrate h-nets used different architectures different tasks. networks used figure depends fine-tuning imagenet-pretrained networks regress boundary probabilities per-patch basis. demonstrate hard-baked rotation equivariance serves strong generalization tool compared previous state-of-the-art architecture without pretraining. tried mimic closely possible shown figure main difference divide number feature maps faster stable training. network extended deeply supervised network side-connections. -convolutions perform weighted averages across relevant feature maps resized match input. binary cross-entropy loss applied side connection stabilize learning. final ‘fusion’ layer created taking weighted linear combination side-connections final output. adapt side-connections h-nets using complex magnitude feature maps taking weighted average. means resultant boundary predictions locally invariant rotation. added small sparsity regularizer cost function found improved results slightly. call harmonic variant h-dsn. also compare number parameters matched h-dsn also compared mean-andcovariance-rbm. technique five main contributions zero-mean unit variance normalization inputs sparsity regularization hidden units averaged ground truth edge annotations average outputs input rotations non-maximum suppression results canny method. perform first methods leave last alone. particular pretrain imagenet attempt kind rotation averaging global equivariance good baseline measure against. tested berkeley segmentation dataset shown table non-pretrained models h-nets deliver superior performance current state-of-the-art architectures including also encode rotation equivariance. noticeable parameters showing restricting investigate properties h-net implementation making sure motivations behind h-net design achieved implementation. rotational stability sanity check measured invariance magnitude response rotation m∈{}. show result rotating random input h-net layer figure response flat periodic small fluctuations inexactness anti-aliasing. filter visualization real parts filters first layer boundary-detection-trained h-net shown figure aligned zero phase ease viewing. since network trained zero-mean unit variance normalized color images weights natural colors would real-world images. nonetheless useful information glean inspecting these. layer filters detect color boundaries blank filters usually sees cnns reoriented copies. also phase histograms phases utilized filters throughout network indicating full figure randomly selected filters phase histograms bsds trained h-dsn. filter aligned oriented circles represent phase. filter copies blank filters usually seen cnns. also balanced distribution phases indicating boundaries deep feature representations uniformly distributed orientation. data ablation investigate h-nets data-efficiency. cnns massively data-hungry. krizhevsky’s landmark paper used million parameters trained million images quantized bits split classes total bits information weight. even vast amount data insufficient training data augmentation needed improve results. experiment rotated mnist dataset show hard-baked rotation equivariance require less data competing methods indeed case interestingly predictably regular cnns trained data augmentation still perform worse h-nets learn global invariance rotation rather local equivariances layer. feature maps visualize feature maps lower layers mnist trained h-net given input feature maps encode complicated structures. left right h-net learns detect edges corners object presence negative space outlines objects. perform trained h-dsn shows equivariance preserved deepest feature maps. also highlights compact representation feature presence pose regular cnns cannot presented convolutional neural network locally equivariant patch-wise translation first time continuous ◦-rotation. achieved restricting filters circular harmonics essentially hard-baking rotation architecture. implanted onto architectures too. circular harmonics pays dividends receive full rotational equivariance using parameters. leads good generalization even less training data. disadvantage we’ve seen higher per-filter computational cost guidance network design balances cost expressive figure feature maps mnist network. arrows display phase colors display magnitude information diverse features encoding edges corners whole objects negative spaces outlines. representation. better interpretability feature maps bonus know transform input image rotations. applied network problem classifying rotated-mnist setting state-of-the-art. also applied network boundary detection achieving state-of-the-art results non-pretrained networks. shown ◦-rotational equivariance possible useful. tensorflowtmimplementation available project website. future work extension work could involve hardbaking transformations equivariance properties harmonic network possibly extending allow expressibility network representations extending benefits seen afforded rotation equivariance larger class models applications. figure data ablation study. rotated mnist dataset experiment test accuracy varying sizes training set. normalize maximum test accuracy method direct comparison falloff training size. clearly h-nets data-efficient regular cnns need data discover rotational equivariance unaided. figure view best color. orientated feature maps h-dsn. color wheel shows orientation coding. note layers boundary orientations colored differently feature different visualization demonstrates consistency orientation within feature across multiple layers. images taken layers clockwise order largest smallest.", "year": 2016}