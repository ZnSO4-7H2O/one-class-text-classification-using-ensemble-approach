{"title": "ParMAC: distributed optimisation of nested functions, with application  to learning binary autoencoders", "tag": ["cs.LG", "cs.DC", "cs.NE", "math.OC", "stat.ML"], "abstract": "Many powerful machine learning models are based on the composition of multiple processing layers, such as deep nets, which gives rise to nonconvex objective functions. A general, recent approach to optimise such \"nested\" functions is the method of auxiliary coordinates (MAC). MAC introduces an auxiliary coordinate for each data point in order to decouple the nested model into independent submodels. This decomposes the optimisation into steps that alternate between training single layers and updating the coordinates. It has the advantage that it reuses existing single-layer algorithms, introduces parallelism, and does not need to use chain-rule gradients, so it works with nondifferentiable layers. With large-scale problems, or when distributing the computation is necessary for faster training, the dataset may not fit in a single machine. It is then essential to limit the amount of communication between machines so it does not obliterate the benefit of parallelism. We describe a general way to achieve this, ParMAC. ParMAC works on a cluster of processing machines with a circular topology and alternates two steps until convergence: one step trains the submodels in parallel using stochastic updates, and the other trains the coordinates in parallel. Only submodel parameters, no data or coordinates, are ever communicated between machines. ParMAC exhibits high parallelism, low communication overhead, and facilitates data shuffling, load balancing, fault tolerance and streaming data processing. We study the convergence of ParMAC and propose a theoretical model of its runtime and parallel speedup. We develop ParMAC to learn binary autoencoders for fast, approximate image retrieval. We implement it in MPI in a distributed system and demonstrate nearly perfect speedups in a 128-processor cluster with a training set of 100 million high-dimensional points.", "text": "many powerful machine learning models based composition multiple processing layers deep nets gives rise nonconvex objective functions. general recent approach optimise nested functions method auxiliary coordinates introduces auxiliary coordinate data point order decouple nested model independent submodels. decomposes optimisation steps alternate training single layers updating coordinates. advantage reuses existing singlelayer algorithms introduces parallelism need chain-rule gradients works nondiﬀerentiable layers. large-scale problems distributing computation necessary faster training dataset single machine. essential limit amount communication machines obliterate beneﬁt parallelism. describe general achieve this parmac. parmac works cluster processing machines circular topology alternates steps convergence step trains submodels parallel using stochastic updates trains coordinates parallel. submodel parameters data coordinates ever communicated machines. parmac exhibits high parallelism communication overhead facilitates data shuﬄing load balancing fault tolerance streaming data processing. study convergence parmac propose theoretical model runtime parallel speedup. illustrate general results speciﬁc algorithm develop parmac learn binary autoencoders application fast approximate image retrieval. implement using message passing interface distributed system demonstrate nearly perfect speedups -processor cluster training million high-dimensional points. speedups achieved agree well prediction theoretical speedup model. serial computing reached plateau parallel distributed architectures becoming widely available machines cores cloud computing machines. combination powerful nested models large datasets ingredient solve diﬃcult problems machine learning computer vision areas underlies recent successes deep learning unfortunately parallel computation easy many good serial algorithms parallelise well. cost communicating greatly exceeds cost computing time energy continue foreseeable future thus good parallel algorithms must minimise communication maximise computation machine creating suﬃciently many subproblems beneﬁt many machines possible. load machine approximately equal. faults become frequent number machines increases particularly inexpensive machines. machines heterogeneous diﬀer memory; case initiatives setihome become important source distributed computation future. data applications additional restrictions. size data means cannot stored single machine distributed-memory architectures necessary. sending data machines prohibitive size data high communication costs. applications data collected stored data must regularly discarded. others sensor networks limited battery life computational power imply data must processed locally. paper focus machine learning models form fk+) i.e. consisting nested mapping input output nested models involve multiple parameterised layers processing include deep neural nets cascades object recognition computer vision phoneme classiﬁcation speech processing wrapper approaches classiﬁcation regression various combinations feature extraction/learning preprocessing prior learning task. nested hierarchical models ubiquitous machine learning provide construct complex models composition simple layers. however training nested models diﬃcult even serial case function composition produces inherently nonconvex functions makes gradient-based optimisation diﬃcult slow sometimes inapplicable starting point recently proposed technique train nested models method auxiliary coordinates reformulates optimisation iterative procedure alternates training submodels independently coordinating them. introduces signiﬁcant model data parallelism often train submodels using existing algorithms convergence guarantees diﬀerentiable functions local stationary point also applies nondiﬀerentiable even discrete layers. applied various nested models however original papers proposing address distributed computing architecture communication machines costlier computation. paper proposes parmac parallel distributed framework learn nested models using implements message passing interface problem learning binary autoencoders demonstrates ability train large datasets achieve large speedups distributed cluster. ﬁrst review related work describe general introduce parmac model extensions then analyse theoretically parmac’s parallel speedup convergence finally describe implementation parmac show experimental results although implementation experiments particular parmac algorithm emphasise contributions apply parmac general situation applies i.e. nested functions layers. distributed optimisation large-scale machine learning steadily gaining interest recent years. work centred convex optimisation particularly objective function form empirical risk minimisation includes many important models machine learning linear regression lasso logistic regression svms. work typically based stochastic gradient descent coordinate descent alternating direction method multipliers resulted several variations parallel parallel parallel admm instructive consider parallel case detail. here typically runs independently data subsets parameter server regularly gathers replica parameters workers averages broadcasts back workers. show that small enough step size technical conditions distance minimum objective function value satisﬁes upper bound. upper bound term decreases number workers increases parallelisation helps another term independent past certain point parallelisation help. practice speedups serial generally modest. also theoretical guarantees parallel restricted shallow models opposed deep nested models composition functions nearly always nonconvex. indeed parallel diverge nonconvex models. intuitive reason that local minima average workers larger objective value individual workers indeed average minima need minimum. practice parallel give reasonable results nonconvex models takes care average replica models close parameter space thus associated optimum easy little work addressed nonconvex models. focused deep nets example google’s distbelief uses asynchronous parallel gradients full model computed backpropagation achieve data parallelism form model parallelism. latter achieved carefully partitioning neural pieces allocating machines compute gradients. diﬃcult requires careful match neural structure target hardware. although managed train huge nets huge datasets using tens thousands cores speedups achieved modest. work used similar techniques gpus another recent trend parallel computation abstractions tailored machine learning spark graphlab petuum tensorflow goal making cloud computing easily available train machine learning models. again often based shallow models trained gradient-based convex optimisation techniques parallel sgd. systems implement form deep neural nets. finally also exist speciﬁc approximation techniques certain types large-scale machine learning problems spectral problems using nystr¨om formula landmark-based methods parmac speciﬁcally designed nested models typically nonconvex include deep nets many models nondiﬀerentiable layers. describe below parmac advantages simple relatively independent target hardware achieving high speedups. many machine learning architectures share fundamental design principle mathematically construct nested mapping inputs outputs form fk+; wk+) parameters deep nets binary autoencoders consisting multiple processing layers. problems traditionally optimised using methods based gradients computed using chain rule. however gradients sometimes inconvenient exist also hard parallelise inherent sequentiality chain rule. method auxiliary coordinates designed optimise nested models without using chain-rule gradients introducing parallelism. solves equivalent appearance diﬀerent problem nested aﬀords embarrassing parallelisation. idea break nested functional relationships judiciously introducing variables equality constraints. solved optimising penalised function using alternating optimisation original parameters coordinates result coordination-minimisation algorithm minimisation step updates parameters splitting nested model independent submodels training using existing algorithms coordination step ensures corresponding inputs outputs submodels eventually match. algorithms developed several nested models deep nets low-dimensional svms binary autoencoders aﬃnity-based loss functions binary hashing parametric nonlinear embeddings paper focus mostly particular case binary autoencoders. deﬁne nonconvex nondiﬀerentiable problem algorithm simple eﬀective. allows demonstrate actual implementation distributed system fundamental properties parmac introduces parallelism; parmac keeps communication machines low; stochastic optimisation step; tradeoﬀ diﬀerent amount parallelism steps. also allows test good theoretical model speedup experiments. ﬁrst give detailed algorithm binary autoencoders generalise hidden layers. maps real vector onto binary code vector bits linear decoder maps back eﬀort reconstruct call binary hash function write minimise following objective function progressively increasing constraints eventually satisﬁed problem solved exactly small enumeration approximately larger alternating optimisation bits initialised solving relaxed problem truncating solution single-bit hash functions solvable ﬁtting linear svm; linear decoders linear least-squares problem. linear simply involves ﬁtting svms linear regressors occurs whenever change compared previous step gives practical stopping criterion. also order generalise well unseen data stop iterating value optimise precision hash function validation decreases. form early stopping guarantees improve initial besides faster. also initialise done running binarising result example. fig. gives algorithm bas. proposed learn good binary hash functions fast approximate information retrieval binary hashing emerged recent years eﬀective fast approximate nearest-neighbour searches image databases. real-valued high-dimensional image vectors mapped onto binary space bits search performed using hamming distances vastly faster speed smaller memory shown carreira-perpi˜n´an raziperchikolaei training beats approximate optimisation approaches relaxing codes step function encoder yields state-ofthe-art binary hash functions unsupervised problems improving established approaches iterative quantisation paper focus linear hash functions used type hash functions literature binary hashing fact computing binary codes test image must fast time. also provide experiment nonlinear hash functions consider general case hidden layers inputs outputs helps think case deep running example ideas apply beyond deep nets. consider regression problem mapping inputs outputs deep given dataset pairs minimise least-squares error layer function form i.e. linear mapping followed squashing nonlinearity applies scalar function sigmoid elementwise vector argument output introduce auxiliary variable data point hidden unit deﬁne following equality-constrained optimisation problem converges minimum constrained problem thus minimum original problem practice follow path loosely. quadratic-penalty objective function seen breaking functional dependences nested mapping unfolding layers. every squared term involves shallow mapping; variables equally scaled improves conditioning problem; derivatives required simpler require backpropagated gradients sometimes gradients all. apply alternating optimisation quadratic-penalty objective thus step results many independent single-layer single-unit submodels trained existing algorithms without extra programming cost. step form generalised proximal operator reduces complex highlycoupled problem—training deep net—to sequence simple uncoupled problems coordinated auxiliary variables large large dataset aﬀords enormous potential parallel computation. turn contribution paper distributed implementation algorithms. seen speciﬁc algorithm depends model objective function auxiliary coordinates introduced. achieve steps closed-form convex nonconvex binary others. however following always hold step subproblems independent data point. step depends part current model. step independent submodels depends problem. example number hidden units deep number hash functions linear decoders submodel depends data coordinates show turn distributed low-communication parmac algorithm give implementation parmac discuss convergence parmac. throughout paper unless otherwise indicated term machine mean single-cpu processing unit local memory disk communicate machines cluster network shared memory. basic idea parmac follows. large datasets distributed systems imperative minimise data movement network communication time generally exceeds computation time modern architectures. types data original training data auxiliary coordinates model parameters usually latter type smaller. parmac never communicate training coordinate data; machine keeps disjoint portion corresponding subset points. model parameters communicated step following circular topology implicitly implements stochastic optimisation. model call entire dataset corresponding coordinates. machine store subset subsets disjoint union entire data i.e. index sets satisfy step simple. step starts machine contain submodels. means step machine processes auxiliary coordinates step subtle. beginning step machine contain submodels portion data coordinates. submodel must access entire data coordinates order update since data cannot leave home machine submodel must data achieve circular topology follows. assume synchronous processing simplicity practice would implement asynchronously. assume arithmetic modulo imaginary clock whose period equals time machine takes process portion submodels. clock tick machines update diﬀerent portion submodels. example clock tick machine updates submodels using also machines need start time step. machine start step data soon received updated submodels step. likewise soon machine ﬁnishes step start step immediately without waiting machines ﬁnish step. however implementation consider steps barriers machines start step time. received i.e. machine updates machine updates submodels machine updates submodels machine updates submodels repeated submodel visited machine thus updated indeed ﬁnished versions submodels reside machines. finally starting step machine must contain submodels achieve this running ﬁnal round communication without computation i.e. machine sends updated submodels successor. thus clock tick machine sends practice asynchronous implementation. machine keeps queue submodels processed repeatedly performs following operations extract submodel queue process send machine’s successor queue empty machine waits nonempty. queue machine initialised portion submodels associated machine. submodel carries counter initially increases every time visits machine. reaches submodel last machine since submodel updated soon visits machine rather computing exact gradient visited machines take step step really carrying stochastic steps submodel. example update done gradient step actually implementing stochastic gradient descent minibatches size point view regard step submodel parallel submodel visit minibatches machine. step submodels visit machine. implies train stochastic gradient descent epoch submodel corresponds submodel visited machines. submodels communicated parallel asynchronously respect other circular topology. epochs entire model parameters communicated times. last round communication needed ensure machine updated version model step. described implemented experiments running epochs step requires rounds communication however epochs round communication submodel consecutive passes within machine’s data. example running epochs submodel means following instead visiting data visits data reduces amount shuﬄing problem data randomly distributed machines eﬀectively directly achieved alltoall broadcasting scatters/gathers data members members group however paper implement using circular topology mechanism described. figure illustration epoch synchronous version parmac’s step example machines submodels corresponds clock tick within machine computes portion submodels sends successor. last tick start next epoch point submodels updated entire dataset. data shuﬄing well known shuﬄing dataset prior epoch improves convergence speed. distributed systems sometimes problem require data movement across machines. shuﬄing easy parmac. within machine simply access local data random order epoch. across machines simply reorganise circular topology randomly beginning epoch could even submodel follow diﬀerent random circular topology. however implement unlikely help unbalance load machines. load balancing simple work steps proportional number data points indeed step submodel must visit every data point epoch. even submodels diﬀer size training submodel proportional step data point separate problem dependent current model thus problems formally identical complexity. hence assumption machines identical data point incurs runtime load balancing trivial points allocated equal portions machine. processing power machine proportional allocate machine subset points proportional i.e. machine gets data points. done practice expect degradation parallel speedup even identical machines submodels type. machines vary various reasons e.g. runtime aﬀected diﬀerences ventilation across machines located diﬀerent areas data centre machines running user processes addition parmac optimisation. another type degradation happen submodels diﬀer signiﬁcantly runtime runtime step driven slow submodels become bottleneck. discussed section group submodels smaller number approximately equal-size aggregate submodels purpose estimating speedup theory. need fastest schedule jobs practice still process individual submodels asynchronously. streaming streaming refers ability discard data data training time. useful online learning allow data refreshed also necessary machine collects data store. circular topology allows remove machines easily used implement streaming. consider forms streaming data added within machine likewise data discarded within machine. data added adding machine topology data discarded removing existing machine topology. forms easily achieved parmac. ﬁrst form within-machine trivial machine always remove data without change system data note private never interacts machines updating submodels. adding discarding data done beginning step. discarding data simply means removing corresponding machine. adding data means inserting machine necessary creating within machine coordinate values {zn} never upload send model ﬁnished. easiest inserting topology step machine simply sending along copy ﬁnal submodels. step proceed usual machines. removing machine easier. remove machine fault tolerance situation similar discarding machine streaming except fault occur time intended. handle little extra bookkeeping assuming support parallel processing library. imagine fault occurs machine need remove happens step need discard faulty machine reconnect circular topology. happens step also discard reconnect addition need rescue submodels updated lose. this revert previously updated copy them resides predecessor circular topology remaining submodels updated machines already updated updated keep track information tagging submodel list machines visited. beginning essentially robustness parmac faults comes in-built redundance. step without data points machine good model still learned remaining data points machines. step revert older copies lost submodels residing machines. asynchronous implementation parmac described earlier relied tagging submodel counter order know whether needs processing communicating. general mechanism parmac asynchronously submodel list machines visit. machine needs upon receiving submodel check list list submodel already visited machine updated data machine simply sends along successor without updating again. list machine updates submodel removes list sends along successor. works even diﬀerent communication topology submodel epoch. section give theoretical model estimate computation communication times parallel speedup parmac. speciﬁcally gives speedup function number machines parameters seems agree well experiments practice model used estimate optimal number machines explore eﬀect speedup diﬀerent parameter settings throughout rest paper call speedup ratio runtime using single machine using machines perfect speedup theoretical model applies general parmac case layers whether diﬀerentiable not; assumes resulting submodels introducing auxiliary coordinates size i.e. computation communication time develop precise quantitative model. consider parmac algorithm operating synchronously independent submodels size step dataset training points distributed identical machines parmac algorithm runs certain number iterations consisting step ignore small overheads estimate total runtime proportional number iterations. hence consider theoretical model runtime iteration parmac algorithm given following parameters communication time submodel step. time send submodel machine another including overheads buﬀering partitioning messages waiting time. assume communication overlap computation i.e. machine either compute communicate given time both. also communication involves time spent sender receiver; interpret time spent given machine ﬁrst receiving submodel sending integers greater equal real values greater model assumes constant equal every submodel data point. reality even submodels mathematical form dimension actual times vary somewhat many factors. however show section model agree quite well experimentally measured speedups. since machines start time amount computation without communication. compute runtime step consider synchronous procedure section tick imaginary clock machine processes portion submodels sends successor. ticks concludes epoch. repeated epochs followed ﬁnal round communication submodels. divisible apply procedure pretending ﬁctitious submodels then runtime tick ⌈m/p⌉ process points portion ⌈m/p⌉ submodels) plus ⌈m/p⌉tw constants understood ratios computation communication independent training size number submodels number machines. ratios depend actual computation within step performance distributed system value ratios vary considerably this means estimated runtime upper bound divisible better organise computation step reduces time machine idle. practice irrelevant implement computation asynchronously. machine keeps queue incoming submodels needs process repeatedly takes submodel processes sends machine’s successor. decrease speedup large caused communication overhead step machines idle tick step. impractical case communication cost speedup although diminishing returns. practice large values likely case case maximum speedup achieved using machines submodels bigger since diminishing returns occur approach maximum practically best value somewhat smaller independent absorbed communication-computation ratios. means depends dataset size computation/communication times therefore invariant parameter transformations leave ratios unchanged. transformations following fig. plots typical speedup curve obtained realistic choice parameter values. displays prototypical speedup shape expect practice wider range parameter settings. dataset size practically small value otherwise curves tend look like typical curve parameter settings representative diﬀerent potential practical situations note following observations time spent communication large relative terms speedup decreased. happen runtime step communication cost large many epochs indeed since step perfectly parallelisable decrease speedup come step. smaller value accordance theorem suggests selecting values make integer particular multiple achieves best speedup eﬃciency machines never idle figure typical form theoretical speedup curve realistic parameter settings speciﬁcally data points submodels epoch step discontinuities curve visible. mark values divisible maximum speedup occurs points; number submodels number epochs step step computation time step communication time step computation time within plot curve corresponds value indicated right plot. practice given speciﬁc problem theoretical speedup curves used determine optimal values number machines use. seen section theoretical curves agree quite well experimentally measured speedups. theoretical curves need estimates computation time communication times assumption speedup model machines identical processing power. model extend case machines diﬀerent noted discussion load balancing work performed machine proportional number data points contains step every submodel runs every submodel must visit machine; step data point separate problem involves submodels hence equalise work machines loading machine amount data proportional processing speed independent number submodels. another assumption model submodels identical size sometimes true. example submodels types encoders decoders since encoders bigger decoders take longer train communicate. still apply speedup model group smaller submodels single submodel size comparable larger submodels equalise much possible submodel sizes reasonable assumption ratio computation times decoder encoder group decoders groups decoders each. group decoders computation communication time equal encoder. gives eﬀective number independent submodels applying model experimental speedups section finally emphasise goal section characterise parallel speedup parmac quantitatively demonstrate runtime gains achievable using machines. practice considerations also important economic cost using machines type machines choice optimisation algorithm steps; fact that size dataset need already distributed across machines; etc. also possible combine parmac other orthogonal techniques. example submodels step convex optimisation problem could techniques described section distributed convex optimisation submodel. would eﬀectively allow larger speedups approximation parmac makes original algorithm using step. since guarantee convergence certain conditions recover original convergence guarantees mac. detail. convergence stationary point given theorem carreira-perpi˜n´an wang quote here theorem consider constrained problem quadratic-penalty function given positive increasing sequence nonnegative sequence starting point suppose quadratic-penalty method ﬁnds approximate minimiser theorem applies general case diﬀerentiable layers standard karush-kuhntucker conditions hold relies standard condition penalty methods nonconvex problems namely must able reduce parameter achieved running suitable optimisation method suﬃciently many iterations. change case parmac? step remains unchanged respect step change obliged distributed stochastic training. need ensure reduce gradient penalised function respect submodel arbitrary tolerance. also guaranteed standard conditions. general convergence conditions stochastic optimisation essentially robbins-monro schedules epoch number give much tighter conditions convergence convergence rate subproblems step convex topic received much attention recently many conditions exist often based techniques nesterov accelerated algorithms stochastic average gradient typically summary convergence parmac stationary point guaranteed theorem added sgd-type condition step. convergence guarantee independent number layers submodels number machines also guarantee parmac’s convergence original theorem without sgd-type conditions still distributed setting achieving signiﬁcant parallelism. done computing gradient step exactly first machine computes exact per-point gradients submodel parallel. then aggregate partial gradients exact gradient submodel. could done parameter server machine parameter server submodel could easily implemented functions. however well known slower using sgd. nondiﬀerentiable layers convergence properties well understood. particular binary autoencoder encoding layer discrete problem np-complete. again modiﬁcation parmac fact encoder decoder trained step whose convergence tolerance achieved sgd-type conditions. indeed experiments show parmac gives almost identical results mac. convergence guarantees important theoretically practical applications large datasets distributed setting typically runs epochs even less sampled random updates. various results exist guarantee convergence deterministic errors rather stochastic errors. results assume bound deterministic errors apply general nonconvex objective functions standard conditions apply particular case incremental gradient method cycle data points ﬁxed sequence. passing data). typically reduces objective function good enough value fast possible since pass data costly. experiments epochs step make parmac similar using exact step. implemented parmac binary autoencoders c/c++ using scientic library basic linear algebra subroutines library mathematical operations linear algebra message passing interface interprocess communication. blas provide wide range mathematical routines basic matrix operations various matrix decompositions least-squares ﬁtting. used versions blas come linux distribution considerably better performance could achieved using lapack optimised version blas widely used frameworks high-performance parallel computing today best option parmac support distributed-memory machines spmd model language independence availability multiple machines small shared-memory multiprocessor machines hybrid clusters. diﬀerent processes cannot directly access other’s memory space data transferred sending messages process another collectively among multiple processes. spmd model useful distributed machine learning means processes share code operate diﬀerent data control using individual process industry standard many implementations mpich openmpi mostly compatible other. used mpich merced shared-memory cluster openmpi ucsd tscc distributed cluster parmac code compiles runs implementations. used highest compiler optimisation level speciﬁcally mpicc -lgsl -lgslcblas -lm. calls compiler option turns available code optimisation ﬂags. results longer compilation time eﬃcient code. code snippet ﬁgure shows main steps parmac algorithm functions starting calls library. programs start code initialising environment ﬁnalising receive data synchronous blocking receive function recv. process calling blocks data arrives. send data buﬀered blocking version send functions bsend. requires allocate enough memory attach system advance. process calling bsend blocks buﬀer copied internal memory; that library takes care sending data appropriately. beneﬁt using version send programmer send messages without worrying buﬀered code simpler. appendix brieﬂy describes important functions arguments. note word synchronous refer process diﬀerent submodels stated earlier synchronised start speciﬁc clock ticks hence processed asynchronously respect other. word synchronous refers mpi’s handling individual receive function done either calling recv block data received pseudocode calling irecv followed wait block data received like this options equivalent purpose ensure receive submodel starting train irecv/mpi wait option slightly ﬂexible would allow additional processing irecv wait possibly achieve performance gain. init; comm rank; comm size; loadsettings; loaddatasets; initializelayers; bsend avoid managing send buﬀers need allocate required amount buﬀer space data copied delivered pack size; allocate enough memory store whole model attach buff malloc); buffer attach; initialise execution environment rank calling process total number processes load parameters load input output datasets initial auxiliary coordinates allocate memory initialise steps stepcounter number submodel carries increases step. reaches certain value stop sending submodel around stops. reset stepcounter submodels beginning w-step. *mpisize) lookup table created randomly stores path submodel epochs iterations pick successor process lookup table successor next lookuptable; load submodel struct send buﬀer loadsubmodel; bsend returns data copied application buﬀer space allocated send buﬀer bsend char successor model distributed-memory used general computing nodes ucsd triton shared computing cluster available public fee. node contains -core intel xeon processors dram hard drive. nodes connected network. used processors. detailed specs table http//idi.ucsd.edu/computing. shared-memory -processor machine located merced. processors communicate shared memory. used large-scale experiment used processors. detailed specs table systems interprocess communication handled shared-memory system faster processors faster communication distributed seen experiments imply shared-memory systems necessarily superior practice simply reﬂects characteristics equipment access parmac speedups function number processors comparable systems. images training test set. extract gist features image. sift-k contains training highresolution colour images test images represented sift features. sift-m contains training test images. sift-b three subsets base vectors search performed learning vectors used train model query vectors. performance measures regarding quality hash functions learnt report following. binary autoencoder error want minimise quadraticpenalty function actually minimise value retrieval precision test using true neighbours nearest images euclidean distance original space retrieved neighbours binary space nearest images hamming distance. cifar sift-k sift-m. sift-b suggested dataset creators report recallr average number queries nearest neighbour ranked within positions case tied distances place query rank. measures computed oﬄine trained. models parameters linear encoders except sift-b also kernel svms. decoder always linear. bits cifar sift-k sift-m bits sift-b. step uses enumeration siftk sift-m alternating optimisation otherwise. initialise binary codes truncated subset training train encoder decoder stochastic optimisation used code bottou bousquet using default parameter settings. step size tuned automatically iteration examining ﬁrst datapoints. multiplicative schedule initial value factor tuned oﬄine trial using small subset data. cifar iterations sift-k sift-m iterations. sift-b iterations. figures show eﬀect sift-k cifar varying number epochs shuﬄing data function number machines learning curves number epochs increases step solved exactly fewer epochs even cause small degradation. reason that although relatively small datasets contain suﬃcient redundance epochs suﬃcient decrease error considerably. also helped accumulated eﬀect epochs iterations. running epochs increases runtime lowers parallel speedup particular model bits therefore submodels compared number machines step less parallelism. fig. shows positive eﬀect data shuﬄing step. shuﬄe minibatches successor machine given random lookup table. shuﬄing generally reduces error increases precision increase runtime. note that even keep circular topology ﬁxed throughout step still small amount shuﬄing. occurs because although submodels process data minibatches direction submodels diﬀerent machines start diﬀerent minibatches. parmac would give identical result matter number machines. however seen ﬁgures seems small intrinsic shuﬄing simply randomises learning curves make better learning curve machine. fundamental advantage parmac distributed optimisation general ability train datasets single machine reduction runtime parallel processing. fig. shows strong scaling speedups achieved experimentally function number machines ﬁxed problem size cifar sift-m even though datasets especially number independent submodels relatively small machines used speedups ﬂatten number epochs increases experiment bottleneck step whose parallelisation ability limited however noted earlier using epochs gives good enough result close exact step. strong scaling total problem size ﬁxed problem size machine inversely proportional number machines weak scaling problem size machine ﬁxed total problem size proportional high speedups easier obtain weak scaling strong scaling particularly remarkable given original nested model model parallelism. speedups vastly larger achieved earlier large-scale nonconvex optimisation approaches google’s distbelief system although admittedly deep nets trained larger bas. fig. shows speedups predicted theoretical model section parameters known values cifar sift-m sift-b time parameters time units trial error achieve reasonably good datasets perimental speedups. speciﬁcally cifar sift-m. although fudge factors rough agreement fact communicating weight vector network orders magnitude slower updating gradient step step quite slower step binary optimisation involves. fig. also shows theoretical prediction sift-b dataset using parameters sift-m since quite larger much larger speedup nearly perfect wide range sift-b largest datasets largest publicly available comparing nearest-neighbour search algorithms known ground-truth vector sift features feature original dataset stored single byte rather double-precision ﬂoats experiments totalling training using linear hash function using rather converting ﬂoats would exceed modiﬁed code convert feature needed. step datapoint processed independently conversion double done point time. step done minibatch time. course possible hard disk additional storage would slow training. auxiliary coordinates must stored algorithms take memory data used bits hash function trained linear before kernel using gaussian radial basis functions ﬁxed bandwidth centres. means trainable parameters weights algorithm change except operates m-dimensional input vector kernel values instead sift features. gaussian kernel values before save memory store unsigned one-byte integer value used centres maximum could memory picked random training set. trials subset training worked well wide enough ensure that limited one-byte precision data point would produce zeros kernel values. learning curves essentially identical systems. nonlinear hash function outperforms linear recall would expect. improvement occurs across whole range recall values note error nested model decrease monotonically. optimises instead penalised function eﬀort minimise increases. based previous results small number epochs larger number submodels step expect nearly perfect speedups. cannot compute actual speedup single-machine runtime enormous. using scaled-down model training estimated training machine would take months. although speedups comparable distributed shared-memory system former times slower. reason distributed system slower processors slower interprocessor communication also figure speedup function number machines cifar sift-m sift-b. experimental result distributed memory system. bottom theoretical result predicted model section dataset size number submodels cifar sift-m sift-b. tscc distributed cluster consists nodes containing processors ram. processors communicate shared-memory within node across network otherwise experiments always allocated processors within node possible. depending whether user requests processors within across nodes tradeoﬀ communication modes. full study issue beyond scope understand parmac algorithm general rather speciﬁc computer architectures. however small experiment evaluate computation communication time spent function number processors node. total number processors allocated following conﬁgurations used hash function sift-b experiment settings trained subset points single iteration. figure shows resulting times. computation time remains constant communication time increases move shared-memory distributed settings expected. hence eﬀect parmac algorithm would increase step runtime correspondingly lower parallel speedup. developing parallel distributed optimisation algorithms nonconvex problems machine learning challenging shown recent eﬀorts large teams researchers important advantage parmac simplicity. data model parallelism arise naturally thanks introduction auxiliary coordinates. corresponding optimisation subproblems often solved reusing existing code black circular topology suﬃcient achieve communication machines. close coupling figure time spent communication computation function number processors node tscc cluster. time shared-memory merced cluster rather algorithm meta-algorithm produce speciﬁc optimisation algorithm given nested problem depending auxiliary coordinates introduced resulting subproblems solved algorithms; dempster example low-dimensional svms wang carreira-perpi˜n´an step small quadratic program data point. however regardless speciﬁcs resulting algorithm typically exhibits step independent submodels step independent coordinates data points. likewise speciﬁcs parmac algorithm depend corresponding algorithm. however always split data auxiliary coordinates machines consist step communication machines step submodels visit machines circular topology eﬀectively training stochastic optimisation. improvements made speciﬁc problems. example possible parallelisation less dependencies reduce communication step sending given machine model portion needs allocating cores within multicore machine accordingly. also step optimisations make parallelisation gpus distributed convex optimisation algorithms. submodels small size better machine operate submodels send together larger message since reduce latency overheads many reﬁnements done industrial implementation. example store communicate reduced-precision values data parameters little eﬀect accuracy done neural nets various system-dependent optimisations possible improving spatial temporal locality code given type size cache machine. paper tried keep implementation simple possible goal understand parallelisation speedups parmac setting general possible rather trying achieve best performance particular dataset model distributed system. parmac eﬃcient communication data coordinates ever sent entire model times iteration. using epoch using epochs performing within machine entire model sent twice iteration. near optimal note following. data cannot communicated every iteration submodel must visit machine hence correct algorithm send entire model least once; parmac twice. also circular topology minimal topology necessary able optimise global model entire dataset machines machine must able communicate machine. edges truly distributed machine importance. broadcast parameters worker machines. bipartite graph bidirectional edges servers workers edges quite larger number machines entire model must sent twice iteration parameter server creates bottleneck parallel distributed computing systems around decades. important class supercomputers carefully designed terms processors memory system connection network. traditionally used solve wide variety large-scale scientiﬁc computation problems weather prediction nuclear reactor modelling astrophysical molecular simulations. another important class clusters inexpensive heterogenous workstations connected ethernet network workstations diﬀering speed memory/disk capacity number cores/gpus etc. used data centres google amazon companies also distributed computation models setihome capitalise computation internet connectivity available individuals willingness donate projects worthy. systems machine learning task tasks running concurrently searches email data centre personal applications individual’s workstation. supercomputers clusters diﬀer considerably across important factors suitability particular problem computation communication speed size memory disk connection network fault tolerance load cost energy consumption etc. present unclear best choices machine learning models expect many diﬀerent possibilities researched immediate future. suggest parmac combination techniques play important role nested models embarrassing parallelism introduces loose demands underlying distributed system. proposed parmac distributed model method auxiliary coordinates training nested nonconvex models general analysed parallel speedup convergence demonstrated mpi-based implementation particular case train binary autoencoders. creates parallelism introducing auxiliary coordinates data point decouple nested terms objective function. parmac able translate parallelism inherent distributed system using data parallelism machine keeps portion original data corresponding auxiliary coordinates; using model parallelism independent submodels visit every machine circular topology eﬀectively executing epochs stochastic optimisation without need parameter server therefore communication bottlenecks. keeps communication machines minimum within iteration. sense parmac seen strategy able existing well-developed distributed optimisation techniques— applicable simple functions—to setting simple functions coupled nesting nonconvex function whose training data distributed machines. convergence properties remain essentially unaltered parmac. parallel speedup theoretically predicted nearly perfect number submodels comparable larger number machines eventually saturate continues increase number machines indeed conﬁrmed experiments. parmac also makes easy account data shuﬄing load balancing streaming fault tolerance. hence expect parmac could basic building block combination techniques distributed optimisation nested models data settings. work supported google faculty research award award iis–. thank dong useful discussions performance evaluation parallel systems quoc useful discussion google’s distbelief system. every program calling functions. programs used pass command-line arguments processes. input argc pointer number arguments; argv pointer argument vector. cator. communicator comm world represents number tasks available application. input comm communicator output size number processes group comm municator. initially process assigned unique integer rank number tasks rank often referred task process becomes associated communicators unique rank within well. input comm communicator output rank rank calling process group comm point-to-point operations involve message passing exactly tasks. task performs send operation task performs matching receive operation. diﬀerent types send receive functions used diﬀerent purposes synchronous send; blocking send blocking receive; non-blocking send non-blocking receive; buﬀered send; combined send-receive. argument list generally takes following formats requested system buﬀer space obtained system issues unique request number. programmer uses system-assigned handle later determine completion non-blocking operation. request pointer predeﬁned structure request. allocate required amount buﬀer space data copied delivered. alleviates problems associated insuﬃcient system buﬀer space. returns data copied application buﬀer space allocated send buﬀer. must used buffer attach routine. send buﬀer. processing continues immediately without waiting message copied application buﬀer. communication request handle returned handling pending message status. program modify application buﬀer subsequent calls wait test indicate non-blocking send completed. identiﬁes area memory serve receive buﬀer. processing continues immediately without actually waiting message received copied application buﬀer. communication request handle returned handling pending message status. program must calls wait test determine non-blocking receive operation completes requested message available application buﬀer.", "year": 2016}