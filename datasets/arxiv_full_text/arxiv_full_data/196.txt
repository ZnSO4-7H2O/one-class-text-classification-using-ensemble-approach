{"title": "Improving Content-Invariance in Gated Autoencoders for 2D and 3D Object  Rotation", "tag": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "abstract": "Content-invariance in mapping codes learned by GAEs is a useful feature for various relation learning tasks. In this paper we show that the content-invariance of mapping codes for images of 2D and 3D rotated objects can be substantially improved by extending the standard GAE loss (symmetric reconstruction error) with a regularization term that penalizes the symmetric cross-reconstruction error. This error term involves reconstruction of pairs with mapping codes obtained from other pairs exhibiting similar transformations. Although this would principally require knowledge of the transformations exhibited by training pairs, our experiments show that a bootstrapping approach can sidestep this issue, and that the regularization term can effectively be used in an unsupervised setting.", "text": "content-invariance mapping codes learned gaes useful feature various relation learning tasks. paper show content-invariance mapping codes images rotated objects substantially improved extending standard loss regularization term penalizes symmetric cross-reconstruction error. error term involves reconstruction pairs mapping codes obtained pairs exhibiting similar transformations. although would principally require knowledge transformations exhibited training pairs experiments show bootstrapping approach sidestep issue regularization term effectively used unsupervised setting. gated autoencoders class unsupervised neural network models especially suited learning relations data instances. capability useful tasks like facial expression modeling motion learning perspective-invariant object recognition another non-trivial task gaes lend analogy-making objective produce data instance given three instances query aptness gaes model relations stems three-way multiplicative connections input sources hidden mapping layer. advantage type connections additive connections allows independence learned representations content transformation respectively words important factor contributing successful gaes relation learning tasks learn representations relations degree invariant content inputs. given observation far-fetched whether models better learning content-invariant mapping codes also better performing relation learning tasks. paper propose novel training objective gaes extending classical training objective content-invariance regularization term penalizes call crossreconstruction error tuples input pairs similar mapping codes. approach leverages fact standard loss tends encode relations inputs mapping codes even weakly content-invariant. note bootstrapping procedure completely unsupervised require labeled input pairs type transformation known advance. show experimentally approach leads stronger content-invariance representations rotations images compared standard gaes. first compare crossreconstruction error tuples input image pairs conveying rotation angles. assess content-invariance mapping space terms cluster separation thirdly mapping codes inputs k-nearest neighbor classiﬁcation task. furthermore effect shown qualitatively using analogy making examples rotated mnist norb data. lastly importantly hinder standard loss. contrary often improved proposed regularization term. paper organized follows. section gives overview usage discusses related models. section described introduced. experiment setup including data preparation training details evaluation described section results presented discussed section section gives conclusion. gaes utilize multiplicative interactions learn correlations within data instances. principle ﬁrst used cross-correlation models motion vision adelson bergen later binocular disparity sanger qian fleet based hand-crafted gabor ﬁlters using multiplicative interactions combination ﬁlter learning higher-order neural networks– computing sums products–were proposed rumelhart giles maxwell smolensky directly related vision. tenenbaum freeman introduced bi-linear models separating person pose face images. bi-linear models two-factor models whose outputs linear either factor held constant property also applies gae. olshausen proposed another variant bi-linear model order learn objects optical ﬂow. similar architecture gated boltzmann machine seen direct predecessor gae. gbms applied image pairs learning transformations modeling facial expression disentangling facial pose expression introduced memisevic derivative standard learning criteria became applicable development denoising autoencoders before-mentioned contributions mostly used relate different data instances series publications exist capture within-image correlations based architectures models closely related energy models based idea square ﬁlter responses network. gaes used analogy-making rotated objects learn transformationinvariant representations classiﬁcation tasks parent-offspring resemblance prediction accelerated motion stacking layers order learn higher-order derivatives domains variants applied linguistics learn negate adjectives activity recognition kinekt sensor robotics learn write numbers learning multi-modal mappings action sound visual stimuli cheung extended common autoencoder penalty term disentangle object classes variations. gated autoencoder given data pairs pairs part pair transformed version other. gated autoencoder learn transformations representing so-called mapping units mapping codes computed trained contain ﬁlter pairs constituting eigenvectors training data transformation. projecting inputs onto ﬁlter pairs results decomposition data subspaces. contains real parts contains imaginary parts eigenvectors speciﬁc transformation amounts number concurrent subspace rotations complex plane. therefore element-wise multiplication ﬁlter responses equation results rotation detectors refer factors. pooling neighboring factors extension common explicitly separating within-subspace pooling acrosssubspace pooling reducing complexity learning using forces four corresponding ﬁlters remain subspace ﬁlters within weight matrix approximately quadrature. result factors pooling representing relative rotation angles less sensitive absolute starting angles. matrix pools across subspaces causing mapping units learn common transformations data. details that please refer memisevic instance reconstructed given instance mapping code symmetric reconstruction error cost function penalizes reconstruction error forward backward transformations training pair. however guarantee mapping encoding speciﬁc transformation results minimal reconstruction error pairs exhibiting transformation. would know transformation exhibited pairs training data objective could easily enforced selecting pairs transformation similar transformations minimizing reconstruction error pair given mapping code pair. call cross-reconstruction error. absence knowledge don’t know priori pairs selected minimizing crossreconstruction error. however assume variation transformation accounts variance mapping space variation content does follows codes close mapping space likely encode similar transformations similar content. case reducing crossreconstruction error pairs close mapping space increase content-invariance mapping codes. assumption course expected hold initial stages training becomes increasingly likely symmetric reconstruction error decreases training. leads following formulation content-invariance regularization rn×l denote mapping codes computed training pairs data pair yielding code given pair reordering m/mi invariance-regularization tested quantitatively qualitatively different data sets different combinations training evaluation data. section introduces used datasets data preparation section gives training details evaluation method described section majority tests performed rotated mnist digits train validation test split original mnist data thereof create different transformation sets. mnistr transformation includes pairs {−−− degree rotation angle resulting discrete transformation classes. mnistr/ angles mnistr increased degrees order obtain complementary classes mnistr transformation include pairs whose mutual rotations exhibit possible discrete angles degrees resulting classes. experiments rotated objects train small norb dataset used consists videos rotating objects categories objects category. training ﬁrst eight objects testing ninth objects category used. videos pairwise frames azimuth angle degrees ﬁxed elevation degrees ﬁxed lighting condition extracted. datasets contrast-normalized zero mean unit variance crucial preparation usage takes bootstrapping approach relying pre-training model. therefore strength number nearest neighbors swap mapping codes minimal beginning training. scheme works well mnist data linear increase parameters training increase parameters maximum values epochs. norb dataset stepwise increase parameters shown work best. trained using stochastic gradient descent input units turned random. enforcing sparsity mapping units factors well weight regularization crucial good performance model. improvement reached limiting weight norms clipping gradients training. cost term used penalizes deviation norm input ﬁlter average ﬁlter norm well deviation zero mean. experiments trained different data sets tested using different eval data sets. mean symmetric reconstruction error mean symmetric crossreconstruction error measured reconstructing test respective eval data using trained training respective data. davies-bouldin index rotation error evaluated based mapping codes inferred projecting test respective eval data mapping space trained training respective data. davies-bouldin index metric quality clustering data points. commonly used evaluate clustering algorithms parameters. example k-means clustering used determine amounts average ratio scatter within clusters distance cluster centers. minimal clusters minimal expansion maximal mutual distance. content information contribute variance content-invariant representations transformations resulting clusters smaller expansion. therefore metric evaluating quality representations learned gae. error gaes predicted rotation angles eval data evaluated follows. k-nearest neighbor classiﬁer used predict discrete rotation angles test pairs eval data. predictions mean distances closest true rotation angles calculated. example predicted rotation angle test pair degrees true angle degrees pair contributes error degrees. table results gated autoencoder without content-invariance regularization different datasets. section describes referenced datasets section describes evaluation criteria. column data speciﬁes dataset used train column eval data shows dataset used evaluation. quantitative results experiments shown table contentinvariance regularization clearly outperforms without almost combinations data eval data. particular interesting note msre also reduced using cir. unexpected result since training without optimizes solely msre rather msre mscre jointly. suggest reducing content-variance mapping space facilitates generalization transformations. better mscre mnistr/ eval data supports assumption none rotation angles training set. comes cost slightly higher msre indicating higher generalization transformations results lower speciﬁcity content types. lower values gae+cir respect show clusters formed pairs identical transformations compact gae+cir. figure shows analogy making examples mnistr dataset. better quality analogies using invariance-regularization indicates mappings learned better represent content-invariant transformations. fourth group shows ambiguous transformation input represent rotation degrees. ambiguity visible transformation results show superimposed rotations gae+cir obviously resolve ambiguity reconstruction less noisy. figure shows analogy making examples norb dataset similar memisevic exarchakis used variant standard concatenated video frames tied weights gating noise. even variant always possible apply transformations inferred image pairs analogy making task experiments show vanilla almost impossible solve task. source images similar query source approximately transformed. mitigates problem making possible learn content-invariant transformations vanilla gae. figure analogies rotated mnist digits. mappings inferred pairs upper-most applied images ﬁrst column group. second column group shows analogies generated vanilla right-most columns show analogies rendered using cir. figure analogies -frame norb videos. mappings inferred pairs upperrow applied images ﬁrst column group. second column group shows analogies generated vanilla right-most columns show analogies rendered using cir. empirical results reported show gaes able learn mapping codes transformations degree content-invariant content-invariance mapping codes substantially improved including regularization term penalizes cross-reconstruction error. although approach principally requires knowledge transformations exhibited data pairs show knowledge necessary practice bootstrapping approach inﬂuence regularization term incrementally increased training allows work unsupervised settings relying assumption similar mapping codes represent similar transformations. interestingly results also show many cases training content-invariance regularization also leads lower standard loss despite fact content-invariance regularization competes reconstruction error training. paper limited experiments learning object rotations images. regularization method obviously limited type data transformations future work assess merits proposed method data using different transformations. aaron courville james bergstra yoshua bengio. spike slab restricted boltzmann machine. proceedings fourteenth international conference artiﬁcial intelligence statistics pages afshin dehghan enrique ortiz ruben villegas mubarak shah. look like? determining parent-offspring resemblance gated autoencoders. proceedings ieee conference computer vision pattern recognition pages alain droniou serena ivaldi olivier sigaud. learning repertoire actions deep neural networks. development learning epigenetic robotics joint ieee international conferences pages ieee geoffrey hinton modeling pixel means covariances using factorized third-order boltzmann machines. computer vision pattern recognition ieee conference pages ieee roland memisevic. multi-view feature learning. john langford joelle pineau editors proceedings international conference machine learning icml pages york july omnipress. decebal constantin mocanu haitham ammar dietwig lowet kurt driessens antonio liotta gerhard weiss karl tuyls. factored four conditional restricted boltzmann machines activity recognition. pattern recognition letters scott reed kihyuk sohn yuting zhang honglak lee. learning disentangle factors variation manifold interaction. proceedings international conference machine learning pages joshua susskind geoffrey hinton roland memisevic marc pollefeys. modeling joint density images variety transformations. computer vision pattern recognition ieee conference pages ieee pascal vincent hugo larochelle isabelle lajoie yoshua bengio pierre-antoine manzagol. stacked denoising autoencoders learning useful representations deep network local denoising criterion. journal machine learning research", "year": 2017}