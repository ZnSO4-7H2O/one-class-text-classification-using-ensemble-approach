{"title": "Towards Machine Intelligence", "tag": ["cs.AI", "cs.LG", "cs.NE"], "abstract": "There exists a theory of a single general-purpose learning algorithm which could explain the principles of its operation. This theory assumes that the brain has some initial rough architecture, a small library of simple innate circuits which are prewired at birth and proposes that all significant mental algorithms can be learned. Given current understanding and observations, this paper reviews and lists the ingredients of such an algorithm from both architectural and functional perspectives.", "text": "abstract. exists theory single general-purpose learning algorithm could explain principles operation. theory assumes brain initial rough architecture small library simple innate circuits prewired birth proposes signiﬁcant mental algorithms learned. given current understanding observations paper reviews lists ingredients algorithm architectural functional perspectives. recently much progress made area supervised learning however greatest challenges remaining artiﬁcial intelligence research advancing ﬁeld unsupervised learning algorithms especially autonomous learning complex spatiotemporal patterns poses great challenge. paper reviews lists ingredients possible general-purpose learning algorithm given current state knowledge. intelligence originates. studied extensively past decades date still consensus principles operation. theories suggest single learning algorithm might suﬃcient explain intelligence theories considered ever since mountcastle’s discovery simple uniform architecture cortex discovery might suggest brain regions perform similar operations region-speciﬁc algorithms. another famous experiment supporting hypothesis showed rewiring auditory part brain ferrets able learn interpret visual inputs knowledge necessary ingredients algorithm shaped neuroscientiﬁc discoveries empirical evaluation eﬀectiveness algorithms metacognition observations. points considered general assumptions reverse-engineering general-purpose learning algorithm. real world almost data unlabeled. although nobody kwons precise rules used human brain learning assume learn mostly unsupervised way. speciﬁcally newborn learns world diﬀerent objects interact might even provide supervised signal him/her appropriate sensory representations need developed ﬁrst. another piece evidence supervised learning obtained simple calculation assuming approximately synapses seconds human lifetime enough capacity store memories rate bits/second therefore seems reasonable brain learns model world directly environment. motivates hypothesis predominance unsupervised learning since acquiring much information absorbing data perceptual inputs even teacher present learning must done learning associations events without supervision. unsupervised learning researched extensively found closely connected process entropy-maximization regularization compression means evolution brains adapted data compactors. particular goal unsupervised learning might codes disentangle input sources describe original information less redundant interpretable throwing much data possible without losing information. example operation observed visual cortex learns patterns appearing natural environment assigns high probability patterns contrast cortex assigns probability random combinations. real world data said near non-linear manifold within higher-dimensional space manifold shape deﬁned data probability distribution. clustering equivalent learning manifolds able separate well enough given task. humans learn concepts sequential order ﬁrst making sense simple patterns representing complex ones terms previously learned abstractions. ability read might serve example. first learn recognize strokes letters words able understand complex sentences. contrast non-compositional approach would attempt read straight patterns piece paper. brain might adapted reﬂect fact world inherently hierarchical. observation also inspired deep learning movement used hierarchical approach model real world data achieving unprecedented performance many tasks. deep learning algorithms automatically compose multiple layers representations data gives rise models yield increasingly abstract associations concepts feature learning among others). main distinction deep approach previous generation machine learning structure data discovered automatically general-purpose learning procedure without need hand-engineer feature detectors scheme agrees well idea unsupervised learning mentioned above. abstract hierarchical representations might natural by-products data compression given theoretical empirical evidence favor deep representation learning could formulate requirement type brain-like architecture deep containing many hierarchical levels. existence cortical columns neocortex linked functional importance arrangement. column typically responds sensory stimulus representing certain body part region sound vision cells belonging cell excited simultaneously therefore acting feature detector. time column active prohibit nearby columns becoming active. lateral inhibition mechanism leads sparse activity patterns. fact strongly active columns inhibited forces learned patterns invariant possible giving rise independent feature detectors cortex might expect sparse distributed representations brain coincidental since possess important properties information-theoretic perspective. distributed important order disentangle underlying causes variation sparsity aﬀects elements learning good features. proven given certain sparsity signal correctly reconstructed even fewer samples sampling theorem requires ever since discovery selective features detectors edge detectors center-surround receptive ﬁelds hubel wiesel learning biologically plausible sparse distributed representations input patterns research topic shown sdrs signiﬁcantly noise-resistant dense representations another important property distributed representations appreciated number distinguishable regions scales exponentially number parameters used describe true non-distributed representations. sparse distributed representations combinatorially much expressive. given observation simple discriminative point view higher levels abstractions sdrs preferred representing inputs since learning procedure produces form preserves much information possible making code short/simple possible in-line fig. eﬃcient learning sdrs; sparse distributed representations simplify learning temporal dependencies; provide mechanism generalization out-ofdomain prediction occam’s razor minimum description length rules postulate simple solutions chosen complex ones allows manipulating sparse representations throughout large network simpliﬁes learning higher level concepts redundancy reduction chinese room argument states learning improve performance measure given task necessarily lead improving understanding task itself. context supervised learning issue since clearly care performance measure. however unsupervised learning considered desired outcome would learn transferrable concepts. could even hypothesized following gradient objective function prohibit learning procedure discovering unknown state-space progress learning equivalent close objective. hypothesis objective problem itself. clearly learning algorithm goal might deﬁned broadly theory curiosity creativity beauty described schmidhuber large network human brain might computationally eﬃcient separate local learning adjusting higher level connections layers/regions functional distinction would reﬂect structural hierarchy predominant deep learning methods described real world. biological technological social transportation types real-world networks neither completely random deﬁnitely regular. instead topology lies somewhere between. so-called small world networks nature’s solution hierarchical structure allowing separate parallel local global updates synapses scalability unsupervised learning lower levels goal-oriented ﬁne-tuning higher regions. study neocortex reveals presence small world networks columnar organization reﬂects local connectivity cerebral cortex.the brain inherently parallel machine without separate instruction-issuing memory storage areas. instead parts neocortex participate both. diﬀerence compared von-neumann architecture describing majority computing systems organized. main bottleneck current systems concerns data movement implies additional bandwidth power latency requirements. cpus typically optimized serial tasks mitigating negative eﬀects architecture deep cache hierarchy losing parallelism involved. gpus brain-like layout equal processing units private memory actually operate parallel without colliding. however problem moving data still exists either inside gpu. problem persists. fact quite easy show virtually impossible achieve peak performance processors data cannot fast enough. moreover data transfers major energy consumption factors parallel gpu-like devices therefore radical approach needed order improve performance signiﬁcantly. von-neumann architecture needs changed memory compute. hardware allows functionality already appeared concept in-place processing assumes however diﬀerent approach also needed thinking algorithms. process communication-aware algorithm design already started advent multi-core cpus gpus fpgas. next step design communication-less algorithms ongoing eﬀort supercomputing community noticed signiﬁcant progress made without reducing information transfer-overhead. given low-level properties learning algorithm overall goal learning learning path look like? kind behavior would considered stepping stone towards machine intelligence describe precise way? even basic question means machine algorithm intelligent needs clariﬁcation. according some goal-directed behavior considered essence intelligence however implies necessary sufﬁcient condition intelligent behavior rationality paper questions statement. humans often rational. creativity fall deﬁnition risk-taking might rational essential innovation. therefore appealing theories universal intelligence broader priors theory curiosity creativity beauty described schmidhuber previous section introduces problems arise objective based learning chinese room argument algorithm attempts inputs outputs without motivation learn anything beyond task given. intelligent algorithm among names) able reveal hidden knowledge might even discoverable humans. section describes functional ingredients learning procedure would violate generality assumption. learning likened formal information-theory based concept information compression. assuming goal build compact useful representations environment) interpretation relates representation learning analogy building compression scheme neocortex. looking task considering general artiﬁcial intelligence general purpose compressor able discover probability distribution source however free lunch theorem states completely generalpurpose learning algorithm exist. words given compressor exists data distribution perform poorly. implies must exist restrictions class problems learning system address well. previous section already mentioned them fortunately general plausible smoothness prior depth prior complete list sensible assumptions). whereas smoothness prior considered type spatial coherence assumption world mostly predictable corresponds temporal generally spatiotemporal coherence. probably important ingredient general-purpose learning procedure. assumption states things close time close space vice versa. purely spatial analogy huge image space tiny fraction possible real images true spatiotemporal patterns. assumption sequence spatial patterns coherent restricts spectrum future spatial states likely. occam’s razor rule principle state simple solutions favored complex ones. therefore learning better representations goal itself even without objective. assumed task given priori best observe learn predict. ﬁrst working examples principle history compression employed recurrent architecture proposed schidmuber ability predict equivalent understanding since given moment cause prediction could inferred given state context. therefore learning predict general requirement intelligent behavior. fact postulated brain constantly predict future states compare predictions sensory inputs readaccordingly. might seem equivalent backpropagating error entire network however biological perspective prediction/expectation readjustment neurons likely operating locally. scientists demonstrated brain predicts consequences movements based next. ﬁndings implications understanding human attention applications robotics. despite fact that practice experienced perceived twice human brains able form stable representation abstract concepts make accurate predictions despite changes context. mental representations help explain rapid movements known saccades. eyes move rapidly approximately three times second order capture visual information. jump image falls onto retina. however experience quicklychanging sequence images instead stable image brain uses mechanism order redirect attention since approximately retina provides sharp image operation extensively researched neuroscientiﬁc perspective provides visible brain activities sensorimotor connections needed order know changes image result internal movement not. hypothesis basic repeating functional unit neocortex sensorimotor model every part brain performs sensory motor processing extent. complex cells visual cortex invariant small changes inputs patterns might mapped purely spatially represent spatiotemporal patterns experiments support claim showing similar mechanism operating diﬀerent type sensory inputs thinking motor command abstract possible show order disambiguate multiple predictions. needs inject additional context. paper assumes predictions associated uncertainty bayesian approach instead assuming single point prediction distribution highly multimodal. additional context equivalent integrating evidence makes predictions speciﬁc. need abstract spatiotemporal concepts illustrated simple example. given images fig. obvious classiﬁcation based purely spatial aspect pattern inadequate. much natural grouping objects function requires ability imagine whether particular object used certain considerations apply objects chairs. much natural learn concepts spatiotemporal ideas rather predominantly depend spatial appearance. considering ability imagine/dream/hallucinate widespread implementation sensorimotor functionality brain surprising. concept manipulating compact spatiotemporal thought might necessary reasoning perspective transfer learning majority analogies make temporal nature. importance learning transformations real-world recognized research community still needs attention. fig. face example spatiotemporal concept micro-saccades sequences low-level spatial patterns fovea pooled temporally mid-level concept nose; macro-saccades task-oriented movement moving nose eyes mouth last functional component postulated paper continuous loop bottom-up predictions top-down context. hypothesis interconnectedness enables perceptual completion higher layers make hypotheses inferences coming lower layers predictions iteratively reﬁned based hypotheses. likened working memory theory non-episodic memories held analogy expectation maximization learning procedure commonly used boltzmann machines samples obtained iteratively alternating unit activations connected layers real-world analogy process solving crossword sudoku puzzle ﬁlling missing words sentence. problems require iterative solution reﬁnement procedure. partial support work provided defense advanced research projects agency would like thank members machine intelligence group research numenta suggestions many interesting discussions.", "year": 2016}