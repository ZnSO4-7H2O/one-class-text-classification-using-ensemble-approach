{"title": "Size-Independent Sample Complexity of Neural Networks", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "We study the sample complexity of learning neural networks, by providing new bounds on their Rademacher complexity assuming norm constraints on the parameter matrix of each layer. Compared to previous work, these complexity bounds have improved dependence on the network depth, and under some additional assumptions, are fully independent of the network size (both depth and width). These results are derived using some novel techniques, which may be of independent interest.", "text": "study sample complexity learning neural networks providing bounds rademacher complexity assuming norm constraints parameter matrix layer. compared previous work complexity bounds improved dependence network depth additional assumptions fully independent network size results derived using novel techniques independent interest. major challenges involving neural networks explaining ability generalize well even large potential overﬁt training data learning theory teaches must inductive bias constrains learn networks speciﬁc conﬁgurations however understanding nature inductive bias still largely open problem. useful starting point consider much restricted class linear predictors particular assuming distribution almost surely well-known generalization error given training examples scales completely independent dimension thus natural whether general case neural networks obtain similar sizeindependent results appropriate norm constraints parameters. also natural question considering size modern neural networks often much larger number training examples. classical results sample complexity neural networks satisfy desideratum strong explicit dependence network size. particular trivial number parameters exceeds number training examples. recently several works aiming improving sample complexity bounds assuming various norm constraints parameter matrices. example neyshabur rademacher complexity tools show parameter matrices layers frobenius norms upperbounded respectively suitable assumptions activation functions generalization error scales neyshabur also showed dependence sometimes avoided anti-symmetric activations unfortunately non-trivial assumption satisﬁed common activations relu. bartlett covering numbers argument show bound scaling summarize although bounds logarithmic dependence network width aware bound literature avoids strong dependence depth even various norms controlled. depth dependency avoided assuming norms sufﬁciently constrained? argue cases must true. this return well-understood case linear predictors consider generalized linear predictors form pletely independent network depth well dimension argue satisfactory sample complexity analysis similar independence properties applied class. simplest choice spectral norm however formally show sec. spectral norm alone weak size-independent bounds even network depth small. instead show controlling suitable norms indeed lead better depth dependence even fully size-independent bounds improving earlier works. speciﬁcally make following contributions avoided applying contraction slightly different object become standard since work bartlett mendelson example networks parameter matrices frobenius norm bound improved bounds assuming control schatten norm parameter matrices observation utilize prediction function computed networks approximated composition shallow network univariate lipschitz functions. example assuming frobenius norms layers bounded improve /m/) best knowledge ﬁrst explicit bound standard neural networks fully size-independent assuming suitable norm constraints. moreover captures depthindependent sample complexity behavior network class discussed earlier. also apply technique depth-independent version bound specifmp bound schatten p-norm largest possible euclidean norm network’s output input vectors norm again upper bounding ﬁrst argument bound independent depth assuming norms suitably constrained. bound terms norm dependencies moreover establishes controlling spectral norm alone cannot lead bounds independent size network. finally bound shows dependence products norms across layers generally inevitable. example refers spectral norm refers frobenius norm refers trace norm. case spectral norm drop subscript kwk. also order follow standard convention kwkf denote frobenius norm. finally given matrix parameter matrix ﬁxed lipschitz continuous function euclidean spaces satisfying above denote depth network width deﬁned maximal column dimensions without loss generality assume lipschitz constant element-wise written application univariate function coordinate input positive-homogeneous element-wise satisﬁes important example relu networks every corresponds applying relu function max{ element. simplify notation sub-network composed layers rademacher complexity. results paper focus rademacher complexity standard tool control uniform convergence given classes predictors shalev-shwartz ben-david details). formally rademacher complexity respect classes neural networks various norm constraints. using standard arguments bounds converted bounds generalization error assuming access sample i.i.d. training examples. example consider class depth-d relu real-valued neural networks layer’s parameter matrix frobenius norm using straightforward manipulations possible show deﬁnition equals also exponential factor follows factor turn follows applying rademacher contraction principle function. unfortunately factor generally unavoidable following theorem arbitrary parameter. perform peeling argument similar before resulting multiplicative factor every peeling step. crucially factors accumulate inside factor result contains factor appropriate tuning reduced formalization argument depends matrix norm using begin case frobenius norm. technical condition argument work perform peeling inside function. captured following lemma bounded neural networks clean factor replaced theorem class real-valued networks depth domain parameter matrix frobenius norm activation functions satisfying lemma remark note simplicity bound thm. stated real-valued networks argument easily carries vector-valued networks composed real-valued lipschitz loss function. case uses variant lemma peel losses proceed manner proof thm. omit precise details brevity. setting also studied neyshabur -norm weights neuron case derive variant lemma fact require network bounded). positive-homogeneity activation function proofs theorem well lemma appear sec. constructions used results section function together inverse depth dependencies scaling thus might tempting improve depth dependence using functions increases subi= ǫixik) difﬁcult section develop general result allows convert depth-dependent bound rademacher complexity neural networks depth-independent assuming schatten denotes element-wise product. therefore would like network compute non-trivial function clearly need bounded away zero still satisfying constraint kwjk fact satisfy requirements simultaneously close -sparse unit vector implies matrices must close rank-. must least parameter matrix rank-. therefore replace parameter matrix appropriate rank- matrix function computed network change much. captured following theorem moreover norm constraints imply latter function lipschitz. therefore class networks considering subset class depth-r networks composed univariate lipschitz functions. fortunately given class bounded complexity effectively bound rademacher complexity composition univariate lipschitz functions formalized following theorem. similar networks ﬁrst parameter matrices rank-. thm. bound complexity turn using rademacher complexity depth-r networks. crucially resulting bound explicit dependence original depth parameter formally following theorem main result section remark parameters divide norm terms thm. closely related notion margin. indeed consider binary multi-class classiﬁcation bounds w.r.t. -lipschitz losses converted bound misclassiﬁcation error rate terms average γ-margin error training data discussion). also viewed maximal margin attainable input domain. section exemplify thm. used obtain depth-independent bounds sample complexity various classes neural networks. general technique follows first prove corollary class depth-d neural networks parameter matrix satisﬁes kwjkf -lipschitz positive-homogeneous element-wise activation functions. assuming loss function satisfy conditions thm. bound completely independent either width depth networks. words possible make bound smaller ﬁxed sample size independent network’s size long bounded. hand bound corollary also bounded next apply thm. results bartlett discussed introduction provide depth-dependent bound using different norms. speciﬁcally obtain following intermediary result deriving generalization bound corollary class depth-d width-h networks -lipschitz positive-homogeneous element-wise activation functions. assuming loss function satisfy conditions thm. constraints correspond frobenius norm) ignore lower-order logarithmic factors bound scaling theorem class depth-d width-h neural networks parameter matrix satisﬁes kwjkp schatten p-norm exists choice -lipschitz loss data points respect probabilistic guarantees neural network bound scaling appropriately complexity compl particular network. note post-hoc guarantees also stated context previous sample complexity bounds neural networks neyshabur achieved instance union bound over doubling scale complexity. refer proof margin bound koltchinskii panchenko example technique. proving results sec. element observation appropriate norm constraints neural network must layer parameter matrix close rank- therefore network viewed composition shallower network univariate lipschitz function. fact generalized whenever network parameter matrix close rank-k view composition shallow network lipschitz function although develop idea paper observation might useful analyzing types neural network classes. functions studying complexity lipschitz functions domain easily veriﬁed setting consider class depth-d networks parameter matrix spectral -lipschitz. using well-known estimates loss assumed bound dependence input dimension hand dependence network depth matrix norm spectral norm. again discussed previous subsection also possible bound post-hoc guarantees without constraining lipschitz parameter learned network advance. proof. denote decomposition diag choose usv⊤ singular vectors values ﬁrst inequalities lemma easy verify. third inequality using unitarial invariance spectral norm given construct piecewise-linear function follows input point nearest rest constructed linear interpolation points easily veriﬁed supx∈ moreover note neighboring points points must neighboring same. therefore function parameterized vector form +}|ux|− speciﬁes whether goes down remains linear segments. number functions |ux|− r/ǫ+ therefore r/ǫ+. recalling majorizes note since ksv⊤k kwrk ksv⊤kp kwrkp former function contained deﬁned theorem; whereas latter function lipschitz constant j=r+ kwjk j=r+ maps input ﬁxed output therefore obtain contained composition -lipschitz functions.", "year": 2017}