{"title": "Minimax Regret Bounds for Reinforcement Learning", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We consider the problem of provably optimal exploration in reinforcement learning for finite horizon MDPs. We show that an optimistic modification to value iteration achieves a regret bound of $\\tilde{O}( \\sqrt{HSAT} + H^2S^2A+H\\sqrt{T})$ where $H$ is the time horizon, $S$ the number of states, $A$ the number of actions and $T$ the number of time-steps. This result improves over the best previous known bound $\\tilde{O}(HS \\sqrt{AT})$ achieved by the UCRL2 algorithm of Jaksch et al., 2010. The key significance of our new results is that when $T\\geq H^3S^3A$ and $SA\\geq H$, it leads to a regret of $\\tilde{O}(\\sqrt{HSAT})$ that matches the established lower bound of $\\Omega(\\sqrt{HSAT})$ up to a logarithmic factor. Our analysis contains two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in $S$), and we define Bernstein-based \"exploration bonuses\" that use the empirical variance of the estimated values at the next states (to improve scaling in $H$).", "text": "show optimistic modiﬁcation value iteration achieves regret bound result improves best previous known bound achieved ucrl algorithm jaksch signiﬁcance results leads regret matches established lower bound logarithmic factor. analysis contains insights. careful application concentration inequalities optimal value function whole rather transitions probabilities deﬁne bernstein-based \"exploration bonuses\" empirical variance estimated values next states consider reinforcement learning problem agent interacting environment order maximize cumulative rewards time model environment markov decision process whose transition dynamics unknown agent. agent interacts environment observes states actions rewards generated system dynamics. leads fundamental trade agent explore poorly-understood states actions gain information improve future performance exploit knowledge optimize short-run rewards. common approach learning problem separate process estimation optimization. paradigm point estimates unknown quantities used place unknown parameters plan made respect estimates. naive optimization respect point estimates lead premature exploitation never learn optimal policy. dithering approaches exploration address failing random action selection. however exploration directed resultant algorithms take exponentially long learn order learn efﬁciently necessary agent prioritizes potentially informative states actions. this important agent maintains notion uncertainty. sense given prior belief optimal solution exploration/exploitation dilemma given dynamic programming extended bayesian belief state however computational demands method become intractable even small problems ﬁnite approximations arbitrarily poor combat failings majority provably efﬁcient learning algorithms employ heuristic principle known optimism face uncertainty algorithms state action afforded optimism imagined value high statistically plausible. agent chooses policy optimistic view world. allows efﬁcient exploration since poorly-understood states actions afforded higher optimistic bonus. agent resolves uncertainty effects optimism reduce agent’s policy approach optimality. almost reinforcement learning algorithms polynomial bounds sample complexity employ optimism guide exploration alternative principle motivated thompson sampling emerged practical competitor optimism. algorithm posterior sampling reinforcement learning maintains posterior distribution mdps episode interaction follows policy optimal single random sample paper present conceptually simple computationally efﬁcient approach optimistic reinforcement learning ﬁnite-horizon mdps report results frequentist regret. algorithm upper conﬁdence bound value iteration similar model-based interval estimation delicate alteration form exploration bonus. particular ucbvi replaces universal scalar bonus mbie-eb empirical variance next-state value function state-action pair. alteration matches established lower bound problem logarithmic factors positive result ﬁrst kind helps address ongoing question fundamental lower bounds reinforcement learning ﬁnite horizon mdps reﬁned analysis contains ingredients high level work addresses noted shortcomings existing algorithms terms dependency demonstrates possible design simple computationally efﬁcient optimistic algorithm simultaneously address careful mention current limitations work provide fruitful ground future research. first study setting episodic ﬁnite horizon mdps general setting weakly communicating systems also assume horizon length mdp. quintuple states actions state transition distribution function real-valued function state-action space horizon length episode. denote probability distribution next state episodes. interaction agent environment every episode follows starting chosen environment agent interacts environment steps following sequence actions chosen observes sequence next-states rewards episode. initial state change arbitrarily episode next. also notation norm fact lower bound general setting weakly communicating mdps doesn’t directly apply setting. similar approach used prove lower bound order ﬁnite-horizon mdps already used denotes value function every step state corresponds expected rewards received policy starting assumption exists always policy attains best possible values deﬁne optimal value function def= supπ policy every step deﬁnes state transition kernel reward function linear operators def= optimality bellman operator def= maxa∈a. ease exposition remove dependence e.g. writing control policy followed learner episode thus regret measures expected loss following policy produced learner instead optimal policy. goal learner follow sequence policies regret small possible. section introduce variants algorithm investigate paper. call algorithm upper conﬁdence bound value iteration ucbvi extension value iteration guarantees resultant value function upper conﬁdence bound optimal value function. algorithm related model based interval estimation algorithm contribution precise design upper conﬁdence sets analysis lead tight regret bounds. ucbvi described algorithm calls ucb-q-values returns ucbs q-values computed value iteration using empirical bellman operator added conﬁdence bonus bonus. consider variants ucbvi depending structure bonus present algorithms ﬁrst ucbvi-ch based upon chernoff-hoeffding’s concentration inequality considers ucbvi bonus bonus_. bonus_ simple bound assumes values bounded like ucrl ucfh instead directly maintains conﬁdence intervals optimal value function. crucial given transition dynamics s-dimensional whereas q-value function onedimensional. however loose form given ucbvi-ch look value function next state consider bounded however much better bounds obtained looking variance next state values. main result relies upon ucbvi bonus bonus_ refer ucbvi-bf relies bernstein-freedman’s concentration inequalities build conﬁdence set. ucbvi-bf builds upon intuition ucbvi-ch also incorporates variance-dependent exploration bonus. leads tighter compared ucbvi-bf bonus built empirical variance estimated next values. idea knowledge optimal value could build tight conﬁdence bounds using variance optimal value function next state place loose bound since however unknown surrogate empirical variance estimated values. data gathered variance estimate converge variance need make sure estimates optimistic times. achieved adding additional bonus guarantees upper bound variance using iterative -bellman-typelaw total variance next-state variances bounded variance h-steps return. thus size bonuses built best known bound jaksch main intuition improved s-dependence bound estimation error next-state value function directly instead transition probabilities. insight apply concentration inequalities bound estimation errors exploration bonuses terms variance next state. fact variances bounded variance return shows estimation errors accumulate instead linearly thus implying improved h-dependence. theorems guarantee statistical efﬁciency ucbvi. addition ucbvi-ch ucbvi-bf computationally tractable. episode algorithms perform optimistic value iteration computational cost order solving known mdp. fact computational cost algorithms reduced selectively recomputing ucbvi sufﬁciently many observations. technique common short paper focus setting ﬁnite horizon mdps. comparison previous optimistic approaches exploration ucrl provide bounds general setting weakly communicating mdps value function. using backward induction prove holds high probability simplify notations sketch proof make numerical constants explicit instead denote numerical constant vary line line. exact values constants provided full proof. also make simpliﬁed notations using represent logarithmic term lnhsat /δ). bound terms. easy check thatpkh andpkh ¯ǫkh sums martingale differences bounded using azuma’s inequality lead regret without dependence size state action space. leading terms regret bound comes exploration bonusespkh estimation errorspkh ekh. step bounding martingalespkh andpkh ¯ǫkh. using azuma’s inequality deduce backward induction straightforward. indeed vkh+ upper bounds necessarily case empirical variance vkh+ upper bound empirical variance ∗h+. however prove induction vkh+ sufﬁciently close guarantee variance terms sufﬁciently close additional bonus make sure still upper-bound precisely deﬁne indices proof relies derivations used proving differences replaced n′kh+ number times state reached time episode additional factor comes fact episode n′kh+ tick once whereas terms expected variances next-state values using recursively total variance conclude quantity nothing variance returns. step detailed now. simplicity exposition sketch neglect second order terms. paper reﬁne familiar concept optimism face uncertainty. contribution design analysis algorithm ucbvi-bf addresses shortcomings existing algorithms optimistic exploration ﬁnite mdps. first apply concentration value whole rather transition estimates leads reduction next apply recursive total variance couple estimates across episode rather time step individually leads reduction logarithmic factors. remains open problem whether match lower bound using bound non-trivial. although push many technical details appendix paper also makes several contributions terms analytical tools useful subsequent work. particular believe construct exploration bonus conﬁdence intervals ucbvi-ch novel literature also constructive approach proof ucbvi-ch bootstraps regret bounds prove vkhs ucbs another analytical contribution paper. authors would like thank marc bellemare wonderful colleagues deepmind many hours discussion insight leading research. also grateful anonymous reviewers helpful comments ﬁxing several mistakes earlier version paper.", "year": 2017}