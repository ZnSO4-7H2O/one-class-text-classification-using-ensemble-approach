{"title": "The emergent algebraic structure of RNNs and embeddings in NLP", "tag": ["cs.CL", "cs.AI", "stat.ML", "97R40"], "abstract": "We examine the algebraic and geometric properties of a uni-directional GRU and word embeddings trained end-to-end on a text classification task. A hyperparameter search over word embedding dimension, GRU hidden dimension, and a linear combination of the GRU outputs is performed. We conclude that words naturally embed themselves in a Lie group and that RNNs form a nonlinear representation of the group. Appealing to these results, we propose a novel class of recurrent-like neural networks and a word embedding scheme.", "text": "examine algebraic geometric properties uni-directional word embeddings trained end-to-end text classiﬁcation task. hyperparameter search word embedding dimension hidden dimension linear combination outputs performed. conclude words naturally embed group rnns form nonlinear representation group. appealing results propose novel class recurrent-like neural networks word embedding scheme. tremendous advances natural language processing enabled novel deep neural network architectures word embeddings. historically convolutional neural network recurrent neural network topologies competed provide state-of-the-art results tasks ranging text classiﬁcation reading comprehension. cnns identify aggregate patterns increasing feature sizes reﬂecting common practice identifying patterns literal idiomatic understanding language; thus adept tasks involving phrase identiﬁcation. rnns instead construct representation sentences successively updating understanding sentence read words appealing formally sequential rule-based construction language. networks display great eﬃcacy certain tasks rnns tend versatile emerged clear victor e.g. language translation typically capable identifying important contextual points attention mechanisms e.g. reading comprehension interest thus turn rnns. rnns nominally solve general problem involving sequential inputs. various speciﬁed tasks specialized constrained implementations tend perform better often improvement simply mitigates exploding/vanishing gradient problem many tasks improvement capable generalizing network’s training task. understanding better certain networks excel certain tasks lead performant networks networks solve problems. advances word embeddings furnished remainder recent progress although possible train word embeddings end-to-end rest network often either prohibitive exploding/vanishing gradients long corpora results poor embeddings rare words embeddings thus typically constructed using powerful heuristically motivated procedures provide pre-trained vectors network trained. rnns themselves understanding better optimal embeddings constructed e.g. end-to-end training provide necessary insight forge better embedding algorithms deployed pre-network training. beyond improving technologies ensuring deep learning advances breakneck pace gaining better understanding systems function crucial allaying public concerns surrounding often inscrutable nature deep neural networks. particularly important rnns since nothing comparable deepdream lucid exists ends goal work fold. first wish understand emergent algebraic structure rnns word embeddings trained end-to-end exhibit. many algebraic structures well understood hints structure would provide perspectives tools deep learning approached. second wish propose novel networks word embedding schemes appealing emergent structure appear. faster reference provides convenient summary intrepretation results outlines class neural network word embedding scheme leveraging results. motivates investigation algebraic structures explains experimental setup. discusses ﬁndings experiments. interprets results motivates proposed network class word embeddings. provides closing remarks discusses followup work gives acknowledgments. make matter notation clear going forward begin referring space words transition analyzing results order consistent notation literature algebraic spaces. embedded words vectors used uni-directional connected dense layer classify account tweets originated. embeddings simple network trained end-to-end avoid imposing artiﬁcial heuristic constraints system. ﬁrst point follows since words embedded continuous space; identity word exists causes trivially hidden state; word inverses exist cause undo action hidden state; successive action using words equivalent action single third word implying multiplicative closure words; words manifestly closed binary action. second point follows given words embed manifold sentences traces paths manifold diﬀerence equation solves bears striking resemble ﬁrst order equation parallel transport t-th hidden state encountered reading sentence conditioned t-th word acting hidden state. since sentences trace path word manifold parallel transport operators representations word manifold take values group must parallel transport hidden states either group base space equipped takes form cells either nested internal memories dependencies extend temporally beyond immediately previous hidden state. particular using sentence generation topic manuscript presently preparation. second propose embedding schemes explicitly embed words elements group. practice embedding schemes would involve representing words constrained matrices optimizing elements subject constraints according loss function constructed invariants matrices applying matrix obtain vectors. prototypical implementation words assumed fundamental representation special orthogonal group conditioned losses sensitive relative actions words subject another manuscript presently preparation. proposals brieﬂy discussed herein focus followup work; focus present work experimental evidence emergent algebraic structure rnns embeddings nlp. first provides function successively updates hidden memory vector characterizing information contained sequence input vectors reads elements sequence. explicitly face value takes form vector space demanding much structure generally places strong constraint network’s behavior would fortuitous structure emerge. generally constrained systems still capable performing required task perform task better least generalize reliably suggestive form rnns assume invites examination determine exist reasonable constraints placed network. highlight suggestiveness form follows represent argument subscript argument treating left action adopting notation second massive exploration hyperparameters presented noted that given word embedding dimension network’s performance seqseq task largely insensitive given algebraic structure generally discrete spaced out. interpreting neurons basis functions output layers elements span functions would expect network’s performance improve admissible dimension representation found addition hidden neurons would simply contribute better learning components proper representation appearing linear combinations neurons contribute minimally improving overall performance. hyperparameter search marginal improvement found hidden dimension suggesting potentially better representation found. groups present candidate consideration since naturally appear variety applications. unitary weight matrices already enjoyed much success mitigating exploding/vanishing gradients problem rnns even constrained explicitly nonlinear representations unitary groups oﬀer competitive results moreover intuitively rnns could plausibly behave group since must learn ignore padding words used square batches training data indicating identity element must exist; existence contractions portmanteaus germanic tradition representing sentences singular words suggest might closed; ability backtrack undo statements suggests language admit natural inverses active controlled forgetting language tied inversion. indeed groups seem reasonably promising. also possible portmanteaus make sense ﬁnite subset pairs words take structure groupoid instead; moreover possible least classiﬁcation tasks information lost successive applications suggesting inverse actually exist leaving either monoid category. also actually admit additional structure additional binary operation rendering ring algebra. whatever structure possesses must additionally continuous since words typically embedded continuous spaces. implies groups semigroups identity algebras plausible algebraic candidates. traditionally words treated categorical objects embedding continuous space computational purposes largely convenience; however relax categorical perspective treat unused word vectors acceptable objects algebraic structure concerned even actively employed language. trained word embeddings uni-directional connected dense layer end-to-end text classiﬁcation scraped tweets using cross-entropy loss function. end-to-end training selected impose heuristic constraints system possible. tweet tokenized using nltk tweettokenizer classiﬁed potential accounts originated. accounts chosen based distinct topics known typically tweet about. tokens occurred fewer times disregarded model. model trained tweets epochs reserved validation testing sets network demonstrated insensitivity initialization hidden state algebraic considerations perform comparative search embedding dimensions hidden dimensions determine impact network’s performance algebraic properties. dimension hyperparameter pair runs increments dedicated ﬁnal energies combined linear layer softmaxing. capitalize universal approximation theorem’s implication neurons serve basis functions i.e. energy function determined basis functions. hidden dimension word embedding dimension hyperparameters scanned over. matrix frozen begin testing emergent algebraic structure. satisfy common requirement stated real hidden states encountered testing data saved randomly test existence words satisfy conditions vectors searched that inserted minimized ratio euclidean norms diﬀerence searched hidden vector correct hidden vector. concreteness loss function random learned word vectors hidden state model parameter trained minimize loss. refer eqs. axiomatic losses. worth noting non-zero hidden state initialization chosen prevent denominators vanishing initial state selected candidate eqs.&. reported losses average across examined. optimization losses eqs. performed epochs. associated condition satisﬁed must exist word vector suﬃciently minimizes axiomatic losses. indeed case attempts learn representation algebraic structure neuron serves basis function necessary neuron individually satisﬁes constraints. clarity recall second motivating point addition neurons representation found simply contributes learning representation better. instead linear combination neurons must. consider possibility task-performant hyperparameter pair capricious pairs. target dimension linear combination refer latent dimension could generally smaller hidden dimension compute linear combination neurons outputs right-multiplied reported results averaged value reduce ﬂuctuations loss diﬀering local minima. trained optimize various combinations algebraic axioms results optimizing single condition frozen axiomatic tests; commutative closure condition however given separate linear combination matrix reasons discussed later. finally geometric structure resulting word vectors explored naively using euclidean metric. sentences trace paths word embedding space natural consider relationships word vectors vectors tangent sentences’ paths. explicitly angles distances computed determine word vectors geometrically distributed. intuitively similar words expected aﬀect hidden states similarly. test this gain insight possible algebraic interpretations word embeddings ratio euclidean norm diﬀerence hidden states produced acting hidden state diﬀerent words euclidean norm original there concern diﬀering treatment output input hidden state since former projected lower dimension latter not. linear combination matrix trained parallel itself would degeneracy product weight matrices successive updates dense layer weight matrices ﬁnal update eﬀect linear combination would absorbed weight matrices. since considering linear combinations freezing weight matrices unnecessary consider role linear combination matrix would play input hidden states necessary output itself. hidden state computed function popular cosine similarity metric distance embeddings. fractional diﬀerence cosine similarity word distance computed performed hyperparameter tuning word embedding dimension hidden dimension optimize classiﬁer’s accuracy. dimension increments contour plot hyperparameter search shown fig.. dimensions. thus training embeddings end-to-end clearly advantageous short text classiﬁcation. worth noting training end-to-end viable primarily short length tweets; longer documents exploding/vanishing gradients typically prohibits training. average fisher information hyperparameter dimension searched region computed determine relative sensitivities model hyperparameters. fisher informamodel performance generally behaved expected across hyperparameter search. indeed higher embedding hidden dimensions tended yield better results. given time resource constraints results averaged many search attempts. consequently unclear pockets entropy indicative anything deeper merely incidental ﬂuctuations. would worthwhile revisit search future work. seven tests conducted hyperparameter pair explore emergent algebraic structure word embeddings exhibit. speciﬁcally tests searched existence identity element existence inverse word word multiplicative closure arbitrary pairs words commutative closure arbitrary pairs words multiplicative closure pairs words co-occur within tweet multiplicative closure sequences words appear tweets existence inverse sequences words appear tweets. tests optimized axiomatic losses deﬁned eqs.. tests broken roughly classes arbitrary solitary words pairs words pairs sequences words co-occurring within tweet. results class shown fig.; results class shown fig.. figure axiomatic error function word embedding hidden dimensions. existence identity element multiple hidden states. note scale. existence inverse word every word acting random hidden states. linear scale. existence third ‘eﬀective’ word performing action randomly chosen words succession acting random states. linear scale. existence third word performing action commutation randomly chosen words acting random states. nonlinear scale. figure axiomatic error function word embedding hidden dimensions. existence third word performing action ordered words comprising tweet acting initial state. linear scale. existence word reverses action ordered words comprising tweet acted initial state. nonlinear scale. existence third word performing action random words co-occurring within tweet acting random states. linear scale. existence inverse word every word acting random hidden states. fig. simply provided side-by-side comparison. identity condition clearly satisﬁed virtually embedding hidden dimensions possible exceptions small embedding dimensions large hidden dimensions. although explicitly check likely even possible exceptions would viable linear combination search. arbitrary pairs words evidently closed multiplication without performing linear combination search minimum error across dimensions. moreover large entropy across search suggest fundamentally interesting notable behavior connections embedding dimension hidden dimension closure property. arbitrary pairs words badly closed commutation unfathomable even linear combination search could rescue property. might consider possibility speciﬁc pairs words might still closed commutation exceptional error handful words commute outright since would push loss near-vanishing denominator. previously stated hidden states initialized zero states separate experiments conﬁrm zero state orbit non-zero state would hope negate vanishing denominator. thus concern principle possible. however explicitly removing examples exploding denominators loss performing linear combination searches still resulted unacceptable errors possibility actually realized. explicitly check closure class tests since class subset class ﬂagrant violation condition would possible successful closure class averaged class results. even though commutative closure satisﬁed curious note error exhibited mostly well-behaved stratiﬁcation. interesting class result arbitrary inverse. embedding dimensions suﬃciently large compared hidden dimension inverses clearly existed even without linear combination search. even remarkable well-behaved stratiﬁcation axiomatic error implying clear relationship embedding dimension hidden dimension emergent algebraic structure model. unreasonable expect inverse condition trivially satisﬁed linear combination search broad range hyperparameter pairs. behavior inverse property immediately apparent class results. stratiﬁcation error virtually identical tested properties acceptable errors suﬃciently large embedding dimensions given hidden dimensions even without linear combination search. optimal hyperparameter pair single pass tuning resulted model accuracy statistically signiﬁcant result since multiple searches averaged random variations validation sets optimization running diﬀering local minima lead ﬂuctuations test accuracies. however selection provided reasonable injection point investigate algebraic properties linear combinations output gru’s neurons. comparison also considered tests linear combination matrix trained assist optimizing composite inverse. learned applied output hidden states properties except commutative closure given linear combination matrix determine existed would render emergent property. combination trained optimize single condition because exists optimal linear combination condition indeed exists underlying algebraic structure incorporating conditions linear combination would optimal conditions. four conditions examined fig. clearly satisﬁed latent dimensions. also reach minimum error region. composite closure intra-sentence closure arbitrary arbitrary multiplicative closure commutative closure highly anti-correlated conditions badly violated. worth noting results fig. remove commutative pairs words error scale error linear combination search virtually identical separately observed commutative pairs removed. also exhibit discussing linear combination searches selected hyperparameter pairs worthwhile noting retraining network performing linear combination search yield diﬀering results. figs.& show linear combination results retraining model hyperparameter pair diﬀerent network performance qualitatively results mostly same common minimizing region conditions satisﬁed least common minimal region. however minimizing region starkly shifted down became sharper composite closure intra-sentence closure arbitrary inverse. more results mostly same. arbitrary closure error drastically increased still highly anti-correlated mostly monotonic despite erratic ﬂuctuations arbitrary closure error. explore geometric distribution word vectors angles distances random pairs words words global average word vector random pairs co-occurring words words co-occurring word vector average adjacent tangent vectors tangent vectors co-occurring tangent vector average computed. magnitudes average word vectors average co-occurring word vectors average tangent vectors also computed. ﬁgures follow generally three categories word vectors explored random word vectors pool word vectors co-occurring word vectors tangent vectors figure frequency distribution norm average vectors. instance norm average word vectors hence singular spike distribution. vector distributions average diﬀerent individual tweets. tangent vectors average word vectors comparable norms. non-zero value average word vector indicates words perfectly distribute throughout space. non-zero value average tangent vectors indicates tweets general progress preferred direction relative origin embedding space; albeit since magnitudes smallest categories investigated preference slight. norm average co-occurring word vectors signiﬁcantly larger norms others categories vectors indicating words tweets typically occupy strongly preferred region embedding space cosine similarity pairs random words co-occurring words shared common distribution albeit notable spikes speciﬁc angles prominent spike co-occurring pairs. prominent spike could potentially explained re-occurrence punctuation within tweets indicate anything importance; potential origin smaller spikes throughout co-occurring distribution unclear. generally pairs strongly preferred orthogonal unsurprising given recent investigations eﬃcacy orthogonal embeddings adjacent pairs tangent vectors however exhibited strong preference obtuse relative words tended slightly positive cosine similarity global average indicative fact words spread uniformly. co-occurring words tended form acute angles respect co-occurring average. meanwhile tangent vectors strongly preferred orthogonal average. strong negative cosine similarity adjacent tangent vectors strong positive cosine similarity words co-occurring average indicate co-occurring words tended form grid structure cone. adjacent words tended perpendicular positive span word basis vectors. course strictly adhered preferred geometry apparent. distributions random pairs words co-occurring words virtually identical plots indicating variation attributable relative orientations vectors rather distances them. plots conﬁrm similar words similar actions hidden states are. strongly linear bi-modal dependence fractional diﬀerence distance words indicates word distance stronger predictor relative meaning words popular cosine similarity. identity inverse closure properties co-occurring words satisﬁed related algebraic structure. since closure satisﬁed arbitrary pairs words essentially possible explanations observed structure either case words considered elements group. since groups also manifolds word vector components interpreted coordinates group. traditionally groups practically handled considering algebra generates them exp. components vectors typically taken coordinates group. hints connection word vectors connection made clear experiments. furthermore rnns learn nonlinear representation group latent space spanned hidden layer. since sentences form paths embedding group it’s reasonable attempt form precise interpretation action rnns. begin considering explicit action hidden states path traversed takes form diﬀerence equation. particular looks similar ﬁnite form diﬀerential equation governing nonlinear parallel transport along path principal ﬁbre bundle base space group exp. tangent vector vector transported second option general requires candidate compelling connect also challenging since generally parallel transport operators taking values group closed. path itself closure would guaranteed since parallel transport operator would element co-occurring subgroup closure arises equivalence class paths. networks solve recurrent-like. updates hidden state generally depend states beyond immediately preceding one; often dependence captured evolving phase space hidden states rather sequences hidden states themselves. latter results nested structure recurrent-like cell similar structure proposed applications currently explored. particular additional structure exists rnns parallel transport states along paths word embedding group geodesics emerge natural candidate sentence paths thus sentence generation could potentially modeled using geodesic equation nonlinear adjoint representation embeddings trained end-to-end work provided highly performant results. unfortunately training embeddings end-tasks longer documents challenging resulting embeddings often poor rare words. however would seem constructing pre-trained word embeddings leveraging emergent group structure observed herein could provide competitive results without need end-to-end training. intuitively unsurprising groups appear candidate construct word embeddings. evidently proximity words governed actions hidden states groups often natural language describe actions vectors. since groups generally non-commutative embedding words group additionally capture ordercontext-dependence. groups also generated algebras group algebra another group recursively form hierarchical tower. arrangement explicitly capture hierarchical structure language expected exhibit. e.g. group structure ﬁrst interpretation given practice group embedding schemes would involve representing words constrained matrices optimizing elements subject constraints according loss function constructed invariants matrices applying matrix obtain vectors. prototypical implementation dubbed liegr words assumed fundamental representation special orthogonal group conditioned losses sensitive relative actions words subject another manuscript presently preparation. results presented herein oﬀer insight rnns word embeddings naturally tend structure text classiﬁcation. beyond elucidating inner machinations deep results used help construct novel network architectures embeddings. however much immediate followup work worth pursuing. particular uniqueness identities inverses multiplicative closure addressed work critical better understand observed emergent algebraic structure. cause hyperparameter stratiﬁcation error complete exploration commutative closure remains outstanding. additionally cause breakdown common optimal latent dimension embedding dimension unclear bi-model linear relationship action words hidden states euclidean distance end-to-end word embeddings invites much investigation. less critical still curious inquiry additive relationship words e.g. king woman queen preserved replaced something new? light group structure words trained tasks seem exhibit would surprising relationship baker-campbell-hausdorﬀ formula applied. author would like thank robin tully john cantrell mark laczin providing useful discussions linguistic mathematical natures work unfolded. robin particular provided essential feedback throughout work helped explore potential free groups computational linguistics outset. john furnished many essential conversations ensured scientiﬁc mathematical consistency experiments provided useful insights results. mark prompted investigation potential emergent monoid structures since appear frequently state machines.", "year": 2018}