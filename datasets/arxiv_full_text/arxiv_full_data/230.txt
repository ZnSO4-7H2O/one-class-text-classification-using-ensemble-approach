{"title": "An Architecture Combining Convolutional Neural Network (CNN) and Support  Vector Machine (SVM) for Image Classification", "tag": ["cs.CV", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Convolutional neural networks (CNNs) are similar to \"ordinary\" neural networks in the sense that they are made up of hidden layers consisting of neurons with \"learnable\" parameters. These neurons receive inputs, performs a dot product, and then follows it with a non-linearity. The whole network expresses the mapping between raw image pixels and their class scores. Conventionally, the Softmax function is the classifier used at the last layer of this network. However, there have been studies (Alalshekmubarak and Smith, 2013; Agarap, 2017; Tang, 2013) conducted to challenge this norm. The cited studies introduce the usage of linear support vector machine (SVM) in an artificial neural network architecture. This project is yet another take on the subject, and is inspired by (Tang, 2013). Empirical data has shown that the CNN-SVM model was able to achieve a test accuracy of ~99.04% using the MNIST dataset (LeCun, Cortes, and Burges, 2010). On the other hand, the CNN-Softmax was able to achieve a test accuracy of ~99.23% using the same dataset. Both models were also tested on the recently-published Fashion-MNIST dataset (Xiao, Rasul, and Vollgraf, 2017), which is suppose to be a more difficult image classification dataset than MNIST (Zalandoresearch, 2017). This proved to be the case as CNN-SVM reached a test accuracy of ~90.72%, while the CNN-Softmax reached a test accuracy of ~91.86%. The said results may be improved if data preprocessing techniques were employed on the datasets, and if the base CNN model was a relatively more sophisticated than the one used in this study.", "text": "claimed artificial neural network architecture produces relatively better results conventional softmax function. course drawback approach restriction binary classification. aims determine optimal hyperplane separating classes dataset multinomial case seemingly ignored. multinomial classification case becomes one-versus-all positive class represents class highest score rest represent negative class. paper emulate architecture proposed combines convolutional neural network linear image classification. however employed study simple -convolutional layer pooling model contrast relatively sophisticated model preprocessing dataset mnist established standard handwritten digit classification dataset widely used benchmarking deep learning models. -class classification problem training examples test cases grayscale. however argued mnist dataset easy overused represent modern tasks. hence proposed fashion-mnist dataset. said dataset consists zalando’s article images distribution number classes color profile mnist. abstract convolutional neural networks similar ordinary neural networks sense made hidden layers consisting neurons learnable parameters. neurons receive inputs performs product follows non-linearity. whole network expresses mapping image pixels class scores. conventionally softmax function classifier used last layer network. however studies conducted challenge norm. cited studies introduce usage linear support vector machine artificial neural network architecture. project another take subject inspired empirical data shown cnn-svm model able achieve test accuracy using mnist dataset. hand cnn-softmax able achieve test accuracy using dataset. models also tested recently-published fashion-mnist dataset suppose difficult image classification dataset mnist. proved case cnn-svm reached test accuracy cnn-softmax reached test accuracy ≈.%. said results improved data preprocessing techniques employed datasets base model relatively sophisticated used study. keywords artificial intelligence; artificial neural networks; classification; image classification; machine learning; mnist dataset; softmax; supervised learning; support vector machine introduction number studies involving deep learning approaches claimed state-of-the-art performances considerable number tasks. include limited image classification natural language processing speech recognition text classification. models used said tasks employ softmax function classification layer. relu max) pool size stride conv size filters stride relu max) pool size stride hidden neurons dropout output classes layer instead conventional softmax function cross entropy function l-svm implemented. output shall translated following case loss computed weight parameters learned using adam. experiments code implementation found https//github.com/afagarap/cnnsvm. experiments study conducted laptop computer intel core i-hq .ghz nvidia geforce gpu. support vector machine support vector machine developed vapnik binary classification. objective find optimal hyperplane separate classes given dataset features manhattan norm penalty parameter actual label predictor function. known l-svm standard hinge loss. differentiable counterpart l-svm provides stable results. convolutional neural network convolutional neural network class deep feed-forward artificial neural networks commonly used computer vision problems image classification. distinction plain multilayer perceptron network usage convolutional layers pooling non-linearities tanh siдmoid relu. convolutional layer consists filter instance intuitively speaking conv layer used slide width height input image compute product input’s region weight learning parameters. turn produce -dimensional activation consists responses filter given regions. consequently pooling layer reduces size input images results conv filter. result number parameters within model also reduced called down-sampling. computation. without such model learn linear mappings. commonly-used activation function days relu function relu commonly-used tanh siдmoid found greatly accelerates convergence stochastic gradient descent compared functions. furthermore compared extensive computation required tanh siдmoid relu implemented simply thresholding matrix values zero figure shows training accuracy cnn-softmax cnnsvm image classification using mnist figure shows training loss. steps models able finish training minutes seconds. cnn-softmax model average training accuracy average training loss cnn-svm model average training accuracy average training loss figure shows training accuracy cnn-softmax cnnsvm image classification using mnist figure shows training loss. steps cnn-softmax model able finish training minutes seconds cnn-svm model able finish training minutes seconds. cnn-softmax model average training accuracy average training loss cnnsvm model average training accuracy average training loss test accuracy mnist dataset corroborate findings cnn-softmax better classification accuracy cnn-svm. result attributed fact data pre-processing mnist dataset. furthermore relatively sophisticated model methodology simple procedure done study. hand test accuracy cnn-softmax model matches findings methodology involve data preprocessing fashion-mnist. conclusion recommendation results study warrants improvement methodology validate review proposed cnn-svm despite contradiction findings quantitatively speaking test accuracies cnn-softmax cnn-svm almost related study. hypothesized data preprocessing relatively sophisticated base model results shall reproduced. acknowledgment expression gratitude yann lecun corinna cortes christopher j.c. burges mnist dataset xiao kashif rasul roland vollgraf fashion-mnist dataset. references martín abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg mané rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda viégas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng. tensorflow large-scale machine learning heterogeneous systems. http//tensorflow.org/ software available tensorflow.org. abien fred agarap. neural network architecture combining gated recurrent unit support vector machine intrusion detection network traffic data. arxiv preprint arxiv. abdulrahman alalshekmubarak leslie smith. novel approach combining recurrent neural network support vector machines time series classification. innovations information technology international conference ieee chorowski dzmitry bahdanau dmitriy serdyuk kyunghyun yoshua bengio. attention-based models speech recognition. advances neural information processing systems. richard hahnloser rahul sarpeshkar misha mahowald rodney douglas sebastian seung. digital selection analogue amplification coexist cortex-inspired silicon circuit. nature alex krizhevsky ilya sutskever geoffrey hinton. imagenet classification deep convolutional neural networks. advances neural information processing systems. tsung-hsien milica gasic nikola mrksic pei-hao david vandyke steve young. semantically conditioned lstm-based natural language generation spoken dialogue systems. arxiv preprint arxiv. xiao kashif rasul roland vollgraf. fashion-mnist novel image dataset benchmarking machine learning algorithms. arxiv preprint arxiv.", "year": 2017}