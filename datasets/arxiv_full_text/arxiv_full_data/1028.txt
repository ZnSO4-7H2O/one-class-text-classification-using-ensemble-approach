{"title": "Tractable Generative Convolutional Arithmetic Circuits", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Casting neural networks in generative frameworks is a highly sought-after endeavor these days. Existing methods, such as Generative Adversarial Networks, capture some of the generative capabilities, but not all. To truly leverage the power of generative models, tractable marginalization is needed, a feature outside the realm of current methods. We present a generative model based on convolutional arithmetic circuits, a variant of convolutional networks that computes high-dimensional functions through tensor decompositions. Our method admits tractable marginalization, combining the expressive power of convolutional networks with all the abilities that may be offered by a generative framework. We focus on the application of classification under missing data, where unknown portions of classified instances are absent at test time. Our model, which theoretically achieves optimal classification, provides state of the art performance when classifying images with missing pixels, as well as promising results when treating speech with occluded samples.", "text": "casting neural networks generative frameworks highly sought-after endeavor days. existing methods generative adversarial networks capture generative capabilities all. truly leverage power generative models tractable marginalization needed feature outside realm current methods. present generative model based convolutional arithmetic circuits variant convolutional networks computes high-dimensional functions tensor decompositions. method admits tractable marginalization combining expressive power convolutional networks abilities offered generative framework. focus application classiﬁcation missing data unknown portions classiﬁed instances absent test time. model theoretically achieves optimal classiﬁcation provides state performance classifying images missing pixels well promising results treating speech occluded samples. many attempts recent years marry generative models neural networks including successful methods generative adversarial networks variational autoencoders nade pixelrnn though methods demonstrated usefulness tasks unclear advantage strictly lies generative nature attribute. broadly combining generative models neural networks could lead methods clear advantage purely discriminative models. class generative models learn also infer discriminative models learn might immediately apparent sole difference leads advantage. jordan question studied respect sample complexity proving cases signiﬁcantly lesser favor generative classiﬁer. wish highlight clear case examining problem classiﬁcation missing data value entries unknown prediction time. settings discriminative classiﬁers typically rely form data imputation i.e. ﬁlling missing values auxiliary method prior prediction. generative classiﬁers hand naturally equipped handle missing values marginalization effectively assessing every possible completion missing values. moreover under mild assumptions method optimal regardless process values become missing application generative models assumes efﬁciently exactly compute known tractable inference well efﬁciently marginalize subset call tractable marginalization. generative models properties speciﬁcally ones mentioned beginning section. known models poses properties limitations. detailed discussion found sec. broad terms known generative models possess following shortcomings insufﬁciently expressive model highdimensional data require explicitly designing dependencies data tractable marginalization. models based neural networks typically solve incapable classical methods e.g. mixture models solve suffer present generative model aimed address shortcomings above. based convolutional arithmetic circuits convacs short variant convnets computes high-dimensional functions tensor decompositions. present required background convacs sec. develop generative interpretation convacs sec. show capable tractable inference well tractable marginalization. sec. formally deﬁne problem classiﬁcation missing data optimality generative classiﬁers task. discuss related works sec. finally present experiments sec. followed concluding summary sec. begin reviewing architecture convolutional arithmetic circuits introduced cohen convacs simply thought regular convnets linear activations product pooling layers instead common non-linear activations average pooling layers. architecture illustrated speciﬁcally point input space network denoted represented n-length sequence s-dimensional vectors called local structures. typically thought image local structure corresponds local patch image. ﬁrst layer network referred representation layer consisting applying representation functions local patch giving rise feature maps. common setting representation functions selected point-wise activation rs×r representation layer reduces standard convolutional layer. following representation layer hidden layers indexed beginning conv operator rl−×× convolutional layer input channels output channels sole exception parameters kernel could spatially unshared following conv layer spatial pooling takes products non-overlapping two-dimensional windows covering output previous layer pooling window size entire spatial dimension reducing output’s shape rl−×× i.e. rl−-dimensional vector. ﬁnal layer maps vector dense linear layer network outputs denoted representing score functions classifying classes through argmaxy shown cohen networks realize functions following form dimensional array speciﬁed indices ranging direct computation intractable storing general tensor consisting entries requires exponential space. however admits low-rank tensor factorization space complexity becomes polynomial. convacs layers following representation layer effectively decompose different network structures correspond different tensor decompositions shallow networks decomposition deep networks hierarchical tucker decomposition immediately follows convacs universal tensor could represented either decomposition given sufﬁciently large ranks network viewpoint correspond sufﬁcient number channels layer. correspondence tensor decompositions gives rise rich theoretical understanding convacs example next section describe generative model intractable likelihood function similar leveraging relations give rise tractable inference algorithm realized convolutional networks. simplest forms tractable generative models mixture models probability distribution deﬁned convex combination mixing components {p}m mixture models also easy learn many able approximate probability distribution given sufﬁcient number components making suitable variety tasks. despite advantages mixture models scale well high dimensional data. propose extend mixture models leveraging fact many high dimensional domains comprised small simple local structures. represent instance deﬁned previous section assume distribution individual local structures efﬁciently modeled mixture model components natural images shown case formally exists hidden variable specifying matching component i-th local structure. thus probability density sampling described generative perspective restriction results latent tree graphical model illustrated hidden layer convac network pair convolution pooling layers corresponds transition between levels tree. speciﬁcally level comprised multiple latent variables spatial position input hidden layer. latent variable input l-th layer takes values i.e. number output channels preceding layer. pooling layers denote parent-child relationship tree latent variables siblings shared parent positioned pooling window. weights convolution layers denote transition matrix parent children i.e. parent latent variable taking values hchild child variables taking values convolutional kernel c-th output channel. discussed sec. enough generative model tractable must also highly expressive. derivation tensor factorization immediate model universal i.e. given sufﬁcient number channels distribution could approximated arbitrarily well. analyses convacs transferred generative counterpart. example universality requires networks exponential size cohen shashua shown networks polynomial size already model type correlations typically found natural images. notably prove app. generative convacs exhibit depth efﬁciency property i.e. deep models exponentially expressive shallower ones. result also instructs design network number channels size pooling windows controls expressivity. stark contrast generative models typically universality proven. conclude starting simple assumptions derived extension mixture models tailored high dimensional data. general model tractable made restriction original convacs. network realizes tractable inference also internal probabilistic meaning ﬁrst glance seems impractical exponential number terms. nevertheless notice bears striking resemblance mixing components mapped representation functions prior probabilities represented coefﬁcients tensor. suggests made tractable simply representing convacs. following discussion sec. essentially wish represent prior probabilities tensor using low-rank factorization. however also ensure represents actual probabilities i.e. non-negative entries every tensor factorization denoted convacs adheres that. address applying non-negative decompositions translates limiting parameters convolutional kernel vast literature relations nonnegative factorizations generative models however important stress apply factorizations derive model analyze expressivity learning parameters generative convacs sharing many traits convnets especially suitable serve classiﬁers. begin introducing class variable model conditional likelihood though possible separate generative models class much efﬁcient leverage relation convnets shared generative convac instead. results single network instead single scalar output representing multiple outputs driven network class. heading predicting class given instance note practice na¨ıve implementation convacs numerically stable. tackled performing computations log-space transforms convacs simnets recently introduced deep learning architecture finally prediction carried returning likely class common setting uniform class priors i.e. corresponds maximal output network. suppose given training ∈)}|s|i= instances labels would like parameters model according maximum likelihood principle. equivalently minimize negative log-likelihood loss function factorized separate loss functions commonly known cross-entropy loss refer discriminative loss corresponds maximizing prior likelihood analogy standard discriminative neural networks. term captures generative nature model accordingly refer generative loss. pθ|y stand y’th output simnet realizing model parameters case uniform class priors empirical estimation written maximum likelihood training oftentimes based dedicated algorithms e.g. expectation minimization leverage resemblance networks convnets speciﬁcally objective instead stochastic gradient descent methods. major advantage generative models discriminative ones lies ability cope missing data speciﬁcally context classiﬁcation. large discriminative methods either attempt complete missing parts data classiﬁcation known data imputation learn directly classify data missing values ﬁrst approaches relies quality data completion much difﬁcult task original classiﬁcation missing data. even completion optimal resulting classiﬁer known sub-optimal second approach make assumption nonetheless assumes distribution missing values train test times similar condition often hold practice. indeed globerson roweis coined term nightmare test time refer common situation classiﬁer must cope missing data whose distribution different encountered training. opposed discriminative methods generative models endowed natural mechanism classiﬁcation missing data. namely generative model simply marginalize missing values effectively classifying possible completions weighing completion according probability. this however requires tractable inference marginalization. already shown sec. generative convacs support former show sec. marginalization efﬁcient. beforehand formulation classiﬁcation missing data. random vector representing object random variable representing label. denote joint distribution speciﬁc realizations thereof. assume sampling speciﬁc instance random binary vector drawn conditioned concretely sample binary mask according distribution considered missing equal zero observed otherwise. formally consider vector whose i’th coordinate deﬁned hold wildcard mi=. classiﬁcation task predict given access solely following works rubin little rubin consider three cases missingness distribution missing completely random independent i.e. function missing random independent missing values i.e. function affected changes mi=; missing random covering rest distributions depends missing values i.e. function least sometimes sensitive changes mi=. joint distribution object label missingness mask given denote event random vector coincides coordinates example all-zero vector covers entire probability space all-one vector corresponds event notations hand position characterize optimal predictor presence missing data claim data distribution missingness distribution optimal classiﬁcation rule terms loss given predicting class maximizes py=y) instance distribution classiﬁer admits simpler form referred marginalized bayes predictor corollary conditions claim distribution optimal classiﬁcation rule written corollary indicates setting frequently encountered practice optimal classiﬁcation require prior knowledge regarding missingness distribution long able realize marginalized bayes predictor equivalently compute likelihoods observed values conditioned labels =y)) classiﬁcation missing data guaranteed optimal regardless corruption process taking place. stark contrast discriminative methods require access missingness distribution training thus able cope unknown conditions test time. faced missingness prediction. however many times wish tackle reverse problem training riddled missing data. tractable generative models offer similar advantage task well learning missing data using marginalized likelihood objective. assumption method results unbiased classiﬁer discussed above generative models optimal classiﬁcation missing data oblivious speciﬁc missingness distribution. however requires tractable marginalization missing values. section show generative convacs bring forth extremely efﬁcient marginalization requiring single forward pass corresponding network. evident network used compute used compute requires slight adaptation representation layer. namely latter would represent observed values usual likelihoods whereas missing values would represented constant ones conclude generative convacs marginalizing missing values efﬁcient plain inference requires single pass corresponding network. accordingly marginalized bayes predictor realized efﬁciently classiﬁcation missing data optimal regardless missingness distribution. many generative models realized convolutional networks. models successful date method generative adversarial networks network trained generate instances data distribution two-player mini-max game. many uses learning generate data points e.g. inpainting super-resolution cannot used computing likelihood data. generative networks offer approximate inference e.g. variational auto-encoders variational lower-bound likelihood function. methods include gsns dpms mpdbms last method especially noteworthy generative classiﬁer approximate marginal likelihoods conditioned class tested classiﬁcation missing data. generative neural networks capable tractable inference tractable marginalization. dinh suggest method designing neural networks realize invertible transformation simple distribution data distribution. inverting network brings forth tractable inference partial integration density function still intractable. another popular method tractable inference central pixelrnn nade factorizing probability according realp izing neural network predicting value given input based construction certain marginal distributions indeed tractable compute not. orderless-nade partially addresses issue using ensembles models different orderings input. however estimate marginal distributions classiﬁer analogue compute class-conditional marginal likelihoods required classiﬁcation missing data. model makes arithmetic circuits long considered foundation constructing generative models sum-product networks another class generative models related though strictly convolutional circuits deﬁned sec. spns realize convac thus universal posses tractable inference lack structure puts disadvantage. hard problem designing structure spns lead many proposals learning structure data indeed improved upon manually designed spns. nevertheless algorithms inconsistent demonstrated under-performance even simple handwritten digit classiﬁcation datasets opposed spns generative convacs easily designed architecture parameters size pooling windows number channels directly related expressivity model detailed sec. outside realm generative networks tractable graphical models e.g. latent tree models common method tractable inference. similar spns straightforward proper structure graphical models particular problem. recently great strides made learning structure parameters latent tree models leveraging relations models tensor factorizations bring forth theoretically efﬁcient consistent learning algorithm. discussed sec. generative convacs also form type latent tree model hierarchical non-negative tensor decompositions. unlike aforementioned algorithms utilize tensor factorizations solely deriving model analyzing expressivity learning parameters conventional means training neural networks. adapting alternative learning schemes according viewed promising avenue future research. demonstrate properties models qualitative quantitative experiments. sec. present state-of-the-art results image classiﬁcation missing data robustness various missingness distributions. additionally show sec. results limited images demonstrating speech recognition. finally app. show visualizations produced models gives insight inner workings. implementation based caffe uses maps efﬁcient code generation well code reproducing experiments available github repository https//github.com/huji-deep/ generative-convacs. experiments provided app. demonstrate effectiveness method classiﬁcation missing data unknown missingness distribution conducting three kinds experiments. ﬁrst experiments conducted mnist handwritten digits classiﬁcation consisting grayscale images. third experiment conducted mnist well small norb object recognition dataset consisting grayscale stereo images toys belonging categories four-legged animals human ﬁgures airplanes trucks cars. results refer models using shallow network cp-cac deep networks ht-cac derived respective factorization formats begin comparing generative approach missing data classical methods namely methods based globerson roweis suggest missing data could regarded feature deletion noise i.e. missing entries replaced zeroes devise learning algorithm takes maximum number missing features account. method later improved upon dekel shamir follow experimental protocol non-zero pixels randomly chosen changed zero carried binary classiﬁcation problem distinguishing pairs handwritten digits. complexity algorithm images digit used training. even though missingness distribution mnar type method guarantied optimal under test results table clearly show performs signiﬁcantly better. additionally method requires training special classiﬁers value whereas method uses single model trained prior knowledge missingness distribution. continue harder task multi-class blind classiﬁcation missing data missingness distribution completely unknown test time single classiﬁer must handle possible distributions. simulate kinds missingness distributions i.i.d. mask ﬁxed probability missing pixel mask composed union possibly overlapping rectangles width height equal randomly assigned position image distributed uniformly. present results method ﬁrst wish demonstrate ineffectiveness learning endto-end purely discriminative classiﬁers task. take baseline standard lenet model bundled caffe train directly datasets different missingness distributions. show convnets learn overcome missingness distributions encountered training fail figure compare convnets trained distribution tested others. denotes training i.i.d. cordistributions randomized parameters. ruption trained probability ptrain tested ptest. missing rectangles training randomized distributions compared testing missing rectangles distribution. generalize distributions. also veriﬁed training multiple missingness distributions slightly improve generalization never possible distributions. illustrate disadvantage discriminative method necessarily incorporates bias towards corruption process seen training makes fail distributions. concluding purely discriminative methods blind classiﬁcation missing data move main results presented discussed sec. common method classiﬁcation under missing data data imputation i.e. ﬁlling missing data preprocessing step followed prediction model trained clean dataset. apply several methods ranging simply ﬁlling missing pixels zeroes mean value utilizing several generative models mentioned sec. suited inpainting. generative models tested norb dataset difﬁculty adapting published implementation work dataset. additionally instead original nade variant orderless nade suggested raiko binarized mnist. evident results best data imputation methods work well relatively percentage missing data collapse performance. figure blind classiﬁcation missing data. testing i.i.d. corruption probability pixel. testing missing rectangles corruption missing rectangles width hight equal accuracies estimated plot goodfellow data imputation algorithms followed convnet. also k-nearest neighbors vote likely class given example. extend missing data comparing distances using obcorrupted instance served entries i.e. clean image training compute better majority modern methods tested practice inefﬁcient even missing data prevents common memory runtime optimizations typically employed address this. finally also compare published results mp-dbms mnist i.i.d. corruption. cases tested lack public implementation. especially notable similar model generative classiﬁer compute class-conditional marginal likelihoods required missing data classiﬁcation unlike model approximately shows importance exact inference. conclude unlike methods tested model achieves highest accuracy signiﬁcant margin times percentage points almost missingness distributions training single network once. comparison trained convnet windows data sample rate predict phoneme center window. model standard convnet reached around accuracy clean dataset half audio missing i.i.d. accuracy convnet model mean imputation goes model utilizing common audio inpainting methods improves results plan explore speech recognition future work. introduced class generative networks call generative convacs based convolutional arithmetic circuits variant convolutional networks computes high-dimensional functions tensor decompositions. principle property models combine expressive power convolutional networks tractable inference well tractable marginalization. latter ability unique amongst contemporary generative networks demonstrate importance theoretical claims well experiments classiﬁcation missing data several datasets. finally currently investigating several avenues future research including semi-supervised learning experimenting convacs architectures progress optimization regularization types networks. animashree anandkumar rong daniel sham kakade matus telgarsky. tensor decompositions learning latent variable models. journal machine learning research ben-nun levy amnon barak rubin. memory access patterns missing piece multi-gpu puzzle. proceedings international conference high performance computing networking storage analysis pages yoshua bengio ´eric thibodeau-laufer guillaume alain jason yosinski. deep generative stochastic networks trainable backprop. international conference machine learning nadav cohen amnon shashua. simnets generalization convolutional networks. advances neural information processing systems nips deep learning workshop nadav cohen amnon shashua. inductive bias deep coninternavolutional networks pooling geometry. tional conference learning representations iclr april goodfellow mehdi mirza aaron courville yoshua bengio. multi-prediction deep boltzmann machines. advances neural information processing systems goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems wolfgang hackbusch. tensor spaces numerical tensor calculus volume springer series computational mathematics. springer science business media berlin heidelberg february furong huang niranjan ioakeim perros robert chen jimeng anima anandkumar. scalable latent tree model application health analytics. nips machine learning healthcare workshop yangqing evan shelhamer jeff donahue sergey karayev jonathan long ross girshick sergio guadarrama trevor darrell. caffe convolutional architecture fast feature embedding. corr abs/. cs.cv rapha¨el mourad christine sinoquet nevin lianwen zhang tengfei philippe leray. survey latent tree models applications. artif. intell. res. cs.lg– andrew michael jordan. discriminative generative classiﬁers comparison logistic regression naive bayes. advances neural information processing systems nips deep learning workshop pedregosa varoquaux gramfort michel thirion grisel blondel prettenhofer weiss dubourg vanderplas passos cournapeau brucher perrot duchesnay. scikit-learn machine learning python. journal machine learning research robert peharz bernhard geiger franz pernkopf. greedy machine part-wise learning sum-product networks. learning knowledge discovery databases pages springer berlin heidelberg berlin heidelberg september jascha sohl-dickstein eric weiss niru maheswaranathan surya ganguli. deep unsupervised learning using nonequilibrium thermodynamics. internation conference machine learning yaniv taigman ming yang marc’aurelio ranzato lior wolf. deepface closing human-level performance face veriﬁcation. computer vision pattern recognition cvpr. ieee computer society june benigno uria marc-alexandre karol gregor iain murray hugo larochelle. neural autoregressive distribution estimation. journal machine learning research matthew zeiler fergus. visualizing understanding convolutional networks. european conference computer vision. springer international publishing thought multi-dimensional array ad...dn number indexing entries array also called modes referred order tensor. number values index particular mode take referred dimension mode. tensor rm⊗...⊗mn mentioned thus order dimension i-th mode. purposes typically assume simply denote fundamental operator tensor analysis tensor product. tensor product operator denoted generalization outer product vectors pair tensors. speciﬁcally tensors order respectively tensor product results tensor order deﬁned d...dp ad...dp +...dp main concept tensor analysis work tensor decompositions. straightforward common tensor decomposition format rank- decomposition also known candecomp/parafac decomposition short decomposition. decomposition natural extension low-rank matrix decomposition general tensors built upon concept linear combination rank- elements. similarly matrices tensors form v⊗···⊗v non-zero vectors regarded n-ordered rank- tensors thus rank-z decomposition tensor naturally deﬁned i=z= parameters {azi rmi}nz decomposition. mentioned above equivalent low-order matrix factorization. simple show tensor represented decomposition minimal known tensor rank. another decomposition paper hierarchical nature known hierarchical tucker decomposition refer decomposition. decomposition combines vectors higher order tensors single step decomposition gradually combining vectors matrices matrices ordered tensors recursively hierarchically fashion. speciﬁcally following describes recursive formula decomposition tensor universality generative convacs section prove universality property generative convacs discussed sec. begin taking note functional analysis deﬁne property called total similar concept total followed proving property invariant cartesian product functions entails universality models corollary. deﬁnition pdfs total exists s.t. words proof. gaussian pdfs diagonal covariance matrices known total gaussian pdfs diagonal covariance matrices claim trivially true. otherwise above exists diagonal gaussians {gij}i∈j∈ s.t. completes proof. corollary total pdfs family generative convacs mixture components approximate arbitrarily well given arbitrarily many components. {aljγ∈rrl−}l∈{...l−}j∈γ∈ level vector rrl− scalars referred ranks decomposition. similar decomposition tensor represented decomposition. moreover given decomposition converted decomposition polynomial increase number parameters. section prove depth efﬁciency property convacs proved cohen applies also generative convacs introduced sec. analysis relies basic knowledge tensor analysis relation convacs. completeness provide short introduction tensor analysis app. prove following theorem generative analog theorem theorem tensor order dimension mode generated recursive formulas simplex constraints introduced sec. deﬁne min{r consider space possir conﬁgurations parameters decomposition {aljγ rl−−}ljγ. space generated tensor cp-rank least almost everywhere differently conﬁgurations cp-rank less form measure zero. exact result holds constrain composition shared i.e. aljγ consider space {alγ rl−−}lγ conﬁgurations. requirement power solely simplifying deﬁnition decomposition. generally instead deﬁning complete binary tree describing order operations canonical decomposition balanced binary tree. measure theoretical arguments original proof. speciﬁcally k-dimensional simplex subset k+dimensional space zero measure respect lebesgue measure rk+. standard method deﬁne measure lebesgue measure lebesgue projection space i.e. measure projection subset simplex latter’s measure deﬁned notice positive measure moreover invertible inverse given xi). case parameter space cartesian product several simplex spaces different dimensions measure deﬁned above measure cartesian product uniquely deﬁned product measure. though standard choice projection function could seen limitation however zero measure sets identical reasonable choice projection speciﬁcally projection invertible differentiable jacobian bounded subset measure zero w.r.t. projection measure zero w.r.t. implies sample weights generative decomposition continuous distribution property holds probability standard parameterization hold probability reasonable parameterization. state prove lemma needed proof theorem lemma min{m polynomial mapping rm×n polynomial function). exists point s.t. rank rk|rank zero measure. denote matricization n-order tensor rows columns correspond even modes respectively. speciﬁcally rm×···mn matrix rows columns rearranging entries tensor ad...dn j=i+ j=i+ additionally matricization linear operator i.e. proof theorem stemming stated facts show cp-rank least sufﬁcient examine matricization prove rank rn/. notice construction according recursive formula ht-decomposition entires polynomial parameters decomposition dimensions accordance discussion measure simplex spaces vector parameter aljγ rl−− instead examine projection aljγ rrl−− notice polynomial mapping w.r.t. ˜aljγ. thus polynomial mapping w.r.t. projected parameters {˜aljγ}ljγ using lemma sufﬁcient show exists parameters rank rn/. denoting convenience construct induction parameters {aljγ}ljγ ranks matrices {}j∈γ∈ least enforcing simassume rank speciﬁc choice mentioned earlier invertible inverse given however simpliﬁed proof notations deﬁned entire range even serve inverse denote rl−. inductive assumption general property rank rank rank ranks matrices least rl−/ rl−/ rl/. writ· noticing {mα} depend aljγ simply pick aljγ thus φljγ rank rl/. completes proof theorem. perspective generative convacs theorem leads following corollary corollary components mixing square integrable probability density functions form linearly independent set. consider deep generative convac model polynomial size whose parameters drawn random continuous distribution. then probability distribution realized network requires exponential size order realized shallow generative convac model. claim holds regardless whether parameters deep model shared not. proof. given coefﬁcient tensor cp-rank lower bound number channels required represent tensor convac following decomposition introduced sec. additionally since mixing components linearly independent products fi|fi linearly independent well entails distribution representable generative convac mixing components unique coefﬁcient tensor theorem parameters polynomial deep generative convac model coefﬁcient tensor polynomial cp-rank requirement polynomial shallow generative convac model realizing distribution exactly forms measure zero. left prove impossible exactly represent distribution exponential coefﬁcient tensor shallow model also impossible approximate follows directly lemma appendix cohen case meets requirement lemma. section give short proofs claims sec. optimality marginalized bayes predictor missingat-random distribution missingness mechanism unknown well general case additional assumptions. addition also present counter example proving data imputation results lead suboptimal classiﬁcation performance. begin introducing several notations augment notations already introduced body article. given speciﬁc mask realization following notations denote partial assignments random vector observed indices i.e. indices denote partial assignment vector length equal number observed indices. similarly denote partial assignment missing indices according vector length equal number missing indices. example notation given realizations deﬁned sec. event using current notation marked partial assignment matches observed values vector according notations place move prove claim describes general solution optimal prediction rule given data missingness distributions without adding additional assumptions. inde pendent marginalization missing values conditional probability deﬁnition expression w.r.t. possible values ﬁxed vectors finally replacing integrals sums proof holds exactly instances discrete. proved setting classiﬁcation marginalization leads optimal performance move show true classiﬁcation dataimputation. though many methods perform dataimputation i.e. complete missing values given observed ones methods seen solution following optimization problem typically approximation given classiﬁer instance missing values according classiﬁcation data-imputation simply result applying output optimal classiﬁer complete data i.e. bayes predictor following prediction rules unconditional argmax conditional argmax table space serves example sub-optimality classiﬁcation data-imputation claim exists data distribution missingness distribution s.t. accuracy classiﬁcation dataimputation almost half accuracy optimal marginalized bayes predictor absolute percentage points. proof. simplicity give example discrete distribution binary small positive number deﬁne according table triplet assigned positive weight normalization deﬁnes distribution x×y. missingness distribution deﬁned s.t. i.e. always observed always missing trivial distribution. given data distribution easily calculate exact accuracy optimal data-imputation classiﬁer marginalized bayes predictor missingness distribution well standard bayes predictor full-observability. first notice whether apply conditional unconditional data-imputation whether equal completion always predicted class always since data-imputation classiﬁers always predict class regardless input probability success simply probability similarly marginalized bayes predictor always predicts regardless input probability success aldouble accuracy achieved data-imputation classiﬁer. additionally notice marginalized bayes predictor achieves almost accuracy bayes predictor full-observability equals exactly following graphical model perspective models allows generate random instances distribution also generate likely patches neuron network effectively explaining role classiﬁcation process. remind reader every neuron network corresponds possible assignment latent variable graphical model. looking likely assignments child nodes graphical tree model generate patch describes neuron. unlike similar suggested methods visualize neural networks often relying brute-force search solving optimization problem likely image method emerges naturally probabilistic interpretation model. conditional samples generates digit visualization top-level layers network small patch matches different neuron network. common wisdom convnets work assuming simple low-level features composed together create complex features subsequent layer denotes features higher abstraction visualization network clearly demonstrate hypothesis true case showing small strokes iteratively composed complete digits. detailed description experiments experiments meaningful could reproduced proﬁcient individuals. providing sufﬁcient details enable others replicate results goal section. hope accomplish making code public well documenting experiments sufﬁcient degree allowing reproduction scratch. complete implementation models presented paper well modiﬁcations open-source projects scripts used process conducting experiments available github repository https//github.com/ huji-deep/generative-convacs. additionally wish invite readers contact authors deem following details insufﬁcient process reproduce results. following give concise descriptions classiﬁcation method used experiments. results experiment mp-dbm taken directly paper conducted hence cover section. direct reader article exact details reproduce results. figure visualization ht-cac model. images visualize different layer model consists several samples generated latent variables different spatial locations conditioned randomly selected channels. leftmost image shows samples taken layer consists single latent variable channels. center image shows samples taken layer consists grid latent variables channels each. image divided quadrants contains samples taken respective latent variable position. rightmost image shows samples layer consists grid latent variables channels image similarly spatial divided different areas matching latent variables layer. binary linear classiﬁers trained formulating optimization quadric program constraint features could deleted i.e. original value changed zero. original source code never published authors kindly agreed share code used reproduced results larger datasets. algorithm couple hyperparameters chosen grid-search crossvalidation process. details exact protocol testing binary classiﬁers missing data please sec. f... k-nearest neighbors classical machine learning algorithm used regression classiﬁcation tasks. underlying mechanism ﬁnding nearest examples training according metric function summarizing function applied targets nearest neighbors produce output used classiﬁcation typically majority voting function returning class found nearest neighbors. experiments classiﬁcation missing data training consists complete examples missing data classiﬁcation time inputs missing values. given input missing values example training modiﬁed euclidean distance metric compare distance metric denon-missing coordinates i.e. xi). process cross-validation chosen experiments. implementation based popular scikit-learn python library widespread successful discriminative method nowadays convolutional neural networks standard convnets represented computational graph consisted different kinds nodes called layers convolutional-like operators applied inputs followed non-linear pointwise activation function e.g. known relu. experiments mnist without missing data used lenet convnet architecture bundled caffe trained iterations using momentum base learning rate remained constant iterations followed linear decrease another iterations followed linear decrease learning rate remaining iterations. model also used l-regularization chosen crossvalidation experiment separately. modiﬁcations made model training procedure. experiments norb used ensemble convnets using following architecture convolution output channels pooling stride relu activation convolution output channels relu activation dropout layer probability average pooling stride convolution output channels relu activation dropout layer probability average pooling stride fully-connected layer output channels relu activation dropout layer probability ends fully-connected layer output channels. stereo images represented two-channel input image network. training used data augmentation consisting randomly scaling rotation transforms. networks trained iterations using momentum base learning rate remained constant iterations followed linear decrease iterations followed linear decrease learning rate remaining iterations. model also used weight decay additional regularization. convnets trained images containing missing values passed network original image missing values zeroed additional binary image separate channel containing missing values spatial position otherwise missing data format sometimes known data imputation. formats representing missing values tested however scheme performed signiﬁcantly better formats. experiments assumed training complete missing values present test set. order design convnets robust speciﬁc missingness distributions simulated missing values training sampling different mask missing values image mini-batch. covered sec. results training convnets directly simulated missingness distributions resulted classiﬁers biased towards speciﬁc distribution used training performed worse distributions compared convnets trained distribution. iterations reduced iterations since effect classiﬁcation accuracy marginal. norb dataset used cifar example lower learning rate modiﬁed code found https//github.com/huji-deep/nice. diffusion dickstein //github.com/sohl-dickstein/ diffusion-probabilistic-models trained mnist using example code without changes. similarly modiﬁcations support general mask missing values kept rest parameters inpainting unchanged. norb used model mnist. tried using cifar example norb however produced exceptions training. modiﬁed code found https//github.com/huji-deep/ diffusion-probabilistic-models. common method handling missing data leveraging available discriminative classiﬁers application data imputation algorithm completion missing values passing results classiﬁer trained uncorrupted dataset. tested different types data imputation algorithms generative data imputation training generative model using complete missing values ﬁnding likely instance coincides observed values i.e. solving following generative stochastic networks used original source code https//github.com/yaoli/gsn trained example model mnist epochs. whereas original article tested completing left right side given image modiﬁed code support general masks. modiﬁed implementation found https//github.com/huji-deep/gsn. estimaal. tion source code https used original //github.com/laurent-dinh/nice trained mnist using example code without changes. similarly modiﬁcation code adapted code support general masks input. additionally original inpainting code required complete theoretical description model please body article. models implemented performing intermediate computations log-space using numerically aware operations. practiced meant models realized simnets architecture consists similarity layers representing gaussian distributions layers representing weighted sums performed log-space input outputs well standard pooling operations. learned parameters layers called offsets represents weights weighted saved log-space. parameters layers optionally shared spatial regions alternatively left parameter sharing all. additionally used implement generative models offsets normalized network architectures tested article consists different gaussian mixture components diagonal covariance matrices non-overlapping patches input size implemented similarity layer speciﬁed simnets architecture added gaussian normalization term. ﬁrst describe architectures used mnist dataset. cp-cac model used following similarity layer layer parameter sharing spatial regions output channels. model ends global pooling operation followed another layer outputs class. ht-cac model starts similarity layer followed sequence four pairs layer followed pooling layer pairs additional layer lowering outputs model outputs number classes. number output channels layer follows ----. layers network parameter sharing except ﬁrst layer uses repeated sharing pattern offsets analogous convolution layer stride models trained losses described sec. using adam variant optimizing parameters base learning rate models trained itersmall offsets might result loss precision. finally applying model different translations input average class predictions. since model marginalize inputs need crop original image instead mask unknown parts translation missing. applying similar trick standard convnets mnist seem improve results. believe method especially model natural treatment overlapping patches like convnets able marginalize missing pixels easily limiting crop translation typically done. experiment focuses binary classiﬁcation problem derived mnist limiting number classes different digits time. non-zero feature deletion distribution suggested globerson roweis i.e. given image uniformly sample non-zero pixels image replace values zeros. type missingness distribution falls mnar type deﬁned sec.. test values given value train separate classiﬁer digit pair classiﬁer randomly picked subset dataset containing images digit training ﬁxed validation images digit. picking best classiﬁer according validation classiﬁer tested test images digits randomly chosen missing values according value experiment repeated times digit pair time using different subset training corrupted test set. conducting different experiments accuracies averaged value reported table experiment focuses complete multi-class digit classiﬁcation mnist dataset presence missing data according different missingness distributions. setting test contains missing values whereas training not. test kinds missingness distributions fall type deﬁned sec.. ﬁrst kind call i.i.d. corruption pixel missing ﬁxed probability second kind call missing rectangles corruption positions rectangles width chosen uniformly picture rectangles overlap another. training stage models tested biased toward speciﬁc missingness distributions chosen test stage classiﬁer tested types missingness distributions without supplying parameters type missingness distribution tested against. rule prevent convnets trained simulated missingness distributions. demonstrate latter lead biased classiﬁers conducted sepnorb dataset trained ht-cac model similarity layer. layers parameter sharing scheme mnist number output channels layer follows ----. training identical mnist models exception using iterations instead additionally used ensemble models trained separately trained using different generative loss weight also used data augmentation methods used training convnets norb used article. standard weight regularization work well models lead adapt better log-space weights minimizing parameter chosen cross-validation. additionally since even large values model still overﬁtting added another form regularization form random marginalization layers. random marginalization layer similar concept dropout instead zeroing activations completely random choses spatial locations random zero activations locations channels. model zeroing activations layer speciﬁc location equivalent marginalizing inputs receptive ﬁeld respective location. used random marginalization layers layers training probability zeroing activations chosen cross-validation layer separately. though might raise concern random marginalization layers could lead biased results toward missingness distributions tested practice addition layers helped improve results cases pixels missing. finally wish discuss optimization tricks minor effects compared above nevertheless useful achieving slightly better results. first instead optimizing directly objective deﬁned smoothing parameter terms follows setting diminish generative capabilities models setting high diminish discriminative performance. cross-validation decided value models trained mnist norb used different value models ranging second found performance increased normalized activations applying operations. speciﬁcally calculate soft-max channels spatial location call activation norm subtract every respective activation. applying operation back activation norm. though might obvious ﬁrst subtracting constant input operation adding output equivalent change mathematical operation. however resolve numerical issue adding large activations arate experiment convnets previous rule ignored train separate convnet classiﬁer type parameter missingness distributions used. tested convnets missingness distributions results conﬁrmed hypothesis.", "year": 2016}