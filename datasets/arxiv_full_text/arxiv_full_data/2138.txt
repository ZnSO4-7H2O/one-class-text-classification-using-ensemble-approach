{"title": "A PAC RL Algorithm for Episodic POMDPs", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Many interesting real world domains involve reinforcement learning (RL) in partially observable environments. Efficient learning in such domains is important, but existing sample complexity bounds for partially observable RL are at least exponential in the episode length. We give, to our knowledge, the first partially observable RL algorithm with a polynomial bound on the number of episodes on which the algorithm may not achieve near-optimal performance. Our algorithm is suitable for an important class of episodic POMDPs. Our approach builds on recent advances in method of moments for latent variable model estimation.", "text": "many interesting real world domains involve reinforcement learning partially observable environments. eﬃcient learning domains important existing sample complexity bounds partially observable least exponential episode length. give knowledge ﬁrst partially observable algorithm polynomial bound number episodes algorithm achieve near-optimal performance. algorithm suitable important class episodic pomdps. approach builds recent advances method moments latent variable model estimation. challenge artiﬁcial intelligence effectively learn make sequence good decisions stochastic unknown environments. reinforcement learning subﬁeld speciﬁcally focused agents learn make good decisions given feedback form reward signal. many important applications robotics education healthcare agent cannot directly observe state environment responsible generating reward signal instead receives incomplete noisy observations. important measure algorithm sample eﬃciency much data/experience needed compute good policy well. measure sample complexity given probably approximately correct framework; algorithm said high probability selects near-optimal action number steps polynomial function problem parameters. substantial progress fully observable setting knowledge exists published work algorithms partially observable settings. lack work partially observable perhaps additional challenge introduced partial observability environment. fully observable settings world often assumed behave markov decision process elegant approach proving algorithm mdps compute ﬁnite sample error bounds parameters. however states partially observable hidden naive approach directly treating pomdp history-based yields state space grows exponentially horizon rather polynomial pomdp parameters hand substantial recent interest progress method moments spectral approaches modeling partially observable systems majority work focused inference prediction little work tackling control setting. method moments approaches latent variable estimation particular interest number models obtain global optima provide ﬁnite sample guarantees accuracy learned model parameters. settings include limited information gathering pomdp domains preference elicitation dialogue management slot-ﬁlling domains medical diagnosis decision making work builds method moments inference techniques requires several non-trivial extensions tackle control setting. particular subtle issue latent state alignment models action learned independent hidden markov models unclear solve correspondence issue across latent states essential performing planning selecting actions. primary contribution provide theoretical analysis proposed algorithm prove possible obtain near-optimal performance number episodes scales polynomial function pomdp parameters. similar fully observable algorithms directly instantiating bounds would yield impractical number samples real application. nevertheless believe understanding sample complexity help guide amount data required task also similar work motivate practical algorithms build ideas. bounds accuracy estimates unlike popular expectation-maximization guaranteed local optima oﬀers ﬁnite sample guarantees. approaches estimating hmms also unfortunately provide accuracy guarantees estimated parameters. pomdp planning methods typically require estimates underlying pomdp parameters would diﬃcult methods computing pomdp policy providing ﬁnite sample guarantee. aside method anandkumar another popular spectral method involves using predictive state representations directly tackle control setting; however asymptotic convergence guarantees ﬁnite sample analysis. also another method moments approach transfer across bandits tasks latent variable estimation problem substantially simpliﬁed state system unchanged selected actions alternatively bayesian methods popular solving pomdps. mdps exist bayesian methods bounds however bounds bayesian methods pomdps. said bayesian methods optimal bayesian sense making best decision given posterior possible future observations translate frequentist ﬁnite sample bound. consider partially observable markov decision process described tuple discrete states discrete actions discrete observations discrete rewards initial belief episode length transition model warmuth approach guarantees estimated probability h-length observation sequences bounded kl-divergence true probability sequence true parameters expressed function number underlying data samples used estimate parameters. think possible estimates control setting modeling hidden state control systems psrs employing forward search approach planning; however remain number subtle issues address ensure approach viable leave interesting direction future work. denote ﬁnite rewards reward matrices entry matrix denotes probability obtaining reward taking action state note setting also treat reward additional observation. objective pomdp planning compute policy achieves large expected future rewards mapping histories prior sequences actions observations rewards actions. many cases capture prior histories using suﬃcient statistic called belief represents probability particular state given prior history actions observations rewards. popular method pomdp planning involves representing value function ﬁnite α-vectors represents expected future rewards following policy associated α-vector initial state pomdp planning proceeds taking ﬁrst action associated policy α-vector yields maximum expected value current belief state computed particular α-vector using reinforcement learning setting transition observation and/or reward model parameters initially unknown. goal learn policy achieves large rewards environment withadvance knowledge world works. ﬁrst assumption setting satisﬁed many real world situations involving agent repeatedly doing task example agent sequentially interact many diﬀerent customers ﬁnite planning problems reward typically realvalued scalar porl must learn reward model. requires assuming mapping states rewards. simplicity assume multinomial distribution discrete rewards. note always discretized real-valued reward ﬁnite values bounded error resulting value function estimates choice makes little restrictions underlying setting. amount time. restrictions setting captured assumptions assumption similar mixing assumption necessary order estimate dynamics states. assumption necessary uniquely determine transition observation reward dynamics. second assumption sound quite strong pomdp settings states reachable complex sequence carefully chosen actions robotic navigation video games. however assumption commonly satisﬁed many important pomdp settings primarily involve information gathering. example preference elicitation user modeling pomdps commonly used identify typically static hidden intent preference state user taking action based resulting information examples include dialog systems medical diagnosis decision support even human-robot collaboration preference modeling settings belief commonly starts non-zero possible user states slowly gets narrowed time. third assumption also signiﬁcant still satisﬁed important class problems overlap settings captured assumption information gathering pomdps state hidden static automatically satisfy full rank assumption transition model since identity matrix. assumption observation reward matrices imply cardinality observations least large size state space. similar assumption made many latent variable estimation settings including control setting indeed observations consist videos images audio signals assumption typically satisﬁed signals common dialog systems user intent modeling situations covered assumption satisfying reward matrix full rank typically trivial reward signal often obtained discretizing real-valued reward. therefore readily acknowledge setting cover generic pomdp reinforcement learning settings believe cover important class problems relevant real applications. contrast many algorithms mdps shown exploration critical order enough data estimate model parameters. however mdps algorithms directly observe many times every action tried every state information steer exploration towards less explored areas. partially observable settings challenging state hidden possible directly observe number times action tried latent state. fortunately recent advances method moments estimation procedures latent variable estimation demonstrated certain uncontrolled settings including many types hidden markov models still possible achieve accuracy estimates underlying latent variable model parameters function amount data samples used perform estimation. intuition this consider starting belief state non-zero probability possible states. repeatedly take action belief given suﬃcient number samples actually taken action state many times control setting subtle uncontrolled setting focus majority recent spectral learning research because wish estimate transition observation models estimate pomdp model parameters. ultimate interest able select good actions. naive approach independently learn transition observation reward parameters separate action restricting pomdp execute single action thereby turning pomdp hmm. however simple aproach fails returned parameters correspond diﬀerent labeling hidden states. example ﬁrst column transition matrix action actually correspond state ﬁrst column transition matrix action truly correspond require labeling must consistent actions since wish compute happens diﬀerent actions executed consecutively. unsatisfactory match labels diﬀerent actions requiring initial belief state probabilities unique well separated state. estimated initial belief action match labels. however strong assumption starting belief state unlikely realized. address challenge mismatched labels transform pomdp induced ﬁxing policy πexplore create alternate hidden state representation directly solves problem alignment hidden states across actions. speciﬁcally make hidden state time induced denoted equal tuple action time step next state subsequent action denote observations induced observation associated hidden state tuple figure shows graphical model original pomdp related graphical model induced hmm. making transformation resulting still satisﬁes markov assumption next state function prior state observation function current state. transformation also desired property possible directly align identity states across selected actions. parameters depend state action built-in correlation diﬀerent actions. discuss theoretical analysis. ready describe algorithm episodic ﬁnite horizon reinforcement learning pomdps eeporl algorithm model-based proceeds phases. ﬁrst phase performs exploration collect samples trying diﬀerent actions diﬀerent states. ﬁrst phase completes extend approach compute estimates induced parameters. estimates obtain near-optimal policy. work proof requires full-rank minimum probability actions. chose perturbed identity matrix simplicity. since πexplore ﬁxed policy pomdp process reduces ﬁrst four steps. steps store observed experience observation previously deﬁned induced hmm. algorithm follows policy πrest remaining steps episode. episodes considered potentially non-optimal choice πrest impact theoretical analysis. however empirically πrest could constructed encourage near optimal behavior given observed data collected current episode. phase completes samples tuple apply extension algorithm parameter estimation anandkumar extension computes estimates bounds transition model computed original method. summarize procedure yields estimated transiated state. possible true observation matrix entries actions hidden state must non-zero true value entries must zero; therefore long suﬃciently accurate estimates observation matrix observation matrix parameters augment states associated action pair. procedure performed algorithm labeling provides connection state original pomdp state. however practical circumstances possible enumerate possible h-step policies. case point-based approaches methods α-vectors enumerate subset possible policies. case additional error \u0001planning ﬁnal error bound ﬁnite policies considered. analysis omit \u0001planning simplicity assume enumerate h-step policies. deﬁnition β-vector taking input root action t-step conditional policies state primary result. full details please refer tech report. starting belief total undiscounted reward following policy episode. maxa similarly mina σ|s| similarly assume algorithm proceeds estimate original pomdp parameters order perform planning compute policy. note estimated parameters computed permutations state. submatrix rows columns submatrix rows correspond actions columns correspond actions estimated pomdp parameters computed follows note require additional normalize procedure since approach leverage guaranteed return well formed probability distributions. normalization procedure divides make valid probability distributions algorithm uses estimated pomdp parameters compute policy. algorithm constructs β-vectors represent expected rewards following particular policy starting action given input permuted state aside slight modiﬁcation β-vectors analogous α-vectors standard pomdp planning. β-vectors form approximate value function underlying pomdp used similar standard α-vectors. phase estimating pomdp parameters β-vectors estimated pomdp value function extract policy acting shortly prove suﬃcient conditions policy near-optimal remaining episodes. policy followed depends computed value function. computationally tractable compute β-vectors incrementally possible h-step policies. case control proceeds ﬁnding quantities directly arise using previously referenced method parameter estimation involve singular values moments induced induced parameters details). brieﬂy overview proof. detailed proofs available supplemental material. ﬁrst show executing eeporl obtain parameter estimates induced bounds estimates function number data points prove induced obtain estimated parameters underlying pomdp show compute policies equivalent original pomdp bound error resulting value function estimates resulting policies approximate model parameters allows compute bound number required samples necessary achieve near-optimal policies high probability phase commence proof bounding error estimates induced parameters. order that introduce lemma proves samples taken phase belong induced transition observation matrices full rank. requirement able apply parameter estimation procedure anandkumar next lemma extension method moments method anandkumar provides bound accuracies estimated induced parameters terms number samples collected. extension inproof. lemma shows error estimates pomdp parameters bounded terms error induced parameters bounded terms number samples lemma lemma together bound error computing estimated value function using estimated pomdp parameters. provided algorithm important class episodic pomdps includes many information gathering domains. knowledge ﬁrst algorithm partially observable settings sample complexity polynomial function pomdp parameters. many areas future work. interested reducing currently required assumptions thereby creating porl algorithms suitable generic settings. direction also require exploring alternatives method moments approaches performing latent variable estimation. also hope theoretical results lead insights practical algorithms partially observable again could easily modify account approximate planning error leave simplicity expect make signiﬁcant impact resulting sample complexity except terms minor changes polynomial terms.", "year": 2016}