{"title": "Multi-Advisor Reinforcement Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We consider tackling a single-agent RL problem by distributing it to $n$ learners. These learners, called advisors, endeavour to solve the problem from a different focus. Their advice, taking the form of action values, is then communicated to an aggregator, which is in control of the system. We show that the local planning method for the advisors is critical and that none of the ones found in the literature is flawless: the egocentric planning overestimates values of states where the other advisors disagree, and the agnostic planning is inefficient around danger zones. We introduce a novel approach called empathic and discuss its theoretical aspects. We empirically examine and validate our theoretical findings on a fruit collection task.", "text": "consider tackling single-agent problem distributing learners. learners called advisors endeavour solve problem different focus. advice taking form action values communicated aggregator control system. show local planning method advisors critical none ones found literature ﬂawless egocentric planning overestimates values states advisors disagree agnostic planning inefﬁcient around danger zones. introduce novel approach called empathic discuss theoretical aspects. empirically examine validate theoretical ﬁndings fruit collection task. person faces complex important problem individual problem solving abilities might sufﬁce. actively seek advice around might consult relatives browse different sources internet and/or hire several people specialised aspects problem. aggregates technical ethical emotional advice order build informed plan hopefully make best possible decision. large number papers tackle decomposition single reinforcement learning task several simpler ones. generally follow method agents trained independently generally greedily local optimality aggregated global policy voting averaging. recent works prove ability solve problems intractable otherwise. section provides survey approaches algorithms ﬁeld. formalised section multi-advisor partitions single-agent multiagent problem widespread divide conquer paradigm. unlike hierarchical approach gives role advisor providing aggregator local q-values actions. advisors said focus reward function state space learning technique etc. mad-rl approach allows therefore tackle task different focuses. person consulted advice enquirer answer egocentrically charge next actions agnostically anticipating future actions equally empathically considering next actions enquirer. approaches modelled local advisors’ planning methods. section shows egocentric planning presents severe overestimation values states advisors disagree creates attractor phenomenon causing system remain static without tie-breaking possibilities. shown navigation task attractors avoided lowering discount factor given value. agnostic planning drawback inefﬁcient dangerous environments gets easily afraid controller performing sequence actions. finally introduce novel empathic planning show converges global optimal bellman equation advisors training full state space. seijen demonstrate fruit collection task distributed architecture signiﬁcantly speeds learning converges better solution distributed baselines. section extends results empirically validates theoretical analysis egocentric planning gets stuck attractors high values; values gets high scores also unstable soon noise introduced; agnostic planning fails efﬁciently gathering fruits near ghosts; despite lack convergence guarantees partial information advisors’ state space novel empathic planning also achieves high scores robust noise. task decomposition literature features numerous ways distribute single-agent problem several specialised advisors state space approximation/reduction reward segmentation algorithm diversiﬁcation algorithm randomization sequencing actions factorisation actions paper mainly focus reward segmentation state space reduction ﬁndings applicable family advisors. subtasks aggregation singh cohn ﬁrst propose merge markov decision processes value functions. makes following strong assumptions positive rewards model-based local optimality supposed known. finally algorithm simply accompanies classical algorithm pruning actions known suboptimal. sprague ballard propose local sarsa online learning algorithm training advisors elude fact online policy cannot locally accurately estimated partial state space endangers convergence properties. russell zimdars study depth theoretical guaranties convergence optimality local q-learning local sarsa algorithms. however work limited fact allow local advisors trained local state space. seijen relax assumption expense optimality guarantees beat hardest atari games pac-man decomposing task hundreds subtasks trained parallel. mad-rl also interpreted generalisation ensemble learning such peterson boosting algorithm framework boosting performed upon policies algorithms. sense article seen precursor policy reuse algorithm rather multi-advisor framework. wiering hasselt combine online algorithms several simple problems show mixture models experts performs generally better single alone. algorithm tackles whole task. algorithms off-policy on-policy actor-critics etc. faußer schwenker continue effort speciﬁc setting actions explicit deterministic transitions. show section planning method choice critical recommendations made accordance task deﬁnition. harutyunyan advisors trained different reward functions potential based reward shaping variants reward function. therefore embedding goals. consequence related bagging procedure. advisors recommendation aggregated horde architecture egocentric planning. aggregator functions tried majority voting ranked voting. laroche féraud follow different approach which instead boosting weak advisors performances aggregating recommendation select best advisor. approach beneﬁcial staggered learning several advisors good policies variance reduction brought committee apply compositional unreal architecture improves state-of-the atari labyrinth domains training deep network auxiliary tasks. unsupervised manner consider learner direct contributor main task. bootstrapped architecture osband also exploits idea multiplying q-value estimations favour deep exploration. result unreal bootstrapped allow break task smaller tractable pieces. summary large variety papers published subjects differing factorise task subtasks. theoretical obstacles identiﬁed singh cohn russell zimdars analysis non-optimality observation general case. article accept non-optimality approach naturally comes simpliﬁcation task brought decomposition analyse pros cons planning methods encountered literature. ﬁrst section lays theoretical foundation multi-advisor domains related works apply distributed models diverse domains racing scheduling dialogue fruit collection fruit collection task centre attention natural empirically validate theoretical ﬁndings domain pac-boy game borrowed seijen pac-boy navigates maze total possible positions possible actions state respectively north west south east. bumping wall simply causes player move without penalty. since pac-boy always starts position potential fruit positions. fruit distribution randomised start episode probability position fruit. game lasts last fruit eaten time step. episode fruits remain ﬁxed eaten pac-boy. randomly-moving ghosts preventing pac-boy eating fruits. state game consists positions pacboy fruits ghosts states. hence global representation system implemented without using function approximation. pac-boy gets reward every eaten fruit penalty touched ghost. markov decision process reinforcement learning framework formalised markov decision process tuple state space action space markovian transition stochastic function immediate reward stochastic function discount factor. trajectory rt∈t− projection task episode. succinctly returnt− goal generate trajectories high discounted cumulative reward also called γtr. needs policy maximising mad-rl structure section deﬁnes multi-advisor framework solving single-agent problem. advisors regarded specialised possibly weak learners concerned part problem. then aggregator responsible merging advisors’ recommendations global policy. overall architecture illustrated figure time step advisor sends aggregator local q-values actions current state. aggregating advisors’ recommendations figure function’s role aggregate advisors’ recommendations policy. formally aggregator deﬁned rn×|a| maps received values action article focuses analysis local qj-functions computed. values design function implements aggregator function encountered ensemble methods literature voting schemes boltzmann policy mixtures course linear value-function combinations analysis restrict wjrj implies decomposition return share advisor’s state representation locally deﬁned local state denoted deﬁne aggregator function greedy qj-functions aggregation recall hereunder main theoretical result seijen theorem ensuring conditions advisors’ training eventually converges. note assigning stationary behaviour advisors sequence random variables markov chain. later analysis assume following. assumption advisors’ environments markov although theorem guarantees convergence guarantee optimality converged solution. moreover ﬁxed point depends advisor model planning methods particular optimisation algorithms used them. section present three planning methods advisor’s level. differ policy evaluate egocentric planning evaluates local greedy policy agnostic planning evaluates random policy empathic planning evaluates aggregator’s greedy policy. common approach literature learn off-policy bootstrapping locally greedy action advisor evaluates local greedy policy. planning referred paper egocentric already employed singh cohn russell zimdars harutyunyan seijen theorem guarantees advisor convergence local optimal value function denoted qego satisﬁes bellman optimality equation study formally explain attractors illustrative example based simple depicted figure initial state system three possible actions stay perform advisor goal perform advisor goal achieving goal trajectory ends. q-values action easy compute qego qego consequence local egocentric planning commands execute action sine die. apparent similarity buridan’s paradox donkey equally thirsty hungry cannot decide drink dies inability make decision. determinism judgement identiﬁed source problem antic philosophy. nevertheless egocentric sub-optimality come actions equally good determinism policy since adding randomness system help. deﬁne generally concept attractors. deﬁnition attractor state following strict inequality holds note condition theorem existence actions allowing system actually static. indeed system might stuck attractor keep moving never achieve goals. understand happen replace state figure attractor similar states action performs random transition attractor actions respectively achieve tasks advisors also happen attractor escapable lack actions keeping system attractor set. instance figure action available remains attractor unstable one. deﬁnition advisor said progressive following condition satisﬁed intuition behind progressive property action worse losing turn nothing. words progress made towards task therefore non-progressing actions regarded advisor worst possible ones. theorem advisors progressive cannot attractor. condition stated theorem restrictive. still exist problems theorem applied resource scheduling advisor responsible progression given task. note mad-rl setting without attractors guarantee optimality egocentric planning. problems fall category. theorem neither applies problems states terminate trajectory goals still incomplete navigation tasks system goes direction opposite goal gets state worse staying position. navigation problem attractors consider three-fruit attractor illustrated figure moving towards fruit makes closer fruits fruits expression action q-value follows qego qego means that qego result aggregator would south wall indeﬁnitely. generally deterministic task action state cancelled shown condition function size action action theorem proved section appendix. theorem state guaranteed attractor a∃a- agnostic planning make prior future actions therefore evaluates random policy. again theorem guarantees convergence local optimisation process local optimal value denoted qagn follows attractor present agnostic planning. nevertheless acting greedily respect qagn guarantees better random policy general optimal. still agnostic planning proved usefulness pac-man novel approach inspired online algorithm found sprague ballard russell zimdars locally predict aggregator’s policy. method referred empathic aggregator control advisors evaluating current aggregator’s greedy policy respect local focus. formally local bellman equilibrium equation following however mad-rl settings involve taking advantage state space reduction speed learning case guarantee convergence function approximated local state space scope. result local estimate ˆfj) used instead reconstruction maxa∈a longer possible global bellman equation experiment intend show value function easier learn mad-rl architecture. consider fruit collection task agent navigate grid receives reward visiting fruit cell deep neural network ﬁtted ground-truth value function various objective functions optimal number turns gather fruits optimal return egocentric optimal mad-rl return. learning problem fully supervised samples allowing show fast capture ignoring burden ﬁnding optimal policy estimating value functions td-backups value iteration. evaluate dnn’s performance actions selected greedily moving agent down left right neighbouring grid cell highest value. section appendix gives details. figure displays performance theoretical optimal policy objective function dashed lines. targets largely surpass mad-rl one. figure also displays performances networks trained limited data samples results completely different. objective target hardest train objective target follows second hardest train egocentric planning mad-rl objective easier train even without state space reduction even without reward/return decomposition additionally target value decomposed training accelerated. finally found mad-rl performance tends dramatically decrease gets close attractors’ presence. consider small experiment show complexity objective function critical decomposing fashion mad-rl make simpler therefore easier train even without state space reduction. section empirically validate ﬁndings section pac-boy domain presented section mad-rl settings associating advisor potential fruit location. local state space consists agent position existence not– fruit. different settings compared baselines linear q-learning dqn-clipped four mad-rl settings egocentric egocentric agnostic empathic implementation experimentation details available appendix section provide links video ﬁles representing trajectory generated epoch various settings. egocentric-γ adopts near optimal policy coming close ghosts without taking risk. fruit collection problem similar travelling salesman problem known np-complete however suboptimal small-γ policy consisting moving towards closest fruits fact near optimal one. regarding ghost avoidance egocentric small gets advantage settings local optimisation guarantees perfect control system near ghosts. interesting outcome presence attractor phenomenon egocentric-γ pac-boy goes straight centre area grid move ghost comes close still knows avoid perfectly. empirical conﬁrmation attractors studied section present real practical issue. empathic almost good egocentric-γ agnostic proves unable reliably ﬁnish last fruits overwhelmed fear ghosts even still away. feature agnostic planning seijen dynamic normalisation depending number fruits left board. finally observe dqn-clipped also struggles last fruits. quantitative analysis displayed figure conﬁrms qualitative video-based impressions. egocentric-γ barely performs better linear q-learning dqn-clipped still optimal performance gets ghosts time time. agnostic closer rarely eats fruits. finally egocentric-γ empathic near-optimal. egocentric-γ trains faster tends ﬁnish game turns faster results noisy rewards using small distort objective function perhaps even importantly reward signal diminishes exponentially function distance goal might critical consequences noisy environments hence following experiment several levels gaussian centred white noise standard deviation applied reward signal turn advisor receives instead. since noise centred white ground truth q-functions remain same estimators obtained sampling corrupted noise variance. empirical results displayed figure shows empathic planning performs better egocentric even noise times larger variance. indeed noise fruit advisors able consistently perceive fruits radius dependent egocentric planning incompatible high values therefore myopic cannot perceive distant fruits. kind limitations expected encountered small values local advisors rely state approximations and/or transitions stochastic. also supports superiority empathic planning general case. article presented mad-rl common ground many recent successful works decomposing single-agent problem simpler problems tackled independent learners. focuses speciﬁcally local planning performed advisors. three found literature novel discussed analysed empirically compared egocentric agnostic empathic. lessons learnt article following ones. egocentric planning convergence guarantees overestimates values states advisors disagree. consequence suffers attractors states no-op action preferred actions making progress subset subtasks. domains resource scheduling identiﬁed attractor-free domains navigation conditions guarantee absence attractor. necessary recall attractor-free setting means system continue making progress towards goals long opportunity egocentric mad-rl system converge optimal solution. agnostic planning also convergence guarantees local agnostic planning equivalent global agnostic planning. however converge solutions. instance dangerous environments considers actions equally likely favours staying away situation random sequence actions signiﬁcant chance ending crossing bridge would avoided. still agnostic planning simplicity enables general value functions seijen empathic planning optimises system according global bellman optimality equation without guarantee convergence advisor state space smaller global state. experiments never encountered case convergence obtained pac-boy domain robustly learns near optimal policy epochs. also safely applied ensemble tasks learners given full state space. references wendelin böhmer jost springenberg joschka boedecker martin riedmiller klaus obermayer. autonomous learning state representations control emerging ﬁeld aims autonomously learn state representations reinforcement learning agents real-world sensor observations. ki-künstliche intelligenz t.g. dietterich. maxq method hierarchical reinforcement learning. proceedings fifteenth international conference machine learning morgan kaufmann fernando fernández manuela veloso. probabilistic policy reuse reinforcement learning agent. proceedings international conference autonomous agents multi-agent aystems xavier glorot yoshua bengio. understanding difﬁculty training deep feedforward neural networks. proceedings international conference artiﬁcial intelligence statistics jaderberg volodymyr mnih wojciech marian czarnecki schaul joel leibo david silver koray kavukcuoglu. reinforcement learning unsupervised auxiliary tasks. arxiv preprint arxiv. romain laroche ghislain putois philippe bretier bernadette bouchon-meunier. hybridisation expertise reinforcement learning dialogue systems. proceedings annual conference international speech communication association volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski stig petersen charles beattie amir sadik ioannis antonoglou helen king dharshan kumaran daan wierstra shane legg demis hassabis. human-level control deep reinforcement learning. nature osband charles blundell alexander pritzel benjamin roy. deep exploration bootstrapped dqn. proceedings advances neural information processing systems nicholas rescher. cosmos logos studies greek philosophy. topics ancient philosophy themen antiken philosophie. gruyter isbn https //books.google.ca/books?id=d-erjyntiac. satinder singh david cohn. dynamically merge markov decision processes. proceedings annual conference advances neural information processing systems nathan sprague dana ballard. multiple-goal reinforcement learning modular sarsa proceedings international joint conference artiﬁcial intelligence richard sutton doina precup satinder singh. mdps semi-mdps framework temporal abstraction reinforcement learning. artiﬁcial intelligence issn http//dx.doi.org/./s--. http//dx.doi.org/./ s--. richard sutton joseph modayil michael delp thomas degris patrick pilarski adam white doina precup. horde scalable real-time architecture learning knowledge unsupervised sensorimotor interaction. proceedings international conference autonomous agents multi-agent systems international foundation autonomous agents multiagent systems harm seijen mehdi fatemi joshua romoff romain laroche tavian barnes jeffrey tsang. hybrid reward architecture reinforcement learning. arxiv preprint arxiv. alexander vezhnevets simon osindero schaul nicolas heess jaderberg david silver koray kavukcuoglu. feudal networks hierarchical reinforcement learning. arxiv preprint arxiv. proof. advisor seen independent learner training trajectories controlled arbitrary behavioural policy. assumption holds advisor’s environment markov off-policy algorithms applied convergence guarantees. proof. logical biconditional demonstrated successively proving converse conditionals. first sufﬁcient condition assume state attractor. deﬁnition state attractor have proves possible preferred action second reciprocal condition assume that state action would preferred optimal policy egocentric planning. then similarly taxi domain dietterich incorporate location fruits state representation using dimensional vector ﬁrst entries used fruit positions last entries used agent’s position. feeds bit-vector input layer dense hidden layers units. output single linear head representing state-value multiple case vector mad-rl target. order assess value function complexity train discount factor setting ﬁxed size random states ground truth values. trained epochs using adam optimizer kingma default parameters. mad-rl setup advisor responsible speciﬁc source reward precisely assign advisor possible fruit location. advisor sees reward fruit assigned position gets eaten. state space consists pac-boy’s position resulting states. addition assign advisor ghost. advisor receives reward pac-boy bumps assigned ghost. state space consists pac-boy’s ghost’s positions resulting states. fruit advisor active fruit assigned position. average fruits average number advisors running beginning episode fruit advisor inactive fruit eaten. learning performed temporal difference updates. small state spaces advisors tabular representation. train learners parallel off-policy learning bellman residuals computed presented section constant parameter. uses \u0001-greedy action selection respect summed values. ghost agents exactly identical also beneﬁt direct knowledge transfer sharing q-tables. notice assumption holds setting that consequence theorem applies egocentric agnostic planning methods. theorem determines sufﬁcient conditions attractor mdp. pac-boy domain cancelling action condition satisﬁed every condition sufﬁcient also necessary since surrounded goals equal value attractor practice attractor becomes stable action enabling remain attraction set. thus condition stuck attractor relaxed hence result example illustrated figure baselines ﬁrst baseline standard algorithm reward clipping input -channel binary image following features walls ghosts fruits pac-boy. second baseline system uses exact input features mad-rl model. speciﬁcally state advisor mad-rl model encoded one-hot vector vectors concatenated resulting sparse binary feature vector size vector used linear function approximation q-learning. refer setting linear q-learning. also tried train deep architectures features success. experimental setting time scale divided epochs lasting transitions each. epoch evaluation phase launched games. theoretical expected maximum score random policy average score around egocentric-γ https//streamable.com/tian egocentric-γ https//streamable.com/sgjkq empathic https//streamable.com/hgey agnostic https//streamable.com/grswh dqn-clipped https//streamable.com/emhy", "year": 2017}