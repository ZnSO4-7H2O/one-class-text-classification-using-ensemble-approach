{"title": "Feature Selection with Conjunctions of Decision Stumps and Learning from  Microarray Data", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "One of the objectives of designing feature selection learning algorithms is to obtain classifiers that depend on a small number of attributes and have verifiable future performance guarantees. There are few, if any, approaches that successfully address the two goals simultaneously. Performance guarantees become crucial for tasks such as microarray data analysis due to very small sample sizes resulting in limited empirical evaluation. To the best of our knowledge, such algorithms that give theoretical bounds on the future performance have not been proposed so far in the context of the classification of gene expression data. In this work, we investigate the premise of learning a conjunction (or disjunction) of decision stumps in Occam's Razor, Sample Compression, and PAC-Bayes learning settings for identifying a small subset of attributes that can be used to perform reliable classification tasks. We apply the proposed approaches for gene identification from DNA microarray data and compare our results to those of well known successful approaches proposed for the task. We show that our algorithm not only finds hypotheses with much smaller number of genes while giving competitive classification accuracy but also have tight risk guarantees on future performance unlike other approaches. The proposed approaches are general and extensible in terms of both designing novel algorithms and application to other domains.", "text": "objectives designing feature selection learning algorithms obtain classiﬁers depend small number attributes veriﬁable future performance guarantees. approaches successfully address goals simultaneously. performance guarantees become crucial tasks microarray data analysis small sample sizes resulting limited empirical evaluation. best knowledge algorithms give theoretical bounds future performance proposed context classiﬁcation gene expression data. work investigate premise learning conjunction decision stumps occam’s razor sample compression pac-bayes learning settings identifying small subset attributes used perform reliable classiﬁcation tasks. apply proposed approaches gene identiﬁcation microarray data compare results well known successful approaches proposed task. show algorithm ﬁnds hypotheses much smaller number genes giving competitive classiﬁcation accuracy also tight risk guarantees future performance unlike approaches. proposed approaches general extensible terms designing novel algorithms application domains. important challenge problem classiﬁcation high-dimensional data design learning algorithm construct accurate classiﬁer depends smallest possible number attributes. further often desired realizable guarantees associated future performance feature selection approaches. recent explosion various technologies generating huge amounts measurements problem obtaining learning algorithms performance guarantees acquired renewed interest. consider case biological domain advent microarray technologies revolutionized outlook investigation analysis genetic diseases. parallel classiﬁcation front many interesting results appeared aiming distinguish types cells based gene expression data case microarrays results colon cancer leukaemia). focusing genes give insight class association microarray sample quite important owing variety reasons. instance small subset genes easier analyze opposed genes output microarray chips. also makes relatively easier deduce biological relationships among well study interactions. approach able identify number genes facilitate customization chips validation experiments– making utilization microarray technology cheaper affordable effective. view diseased versus normal sample genes considered indicators disease’s cause. subsequent validation study focused genes behavior interactions lead better understanding disease. attempts direction yielded interesting results. instance recent algorithm proposed wang involving identiﬁcation gene subset based importance ranking subsequently combinations genes classiﬁcation. another example approach tibshirani based nearest shrunken centroids. kernel based approaches bahsic algorithm extensions short time-series domains) also appeared. traditional methods used classifying high-dimensional data often characterized either ﬁlters wrappers depending whether attribute selection performed independent conjunction with base learning algorithm. despite acceptable empirical results achieved approaches theoretical justiﬁcation performance come guarantee well perform future. really needed learning algorithm provably good performance guarantees presence many irrelevant attributes. focus work presented here. main contributions work come form formulation feature selection strategies within well established learning settings resulting learning algorithms combine tasks feature selection discriminative learning. consequently obtain feature selection algorithms classiﬁcation tight realizable guarantees generalization error. proposed approaches step towards general learning strategies combine feature selection classiﬁcation algorithm tight realizable guarantees. apply approaches task classifying microarray data attributes data sample correspond expression level measurements various genes. fact choice decision stumps learning bias part motivated application. framework general extensible variety ways. instance learning strategies proposed work readily extended similar tasks beneﬁt learning bias. immediate example would classifying data microarray technologies case chromatin immunoprecipitation experiments. similarly learning biases conjunctions decision stumps also explored frameworks leading novel learning algorithms. learning class conjunctions features draw motivation guarantee exists class following form exists conjunction depends input attributes correctly classiﬁes training examples greedy covering algorithm haussler conjunction attributes makes training errors. note absence dependence number input attributes. method guaranteed attributes hence depends number available samples number attributes analyzed. propose learning algorithms building small conjunctions decision stumps. examine three approaches obtain optimal classiﬁer based premise mainly vary coding strategies threshold decision stump. ﬁrst approaches attempt encoding threshold either message strings using training examples third strategy attempts examine optimal classiﬁer obtained trading sparsity classiﬁer magnitude separating margin decision stump. case derive upper bound generalization error classiﬁer subsequently guide respective algorithm. finally present empirical results microarray data classiﬁcation tasks compare results state-of-the-art approaches proposed task including support vector machine coupled feature selectors adaboost. preliminary results work appeared section gives basic deﬁnitions notions learning setting utilize also characterizes hypothesis class conjunctions decision stumps. subsequent learning algorithms proposed learn hypothesis class. section proposes occam’s razor approach learn conjunctions decision stumps leading upper bound generalization error framework. section proposes alternate encoding strategy message strings using sample compression framework gives corresponding risk bound. section propose pac-bayes approach learn conjunction decision stumps enables learning algorithm perform explicit non-trivial margin-sparsity trade-off obtain general classiﬁers. section proposes algorithms learn three learning settings proposed sections along time complexity analysis. note learning strategies proposed section affect respective theoretical guarantees learning settings. algorithms evaluated empirically real world microarray datasets section vii. section viii presents discussion results also provides analysis biological relevance selected genes case dataset agreement published ﬁndings. finally conclude section input space consists n-dimensional vectors real-valued component attribute instance refer expression level gene hence respectively priori lower upper bounds values output space classiﬁcation labels assigned input vector focus binary classiﬁcation problems. thus example input vector classiﬁcation label chosen i.i.d. unknown distribution true risk classiﬁer deﬁned probability misclassiﬁes example drawn according focus learning algorithms construct conjunction decision stumps training set. decision stump threshold classiﬁer deﬁned single attribute formally decision stump identiﬁed attribute index threshold value direction given input example output finally algorithm builds conjunction used build disjunction exchanging role positive negative labeled examples. order keep description simple describe case conjunction. however case disjunction follows symmetrically. ﬁrst approach towards learning conjunction decision stumps occam’s razor approach. basically wish obtain hypothesis coded using least number bits. ﬁrst propose occam’s razor risk bound ultimately guide learning algorithm. starting point occam’s razor bound langford tighter version bound proposed blumer also general sense applies prior distribution countable class classiﬁers. case decision-stumps’ conjunctions speciﬁed terms discrete-valued vectors continuousvalued vector ﬁnite-precision string specify threshold values denote prior probability assigned conjunction described choose prior following form prior probability assigned string given chosen message strings given chosen denotes possible attribute index vectors denotes binary direction vectors dimension reasons motivating choice prior following. ﬁrst factors come belief ﬁnal classiﬁer constructed group attributes speciﬁed depend number attributes group. complete ignorance number decision stumps ﬁnal classiﬁer likely have choose however choose decreases increase reasons believe number decision stumps ﬁnal classiﬁer much smaller since usually case propose specify distribution strings consider problem coding threshold value predeﬁned interval permitted choose interval equally good threshold values. propose following diadic coding scheme identiﬁcation threshold value belongs interval. number bits code. then code bits speciﬁes value among threshold values denote respective priori minimum maximum values attribute take. values obtained deﬁnition data. hence attribute given interval threshold values take smallest number bits exists threshold value falls interval need ⌊log/)⌋ bits obtain threshold value falls hence specify threshold decision stump need specify number bits li-bit string identiﬁes threshold values λli. risk bound depend actually code depends priori probabilities assign possible realization choose following distribution distribution chosen string length advantage decreasing slowly risk bound deteriorate rapidly increases. choices clearly possible. however note dominant contribution comes term yielding risk bound depends linearly finally emphasize risk bound theorem used conjunction distribution messages given provides guide choosing optimal classiﬁer. note risk bound suggests non-trivial trade-off number attributes length message string used encode classiﬁer. indeed risk bound smaller conjunction large number attributes small message strings conjunction small number attributes large message strings. basic idea sample compression framework obtain learning algorithms property generated classiﬁer often reconstructed small subset training examples. formally learning algorithm said sample-compression algorithm exists compression function reconstruction function training sample information message contains additional information needed reconstruct classiﬁer compression given training sample deﬁne compression def= m}∀j i|i| denotes vector indices number indices present given arbitrary compression arbitrary information message reconstruction function learning algorithm must output classiﬁer. information message chosen consists distinct messages attached compression existence reconstruction function assures classiﬁer returned always identiﬁed compression information message sample compression settings learning decision stumps’ conjunctions message string consists attributes directions deﬁned above. however thresholds speciﬁed training examples. hence attributes thresholds compression consists training examples starting point following generic sample compression bound need specify distribution messages conjunction decision stumps. note order specify conjunction decision stumps compression consists example decision stump. decision stump attribute corresponding threshold value determined numerical value attribute takes training example. subset attributes speciﬁes decision stumps compression given vector deﬁned previous section. moreover since decision stump corresponding example compression |k|. assign equal probability possible attributes selected attributes. moreover assign equal probability direction decision stump hence following distribution messages occam’s razor sample compression sense obtaining sparse classiﬁers minimum number stumps. sparsity enforced selecting classiﬁers minimal encoding message strings compression respective cases. examine sacriﬁcing sparsity terms larger separating margin around decision boundary lead classiﬁers smaller generalization error. learning algorithm based pac-bayes approach aims providing probably approximately correct guarantees bayesian learning algorithms speciﬁed terms prior distribution data-dependent posterior distribution space classiﬁers. formulate learning algorithm outputs stochastic classiﬁer called gibbs classiﬁer deﬁned datadependent posterior classiﬁer partly stochastic sense formulate posterior threshold values utilized decision stumps still retaining deterministic nature selected attributes directions decision stumps. bound risk gibbs classiﬁers easily turned bound risk bayes classiﬁers posterior basically performs majority vote binary classiﬁers misclassiﬁes example least half binary classiﬁers misclassiﬁes follows error rate least half error rate hence case seen decision stump conjunctions speciﬁed terms mixture discrete parameters continuous parameters denote probability density function associated prior class decision stump conjunctions consider priors form factors relating discrete components rationale case occam’s razor approach. however case threshold decision stumps consider explicitly continuous uniform prior. occam’s razor case assume attribute value constrained priori obtained deﬁnition data. hence chosen uniform prior probability density explains last factors pkd. given training learner choose attribute group direction vector deterministically. pose problem choosing threshold similar manner case occam’s razor approach section difference learner identiﬁes interval selects threshold stochastically. attribute margin interval chosen learner. deterministic decision stump conjunction classiﬁer speciﬁed choosing thresholds values uniformly. tempting point choose however pac-bayes theorem offers better guarantee another type deterministic classiﬁer below. limit seen divergence continuous components vanishes. furthermore divergence discrete components small small values small). hence divergence choices exhibits tradeoff margins sparsity gibbs classiﬁers. theorem suggests smallest guarantee risk minimize trivial combination proposed theoretical frameworks attempting obtain optimal classiﬁers based various optimization criteria detail learning algorithms approaches. ideally would like conjunction decision stumps minimizes respective risk bounds approach. unfortunately cannot done efﬁciently cases since problem least hard minimum cover problem mentioned marchand shawe-taylor hence covering greedy heuristic. consists choosing decision stump largest utility where negative examples covered feature positive examples misclassiﬁed feature learning parameter gives penalty misclassiﬁed positive example. feature largest found remove training repeat either negative examples present maximum number features reached. heuristic also used marchand shawe-taylor context sample compression classiﬁer called covering machine. sample compression approach utility function propose following learning strategy occam’s razor learning conjunctions decision stumps. ﬁxed negative examples positive examples. start subset covered decision stump subset covered decision stump number bits used code threshold decision stump choose decision stump maximizes utility occam penalty suffered covering positive example cost using bits decision stump found decision stump maximizing update repeat next decision stump either maximum number decision stumps reached best values learning parameters determined cross-validation. case however need keep gibbs risk instead risk deterministic classiﬁer. since gibbs risk soft measure uses piece-wise linear functions instead hard indicator functions cannot make hard utility function equation instead need softer version utility function take account covering example partly. negative example falls linear region fact partly covered vice versa positive example. typically covering contribution decision stump increase utility positive-side error decrease moreover want decrease utility decision stump amount would become large whenever small separating margin. expression suggests amount proportional ln/). furthermore compare margin term fraction remaining negative examples decision stump covered hence covering contribution negative examples remains covered considering decision stump parameter represents penalty misclassifying positive example another parameter controls importance large margin. learning parameters chosen cross-validation. ﬁxed values parameters soft greedy algorithm simply consists adding current gibbs classiﬁer decision stump maximum added utility either maximum number decision stumps reached negative examples covered. understood that soft greedy algorithm remove example time complexity analysis analyze time complexity algorithm ﬁxed attribute ﬁrst sort examples respect values attribute consideration. takes time. then examine potential value corresponding examine potential values gives time complexity largest number examples falling calculating covering error contributions ﬁnding best interval takes time. moreover allow giving time complexity attribute. finally attributes. hence overall time complexity algorithm note however microarray data moreover best stump found remove examples covered stump training repeat algorithm. know greedy algorithms kind following guarantee exist decision stumps covers examples greedy algorithm decision stumps. since almost always running time whole algorithm almost always good news since time complexity algorithm roughly linear fixed-margin heuristic order show prefer uniformly distributed threshold opposed ﬁxed middle interval stump alternate algorithm call ﬁxed margin heuristic. algorithm similar described additional parameter parameter decides ﬁxed margin boundary around threshold i.e. decides length interval algorithm still chooses attribute vector direction vector vectors however ai’s bi’s stump chosen that threshold ﬁxed middle interval hence stump interval ﬁxed similar analysis previous subsection yields time complexity algorithm. proposed approaches learning conjunctions decision stumps tested real-world binary microarray datasets viz. colon tumor leukaemia medulloblastomas data lung breaster data colon tumor data provides expression levels tumor normal colon tissues measured human genes. genes identiﬁed highest minimal intensity across tissues. leuk data provides expression levels human genes samples patients acute lymphoblastic leukemia samples patients acute myeloid leukemia data sets microarray samples containing expression levels human genes. data contains classic desmoplastic medulloblastomas whereas data contains medulloblastomas survivors treatment failures lung dataset consists gene expression levels genes patients adenocarcinoma squamous cell cancer data missing values replaced zeros. finally breaster dataset breast tumor data west used estrogen receptor status label various samples. data consists expression levels genes patients positive estrogen receptor samples negative estrogen receptor samples. number examples number genes data given genes columns respectively data table. algorithms referred occam pac-bayes tables utilize respective theoretical frameworks discussed sections along respective learning strategies section compared learning algorithm linear-kernel soft-margin trained attributes subset attributes chosen ﬁlter method golub ﬁlter method consists ranking attributes function difference positive-example mean negative-example mean ﬁrst attributes. resulting learning algorithm named svm+gs used furey task. guyon claimed obtaining better results recursive feature elimination method pointed ambroise mclachlan work contained methodological ﬂaw. recursive feature elimination algorithm bias removed present results well comparison finally also compare results state-of-the-art adaboost algorithm. this implementation weka data mining software algorithm tested random permutations datasets -fold cross validation method. training sets testing sets algorithms. learning parameters algorithms gene subsets chosen training sets only. done performing second -fold training set. gene subset selection procedure svm+gs considered ﬁrst genes ranked according criterion golub chosen value gave smallest -fold error training set. errs column algorithm tables refer average -fold cross-validation error respective algorithm standard deviation two-sided conﬁdence interval. bits column table refer number bits used occam’ razor approach. g-errs b-errs columns table refer average nested -fold error optimal gibbs classiﬁer corresponding bayes classiﬁer standard deviation two-sided interval respectively. adaboost iterations datasets tried reported results correspond best obtained -fold error. size values reported correspond number attributes selected frequently respective algorithms permutation runs. choosing cross-validation number boosting iteration somewhat inconsistent adaboost’s goal minimizing empirical exponential risk. indeed comply adaboost’s goal choose large-enough number boosting rounds assures convergence empirical exponential risk minimum value. however shown zhang boosting known overﬁt number attributes exceeds number examples. happens case microarray experiments frequently number genes exceeds number samples also case datasets mentioned above. early stopping recommended approach cases hence followed method described obtained best number boosting iterations. further table gives result single deterministic algorithm using ﬁxed-margin heuristic described above. table gives results pac-bayes bound values results obtained single pac-bayes algorithm respective microarray data sets. recall pac-bayes bound provides uniform upper bound risk gibbs classiﬁer. column labels refer quantities although errors reported single nested -fold run. ratio column table refers average value obtained decision stumps used classiﬁers testing folds bound columns tables refer average risk bound theorem multiplied total number examples respective data sets. note again results single permutation datasets presented illustrate practicality risk bound rationale choosing ﬁxed-margin heuristic current learning strategy. note risk bounds quite effective relevance misconstrued observing results current scenario. limiting factor current analysis unavailability microarray data larger number examples. number examples increase risk bound theorem gives tighter guarantees. consider instance datasets lung colon cancer examples. classiﬁer performance examples would bound percent error instead current percent respectively. illustrates bound effective guarantee used datasets examples. similarly dataset examples breast cancer similar performance bound percent instead current percent. hence current limitation practical application bound comes limited data availability. number examples increase bounds provides tighter guarantees become signiﬁcant. results clearly show even though occam able sparse classiﬁers able obtain acceptable classiﬁcation accuracies. possible explanation approaches focus succinct classiﬁer respective criterion. sample compression approach tries minimize number genes used take account magnitude separating margin hence compromises accuracy. hand occam’s razor approach tries classiﬁer depends margin indirectly. approaches based sample compression well minimum description length shown encouraging results various domains. alternate explanation suboptimal performance seen terms extremely limited sample sizes. result gain accuracy offset cost adding additional features conjunction. pac-bayes approach seems alleviate problems performing signiﬁcant margin-sparsity tradeoff. advantage adding feature seen terms combination gain margin empirical risk. compared strategy used regularization approaches. classiﬁcation accuracy pac-bayes algorithm competitive best performing classiﬁer added advantage quite importantly using genes. pac-bayes approach expect bayes classiﬁer generally perform better gibbs classiﬁer. reﬂected extent empirical results colon leukaemia datasets. however means prove always case. noted exist several different utility functions proposed learning approaches. tried reported results ones found best noteworthy observation regard adaboost gene subset identiﬁed algorithm almost always include ones found proposed pac-bayes approach decision stumps. notably gene cyclin well known marker cancer found lung cancer dataset discriminating factor commonly found approaches. cases size classiﬁer almost always restricted observations give insights absolute peaks worth investigating also experimentally validates proposed approaches. table details genes identiﬁed ﬁnal pac-bayes classiﬁer learned dataset parameter selection phase. prominent markers identiﬁed classiﬁer. main genes identiﬁed pac-bayes approach ones identiﬁed previous studies disease— giving conﬁdence proposed approach. discovered genes case include human monocyte-derived neutrophil-activating protein mrna case colon cancer dataset oestrogen receptor case breast cancer data at-ribosomal protein at-cadherin- at-nptx neuronal pentraxin case medulloblastomas datasets genes identiﬁed biological relevance instance identiﬁcation adipsin laf- hoxc regard all/aml algorithm agreement ﬁndings chow hiwatari lawrence largman respectively studies followed. further case breast cancer estrogen receptors shown interact brca regulate vegf transcription secretion breast cancer cells interactions investigated studies also done. instance moggs discovered putative estrogen-response elements keratin context gene identiﬁed pac-bayes classiﬁer hsa˙ m-human monocyte-derived neutrophil-activating protein mrna at-ribosomal protein at-neural cell adhesion molecule phosphatidylinositol-linked isoform precursor at-cadherin- at-nptx neuronal pentraxin at-haes- mrna at-high conductance inward rectiﬁer potassium channel alpha subunit mrna at-df component complement at-lymphoid nuclear protein mrna at-homeo protein mrna genex-image -cyclin e-responsive genes identiﬁed microarray analysis mda-md- cells re-express erα. important role played cytokeratins cancer development also widely known furthermore importance monap case colon cancer adipsin case leukaemia data conﬁrmed various rank based algorithms detailed implementation rankgene program analyzes ranks genes gene expression data using eight ranking criteria including information gain gini index minority minority twoing rule t-statistic variances one-dimensional support vector machine case colon cancer data monap identiﬁed ranked gene four eight criteria second eighth similarly case leukaemia data adipsin ranked ﬁfth seventh observations provides strong validation approaches. learning high-dimensional data microarrays quite challenging especially identify attributes characterizes differences classes data. investigated premise learning conjunctions decision stumps proposed three formulations based different learning principles. observed approaches solely optimize sparsity message code regard classiﬁer’s empirical risk limits algorithm terms generalization performance least present case small dataset sizes. trading-off sparsity classiﬁer separating margin addition empirical risk pac-bayes approach seem alleviate problem signiﬁcant extent. allows pac-bayes algorithm yield competitive classiﬁcation performance time utilizing signiﬁcantly fewer attributes. opposed traditional feature selection methods proposed approaches accompanied theoretical justiﬁcation performance. moreover proposed algorithms embed feature selection part learning process itself. furthermore generalization error bounds practical potentially guide model selection. applied classify microarray data genes identiﬁed proposed approaches found biologically signiﬁcant experimentally validated various studies empirical justiﬁcation approaches successfully perform meaningful feature selection. consequently represents signiﬁcant improvement direction successful integration machine learning approaches high-throughput data provide meaningful theoretically justiﬁable reliable results. approaches yield compressed view terms small number biological markers lead targeted well focussed study issue interest. instance approach utilized identifying gene subsets microarray experiments validated using focused rt-pcr techniques otherwise costly impractical perform full genes. finally mentioned previously approaches presented wider relevance signiﬁcant implications direction designing theoretically justiﬁed feature selection algorithms. approaches combines feature selection learning process provide generalization guarantees resulting classiﬁers simultaneously. property assumes even signiﬁcance wake limited size microarray datasets since limits amount empirical evaluation reliably performed otherwise. natural extensions approaches learning bias proposed would similar domains including forms microarray experiments chromatin immunoprecipitation promoter arrays protein arrays. within learning settings learning biases also explored classiﬁers represented features sets features built subsets attributes. work supported national science engineering research council canada canadian institutes health research canada research chair medical genomics alon barkai d.a. notterman gish ybarra mack a.j. levine. broad patterns gene expression revealed clustering analysis tumor normal colon tissues probed oligonucleotide arrays. proc. natl. acad. sci. avrim blum john langford. pac-mdl bounds. proceedings annual conference learning theory colt washington august volume lecture notes artiﬁcial intelligence pages springer berlin blumer ehrenfeucht haussler warmuth. occam’s razor. information processing letters chow moler mian. identifying marker genes transcription proﬁling data using mixture feature eisen brown. arrays analysis gene expression. methods enzymology furey cristianini duffy bednarski schummer haussler. support vector machine classiﬁcation garber troyanskaya schluens petersen thaesler pacyna-gengelbach rijn rosen perou whyte altman brown botstein petersen. diversity gene expression adenocarcinoma lung. proc. natl. acad. sci. golub slonim tamayo huard gaasenbeek mesirov coller downing caligiuri bloomﬁeld lander. molecular classiﬁcation cancer class discovery class prediction gene expression monitoring. science mitsuteru hiwatari tomohiko taki takeshi taketani masafumi taniwaki kenichi sugita mayuko okuya mitsuoki eguchi kohmei yasuhide hayashi. fusion af-related gene childhood acute lymphoblastic leukemia oncogene kawai chun avraham avraham. direct interaction brca estrogen receptor regulates vascular endothelial growth factor transcription secretion breast cancer cells. oncogene lawrence largman. homeobox genes normal hematopoiesis leukemia. blood lipshutz fodor gingeras lockhart. high density synthetic oligonucleotide arrays. nature genetics david mcallester. pac-bayesian theorems. machine learning moggs murphy moore stuckey antrobus kimber orphanides. anti-proliferative effect estrogen breast cancer cells re-express eralpha mediated aberrant regulation cell cycle genes. journal molecular endocrinology pomeroy tamayo gaasenbeek sturla angelo mclaughlin goumnerova black allen zagzag olson curran wetmore biegel poggio mukherjee rifkin califano stolovitzky louis mesirov lander golub. prediction central nervous system embryonal tumour outcome based gene expression. nature mohak shah jacques corbeil. general framework analyzing data short time-series microarray experiments. ieee/acm transactions computational biology bioinformatics appear http//doi.ieeecomputersociety. org/./tcbb... west blanchette dressman huang ishida spang zuzan olson marks nevins. predicting clinical status human breast cancer using gene expression proﬁles. proc. natl. acad. sci.", "year": 2010}